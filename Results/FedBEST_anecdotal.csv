Function,Generated,Ground Truth
"@frappe.whitelist() 
 def setup_complete(args): 
    if cint(frappe.db.get_single_value(u'System   Settings', u'setup_complete')): 
      return 
   args = process_args(args) 
   try: 
      if (args.language and (args.language != u'english')): 
         set_default_language(get_language_code(args.lang)) 
      frappe.clear_cache() 
      update_system_settings(args) 
      update_user_name(args) 
      for method in frappe.get_hooks(u'setup_wizard_complete'): 
         frappe.get_attr(method)(args) 
      disable_future_access() 
      frappe.db.commit() 
      frappe.clear_cache() 
   except: 
      frappe.db.rollback() 
      if args: 
         traceback = frappe.get_traceback() 
         for hook in frappe.get_hooks(u'setup_wizard_exception'): 
            frappe.get_attr(hook)(traceback, args) 
      raise 
   else: 
      for hook in frappe.get_hooks(u'setup_wizard_success'): 
         frappe.get_attr(hook)(args)"," 'Setup complete hook. 
 :param args: dict 
 :returns: None'","'Calls hooks for `setup_wizard_complete`, sets home page as `desktop` 
 and clears cache. If wizard breaks, calls `setup_wizard_exception` hook'"
"def main(): 
    module = AnsibleModule(argument_spec=ClcGroup._define_module_argument_spec(), supports_check_mode=True) 
   clc_group = ClcGroup(module) 
   clc_group.process_request()", 'Create a Clc group',"'The main function.  Instantiates the module and calls process_request. 
 :return: none'"
"def __guess_dataset_by_filename(filename): 
    try: 
      fields = os.path.split(filename) 
      if fields: 
         if (fields[(-1)].startswith('dataset_') and fields[(-1)].endswith('.dat')): 
            return Dataset.get(int(fields[(-1)][len('dataset_'):(- len('.dat'))])) 
   except: 
      pass 
   return None"," 'Given a filename, try to guess the dataset it belongs to. 
 Returns the dataset, or None if it can\'t be guessed.'",'Return a guessed dataset by filename'
"def resolve_possible_pending_xrefs(app, fromdocname, maybe_xrefs): 
    result = [] 
   for node in maybe_xrefs: 
      if isinstance(node, addnodes.pending_xref): 
         result.extend(resolve_pending_xref(app, fromdocname, node.deepcopy())) 
      else: 
         result.append(node) 
   return result"," 'Resolve all pending xrefs in the given nodes. 
 :param app: the docutils application 
 :param fromdocname: the name of the current document 
 :param maybe_xrefs: a list of nodes to resolve 
 :returns: a list of nodes'","'If any node is a pending_xref, attempt to resolve it. If it cannot be 
 resolved, replace it with its children.'"
"def prepare_to_run(): 
    (is_travis, non_pr) = check_environment() 
   if (not is_travis): 
      return 
   if (not non_pr): 
      print('Running   in   Travis   during   non-merge   to   master,   doing   nothing.') 
      sys.exit(0) 
   decrypt_keyfile()"," 'Prepare to run. 
 This function is called before running the tests. 
 It will decrypt the keyfile if it is encrypted. 
 It will also check if this is a merge to master and if it is, 
 it will not run the tests.'","'Prepare to run system tests. 
 If on Travis during a PR, exit the entire program; there is 
 no need to run the system tests. 
 If on Travis during a build for a non-PR merge to master, 
 decrypts stored keyfile.'"
"def test_download_exit_status_code_when_no_requirements(script): 
    result = script.pip('download', expect_error=True) 
   assert ('You   must   give   at   least   one   requirement   to   download' in result.stderr) 
   assert (result.returncode == ERROR)", 'Test download with no requirements','Test download exit status code when no requirements specified'
"def main(): 
    with open('../show_version.txt') as show_ver_file: 
      show_ver = show_ver_file.read() 
   print obtain_os_version(show_ver)", 'This is a test script to show the version of the OS.',"'Obtain the OS version from the show version output 
 Print output to STDOUT'"
"def _out(msg): 
    stdout.write(msg.encode('utf-8'))"," 'Print the given message to the standard output. 
 :param msg: The message to print.'",'Output a string'
"@hook.command('bingimage', 'bis') 
 def bingimage(text, bot): 
    api_key = bot.config.get('api_keys', {}).get('bing_azure') 
   show_nsfw = text.endswith('   nsfw') 
   if show_nsfw: 
      text = text[:(-5)].strip().lower() 
   rating = (NSFW_FILTER if show_nsfw else DEFAULT_FILTER) 
   if (not api_key): 
      return 'Error:   No   Bing   Azure   API   details.' 
   params = {'Sources': bingify('image'), 'Query': bingify(text), 'Adult': bingify(rating), '$format': 'json'} 
   request = requests.get(API_URL, params=params, auth=(api_key, api_key)) 
   j = request.json()['d']['results'][0] 
   if (not j['Image']): 
      return 'No   results.' 
   result = random.choice(j['Image'][:10]) 
   tags = [] 
   tags.append('{}x{}px'.format(result['Width'], result['Height'])) 
   tags.append(result['ContentType']) 
   tags.append(filesize.size(int(result['FileSize']), system=filesize.alternative)) 
   if ('explicit' in result['Thumbnail']['MediaUrl']): 
      tags.append('NSFW') 
   tag_text = ',   '.join(tags) 
   return '{}   ({})'.format(unescape(result['MediaUrl']), tag_text)"," 'Bing image search. 
 :param text: The query to search for. 
 :param bot: The bot to send the result to. 
 :returns: The image URL.'",'<query> - returns the first bing image search result for <query>'
"def create_option_values_for_optionable(optionable_type, **options): 
    if (not issubclass(optionable_type, Optionable)): 
      raise TypeError(u'The   given   `optionable_type`   was   not   a   subclass   of   `Optionable`:   {}'.format(optionable_type)) 
   option_values = {} 
   registration_function = _options_registration_function(option_values) 
   optionable_type.register_options(registration_function) 
   option_values.update(**options) 
   return create_option_values(option_values)"," 'Create option values for an optionable. 
 :param optionable_type: The type of the optionable. 
 :param options: The options to create. 
 :return: The created option values. 
 :rtype: dict[str, Any]'","'Create a fake OptionValueContainer with appropriate defaults for the given `Optionable` type."" 
 :param type optionable_type: An :class:`pants.option.optionable.Optionable` subclass. 
 :param **options: Keyword args representing option values explicitly set via the command line. 
 :returns: A fake `OptionValueContainer`, ie: the value returned from `get_options()`.'"
"def probitloglike(params, endog, exog): 
    q = ((2 * endog) - 1) 
   X = exog 
   return np.add.reduce(stats.norm.logcdf((q * np.dot(X, params))))", 'Log-likelihood of a probit model','Log likelihood for the probit'
"def test_pix2world(): 
    filename = get_pkg_data_filename(u'data/sip2.fits') 
   with catch_warnings(wcs.wcs.FITSFixedWarning) as caught_warnings: 
      ww = wcs.WCS(filename) 
      assert (len(caught_warnings) == 1) 
   n = 3 
   pixels = (np.arange(n) * np.ones((2, n))).T 
   result = ww.wcs_pix2world(pixels, 0, ra_dec_order=True) 
   ww.wcs_pix2world(pixels[..., 0], pixels[..., 1], 0, ra_dec_order=True) 
   close_enough = 1e-08 
   answer = np.array([[0.00024976, 0.00023018], [0.00023043, (-0.00024997)]]) 
   assert np.all((np.abs((ww.wcs.pc - answer)) < close_enough)) 
   answer = np.array([[202.39265216, 47.17756518], [202.39335826, 47.17754619], [202.39406436, 47.1775272]]) 
   assert np.all((np.abs((result - answer)) < close_enough))", 'Test the wcs.WCS.wcs_pix2world() function','From github issue #1463'
"def influence_plot(results, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs): 
    (fig, ax) = utils.create_mpl_ax(ax) 
   infl = results.get_influence() 
   if criterion.lower().startswith('coo'): 
      psize = infl.cooks_distance[0] 
   elif criterion.lower().startswith('dff'): 
      psize = np.abs(infl.dffits[0]) 
   else: 
      raise ValueError(('Criterion   %s   not   understood' % criterion)) 
   old_range = np.ptp(psize) 
   new_range = ((size ** 2) - (8 ** 2)) 
   psize = ((((psize - psize.min()) * new_range) / old_range) + (8 ** 2)) 
   leverage = infl.hat_matrix_diag 
   if external: 
      resids = infl.resid_studentized_external 
   else: 
      resids = infl.resid_studentized_internal 
   from scipy import stats 
   cutoff = stats.t.ppf((1.0 - (alpha / 2)), results.df_resid) 
   large_resid = (np.abs(resids) > cutoff) 
   large_leverage = (leverage > _high_leverage(results)) 
   large_points = np.logical_or(large_resid, large_leverage) 
   ax.scatter(leverage, resids, s=psize, alpha=plot_alpha) 
   labels = results.model.data.row_labels 
   if (labels is None): 
      labels = lrange(len(resids)) 
   ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(leverage, resids), lzip((- ((psize / 2) ** 0.5)), ((psize / 2) ** 0.5)), 'x-large', ax) 
   font = {'fontsize': 16, 'color': 'black'} 
   ax.set_ylabel('Studentized   Residuals', **font) 
   ax.set_xlabel('H   Leverage', **font) 
   ax.set_title('Influence   Plot', **font) 
   return fig"," 'Influence plot. 
 Parameters 
 results : Result object 
 The results object to be plotted. 
 external : boolean 
 Whether or not to plot the external residuals. 
 alpha : float 
 The significance level. 
 criterion : string 
 The criterion to use for the influence plot. 
 size : int 
 The size of the influence plot. 
 plot_alpha : float 
 The alpha of the points in the influence plot. 
 ax : matplotlib axes 
 The axes to plot on. 
 kwargs : dict 
 Keyword arguments to pass to matplotlib. 
 Returns 
 fig : matplotlib figure 
 The figure containing the influence plot. 
 Examples 
 >>> from sklearn.linear_model import Lasso 
 >>> from sklearn.datasets import make_regression 
 >>> X, y = make_regression(n_samples=100, n_features=10, random_state=0) 
 >>> clf = Lasso(alpha=0.5) 
 >>> clf.fit(X, y) 
 >>> influence_plot(clf.results,","'Plot of influence in regression. Plots studentized resids vs. leverage. 
 Parameters 
 results : results instance 
 A fitted model. 
 external : bool 
 Whether to use externally or internally studentized residuals. It is 
 recommended to leave external as True. 
 alpha : float 
 The alpha value to identify large studentized residuals. Large means 
 abs(resid_studentized) > t.ppf(1-alpha/2, dof=results.df_resid) 
 criterion : str {\'DFFITS\', \'Cooks\'} 
 Which criterion to base the size of the points on. Options are 
 DFFITS or Cook\'s D. 
 size : float 
 The range of `criterion` is mapped to 10**2 - size**2 in points. 
 plot_alpha : float 
 The `alpha` of the plotted points. 
 ax : matplotlib Axes instance 
 An instance of a matplotlib Axes. 
 Returns 
 fig : matplotlib figure 
 The matplotlib figure that contains the Axes. 
 Notes 
 Row labels for the observations in which the leverage, measured by the 
 diagonal of the hat matrix, is high or the residuals are large, as the 
 combination of large residuals and a high influence value indicates an 
 influence point. The value of large residuals can be controlled using the 
 `alpha` parameter. Large leverage points are identified as 
 hat_i > 2 * (df_model + 1)/nobs.'"
"def send_file(name, data): 
    nf = NetlogFile(name) 
   nf.sock.sendall(data) 
   nf.close()"," 'Send data to a NetlogFile object. 
 :param name: The name of the file to send to. 
 :param data: The data to send to the file. 
 :type name: str 
 :type data: bytes'",'Send file to result server'
"def root_create(request): 
    root = get_or_create_root() 
   return redirect('wiki:get', path=root.path)"," 'Redirect to the root page. 
 :param request: The request object.'","'In the edX wiki, we don\'t show the root_create view. Instead, we 
 just create the root automatically if it doesn\'t exist.'"
"def block_device_mapping_update_or_create(context, values): 
    return IMPL.block_device_mapping_update_or_create(context, values)", 'Update or create a block device mapping.',"'Update an entry of block device mapping. 
 If not existed, create a new entry'"
"@profiler.trace 
 @memoized 
 def flavor_list(request, is_public=True, get_extras=False): 
    flavors = novaclient(request).flavors.list(is_public=is_public) 
   if get_extras: 
      for flavor in flavors: 
         flavor.extras = flavor_get_extras(request, flavor.id, True, flavor) 
   return flavors"," 'List flavors. 
 :param request: The request to use. 
 :param is_public: If True, only public flavors are returned. 
 :param get_extras: If True, return extra details about the flavors. 
 :returns: A list of :class:`Flavor` instances.'",'Get the list of available instance sizes (flavors).'
"def build_content_type(format, encoding=u'utf-8'): 
    if (u'charset' in format): 
      return format 
   if (format in (u'application/json', u'text/javascript')): 
      return format 
   return (u'%s;   charset=%s' % (format, encoding))"," 'Build a content-type header for a given format. 
 :param format: The format to use for the content type. 
 :param encoding: The encoding to use for the content type. 
 :returns: The content type header.'",'Appends character encoding to the provided format if not already present.'
"def assign_random_category(resource): 
    random_index = randint(0, (TopicCategory.objects.all().count() - 1)) 
   tc = TopicCategory.objects.all()[random_index] 
   resource.category = tc 
   resource.save()", 'Assign a random topic category to a resource.','Assign a random category to a resource'
"@contextlib.contextmanager 
 def temporary_mutation(obj, **kwargs): 
    NOT_PRESENT = object() 
   old_values = {} 
   for (attr, new_value) in kwargs.items(): 
      old_values[attr] = getattr(obj, attr, NOT_PRESENT) 
      setattr(obj, attr, new_value) 
   try: 
      (yield) 
   finally: 
      for (attr, old_value) in old_values.items(): 
         if (old_value is NOT_PRESENT): 
            del obj[attr] 
         else: 
            setattr(obj, attr, old_value)"," 'Context manager that temporarily mutates an object. 
 This is useful for testing context managers that need to 
 mutate an object. 
 :param obj: The object to mutate. 
 :param kwargs: The keyword arguments to use when mutating the object. 
 :type kwargs: dict 
 :returns: The object that was mutated. 
 :rtype: object 
 :raises: AttributeError if the object is not an instance of the given class.'","'Temporarily set the attr on a particular object to a given value then 
 revert when finished. 
 One use of this is to temporarily set the read_deleted flag on a context 
 object: 
 with temporary_mutation(context, read_deleted=""yes""): 
 do_something_that_needed_deleted_objects()'"
"def is_harvard_notes_enabled(course): 
    modules = set(['textannotation', 'imageannotation', 'videoannotation']) 
   return bool(modules.intersection(course.advanced_modules))"," 'Check whether HarvardNotes is enabled in a course. 
 Args: 
 course: Course to check. 
 Returns: 
 True if HarvardNotes is enabled, False otherwise.'","'Returns True if Harvard Annotation Tool is enabled for the course, 
 False otherwise. 
 Checks for \'textannotation\', \'imageannotation\', \'videoannotation\' in the list 
 of advanced modules of the course.'"
"def seqs_from_file(ids, file_lines): 
    for (label, seq) in parse_fasta(file_lines): 
      if (id_from_fasta_label_line(label) in ids): 
         (yield (label, seq))", 'Generate a sequence object for each line in the file.','Extract labels and seqs from file'
"@frappe.whitelist() 
 def setup_complete(args): 
    if cint(frappe.db.get_single_value(u'System   Settings', u'setup_complete')): 
      return 
   args = process_args(args) 
   try: 
      if (args.language and (args.language != u'english')): 
         set_default_language(get_language_code(args.lang)) 
      frappe.clear_cache() 
      update_system_settings(args) 
      update_user_name(args) 
      for method in frappe.get_hooks(u'setup_wizard_complete'): 
         frappe.get_attr(method)(args) 
      disable_future_access() 
      frappe.db.commit() 
      frappe.clear_cache() 
   except: 
      frappe.db.rollback() 
      if args: 
         traceback = frappe.get_traceback() 
         for hook in frappe.get_hooks(u'setup_wizard_exception'): 
            frappe.get_attr(hook)(traceback, args) 
      raise 
   else: 
      for hook in frappe.get_hooks(u'setup_wizard_success'): 
         frappe.get_attr(hook)(args)", 'Called when setup is complete.',"'Calls hooks for `setup_wizard_complete`, sets home page as `desktop` 
 and clears cache. If wizard breaks, calls `setup_wizard_exception` hook'"
"def main(): 
    module = AnsibleModule(argument_spec=ClcGroup._define_module_argument_spec(), supports_check_mode=True) 
   clc_group = ClcGroup(module) 
   clc_group.process_request()", 'Create a new ClcGroup',"'The main function.  Instantiates the module and calls process_request. 
 :return: none'"
"def __guess_dataset_by_filename(filename): 
    try: 
      fields = os.path.split(filename) 
      if fields: 
         if (fields[(-1)].startswith('dataset_') and fields[(-1)].endswith('.dat')): 
            return Dataset.get(int(fields[(-1)][len('dataset_'):(- len('.dat'))])) 
   except: 
      pass 
   return None"," 'Guesses the dataset from filename. 
 :param filename: 
 :return: 
 :rtype: 
 :returns: 
 :raises: 
 :raises: ValueError'",'Return a guessed dataset by filename'
"def resolve_possible_pending_xrefs(app, fromdocname, maybe_xrefs): 
    result = [] 
   for node in maybe_xrefs: 
      if isinstance(node, addnodes.pending_xref): 
         result.extend(resolve_pending_xref(app, fromdocname, node.deepcopy())) 
      else: 
         result.append(node) 
   return result"," 'Resolve possible pending xrefs in a list of nodes. 
 :param app: The application context. 
 :param fromdocname: The name of the document being processed. 
 :param maybe_xrefs: A list of nodes to resolve. 
 :return: A list of resolved nodes. 
 :rtype: list'","'If any node is a pending_xref, attempt to resolve it. If it cannot be 
 resolved, replace it with its children.'"
"def prepare_to_run(): 
    (is_travis, non_pr) = check_environment() 
   if (not is_travis): 
      return 
   if (not non_pr): 
      print('Running   in   Travis   during   non-merge   to   master,   doing   nothing.') 
      sys.exit(0) 
   decrypt_keyfile()"," 'Prepare to run. 
 This is called by the travis-ci build system. 
 It will decrypt the private key file if it is encrypted. 
 If it is not encrypted, it will do nothing.'","'Prepare to run system tests. 
 If on Travis during a PR, exit the entire program; there is 
 no need to run the system tests. 
 If on Travis during a build for a non-PR merge to master, 
 decrypts stored keyfile.'"
"def test_download_exit_status_code_when_no_requirements(script): 
    result = script.pip('download', expect_error=True) 
   assert ('You   must   give   at   least   one   requirement   to   download' in result.stderr) 
   assert (result.returncode == ERROR)", 'Verify that the download fails with an error if no requirements are given.','Test download exit status code when no requirements specified'
"def main(): 
    with open('../show_version.txt') as show_ver_file: 
      show_ver = show_ver_file.read() 
   print obtain_os_version(show_ver)", 'This is the main function',"'Obtain the OS version from the show version output 
 Print output to STDOUT'"
"def _out(msg): 
    stdout.write(msg.encode('utf-8'))", 'Write message to stdout','Output a string'
"@hook.command('bingimage', 'bis') 
 def bingimage(text, bot): 
    api_key = bot.config.get('api_keys', {}).get('bing_azure') 
   show_nsfw = text.endswith('   nsfw') 
   if show_nsfw: 
      text = text[:(-5)].strip().lower() 
   rating = (NSFW_FILTER if show_nsfw else DEFAULT_FILTER) 
   if (not api_key): 
      return 'Error:   No   Bing   Azure   API   details.' 
   params = {'Sources': bingify('image'), 'Query': bingify(text), 'Adult': bingify(rating), '$format': 'json'} 
   request = requests.get(API_URL, params=params, auth=(api_key, api_key)) 
   j = request.json()['d']['results'][0] 
   if (not j['Image']): 
      return 'No   results.' 
   result = random.choice(j['Image'][:10]) 
   tags = [] 
   tags.append('{}x{}px'.format(result['Width'], result['Height'])) 
   tags.append(result['ContentType']) 
   tags.append(filesize.size(int(result['FileSize']), system=filesize.alternative)) 
   if ('explicit' in result['Thumbnail']['MediaUrl']): 
      tags.append('NSFW') 
   tag_text = ',   '.join(tags) 
   return '{}   ({})'.format(unescape(result['MediaUrl']), tag_text)", 'Bing Image Search','<query> - returns the first bing image search result for <query>'
"def create_option_values_for_optionable(optionable_type, **options): 
    if (not issubclass(optionable_type, Optionable)): 
      raise TypeError(u'The   given   `optionable_type`   was   not   a   subclass   of   `Optionable`:   {}'.format(optionable_type)) 
   option_values = {} 
   registration_function = _options_registration_function(option_values) 
   optionable_type.register_options(registration_function) 
   option_values.update(**options) 
   return create_option_values(option_values)"," 'Creates a dictionary of option values for an Optionable subclass. 
 :param optionable_type: A subclass of Optionable. 
 :param options: A dictionary of option values. 
 :returns: A dictionary of option values. 
 :raises: TypeError if `optionable_type` is not a subclass of Optionable.'","'Create a fake OptionValueContainer with appropriate defaults for the given `Optionable` type."" 
 :param type optionable_type: An :class:`pants.option.optionable.Optionable` subclass. 
 :param **options: Keyword args representing option values explicitly set via the command line. 
 :returns: A fake `OptionValueContainer`, ie: the value returned from `get_options()`.'"
"def probitloglike(params, endog, exog): 
    q = ((2 * endog) - 1) 
   X = exog 
   return np.add.reduce(stats.norm.logcdf((q * np.dot(X, params))))"," 'Probit log-likelihood. 
 Parameters 
 params : ndarray 
 The parameters of the probit model. 
 endog : ndarray 
 The observed probit values. 
 exog : ndarray 
 The explanatory variables. 
 Returns 
 loglike : ndarray 
 The log-likelihood of the data. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Probit_regression 
 Examples 
 >>> from statsmodels.regression.probit import probitloglike 
 >>> from scipy.stats import norm 
 >>> params = np.array([1, 1]) 
 >>> endog = np.array([1, 0]) 
 >>> exog = np.array([1, 1]) 
 >>> loglike = probitloglike(params, endog, exog) 
 >>> loglike.shape 
 (2,) 
 >>> loglike.sum() 
 1.00000000000000011'",'Log likelihood for the probit'
"def test_pix2world(): 
    filename = get_pkg_data_filename(u'data/sip2.fits') 
   with catch_warnings(wcs.wcs.FITSFixedWarning) as caught_warnings: 
      ww = wcs.WCS(filename) 
      assert (len(caught_warnings) == 1) 
   n = 3 
   pixels = (np.arange(n) * np.ones((2, n))).T 
   result = ww.wcs_pix2world(pixels, 0, ra_dec_order=True) 
   ww.wcs_pix2world(pixels[..., 0], pixels[..., 1], 0, ra_dec_order=True) 
   close_enough = 1e-08 
   answer = np.array([[0.00024976, 0.00023018], [0.00023043, (-0.00024997)]]) 
   assert np.all((np.abs((ww.wcs.pc - answer)) < close_enough)) 
   answer = np.array([[202.39265216, 47.17756518], [202.39335826, 47.17754619], [202.39406436, 47.1775272]]) 
   assert np.all((np.abs((result - answer)) < close_enough))", 'Test wcs.wcs.wcs_pix2world.','From github issue #1463'
"def influence_plot(results, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs): 
    (fig, ax) = utils.create_mpl_ax(ax) 
   infl = results.get_influence() 
   if criterion.lower().startswith('coo'): 
      psize = infl.cooks_distance[0] 
   elif criterion.lower().startswith('dff'): 
      psize = np.abs(infl.dffits[0]) 
   else: 
      raise ValueError(('Criterion   %s   not   understood' % criterion)) 
   old_range = np.ptp(psize) 
   new_range = ((size ** 2) - (8 ** 2)) 
   psize = ((((psize - psize.min()) * new_range) / old_range) + (8 ** 2)) 
   leverage = infl.hat_matrix_diag 
   if external: 
      resids = infl.resid_studentized_external 
   else: 
      resids = infl.resid_studentized_internal 
   from scipy import stats 
   cutoff = stats.t.ppf((1.0 - (alpha / 2)), results.df_resid) 
   large_resid = (np.abs(resids) > cutoff) 
   large_leverage = (leverage > _high_leverage(results)) 
   large_points = np.logical_or(large_resid, large_leverage) 
   ax.scatter(leverage, resids, s=psize, alpha=plot_alpha) 
   labels = results.model.data.row_labels 
   if (labels is None): 
      labels = lrange(len(resids)) 
   ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(leverage, resids), lzip((- ((psize / 2) ** 0.5)), ((psize / 2) ** 0.5)), 'x-large', ax) 
   font = {'fontsize': 16, 'color': 'black'} 
   ax.set_ylabel('Studentized   Residuals', **font) 
   ax.set_xlabel('H   Leverage', **font) 
   ax.set_title('Influence   Plot', **font) 
   return fig"," 'Plot the influence plot of the results. 
 Parameters 
 results : Results object 
 The results object to plot. 
 external : bool 
 Whether to plot the external residuals. 
 alpha : float 
 The significance level. 
 criterion : string 
 The criterion to use for the influence plot. 
 size : int 
 The number of points to use for the influence plot. 
 plot_alpha : float 
 The alpha of the points in the influence plot. 
 ax : matplotlib axes object 
 The axes to plot the influence plot on. 
 kwargs 
 See :func:`~statsmodels.graphics.gof.influence_plot` for more details. 
 Returns 
 fig : matplotlib figure 
 The figure containing the influence plot. 
 Examples 
 >>> from statsmodels.graphics.gof import influence_plot 
 >>> results = statsmodels.stats.api.residuals.StudentizedResiduals(x=range(10), y=range(10), model=statsmodels.api.sm.OLS(x, y).fit()) 
 >>> influence_plot(results, alpha","'Plot of influence in regression. Plots studentized resids vs. leverage. 
 Parameters 
 results : results instance 
 A fitted model. 
 external : bool 
 Whether to use externally or internally studentized residuals. It is 
 recommended to leave external as True. 
 alpha : float 
 The alpha value to identify large studentized residuals. Large means 
 abs(resid_studentized) > t.ppf(1-alpha/2, dof=results.df_resid) 
 criterion : str {\'DFFITS\', \'Cooks\'} 
 Which criterion to base the size of the points on. Options are 
 DFFITS or Cook\'s D. 
 size : float 
 The range of `criterion` is mapped to 10**2 - size**2 in points. 
 plot_alpha : float 
 The `alpha` of the plotted points. 
 ax : matplotlib Axes instance 
 An instance of a matplotlib Axes. 
 Returns 
 fig : matplotlib figure 
 The matplotlib figure that contains the Axes. 
 Notes 
 Row labels for the observations in which the leverage, measured by the 
 diagonal of the hat matrix, is high or the residuals are large, as the 
 combination of large residuals and a high influence value indicates an 
 influence point. The value of large residuals can be controlled using the 
 `alpha` parameter. Large leverage points are identified as 
 hat_i > 2 * (df_model + 1)/nobs.'"
"def send_file(name, data): 
    nf = NetlogFile(name) 
   nf.sock.sendall(data) 
   nf.close()"," 'Send a file to a netlog server. 
 :param name: The name of the file to send. 
 :param data: The data to send. 
 :type data: bytes'",'Send file to result server'
"def root_create(request): 
    root = get_or_create_root() 
   return redirect('wiki:get', path=root.path)", 'Redirect to the root page.',"'In the edX wiki, we don\'t show the root_create view. Instead, we 
 just create the root automatically if it doesn\'t exist.'"
"def block_device_mapping_update_or_create(context, values): 
    return IMPL.block_device_mapping_update_or_create(context, values)", 'Create or update a block device mapping.',"'Update an entry of block device mapping. 
 If not existed, create a new entry'"
"@profiler.trace 
 @memoized 
 def flavor_list(request, is_public=True, get_extras=False): 
    flavors = novaclient(request).flavors.list(is_public=is_public) 
   if get_extras: 
      for flavor in flavors: 
         flavor.extras = flavor_get_extras(request, flavor.id, True, flavor) 
   return flavors"," 'List flavors. 
 :param request: nova client request 
 :param is_public: True to list public flavors, False to list private flavors 
 :param get_extras: True to return extra attributes for each flavor 
 :returns: list of flavors'",'Get the list of available instance sizes (flavors).'
"def build_content_type(format, encoding=u'utf-8'): 
    if (u'charset' in format): 
      return format 
   if (format in (u'application/json', u'text/javascript')): 
      return format 
   return (u'%s;   charset=%s' % (format, encoding))", 'Returns a content type for the given format.','Appends character encoding to the provided format if not already present.'
"def assign_random_category(resource): 
    random_index = randint(0, (TopicCategory.objects.all().count() - 1)) 
   tc = TopicCategory.objects.all()[random_index] 
   resource.category = tc 
   resource.save()", 'Assign a random category to the given resource.','Assign a random category to a resource'
"@contextlib.contextmanager 
 def temporary_mutation(obj, **kwargs): 
    NOT_PRESENT = object() 
   old_values = {} 
   for (attr, new_value) in kwargs.items(): 
      old_values[attr] = getattr(obj, attr, NOT_PRESENT) 
      setattr(obj, attr, new_value) 
   try: 
      (yield) 
   finally: 
      for (attr, old_value) in old_values.items(): 
         if (old_value is NOT_PRESENT): 
            del obj[attr] 
         else: 
            setattr(obj, attr, old_value)"," 'Context manager for temporarily mutating a class attribute. 
 This context manager will temporarily mutate the class attribute 
 to the new value and then restore it to the old value when the 
 context manager exits. 
 :param obj: The object to mutate. 
 :param kwargs: The kwargs to mutate the object with. 
 :returns: The context manager. 
 :rtype: contextlib.contextmanager'","'Temporarily set the attr on a particular object to a given value then 
 revert when finished. 
 One use of this is to temporarily set the read_deleted flag on a context 
 object: 
 with temporary_mutation(context, read_deleted=""yes""): 
 do_something_that_needed_deleted_objects()'"
"def is_harvard_notes_enabled(course): 
    modules = set(['textannotation', 'imageannotation', 'videoannotation']) 
   return bool(modules.intersection(course.advanced_modules))", 'Returns true if the course has advanced modules enabled.',"'Returns True if Harvard Annotation Tool is enabled for the course, 
 False otherwise. 
 Checks for \'textannotation\', \'imageannotation\', \'videoannotation\' in the list 
 of advanced modules of the course.'"
"def seqs_from_file(ids, file_lines): 
    for (label, seq) in parse_fasta(file_lines): 
      if (id_from_fasta_label_line(label) in ids): 
         (yield (label, seq))"," 'Parse a file of fasta lines and return a list of (label, seq) tuples. 
 :param ids: a list of strings of the form ``[label1, label2, ...]`` 
 :param file_lines: a list of strings of the form ``[label, seq, ...]`` 
 :return: a list of (label, seq) tuples'",'Extract labels and seqs from file'
"def dice_coe(output, target, epsilon=1e-10): 
    inse = tf.reduce_sum((output * target)) 
   l = tf.reduce_sum((output * output)) 
   r = tf.reduce_sum((target * target)) 
   dice = ((2 * inse) / (l + r)) 
   if (epsilon == 0): 
      return dice 
   else: 
      return tf.clip_by_value(dice, 0, (1.0 - epsilon))"," 'Dice coefficient between two tensors. 
 Computes the dice coefficient between two tensors. 
 This is the harmonic mean of the product and the 
 sum of the squares of the two tensors. 
 Parameters 
 output : Tensor 
 The first tensor. 
 target : Tensor 
 The second tensor. 
 epsilon : float 
 Epsilon to clip the dice coefficient. 
 Returns 
 dice : Tensor 
 The dice coefficient. 
 Raises 
 ValueError 
 If the dimensions of the tensors are not the same. 
 Examples 
 >>> x = tf.constant([1, 2, 3]) 
 >>> y = tf.constant([4, 5, 6]) 
 >>> dice_coe(x, y) 
 0.5 
 >>> dice_coe(x, y, epsilon=1e-6) 
 0.50000000000000002'","'SÃ¸rensenâDice coefficient for comparing the similarity of two distributions, 
 usually be used for binary image segmentation i.e. labels are binary. 
 The coefficient = [0, 1], 1 if totally match. 
 Parameters 
 output : tensor 
 A distribution with shape: [batch_size, ....], (any dimensions). 
 target : tensor 
 A distribution with shape: [batch_size, ....], (any dimensions). 
 epsilon : float 
 An optional name to attach to this layer. 
 Examples 
 >>> outputs = tl.act.pixel_wise_softmax(network.outputs) 
 >>> dice_loss = 1 - tl.cost.dice_coe(outputs, y_, epsilon=1e-5) 
 References 
 - `wiki-dice <https://en.wikipedia.org/wiki/SÃ¸rensenâDice_coefficient>`_'"
"def _skip_create_test_db(self, verbosity=1, autoclobber=False, serialize=True): 
    if callable(getattr(self.connection.features, 'confirm', None)): 
      self.connection.features.confirm() 
   elif hasattr(self, '_rollback_works'): 
      can_rollback = self._rollback_works() 
      self.connection.settings_dict['SUPPORTS_TRANSACTIONS'] = can_rollback 
   return self._get_test_db_name()"," 'Skip the creation of the test database. 
 If the connection supports transactions, and the test database is 
 not already created, this will create the test database. 
 If the connection does not support transactions, this will not create 
 the test database.'","'``create_test_db`` implementation that skips both creation and flushing 
 The idea is to re-use the perfectly good test DB already created by an 
 earlier test run, cutting the time spent before any tests run from 5-13s 
 (depending on your I/O luck) down to 3.'"
"def upload_mission(aFileName): 
    missionlist = readmission(aFileName) 
   print ('\nUpload   mission   from   a   file:   %s' % import_mission_filename) 
   print '   Clear   mission' 
   cmds = vehicle.commands 
   cmds.clear() 
   for command in missionlist: 
      cmds.add(command) 
   print '   Upload   mission' 
   vehicle.commands.upload()", 'Upload a mission from a file.','Upload a mission from a file.'
"def evaluator(variables, functions, math_expr, case_sensitive=False): 
    if (math_expr.strip() == ''): 
      return float('nan') 
   math_interpreter = ParseAugmenter(math_expr, case_sensitive) 
   math_interpreter.parse_algebra() 
   (all_variables, all_functions) = add_defaults(variables, functions, case_sensitive) 
   math_interpreter.check_variables(all_variables, all_functions) 
   if case_sensitive: 
      casify = (lambda x: x) 
   else: 
      casify = (lambda x: x.lower()) 
   evaluate_actions = {'number': eval_number, 'variable': (lambda x: all_variables[casify(x[0])]), 'function': (lambda x: all_functions[casify(x[0])](x[1])), 'atom': eval_atom, 'power': eval_power, 'parallel': eval_parallel, 'product': eval_product, 'sum': eval_sum} 
   return math_interpreter.reduce_tree(evaluate_actions)"," 'Evaluate a mathematical expression. 
 Parameters 
 variables : dict 
 Dictionary of variables. 
 functions : dict 
 Dictionary of functions. 
 math_expr : str 
 Mathematical expression to evaluate. 
 case_sensitive : bool 
 Whether or not to case sensitive. 
 Returns 
 float 
 Result of evaluation.'","'Evaluate an expression; that is, take a string of math and return a float. 
 -Variables are passed as a dictionary from string to value. They must be 
 python numbers. 
 -Unary functions are passed as a dictionary from string to function.'"
"def make_pie_chart(data, dir_path, level, prefs, pref_colors, background_color, label_color, generate_image_type, plot_width, plot_height, bar_width, dpi, include_html_legend, file_prefix=None, props={}, others_key='All   Other   Categories', others_color='#eeeeee', should_capitalize=True): 
    if (not data): 
      raise ValueError('No   data   available   for   pie   chart.') 
   all_fracs = [] 
   all_labels = [] 
   colors = [] 
   for (color_ix, (c_label, c_frac)) in enumerate(data): 
      if (c_label == others_key): 
         colors.append(others_color) 
      else: 
         colors.append(data_colors[pref_colors[c_label]].toHex()) 
      all_fracs.append(c_frac) 
      if should_capitalize: 
         capital = ('%s   (%.2f%%)' % (c_label.capitalize(), (c_frac * 100.0))) 
         all_labels.append(capital) 
      else: 
         all_labels.append(('%s   (%.2f%%)' % (c_label, (c_frac * 100.0)))) 
   rc('font', size='10') 
   rc('text', color=label_color) 
   rc('patch', linewidth=0.1) 
   rc('axes', linewidth=0.5, edgecolor=label_color) 
   rc('text', usetex=False) 
   fig = figure(randrange(10000), figsize=(plot_width, plot_height)) 
   fp = FontProperties() 
   fp.set_size('8') 
   if (len(data) > 30): 
      loc = 4 
   else: 
      loc = 5 
   mtitle = 'Pie   Chart' 
   if ('title' in props): 
      mtitle = props['title'] 
   axis('off') 
   title(mtitle, fontsize='10', color=label_color) 
   ax = axes([0.0, 0.0, 0.5, 1]) 
   p1 = pie(all_fracs, shadow=False, colors=colors) 
   if (file_prefix is None): 
      img_name = make_img_name(file_ext='.png') 
   else: 
      img_name = file_prefix 
   img_abs = os.path.join(dir_path, 'charts', img_name) 
   savefig(img_abs, dpi=dpi, facecolor=background_color) 
   eps_link = '' 
   eps_abs = '' 
   if (file_prefix is None): 
      eps_img_name = make_img_name(file_ext=('.%s' % generate_image_type)) 
   else: 
      eps_img_name = (file_prefix + ('.%s' % generate_image_type)) 
   savefig(os.path.join(dir_path, 'charts', eps_img_name), facecolor=background_color) 
   if (generate_image_type == 'eps'): 
      strip_eps_font(os.path.join(dir_path, 'charts', eps_img_name)) 
   eps_abs = os.path.join(dir_path, 'charts', eps_img_name) 
   eps_link = (PDF_LINK % (os.path.join('charts', eps_img_name), ('View   Figure   (.%s)' % generate_image_type))) 
   close(fig) 
   clf() 
   updated_taxa = [] 
   updated_colors = [] 
   for i in data: 
      if (i[0] != others_key): 
         updated_taxa.append(i[0].replace('""', '')) 
         updated_colors.append(data_colors[pref_colors[i[0]]].toHex()) 
      else: 
         updated_taxa.append(others_key) 
         updated_colors.append(others_color) 
   if include_html_legend: 
      legend_fname_png = make_legend(updated_taxa, updated_colors, plot_width, plot_height, label_color, background_color, img_abs, 'png', 80) 
      legend_fpath_png = os.path.join('charts', legend_fname_png) 
   legend_fname = make_legend(updated_taxa, updated_colors, plot_width, plot_height, label_color, background_color, img_abs, generate_image_type, dpi) 
   legend_fpath = os.path.join('charts', legend_fname) 
   legend_link = (LEGEND_LINK % (legend_fpath, ('View   Legend   (.%s)' % generate_image_type))) 
   points_id = '' 
   xmap_html = '' 
   if (not include_html_legend): 
      IMG_TEXT = (IMG_SRC_minus_legend % (os.path.join('charts', img_name), points_id)) 
   else: 
      IMG_TEXT = (IMG_SRC_2 % (os.path.join('charts', img_name), points_id, legend_fpath_png)) 
   return (eps_link, legend_link, IMG_TEXT, xmap_html)"," 'Generate a pie chart. 
 Parameters 
 data : list of tuples 
 A list of tuples, where each tuple has the form (label, fraction). 
 dir_path : str 
 The directory to place the chart in. 
 level : str 
 The level of the directory to place the chart in. 
 prefs : dict 
 A dictionary of the form: 
 { \'label_color\' : \'#000000\', \'background_color\' : \'#000000\', \'font_color\' : \'#000000\', \'font_size\' : 10, \'font_weight\' : \'bold\', \'font_name\' : \'Helvetica\'} 
 A dictionary of the form: 
 { \'label_color\' : \'#000000\', \'background_color\' : \'#000000\', \'font_color\' : \'#000000\', \'font_size\' : 10","'Write interactive piechart 
 data: [fraction:label,...] 
 trunc_len: truncates labels after this many chars'"
"def ismount(path): 
    try: 
      return ismount_raw(path) 
   except OSError: 
      return False", 'Return True if path is a mount point.',"'Test whether a path is a mount point. This will catch any 
 exceptions and translate them into a False return value 
 Use ismount_raw to have the exceptions raised instead.'"
"def libvlc_vlm_get_media_instance_title(p_instance, psz_name, i_instance): 
    f = (_Cfunctions.get('libvlc_vlm_get_media_instance_title', None) or _Cfunction('libvlc_vlm_get_media_instance_title', ((1,), (1,), (1,)), None, ctypes.c_int, Instance, ctypes.c_char_p, ctypes.c_int)) 
   return f(p_instance, psz_name, i_instance)"," 'Get the title of a media instance. 
 @param p_instance: instance to get the title from 
 @param psz_name: name of the media instance to get the title from 
 @param i_instance: instance number 
 @return: the title of the media instance, or None if not found'","'Get vlm_media instance title number by name or instance id. 
 @param p_instance: a libvlc instance. 
 @param psz_name: name of vlm media instance. 
 @param i_instance: instance id. 
 @return: title as number or -1 on error. 
 @bug: will always return 0.'"
"def IE_Dispatcher(s): 
    if (len(s) < 1): 
      return Raw(s) 
   ietype = ord(s[0]) 
   cls = ietypecls.get(ietype, Raw) 
   if ((cls == Raw) and ((ietype & 128) == 128)): 
      cls = IE_NotImplementedTLV 
   return cls(s)"," 'Convert a string of bytes to a subclass of the IE class. 
 :param s: A string of bytes 
 :type s: str 
 :return: An instance of the IE class 
 :rtype: :class:`tlv.IE`'",'Choose the correct Information Element class.'
"def combine_path_lists(*path_seqs): 
    results = [] 
   for path in combine_lists(*path_seqs): 
      expanded = expand_path(path) 
      paths = (sorted(glob.glob(expanded)) or [expanded]) 
      results.extend(paths) 
   return results"," 'Combine path lists. 
 This function is a bit like the ``join`` function from the ``pathlib`` 
 module.  It combines a list of path lists into a single path list.  It 
 assumes that the input path lists are sorted, and that the input path 
 lists are of the same length.  It will raise an exception if this is 
 not the case. 
 :param path_seqs: A sequence of path lists. 
 :return: A single path list. 
 :rtype: list of str'","'Concatenate the given sequences into a list. Ignore None values. 
 Resolve ``~`` (home dir) and environment variables, and expand globs 
 that refer to the local filesystem. 
 .. versionchanged:: 0.4.6 
 Can take single strings as well as lists.'"
"def first_ip(network): 
    atoi = (lambda addr: struct.unpack('!I', socket.inet_aton(addr))[0]) 
   itoa = (lambda addr: socket.inet_ntoa(struct.pack('!I', addr))) 
   (address, netmask) = network.split('/') 
   netmask_i = ((4294967295 << (32 - atoi(netmask))) & 4294967295) 
   return itoa(((atoi(address) & netmask_i) + 1))"," 'Return the first IP address in a subnet. 
 :param network: A string of the form \'192.168.0.0/24\'. 
 :returns: The first IP address in the subnet.'","'Return the first IPv4 address in network 
 Args: 
 network (str): network in CIDR format 
 Returns: 
 str: first IPv4 address'"
"def load_sparse_dataset(name, normalize=True, transfer=False, randomize_valid=False, randomize_test=False): 
    assert (name in ['harry', 'terry', 'ule']) 
   common = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'UTLC', 'sparse', (name + '_')) 
   (trname, vname, tename) = [((common + subset) + '.npy') for subset in ['train', 'valid', 'test']] 
   train = load_sparse(trname) 
   valid = load_sparse(vname) 
   test = load_sparse(tename) 
   if randomize_valid: 
      rng = make_np_rng(None, [1, 2, 3, 4], which_method='permutation') 
      perm = rng.permutation(valid.shape[0]) 
      valid = valid[perm] 
   if randomize_test: 
      rng = make_np_rng(None, [1, 2, 3, 4], which_method='permutation') 
      perm = rng.permutation(test.shape[0]) 
      test = test[perm] 
   if normalize: 
      if (name == 'ule'): 
         train = (train.astype(theano.config.floatX) / 255) 
         valid = (valid.astype(theano.config.floatX) / 255) 
         test = (test.astype(theano.config.floatX) / 255) 
      elif (name == 'harry'): 
         train = train.astype(theano.config.floatX) 
         valid = valid.astype(theano.config.floatX) 
         test = test.astype(theano.config.floatX) 
         std = 0.6933604603392579 
         train = (train / std) 
         valid = (valid / std) 
         test = (test / std) 
      elif (name == 'terry'): 
         train = train.astype(theano.config.floatX) 
         valid = valid.astype(theano.config.floatX) 
         test = test.astype(theano.config.floatX) 
         train = (train / 300) 
         valid = (valid / 300) 
         test = (test / 300) 
      else: 
         raise Exception(""This   dataset   don't   have   its   normalization   defined"") 
   if transfer: 
      fname = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'UTLC', 'filetensor', (name + '_transfer.ft')) 
      transfer = load_filetensor(fname) 
      return (train, valid, test, transfer) 
   else: 
      return (train, valid, test)"," 'Loads the sparse datasets from the UTLC dataset 
 Parameters 
 name : str 
 The name of the dataset to load 
 normalize : bool 
 If True, the dataset is normalized to [0, 1] 
 transfer : bool 
 If True, the transfer dataset is loaded 
 randomize_valid : bool 
 If True, the validation set is randomized 
 randomize_test : bool 
 If True, the test set is randomized'","'Load the train,valid,test data for the dataset `name` and return it in 
 sparse format. 
 We suppose the data was created with ift6266h11/pretraitement/to_npy.py 
 that shuffle the train. So the train should already be shuffled. 
 name : \'avicenna\', \'harry\', \'rita\', \'sylvester\' or \'ule\' 
 Which dataset to load 
 normalize : bool 
 If True, we normalize the train dataset before returning it 
 transfer : 
 If True also return the transfer label 
 randomize_valid : bool 
 Do we randomize the order of the valid set?  We always use the same 
 random order If False, return in the same order as downloaded on the 
 web 
 randomize_test : bool 
 Do we randomize the order of the test set?  We always use the same 
 random order If False, return in the same order as downloaded on the 
 web 
 Returns 
 train, valid, test : ndarrays 
 Datasets returned if transfer = False 
 train, valid, test, transfer : ndarrays 
 Datasets returned if transfer = False'"
"def libvlc_video_get_spu_description(p_mi): 
    f = (_Cfunctions.get('libvlc_video_get_spu_description', None) or _Cfunction('libvlc_video_get_spu_description', ((1,),), None, ctypes.POINTER(TrackDescription), MediaPlayer)) 
   return f(p_mi)"," 'Get the SPU description of a video track. 
 @param p_mi: the media player. 
 @return: the SPU description, or None if there is no SPU description.'","'Get the description of available video subtitles. 
 @param p_mi: the media player. 
 @return: list containing description of available video subtitles. It must be freed with L{libvlc_track_description_list_release}().'"
"def sum_parts(data): 
    arr = np.asarray(data, dtype=np.float32) 
   out = cuda.device_array(1, dtype=np.float32) 
   gpu_single_block_sum[(1, gpu_block_sum_max_blockdim)](arr, out) 
   return out.copy_to_host()[0]"," 'Sum of the parts of the given array. 
 Parameters 
 data : array 
 The array to sum. 
 Returns 
 out : array 
 The sum of the parts of the array. 
 Notes 
 The sum of the parts of a given array is the sum of the parts of each 
 element in the array. 
 Examples 
 >>> from sympy.physics.quantum.gpu import gpu_single_block_sum 
 >>> from sympy.physics.quantum.gpu import gpu_single_block_sum_parts 
 >>> data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 >>> gpu_single_block_sum_parts(data) 
 array([ 3.,  5.,  7.]) 
 >>> gpu_single_block_sum_parts(data) 
 array([ 1.,  3.,  5.])'",'Driver for ``gpu_single_block_sum`` kernel'
"def test_contains_one_of(): 
    assert hug.validate.contains_one_of('no', 'way')(TEST_SCHEMA) 
   assert (not hug.validate.contains_one_of('last', 'place')(TEST_SCHEMA))", 'Test contains_one_of','Test to ensure hug\'s contains_one_of validation function works as expected to ensure presence of a field'
"def getTransferClosestNestedRing(extrusionHalfWidth, nestedRings, oldOrderedLocation, skein, threadSequence): 
    if (len(nestedRings) > 0): 
      oldOrderedLocation.z = nestedRings[0].z 
   closestDistance = 9.876543219876543e+17 
   closestNestedRing = None 
   for remainingNestedRing in nestedRings: 
      distance = getClosestDistanceIndexToLine(oldOrderedLocation.dropAxis(), remainingNestedRing.boundary).distance 
      if (distance < closestDistance): 
         closestDistance = distance 
         closestNestedRing = remainingNestedRing 
   nestedRings.remove(closestNestedRing) 
   closestNestedRing.addToThreads(extrusionHalfWidth, oldOrderedLocation, skein, threadSequence) 
   return closestNestedRing"," 'Find the closest nested ring to the oldOrderedLocation and add it to the 
 thread sequence. 
 :param extrusionHalfWidth: half width of extrusion 
 :type extrusionHalfWidth: float 
 :param nestedRings: list of nested rings 
 :type nestedRings: list of :class:`~rhino.geometry.NestedRing` 
 :param oldOrderedLocation: location of old thread sequence 
 :type oldOrderedLocation: :class:`~rhino.geometry.Point3d` 
 :param skein: skein of thread sequence 
 :type skein: :class:`~rhino.geometry.Skein` 
 :param threadSequence: thread sequence 
 :type threadSequence: :class:`~rhino.Geometry.ThreadSequence` 
 :rtype: :class:`~rhino.Geometry.NestedRing` 
 :return: closest nested ring'",'Get and transfer the closest remaining nested ring.'
"def org_organisation_requires(required=False, realms=None, updateable=False): 
    requires = IS_ONE_OF(current.db, 'org_organisation.id', org_OrganisationRepresent(), realms=realms, updateable=updateable, orderby='org_organisation.name', sort=True) 
   if (not required): 
      requires = IS_EMPTY_OR(requires) 
   return requires"," 'Requires a specific organisation, or no organisation.'","'@param required: Whether the selection is optional or mandatory 
 @param realms: Whether the list should be filtered to just those 
 belonging to a list of realm entities 
 @param updateable: Whether the list should be filtered to just those 
 which the user has Write access to 
 @ToDo: Option to remove Branches 
 @ToDo: Option to only include Branches'"
"def get_id(sensorid, feedtag, feedname, feedid, feeduserid): 
    return 'emoncms{}_{}_{}_{}_{}'.format(sensorid, feedtag, feedname, feedid, feeduserid)", 'Returns the id for a sensor feed.','Return unique identifier for feed / sensor.'
"def set_network(ip, netmask, gateway): 
    return __execute_cmd('setniccfg   -s   {0}   {1}   {2}'.format(ip, netmask, gateway))"," 'Set network configuration. 
 ip: IP address 
 netmask: netmask 
 gateway: gateway 
 :return: None'","'Configure Network 
 CLI Example: 
 .. code-block:: bash 
 salt dell drac.set_network [DRAC IP] [NETMASK] [GATEWAY] 
 salt dell drac.set_network 192.168.0.2 255.255.255.0 192.168.0.1'"
"def pickle(obj, fname, protocol=2): 
    with smart_open(fname, 'wb') as fout: 
      _pickle.dump(obj, fout, protocol=protocol)"," 'Pickle an object to a file. 
 :param obj: the object to pickle 
 :param fname: the name of the file to pickle to 
 :param protocol: the protocol version to use (2 or 3)'","'Pickle object `obj` to file `fname`. 
 `protocol` defaults to 2 so pickled objects are compatible across 
 Python 2.x and 3.x.'"
"def split_file_dummy(changed_file): 
    return (None, changed_file)"," 'Returns (None, changed_file) for a dummy file.'","'Split the repository-relative filename into a tuple of (branchname, 
 branch_relative_filename). If you have no branches, this should just 
 return (None, changed_file).'"
"def _merge_entries(path, tree1, tree2): 
    entries1 = _tree_entries(path, tree1) 
   entries2 = _tree_entries(path, tree2) 
   i1 = i2 = 0 
   len1 = len(entries1) 
   len2 = len(entries2) 
   result = [] 
   while ((i1 < len1) and (i2 < len2)): 
      entry1 = entries1[i1] 
      entry2 = entries2[i2] 
      if (entry1.path < entry2.path): 
         result.append((entry1, _NULL_ENTRY)) 
         i1 += 1 
      elif (entry1.path > entry2.path): 
         result.append((_NULL_ENTRY, entry2)) 
         i2 += 1 
      else: 
         result.append((entry1, entry2)) 
         i1 += 1 
         i2 += 1 
   for i in range(i1, len1): 
      result.append((entries1[i], _NULL_ENTRY)) 
   for i in range(i2, len2): 
      result.append((_NULL_ENTRY, entries2[i])) 
   return result"," 'Merge two trees, returning a list of tuples, where each tuple is a 
 pair of entries. 
 The entries are sorted by path, so that the result is a complete tree. 
 If a path is missing in one tree, it will be added to the result. 
 If a path is missing in the other tree, it will not be added to the 
 result. 
 The result is sorted by path, so that the result is a complete tree. 
 The result is a list of tuples, where each tuple is a pair of entries. 
 Each entry is a :class:`Entry` instance. 
 The entries are sorted by path, so that the result is a complete tree. 
 The result is a list of tuples, where each tuple is a pair of entries. 
 Each entry is a :class:`Entry` instance.'","'Merge the entries of two trees. 
 :param path: A path to prepend to all tree entry names. 
 :param tree1: The first Tree object to iterate, or None. 
 :param tree2: The second Tree object to iterate, or None. 
 :return: A list of pairs of TreeEntry objects for each pair of entries in 
 the trees. If an entry exists in one tree but not the other, the other 
 entry will have all attributes set to None. If neither entry\'s path is 
 None, they are guaranteed to match.'"
"def register_onaccept(form): 
    req_vars = form.request_vars 
   position = req_vars.get('position', '') 
   reason = req_vars.get('reason', '') 
   db = current.db 
   table = db.auth_user 
   db((table.id == form.vars.id)).update(comments=('%s   |   %s' % (position, reason)))", 'Update user comments on acceptance','Tasks to be performed after a new user registers'
"def gf_mul_ground(f, a, p, K): 
    if (not a): 
      return [] 
   else: 
      return [((a * b) % p) for b in f]"," 'Multiply ``f`` by ``a`` and return the result in ``gf(p)[0]``. 
 Examples 
 >>> from sympy.polys.galoistools import gf_mul_ground 
 >>> gf_mul_ground([1, 2, 3], 4, 5, QQ) 
 [2, 6, 9] 
 >>> gf_mul_ground([1, 2, 3], 4, 5, ZZ) 
 []'","'Compute ``f * a`` where ``f`` in ``GF(p)[x]`` and ``a`` in ``GF(p)``. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import gf_mul_ground 
 >>> gf_mul_ground([3, 2, 4], 2, 5, ZZ) 
 [1, 4, 3]'"
"def close_review_requests(payload, server_url): 
    review_request_id_to_commits_map = defaultdict(list) 
   branch_name = payload.get(u'repository_path') 
   if (not branch_name): 
      return review_request_id_to_commits_map 
   revisions = payload.get(u'revisions', []) 
   for revision in revisions: 
      revision_id = revision.get(u'revision') 
      if (len(revision_id) > 7): 
         revision_id = revision_id[:7] 
      commit_message = revision.get(u'message') 
      review_request_id = get_review_request_id(commit_message, server_url) 
      review_request_id_to_commits_map[review_request_id].append((u'%s   (%s)' % (branch_name, revision_id))) 
   return review_request_id_to_commits_map"," 'Returns a dictionary mapping review request IDs to the commit messages 
 that correspond to the commits in the review request.'",'Closes all review requests for the Google Code repository.'
"def encoding(argument): 
    try: 
      codecs.lookup(argument) 
   except LookupError: 
      raise ValueError(('unknown   encoding:   ""%s""' % argument)) 
   return argument"," 'Look up the given encoding. 
 :param argument: 
 The encoding to look up. 
 :raises: 
 LookupError if the encoding is not known. 
 :returns: 
 The encoding name, or None if the encoding is not known.'","'Verfies the encoding argument by lookup. 
 (Directive option conversion function.) 
 Raises ValueError for unknown encodings.'"
"def setConfigOptions(**opts): 
    for (k, v) in opts.items(): 
      setConfigOption(k, v)"," 'Set config options. 
 Parameters 
 opts : dict 
 Options to set. 
 Example 
 >>> from pylons import config 
 >>> config.setConfigOptions(app_name=""pylons"") 
 >>> config.app_name 
 \'pylons\' 
 >>> config.setConfigOptions(app_name=""myapp"") 
 >>> config.app_name 
 \'myapp\''","'Set global configuration options. 
 Each keyword argument sets one global option.'"
"def _getReportItem(itemName, results): 
    subKeys = itemName.split(':') 
   subResults = results 
   for subKey in subKeys: 
      subResults = subResults[subKey] 
   return subResults"," 'Returns the sub-result for a given report item. 
 :param itemName: Name of the report item. 
 :param results: Dictionary of results. 
 :return: Sub-result dictionary.'","'Get a specific item by name out of the results dict. 
 The format of itemName is a string of dictionary keys separated by colons, 
 each key being one level deeper into the results dict. For example, 
 \'key1:key2\' would fetch results[\'key1\'][\'key2\']. 
 If itemName is not found in results, then None is returned'"
"def getInnerText(node): 
    inner_text = [] 
   for child in node.childNodes: 
      if ((child.nodeType == child.TEXT_NODE) or (child.nodeType == child.CDATA_SECTION_NODE)): 
         inner_text.append(child.data) 
      elif (child.nodeType == child.ELEMENT_NODE): 
         inner_text.extend(getInnerText(child)) 
      else: 
         pass 
   return ''.join(inner_text)"," 'Returns the inner text of the given node. 
 :param node: The node to get the inner text of. 
 :type node: :class:`xml.etree.ElementTree.Element` 
 :rtype: :class:`str` 
 :returns: The inner text of the given node. 
 :raises: :class:`xml.etree.ElementTree.ParseError` 
 :raises: :class:`xml.etree.ElementTree.ParseError` 
 :raises: :class:`xml.etree.ElementTree.ParseError` 
 :raises: :class:`xml.etree.ElementTree.ParseError` 
 :raises: :class:`xml.etree.ElementTree.ParseError` 
 :raises: :class:`xml.etree.ElementTree.ParseError` 
 :raises: :class:`xml.etree.ElementTree.ParseError` 
 :raises: :class:`xml.etree.ElementTree.ParseError` 
 :raises: :class:`xml.etree.ElementTree.ParseError` 
 :raises: :",'Get all the inner text of a DOM node (recursively).'
"def resolve_name(name, namespace_, remappings=None): 
    if (not name): 
      return namespace(namespace_) 
   name = canonicalize_name(name) 
   if (name[0] == SEP): 
      resolved_name = name 
   elif is_private(name): 
      resolved_name = canonicalize_name(((namespace_ + SEP) + name[1:])) 
   else: 
      resolved_name = (namespace(namespace_) + name) 
   if (remappings and (resolved_name in remappings)): 
      return remappings[resolved_name] 
   else: 
      return resolved_name"," 'Resolve a name in a namespace. 
 :param name: The name to resolve. 
 :param namespace_: The namespace to resolve the name in. 
 :param remappings: A mapping of old names to new names. 
 :return: The resolved name.'","'Resolve a ROS name to its global, canonical form. Private ~names 
 are resolved relative to the node name. 
 @param name: name to resolve. 
 @type  name: str 
 @param namespace_: node name to resolve relative to. 
 @type  namespace_: str 
 @param remappings: Map of resolved remappings. Use None to indicate no remapping. 
 @return: Resolved name. If name is empty/None, resolve_name 
 returns parent namespace_. If namespace_ is empty/None, 
 @rtype: str'"
"def separate_qtranslate_content(text): 
    qt_start = u'<!--:' 
   qt_end = u'-->' 
   qt_end_with_lang_len = 5 
   qt_chunks = text.split(qt_start) 
   content_by_lang = {} 
   common_txt_list = [] 
   for c in qt_chunks: 
      if (not c.strip()): 
         continue 
      if c.startswith(qt_end): 
         lang = u'' 
         c = c.lstrip(qt_end) 
         if (not c): 
            continue 
      elif c[2:].startswith(qt_end): 
         lang = c[:2] 
         c = c[qt_end_with_lang_len:] 
      else: 
         lang = u'' 
      if (not lang): 
         common_txt_list.append(c) 
         for l in content_by_lang.keys(): 
            content_by_lang[l].append(c) 
      else: 
         content_by_lang[lang] = (content_by_lang.get(lang, common_txt_list) + [c]) 
   if (common_txt_list and (not content_by_lang)): 
      content_by_lang[u''] = common_txt_list 
   for l in content_by_lang.keys(): 
      content_by_lang[l] = u'   '.join(content_by_lang[l]) 
   return content_by_lang"," 'Separate the qtranslate content from the original content. 
 :param text: the original content 
 :return: a dictionary of language to the content'","'Parse the content of a wordpress post or page and separate qtranslate languages. 
 qtranslate tags: <!--:LL-->blabla<!--:-->'"
"def get(item, default=None): 
    def getter(value): 
      if ET.iselement(value): 
         value = value.attrib 
      try: 
         if _is_re_match(value): 
            return value.group(item) 
         else: 
            return value[item] 
      except (KeyError, IndexError): 
         return default 
      except (TypeError, AttributeError) as err: 
         raise ValueError(err) 
   return transform(getter)"," 'Get the value of an attribute from an ET.Element. 
 :param item: The name of the attribute to get. 
 :param default: The default value to return if the attribute is not found. 
 :type default: str 
 :return: The value of the attribute. 
 :rtype: str'","'Get item from value (value[item]). 
 If the item is not found, return the default. 
 Handles XML elements, regex matches and anything that has __getitem__.'"
"def detect_paragraph_type(txt): 
    txt = txt.replace('\r\n', '\n') 
   txt = txt.replace('\r', '\n') 
   txt_line_count = len(re.findall('(?mu)^\\s*.+$', txt)) 
   docanalysis = DocAnalysis('txt', txt) 
   hardbreaks = docanalysis.line_histogram(0.55) 
   if hardbreaks: 
      tab_line_count = len(re.findall('(?mu)^( DCTB |\\s{2,}).+$', txt)) 
      print_percent = (tab_line_count / float(txt_line_count)) 
      empty_line_count = len(re.findall('(?mu)^\\s*$', txt)) 
      block_percent = (empty_line_count / float(txt_line_count)) 
      if (print_percent >= block_percent): 
         if (0.15 <= print_percent <= 0.75): 
            return 'print' 
      elif (0.15 <= block_percent <= 0.75): 
         return 'block' 
      return 'unformatted' 
   return 'single'"," 'Detects paragraph type based on the following rules: 
 * If the text contains a lot of hard breaks, the paragraph type is 
 ""print"". 
 * If the text contains a lot of empty lines, the paragraph type is 
 ""block"". 
 * If the text contains a lot of tab lines, the paragraph type is 
 ""block"". 
 * If the text contains a lot of spaces, the paragraph type is 
 ""block"". 
 * If the text contains a lot of single lines, the paragraph type is 
 ""single"". 
 * If the text contains a lot of paragraphs, the paragraph type is 
 ""unformatted"". 
 * If the text contains a lot of lines, the paragraph type is 
 ""unformatted"". 
 * If the text contains a lot of lines with no hard breaks, the paragraph 
 type is ""unformatted"". 
 * If the text contains a lot of lines with no empty lines, the paragraph 
 type is ""unformatted"". 
 * If the text contains a lot of lines with no tab lines, the paragraph 
 type is ""unformatted"". 
 * If the text contains","'Tries to determine the paragraph type of the document. 
 block: Paragraphs are separated by a blank line. 
 single: Each line is a paragraph. 
 print: Each paragraph starts with a 2+ spaces or a tab 
 and ends when a new paragraph is reached. 
 unformatted: most lines have hard line breaks, few/no blank lines or indents 
 returns block, single, print, unformatted'"
"def unique_file_name(base_name, extension=''): 
    idcount = 0 
   if (extension and (not extension.startswith('.'))): 
      extension = ('.%s' % extension) 
   fname = (base_name + extension) 
   while os.path.exists(fname): 
      fname = ('%s-%d%s' % (base_name, idcount, extension)) 
      idcount += 1 
   return fname"," 'Return a unique file name. 
 This is the same as os.path.join(os.path.dirname(base_name), 
 unique_file_name(base_name, extension)) 
 :param base_name: base name 
 :param extension: extension 
 :return: unique name'","'Creates a unique file name based on the specified base name. 
 @base_name - The base name to use for the unique file name. 
 @extension - The file extension to use for the unique file name. 
 Returns a unique file string.'"
"def UniqueIterator(iterator, key=(lambda x: x)): 
    so_far = set() 
   def no_dups(x): 
      k = key(x) 
      if (k in so_far): 
         return False 
      else: 
         so_far.add(k) 
         return True 
   return IteratorFilter(iterator, no_dups)"," 'Returns an iterator that only yields unique values. 
 This is useful for iterating over the keys of a dict. 
 :param iterator: An iterator to filter. 
 :param key: A function that takes an item and returns a boolean. 
 If ``key`` returns ``True``, the item is yielded. 
 :return: An iterator that only yields unique values. 
 :rtype: Iterator'","'Takes an iterator and returns an iterator that returns only the 
 first occurence of each entry'"
"def _putmask_smart(v, m, n): 
    if (not is_list_like(n)): 
      n = np.array(([n] * len(m))) 
   elif (isinstance(n, np.ndarray) and (n.ndim == 0)): 
      n = np.repeat(np.array(n, ndmin=1), len(m)) 
   try: 
      nn = n[m] 
      if (not _is_na_compat(v, nn[0])): 
         raise ValueError 
      nn_at = nn.astype(v.dtype) 
      if (not is_numeric_v_string_like(nn, nn_at)): 
         comp = (nn == nn_at) 
         if (is_list_like(comp) and comp.all()): 
            nv = v.copy() 
            nv[m] = nn_at 
            return nv 
   except (ValueError, IndexError, TypeError): 
      pass 
   (dtype, _) = _maybe_promote(n.dtype) 
   if (is_extension_type(v.dtype) and is_object_dtype(dtype)): 
      nv = v.get_values(dtype) 
   else: 
      nv = v.astype(dtype) 
   try: 
      nv[m] = n[m] 
   except ValueError: 
      (idx,) = np.where(np.squeeze(m)) 
      for (mask_index, new_val) in zip(idx, n[m]): 
         nv[mask_index] = new_val 
   return nv"," 'Like putmask, but with a smarter mask. 
 The mask is a list of indices. 
 The value of the mask is not checked for validity, so this is 
 a more general version of putmask. 
 Parameters 
 v : array-like 
 The array to mask. 
 m : array-like 
 The mask. 
 n : array-like 
 The replacement value. 
 Returns 
 nv : array-like 
 The array with the masked values replaced by n. 
 Notes 
 If v is a string, the string is converted to a float or int, 
 depending on the dtype of n. 
 If v is a float or int, the dtype of n is used. 
 If v is a float or int, the dtype of v is used. 
 If v is a string, the dtype of n is used. 
 If v is a string, the dtype of v is used. 
 Examples 
 >>> from numpy.lib import recycle_array_info 
 >>> a = np.array([1, 2, 3, 4, 5, 6, 7","'Return a new block, try to preserve dtype if possible. 
 Parameters 
 v : `values`, updated in-place (array like) 
 m : `mask`, applies to both sides (array like) 
 n : `new values` either scalar or an array like aligned with `values`'"
"def _traverse_generic(start_node, get_parents, get_children, filter_func=None, yield_descendants_of_unyielded=False): 
    filter_func = (filter_func or (lambda __: True)) 
   stack = deque([start_node]) 
   yield_results = {} 
   while stack: 
      current_node = stack.pop() 
      if (get_parents and (current_node != start_node)): 
         parents = get_parents(current_node) 
         if (not all(((parent in yield_results) for parent in parents))): 
            continue 
         elif ((not yield_descendants_of_unyielded) and (not any((yield_results[parent] for parent in parents)))): 
            continue 
      if (current_node not in yield_results): 
         if get_parents: 
            unvisited_children = list(get_children(current_node)) 
         else: 
            unvisited_children = list((child for child in get_children(current_node) if (child not in yield_results))) 
         unvisited_children.reverse() 
         stack.extend(unvisited_children) 
         should_yield_node = filter_func(current_node) 
         if should_yield_node: 
            (yield current_node) 
         yield_results[current_node] = should_yield_node"," 'Traverse the tree of a given node and yield the nodes. 
 :param start_node: The node to start the traversal from. 
 :param get_parents: A function that returns the parents of a node. 
 :param get_children: A function that returns the children of a node. 
 :param filter_func: A function that returns True if the node should be 
 yielded. 
 :param yield_descendants_of_unyielded: If True, yield the descendants of 
 unyielded nodes. 
 :yield: The nodes visited. 
 :return: None'","'Helper function to avoid duplicating functionality between 
 traverse_depth_first and traverse_topologically. 
 If get_parents is None, do a pre-order traversal. 
 Else, do a topological traversal. 
 The topological traversal has a worse time complexity than 
 pre-order does, as it needs to check whether each node\'s 
 parents have been visited. 
 Arguments: 
 See description in traverse_topologically.'"
"def getQuadraticPath(elementNode): 
    end = evaluate.getVector3FromElementNode(elementNode) 
   previousElementNode = elementNode.getPreviousElementNode() 
   if (previousElementNode == None): 
      print 'Warning,   can   not   get   previousElementNode   in   getQuadraticPath   in   quadratic   for:' 
      print elementNode 
      return [end] 
   begin = elementNode.getPreviousVertex(Vector3()) 
   controlPoint = evaluate.getVector3ByPrefix(None, elementNode, 'controlPoint') 
   if (controlPoint == None): 
      oldControlPoint = evaluate.getVector3ByPrefixes(previousElementNode, ['controlPoint', 'controlPoint1'], None) 
      if (oldControlPoint == None): 
         oldControlPoint = end 
      controlPoint = ((begin + begin) - oldControlPoint) 
      evaluate.addVector3ToElementNode(elementNode, 'controlPoint', controlPoint) 
   return svg_reader.getQuadraticPoints(begin, controlPoint, end, lineation.getNumberOfBezierPoints(begin, elementNode, end))"," 'Returns a list of points for a quadratic path. 
 The points are ordered as follows: 
 begin, controlPoint, end, controlPoint1, end, controlPoint2, end, controlPoint3, ... 
 The control points are calculated as the following: 
 controlPoint = (begin + begin) - oldControlPoint 
 oldControlPoint = (begin + begin) - controlPoint 
 controlPoint1 = (end + end) - oldControlPoint 
 oldControlPoint = (end + end) - controlPoint 
 controlPoint2 = (controlPoint + controlPoint) - oldControlPoint 
 oldControlPoint = (controlPoint + controlPoint) - controlPoint1 
 controlPoint3 = (controlPoint2 + controlPoint2) - oldControlPoint 
 oldControlPoint = (controlPoint2 + controlPoint2) - controlPoint2'",'Get the quadratic path.'
"@opt.register_specialize('stabilize', 'fast_compile') 
 @gof.local_optimizer([tensor.Elemwise]) 
 def local_logsoftmax(node): 
    if (isinstance(node.op, tensor.Elemwise) and isinstance(node.op.scalar_op, scalar.basic.Log) and (len(node.inputs) == 1) and (node.inputs[0].owner is not None) and isinstance(node.inputs[0].owner.op, Softmax)): 
      inVars = node.inputs[0].owner.inputs[0] 
      new_op = LogSoftmax() 
      ret = new_op(inVars) 
      ret.tag.values_eq_approx = values_eq_approx_remove_inf 
      copy_stack_trace([node.inputs[0], node.outputs[0]], ret) 
      return [ret]"," 'Stabilize the log softmax by replacing the log with log_1 + log_2 
 This is a specialization of the fast_compile pass. 
 The original log softmax op is replaced with a new op that is 
 equivalent to log(1 + exp(inVars)) 
 Parameters 
 node : Node 
 The node to be optimized 
 Returns 
 list of nodes 
 The optimized node'","'Detect Log(Softmax(x)) and replace it with LogSoftmax(x) 
 Note: only forward pass is affected'"
"def stChromaFeaturesInit(nfft, fs): 
    freqs = numpy.array([(((f + 1) * fs) / (2 * nfft)) for f in range(nfft)]) 
   Cp = 27.5 
   nChroma = numpy.round((12.0 * numpy.log2((freqs / Cp)))).astype(int) 
   nFreqsPerChroma = numpy.zeros((nChroma.shape[0],)) 
   uChroma = numpy.unique(nChroma) 
   for u in uChroma: 
      idx = numpy.nonzero((nChroma == u)) 
      nFreqsPerChroma[idx] = idx[0].shape 
   return (nChroma, nFreqsPerChroma)"," 'Initialize chroma features with nfft and fs. 
 Parameters 
 nfft : int 
 The length of the FFT. 
 fs : float 
 The sampling frequency. 
 Returns 
 (nChroma, nFreqsPerChroma) : tuple 
 nChroma : int 
 The number of chroma bands. 
 nFreqsPerChroma : int 
 The number of frequencies per chroma band.'",'This function initializes the chroma matrices used in the calculation of the chroma features'
"def _set_nxm_headers(nxm_headers): 
    def _set_nxm_headers_dec(self): 
      self.nxm_headers = nxm_headers 
      return self 
   return _set_nxm_headers_dec"," 'Sets the nxm headers for the NXM object. 
 :param nxm_headers: dict of nxm headers to set. 
 :returns: self 
 :rtype: :class:`~pyxb.binding.basis.datatypes.anyType`'",'Annotate corresponding NXM header'
"def gettext(string, **variables): 
    return get_i18n().gettext(string, **variables)", 'Returns a translated string.','See :meth:`I18n.gettext`.'
"def _modified_weiszfeld_step(X, x_old): 
    diff = (X - x_old) 
   diff_norm = np.sqrt(np.sum((diff ** 2), axis=1)) 
   mask = (diff_norm >= _EPSILON) 
   is_x_old_in_X = int((mask.sum() < X.shape[0])) 
   diff = diff[mask] 
   diff_norm = diff_norm[mask][:, np.newaxis] 
   quotient_norm = linalg.norm(np.sum((diff / diff_norm), axis=0)) 
   if (quotient_norm > _EPSILON): 
      new_direction = (np.sum((X[mask, :] / diff_norm), axis=0) / np.sum((1 / diff_norm), axis=0)) 
   else: 
      new_direction = 1.0 
      quotient_norm = 1.0 
   return ((max(0.0, (1.0 - (is_x_old_in_X / quotient_norm))) * new_direction) + (min(1.0, (is_x_old_in_X / quotient_norm)) * x_old))"," 'Modified Weiszfeld step. 
 :param X: 
 :param x_old: 
 :return: 
 :rtype: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError: 
 :raises ValueError","'Modified Weiszfeld step. 
 This function defines one iteration step in order to approximate the 
 spatial median (L1 median). It is a form of an iteratively re-weighted 
 least squares method. 
 Parameters 
 X : array, shape = [n_samples, n_features] 
 Training vector, where n_samples is the number of samples and 
 n_features is the number of features. 
 x_old : array, shape = [n_features] 
 Current start vector. 
 Returns 
 x_new : array, shape = [n_features] 
 New iteration step. 
 References 
 - On Computation of Spatial Median for Robust Data Mining, 2005 
 T. KÃ¤rkkÃ¤inen and S. ÃyrÃ¤mÃ¶ 
 http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf'"
"def translate_exception(req, e): 
    if (not hasattr(req, 'best_match_language')): 
      return e 
   locale = req.best_match_language() 
   if isinstance(e, webob.exc.HTTPError): 
      e.explanation = i18n.translate(e.explanation, locale) 
      e.detail = i18n.translate(e.detail, locale) 
      if getattr(e, 'body_template', None): 
         e.body_template = i18n.translate(e.body_template, locale) 
   return e", 'Translate a webob.exc.HTTPError to a localized version.','Translates all translatable elements of the given exception.'
"def rand_uuid_hex(): 
    return uuid.uuid4().hex", 'Returns a random UUID in hex format.',"'Generate a random UUID hex string 
 :return: a random UUID (e.g. \'0b98cf96d90447bda4b46f31aeb1508c\') 
 :rtype: string'"
"def current_time(): 
    return time.time()", 'Returns the current time in seconds.',"'Retrieve the current time, this function is mocked out in unit testing.'"
"def check_packages(module, xbps_path, packages, state): 
    would_be_changed = [] 
   for package in packages: 
      (installed, updated) = query_package(module, xbps_path, package) 
      if (((state in ['present', 'latest']) and (not installed)) or ((state == 'absent') and installed) or ((state == 'latest') and (not updated))): 
         would_be_changed.append(package) 
   if would_be_changed: 
      if (state == 'absent'): 
         state = 'removed' 
      module.exit_json(changed=True, msg=('%s   package(s)   would   be   %s' % (len(would_be_changed), state)), packages=would_be_changed) 
   else: 
      module.exit_json(changed=False, msg=('package(s)   already   %s' % state), packages=[])"," 'Check if packages should be installed, updated, or removed. 
 :param module: Ansible module 
 :param xbps_path: Path to xbps 
 :param packages: List of packages to check 
 :param state: State of packages (present, latest, absent) 
 :returns: None'",'Returns change status of command'
"def send_summary(): 
    for d in frappe.get_all(u'Daily   Work   Summary', dict(status=u'Open')): 
      daily_work_summary = frappe.get_doc(u'Daily   Work   Summary', d.name) 
      daily_work_summary.send_summary()", 'Send summary emails to Daily Work Summary','Send summary to everyone'
"def set_driver(drivers, provider, module, klass): 
    if (provider in drivers): 
      raise AttributeError(('Provider   %s   already   registered' % provider)) 
   drivers[provider] = (module, klass) 
   try: 
      driver = get_driver(drivers, provider) 
   except (ImportError, AttributeError): 
      exp = sys.exc_info()[1] 
      drivers.pop(provider) 
      raise exp 
   return driver"," 'Set a driver for a given provider. 
 :param drivers: A dictionary of drivers 
 :type drivers: dict 
 :param provider: The provider to register 
 :type provider: str 
 :param module: The module that contains the driver 
 :type module: str 
 :param klass: The class that contains the driver 
 :type klass: str'","'Sets a driver. 
 :param drivers: Dictionary to store providers. 
 :param provider: Id of provider to set driver for 
 :type provider: :class:`libcloud.types.Provider` 
 :param module: The module which contains the driver 
 :type module: L 
 :param klass: The driver class name 
 :type klass:'"
"def classname_for_table(base, tablename, table): 
    return str(tablename)"," 'Return a class name for the given table. 
 :param base: The base class for the table. 
 :param tablename: The name of the table. 
 :param table: The table definition.'","'Return the class name that should be used, given the name 
 of a table. 
 The default implementation is:: 
 return str(tablename) 
 Alternate implementations can be specified using the 
 :paramref:`.AutomapBase.prepare.classname_for_table` 
 parameter. 
 :param base: the :class:`.AutomapBase` class doing the prepare. 
 :param tablename: string name of the :class:`.Table`. 
 :param table: the :class:`.Table` object itself. 
 :return: a string class name. 
 .. note:: 
 In Python 2, the string used for the class name **must** be a non-Unicode 
 object, e.g. a ``str()`` object.  The ``.name`` attribute of 
 :class:`.Table` is typically a Python unicode subclass, so the ``str()`` 
 function should be applied to this name, after accounting for any non-ASCII 
 characters.'"
"def getVertexGivenLine(line): 
    splitLine = line.split() 
   return Vector3(float(splitLine[1]), float(splitLine[2]), float(splitLine[3]))"," 'Returns the vertex of a line given its line equation. 
 :param line: string representing the line equation 
 :return: Vector3 representing the vertex of the line'",'Get vertex given obj vertex line.'
"def filter_new_cons(packet): 
    flags = [] 
   TCP_FIN = 1 
   TCP_SYN = 2 
   TCP_RST = 4 
   TCP_PSH = 8 
   TCP_ACK = 16 
   TCP_URG = 32 
   TCP_ECE = 64 
   TCP_CWK = 128 
   if (packet['tcp']['flags'] & TCP_FIN): 
      flags.append('FIN') 
   elif (packet['tcp']['flags'] & TCP_SYN): 
      flags.append('SYN') 
   elif (packet['tcp']['flags'] & TCP_RST): 
      flags.append('RST') 
   elif (packet['tcp']['flags'] & TCP_PSH): 
      flags.append('PSH') 
   elif (packet['tcp']['flags'] & TCP_ACK): 
      flags.append('ACK') 
   elif (packet['tcp']['flags'] & TCP_URG): 
      flags.append('URG') 
   elif (packet['tcp']['flags'] & TCP_ECE): 
      flags.append('ECE') 
   elif (packet['tcp']['flags'] & TCP_CWK): 
      flags.append('CWK') 
   else: 
      print('UNKNOWN   PACKET') 
   if (packet['tcp']['d_port'] == 4505): 
      if (('SYN' in flags) and (len(flags) == 1)): 
         return 10 
      elif ('FIN' in flags): 
         return 12 
   elif (packet['tcp']['d_port'] == 4506): 
      if (('SYN' in flags) and (len(flags) == 1)): 
         return 100 
      elif ('FIN' in flags): 
         return 120 
   else: 
      return None"," 'Filter out TCP SYN packets with flags not equal to SYN and FIN. 
 Return 10 or 12 if we are looking for a TCP SYN or FIN packet. 
 Return 100 or 120 if we are looking for a TCP SYN or FIN packet 
 with the TCP flags set to SYN and FIN.'","'filter packets by there tcp-state and 
 returns codes for specific states'"
"def memcache_set(request): 
    keys = [] 
   for i in request['item']: 
      if ('MemcacheSetRequest_Item' in i): 
         key = i['MemcacheSetRequest_Item']['key'] 
      else: 
         key = i['Item']['key'] 
      keys.append(truncate(key)) 
   return '\n'.join(keys)", 'Return a string of all the keys to be set in memcached.',"'Pretty-format a memcache.set() request. 
 Arguments: 
 request - The memcache.set() request object, e.g., 
 {\'item\': [{\'Item\': {\'flags\': \'0L\', \'key\': \'memcache_key\' ... 
 Returns: 
 The keys of the memcache.get() response as a string. If there are 
 multiple keys, they are separated by newline characters.'"
"def splantider(tck, n=1): 
    if isinstance(tck, BSpline): 
      return tck.antiderivative(n) 
   else: 
      return _impl.splantider(tck, n)"," 'Returns the n-th antiderivative of tck. 
 Parameters 
 tck : BSpline 
 The spline to antidifferentiate. 
 n : int 
 The order of the antiderivative. 
 Returns 
 spl : BSpline 
 The antiderivative of tck.'","'Compute the spline for the antiderivative (integral) of a given spline. 
 Parameters 
 tck : BSpline instance or a tuple of (t, c, k) 
 Spline whose antiderivative to compute 
 n : int, optional 
 Order of antiderivative to evaluate. Default: 1 
 Returns 
 BSpline instance or a tuple of (t2, c2, k2) 
 Spline of order k2=k+n representing the antiderivative of the input 
 spline. 
 A tuple is returned iff the input argument `tck` is a tuple, otherwise 
 a BSpline object is constructed and returned. 
 See Also 
 splder, splev, spalde 
 BSpline 
 Notes 
 The `splder` function is the inverse operation of this function. 
 Namely, ``splder(splantider(tck))`` is identical to `tck`, modulo 
 rounding error. 
 .. versionadded:: 0.13.0 
 Examples 
 >>> from scipy.interpolate import splrep, splder, splantider, splev 
 >>> x = np.linspace(0, np.pi/2, 70) 
 >>> y = 1 / np.sqrt(1 - 0.8*np.sin(x)**2) 
 >>> spl = splrep(x, y) 
 The derivative is the inverse operation of the antiderivative, 
 although some floating point error accumulates: 
 >>> splev(1.7, spl), splev(1.7, splder(splantider(spl))) 
 (array(2.1565429877197317), array(2.1565429877201865)) 
 Antiderivative can be used to evaluate definite integrals: 
 >>> ispl = splantider(spl) 
 >>> splev(np.pi/2, ispl) - splev(0, ispl) 
 2.2572053588768486 
 This is indeed an approximation to the complete elliptic integral 
 :math:`K(m) = \int_0^{\pi/2} [1 - m\sin^2 x]^{-1/2} dx`: 
 >>> from scipy.special import ellipk 
 >>> ellipk(0.8) 
 2.2572053268208538'"
"@contextfunction 
 def core_generic_list(context, objects, skip_group=False, tag=None): 
    if tag: 
      return tag(context, objects) 
   request = context['request'] 
   response_format = 'html' 
   if ('response_format' in context): 
      response_format = context['response_format'] 
   return Markup(render_to_string('core/tags/generic_list', {'objects': objects, 'skip_group': skip_group}, context_instance=RequestContext(request), response_format=response_format))"," 'Render a list of objects. 
 :param objects: A list of objects. 
 :param skip_group: Whether to skip the group header. 
 :param tag: A callable that returns a list of objects. 
 :param context: A dictionary of context variables.'",'Print a list of objects'
"def _get_sysfs_netdev_path(pci_addr, pf_interface): 
    if pf_interface: 
      return ('/sys/bus/pci/devices/%s/physfn/net' % pci_addr) 
   return ('/sys/bus/pci/devices/%s/net' % pci_addr)", 'Returns the sysfs path for the given PCI address and PF interface.',"'Get the sysfs path based on the PCI address of the device. 
 Assumes a networking device - will not check for the existence of the path.'"
"def unpack(desc, formodulename=''): 
    t = desc.type 
   if unpacker_coercions.has_key(t): 
      desc = desc.AECoerceDesc(unpacker_coercions[t]) 
      t = desc.type 
   if (t == typeAEList): 
      l = [] 
      for i in range(desc.AECountItems()): 
         (keyword, item) = desc.AEGetNthDesc((i + 1), '****') 
         l.append(unpack(item, formodulename)) 
      return l 
   if (t == typeAERecord): 
      d = {} 
      for i in range(desc.AECountItems()): 
         (keyword, item) = desc.AEGetNthDesc((i + 1), '****') 
         d[keyword] = unpack(item, formodulename) 
      return d 
   if (t == typeAEText): 
      record = desc.AECoerceDesc('reco') 
      return mkaetext(unpack(record, formodulename)) 
   if (t == typeAlias): 
      return Carbon.File.Alias(rawdata=desc.data) 
   if (t == typeBoolean): 
      return struct.unpack('b', desc.data)[0] 
   if (t == typeChar): 
      return desc.data 
   if (t == typeUnicodeText): 
      return unicode(desc.data, 'utf16') 
   if (t == typeEnumeration): 
      return mkenum(desc.data) 
   if (t == typeFalse): 
      return 0 
   if (t == typeFloat): 
      data = desc.data 
      return struct.unpack('d', data)[0] 
   if (t == typeFSS): 
      return Carbon.File.FSSpec(rawdata=desc.data) 
   if (t == typeFSRef): 
      return Carbon.File.FSRef(rawdata=desc.data) 
   if (t == typeInsertionLoc): 
      record = desc.AECoerceDesc('reco') 
      return mkinsertionloc(unpack(record, formodulename)) 
   if (t == typeIntlText): 
      (script, language) = struct.unpack('hh', desc.data[:4]) 
      return aetypes.IntlText(script, language, desc.data[4:]) 
   if (t == typeIntlWritingCode): 
      (script, language) = struct.unpack('hh', desc.data) 
      return aetypes.IntlWritingCode(script, language) 
   if (t == typeKeyword): 
      return mkkeyword(desc.data) 
   if (t == typeLongInteger): 
      return struct.unpack('l', desc.data)[0] 
   if (t == typeLongDateTime): 
      (a, b) = struct.unpack('lL', desc.data) 
      return ((long(a) << 32) + b) 
   if (t == typeNull): 
      return None 
   if (t == typeMagnitude): 
      v = struct.unpack('l', desc.data) 
      if (v < 0): 
         v = (4294967296L + v) 
      return v 
   if (t == typeObjectSpecifier): 
      record = desc.AECoerceDesc('reco') 
      if formodulename: 
         return mkobjectfrommodule(unpack(record, formodulename), formodulename) 
      return mkobject(unpack(record, formodulename)) 
   if (t == typeQDPoint): 
      (v, h) = struct.unpack('hh', desc.data) 
      return aetypes.QDPoint(v, h) 
   if (t == typeQDRectangle): 
      (v0, h0, v1, h1) = struct.unpack('hhhh', desc.data) 
      return aetypes.QDRectangle(v0, h0, v1, h1) 
   if (t == typeRGBColor): 
      (r, g, b) = struct.unpack('hhh', desc.data) 
      return aetypes.RGBColor(r, g, b) 
   if (t == typeShortFloat): 
      return struct.unpack('f', desc.data)[0] 
   if (t == typeShortInteger): 
      return struct.unpack('h', desc.data)[0] 
   if (t == typeTargetID): 
      return mktargetid(desc.data) 
   if (t == typeTrue): 
      return 1 
   if (t == typeType): 
      return mktype(desc.data, formodulename) 
   if (t == 'rang'): 
      record = desc.AECoerceDesc('reco') 
      return mkrange(unpack(record, formodulename)) 
   if (t == 'cmpd'): 
      record = desc.AECoerceDesc('reco') 
      return mkcomparison(unpack(record, formodulename)) 
   if (t == 'logi'): 
      record = desc.AECoerceDesc('reco') 
      return mklogical(unpack(record, formodulename)) 
   return mkunknown(desc.type, desc.data)", 'unpack a description','Unpack an AE descriptor to a python object'
"def convert_labels(labels, transl): 
    return [transl[l] for l in labels]", 'Convert labels to the translator\'s language','Convert between strings and numbers.'
"def absent(name, path, mask, cmd, user='root'): 
    mask = ','.join(mask) 
   ret = {'name': name, 'result': True, 'changes': {}, 'comment': ''} 
   if __opts__['test']: 
      status = _check_cron(user, path, mask, cmd) 
      ret['result'] = None 
      if (status == 'absent'): 
         ret['result'] = True 
         ret['comment'] = 'Incron   {0}   is   absent'.format(name) 
      elif ((status == 'present') or (status == 'update')): 
         ret['comment'] = 'Incron   {0}   is   set   to   be   removed'.format(name) 
      return ret 
   data = __salt__['incron.rm_job'](user=user, path=path, mask=mask, cmd=cmd) 
   if (data == 'absent'): 
      ret['comment'] = 'Incron   {0}   already   absent'.format(name) 
      return ret 
   if (data == 'removed'): 
      ret['comment'] = ""Incron   {0}   removed   from   {1}'s   crontab"".format(name, user) 
      ret['changes'] = {user: name} 
      return ret 
   ret['comment'] = 'Incron   {0}   for   user   {1}   failed   to   commit   with   error   {2}'.format(name, user, data) 
   ret['result'] = False 
   return ret"," 'Remove an incron job. 
 :param name: Name of the job to remove. 
 :param path: Path to the incron file. 
 :param mask: Mask of the incron file. 
 :param cmd: Command to run. 
 :param user: User to run the command as. 
 :returns: Dictionary of results.'","'Verifies that the specified incron job is absent for the specified user; only 
 the name is matched when removing a incron job. 
 name 
 Unique comment describing the entry 
 path 
 The path that should be watched 
 user 
 The name of the user who\'s crontab needs to be modified, defaults to 
 the root user 
 mask 
 The mask of events that should be monitored for 
 cmd 
 The cmd that should be executed'"
"@blueprint.route('/users') 
 def list_all_users(): 
    return _list_users()", 'List all users.','Return a list of all known user names.'
"def _get_frame_class(frame): 
    import inspect 
   if isinstance(frame, six.string_types): 
      frame_names = frame_transform_graph.get_names() 
      if (frame not in frame_names): 
         raise ValueError(u'Coordinate   frame   {0}   not   in   allowed   values   {1}'.format(frame, sorted(frame_names))) 
      frame_cls = frame_transform_graph.lookup_name(frame) 
   elif (inspect.isclass(frame) and issubclass(frame, BaseCoordinateFrame)): 
      frame_cls = frame 
   else: 
      raise ValueError(u'Coordinate   frame   must   be   a   frame   name   or   frame   class') 
   return frame_cls"," 'Returns the class of the frame if it is a string, or the class of the 
 frame itself if it is a class.'","'Get a frame class from the input `frame`, which could be a frame name 
 string, or frame class.'"
"def dmp_ground_TC(f, u, K): 
    while u: 
      f = dmp_TC(f, K) 
      u -= 1 
   return dup_TC(f, K)"," 'Returns the ground form of f(x) in TC(K[x]). 
 Examples 
 >>> from sympy.polys import ring, ZZ 
 >>> R, x = ring(""x"", ZZ) 
 >>> R.dmp_ground_TC(x**3 - 2*x**2 - 1, 2, R) 
 1/2*x**2 - 1'","'Return the ground trailing coefficient. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.densebasic import dmp_ground_TC 
 >>> f = ZZ.map([[[1], [2, 3]]]) 
 >>> dmp_ground_TC(f, 2, ZZ) 
 3'"
"def load_random_chromosome(chr_name): 
    cur_chromosome = BasicChromosome.Chromosome(chr_name) 
   num_segments = random.randrange(num_possible_segments) 
   for seg in range(num_segments): 
      if (seg == 0): 
         cur_segment = BasicChromosome.TelomereSegment() 
      elif (seg == (num_segments - 1)): 
         cur_segment = BasicChromosome.TelomereSegment(1) 
      else: 
         cur_segment = BasicChromosome.ChromosomeSegment() 
      color_chance = random.random() 
      if (color_chance <= color_prob): 
         fill_color = random.choice(color_choices) 
         cur_segment.fill_color = fill_color 
      id_chance = random.random() 
      if (id_chance <= id_prob): 
         id = get_random_id() 
         cur_segment.label = id 
      cur_chromosome.add(cur_segment) 
   return (cur_chromosome, num_segments)"," 'Loads a random chromosome with a random number of segments. 
 :param chr_name: chromosome name 
 :return: chromosome and number of segments'",'Generate a chromosome with random information about it.'
"def _selective_search_IJCV_top_k(split, year, top_k): 
    imdb = datasets.pascal_voc(split, year) 
   imdb.roidb_handler = imdb.selective_search_IJCV_roidb 
   imdb.config['top_k'] = top_k 
   return imdb"," 'Returns a new instance of the dataset with the selective search 
 IJCV paper\'s top-k results. 
 Parameters 
 split : str 
 The dataset split. 
 year : int 
 The year of the dataset. 
 top_k : int 
 The top-k results to return. 
 Returns 
 imdb : VOCdevkit.VOC.ImageNet 
 A new instance of the dataset with the selective search IJCV paper\'s 
 top-k results.'","'Return an imdb that uses the top k proposals from the selective search 
 IJCV code.'"
"def is_file_ignored(opts, fname): 
    if opts['file_ignore_regex']: 
      for regex in opts['file_ignore_regex']: 
         if re.search(regex, fname): 
            log.debug('File   matching   file_ignore_regex.   Skipping:   {0}'.format(fname)) 
            return True 
   if opts['file_ignore_glob']: 
      for glob in opts['file_ignore_glob']: 
         if fnmatch.fnmatch(fname, glob): 
            log.debug('File   matching   file_ignore_glob.   Skipping:   {0}'.format(fname)) 
            return True 
   return False"," 'Check if a file should be ignored by the file_ignore_regex or file_ignore_glob 
 options. 
 :param opts: The options passed to the build. 
 :param fname: The filename to check. 
 :return: True if the file should be ignored, False otherwise.'","'If file_ignore_regex or file_ignore_glob were given in config, 
 compare the given file path against all of them and return True 
 on the first match.'"
"def _warn_iers(ierserr): 
    msg = u'{0}   Assuming   UT1-UTC=0   for   coordinate   transformations.' 
   warnings.warn(msg.format(ierserr.args[0]), AstropyWarning)"," 'Warn if the ierser is not zero. 
 :param ierserr: IERS error message. 
 :type ierserr: str'","'Generate a warning for an IERSRangeerror 
 Parameters 
 ierserr : An `~astropy.utils.iers.IERSRangeError`'"
"def mean(x, axis=None, keepdims=False): 
    axis = _normalize_axis(axis, ndim(x)) 
   if (x.dtype.base_dtype == tf.bool): 
      x = tf.cast(x, floatx()) 
   return tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)"," 'Mean over the specified axis of a tensor. 
 If the axis is None, the mean is taken over the flattened 
 tensor. 
 Args: 
 x: A tensor. 
 axis: An integer or tuple/list of integers. The axis or axes 
 along which to take the mean. If None, the mean is taken 
 over the flattened tensor. 
 keepdims: If True, retain the reduced dimensions of the tensor. 
 Returns: 
 A tensor with the same type as x. 
 Raises: 
 TypeError: If x is not a tensor. 
 ValueError: If the type of x is not a real number. 
 Examples: 
 >>> x = tf.constant([1, 2, 3]) 
 >>> tf.reduce_mean(x) 
 1.5 
 >>> tf.reduce_mean(x, 0) 
 1.5 
 >>> tf.reduce_mean(x, 1) 
 2.0 
 >>> tf.reduce_mean(x, 1, keepdims=True) 
 [1.0","'Mean of a tensor, alongside the specified axis. 
 # Arguments 
 x: A tensor or variable. 
 axis: A list of integer. Axes to compute the mean. 
 keepdims: A boolean, whether to keep the dimensions or not. 
 If `keepdims` is `False`, the rank of the tensor is reduced 
 by 1 for each entry in `axis`. If `keep_dims` is `True`, 
 the reduced dimensions are retained with length 1. 
 # Returns 
 A tensor with the mean of elements of `x`.'"
"@verbose 
 def morph_source_spaces(src_from, subject_to, surf='white', subject_from=None, subjects_dir=None, verbose=None): 
    src_from = _ensure_src(src_from) 
   subject_from = _ensure_src_subject(src_from, subject_from) 
   subjects_dir = get_subjects_dir(subjects_dir, raise_error=True) 
   src_out = list() 
   for fro in src_from: 
      (hemi, idx, id_) = _get_hemi(fro) 
      to = op.join(subjects_dir, subject_to, 'surf', ('%s.%s' % (hemi, surf))) 
      logger.info(('Reading   destination   surface   %s' % (to,))) 
      to = read_surface(to, return_dict=True, verbose=False)[(-1)] 
      complete_surface_info(to, copy=False) 
      best = _get_vertex_map_nn(fro, subject_from, subject_to, hemi, subjects_dir, to['neighbor_tri']) 
      for key in ('neighbor_tri', 'tri_area', 'tri_cent', 'tri_nn', 'use_tris'): 
         del to[key] 
      to['vertno'] = np.sort(best[fro['vertno']]) 
      to['inuse'] = np.zeros(len(to['rr']), int) 
      to['inuse'][to['vertno']] = True 
      to['use_tris'] = best[fro['use_tris']] 
      to.update(nuse=len(to['vertno']), nuse_tri=len(to['use_tris']), nearest=None, nearest_dist=None, patch_inds=None, pinfo=None, dist=None, id=id_, dist_limit=None, type='surf', coord_frame=FIFF.FIFFV_COORD_MRI, subject_his_id=subject_to, rr=(to['rr'] / 1000.0)) 
      src_out.append(to) 
      logger.info('[done]\n') 
   info = dict(working_dir=os.getcwd(), command_line=_get_call_line(in_verbose=True)) 
   return SourceSpaces(src_out, info=info)"," 'Reads the subject surface from the specified subject and morphs it to the 
 specified destination surface. 
 Parameters 
 src_from : string 
 The name of the source surface. 
 subject_to : string 
 The name of the destination subject. 
 surf : string 
 The name of the destination surface. 
 subject_from : string 
 The name of the source subject. 
 subjects_dir : string 
 The directory containing the subject surface files. 
 verbose : bool 
 If True, then print out a progress bar. 
 Returns 
 SourceSpaces 
 A SourceSpaces object. 
 Notes 
 This function reads the source surface from the specified subject and 
 morphs it to the specified destination surface. 
 Examples 
 >>> from nipype.interfaces.morph.source import SourceSpaces 
 >>> from nipype.interfaces.morph.morph_source_spaces import morph_source_spaces 
 >>> source = SourceSpaces(src_from=['left_frontal', 'left_temporal', 'right_temporal'], subject_to='subject_01","'Morph an existing source space to a different subject. 
 .. warning:: This can be used in place of morphing source estimates for 
 multiple subjects, but there may be consequences in terms 
 of dipole topology. 
 Parameters 
 src_from : instance of SourceSpaces 
 Surface source spaces to morph. 
 subject_to : str 
 The destination subject. 
 surf : str 
 The brain surface to use for the new source space. 
 subject_from : str | None 
 The ""from"" subject. For most source spaces this shouldn\'t need 
 to be provided, since it is stored in the source space itself. 
 subjects_dir : string, or None 
 Path to SUBJECTS_DIR if it is not set in the environment. 
 verbose : bool, str, int, or None 
 If not None, override default verbose level (see :func:`mne.verbose` 
 and :ref:`Logging documentation <tut_logging>` for more). 
 Returns 
 src : instance of SourceSpaces 
 The morphed source spaces. 
 Notes 
 .. versionadded:: 0.10.0'"
"def _read(filepath_or_buffer, kwds): 
    encoding = kwds.get('encoding', None) 
   if (encoding is not None): 
      encoding = re.sub('_', '-', encoding).lower() 
      kwds['encoding'] = encoding 
   compression = kwds.get('compression') 
   compression = _infer_compression(filepath_or_buffer, compression) 
   (filepath_or_buffer, _, compression) = get_filepath_or_buffer(filepath_or_buffer, encoding, compression) 
   kwds['compression'] = compression 
   if (kwds.get('date_parser', None) is not None): 
      if isinstance(kwds['parse_dates'], bool): 
         kwds['parse_dates'] = True 
   iterator = kwds.get('iterator', False) 
   chunksize = kwds.get('chunksize', None) 
   nrows = _validate_nrows(kwds.pop('nrows', None)) 
   parser = TextFileReader(filepath_or_buffer, **kwds) 
   if ((nrows is not None) and (chunksize is not None)): 
      raise NotImplementedError(""'nrows'   and   'chunksize'   cannot   be   used   together   yet."") 
   elif (nrows is not None): 
      try: 
         data = parser.read(nrows) 
      finally: 
         parser.close() 
      return data 
   elif (chunksize or iterator): 
      return parser 
   try: 
      data = parser.read() 
   finally: 
      parser.close() 
   return data"," 'Read a text file. 
 Parameters 
 filepath_or_buffer : str, buffer 
 The path to a file or a buffer. 
 kwds : dict 
 Keyword arguments. 
 encoding : str, optional 
 The encoding of the file. 
 compression : str, optional 
 The compression method of the file. 
 date_parser : function, optional 
 A function to parse the date column. 
 iterator : bool, optional 
 Whether to return an iterator. 
 chunksize : int, optional 
 The number of rows to read at a time. 
 nrows : int, optional 
 The number of rows to read. 
 Returns 
 data : array 
 The data from the file. 
 Examples 
 Read the file in chunks: 
 >>> df = pd.read_csv(csv_filepath, chunksize=10000, iterator=True) 
 Read the file in chunks and return the iterator: 
 >>> df = pd.read_csv(csv_filepath, chunksize=10000, iterator=True) 
 >>> for i in",'Generic reader of line files.'
"def get_details_for_etag(options): 
    (tags, noserver) = read_etag_file(options) 
   if (noserver and (not options.noserver)): 
      options.noserver = noserver 
   m = re.match('(?:W/)?""?(.*)""?$', options.etag) 
   if m: 
      options.etag = m.group(1) 
   etag = options.etag 
   if (etag in tags): 
      print ('Found   etag   [%s]   for   version   %s' % (etag, tags[etag][0]['version'])) 
      return tags[etag] 
   short = etag[etag.index('-'):] 
   for t in tags: 
      if (t.find(short) != (-1)): 
         print ('Partial   ETag   match:   [%s],[%s]   for   version   %s' % (etag, t, tags[t][0]['version'])) 
         return tags[t] 
   return None"," 'Get details for an etag. 
 :param options: the command line options 
 :type options: :class:`~.options.Options` 
 :return: details about the etag or None if it is not found 
 :rtype: dict'",'Get the stack address for a specific ETag from the configuration file.'
"def __virtual__(): 
    if (get_configured_provider() is False): 
      return False 
   if (get_dependencies() is False): 
      return False 
   for (provider, details) in six.iteritems(__opts__['providers']): 
      if ('dimensiondata' not in details): 
         continue 
   return __virtualname__", 'Check if dimension data is available','Set up the libcloud functions and check for dimensiondata configurations.'
"def rs_diff(p, x): 
    R = p.ring 
   n = R.gens.index(x) 
   p1 = R.zero 
   mn = ([0] * R.ngens) 
   mn[n] = 1 
   mn = tuple(mn) 
   for expv in p: 
      if expv[n]: 
         e = monomial_ldiv(expv, mn) 
         p1[e] = (p[expv] * expv[n]) 
   return p1"," 'Return a polynomial with the same coefficients as `p` and 
 the same leading coefficient as `x`'","'Return partial derivative of ``p`` with respect to ``x``. 
 Parameters 
 x : :class:`PolyElement` with respect to which ``p`` is differentiated. 
 Examples 
 >>> from sympy.polys.domains import QQ 
 >>> from sympy.polys.rings import ring 
 >>> from sympy.polys.ring_series import rs_diff 
 >>> R, x, y = ring(\'x, y\', QQ) 
 >>> p = x + x**2*y**3 
 >>> rs_diff(p, x) 
 2*x*y**3 + 1'"
"def _generate_python_path(pkg, rospack): 
    if (pkg in _bootstrapped): 
      return [] 
   m = rospack.get_manifest(pkg) 
   if m.is_catkin: 
      _bootstrapped.append(pkg) 
      return [] 
   packages = get_depends(pkg, rospack) 
   packages.append(pkg) 
   paths = [] 
   try: 
      for p in packages: 
         m = rospack.get_manifest(p) 
         d = rospack.get_path(p) 
         _append_package_paths(m, paths, d) 
         _bootstrapped.append(p) 
   except: 
      if (pkg in _bootstrapped): 
         _bootstrapped.remove(pkg) 
      raise 
   return paths"," 'Generate a list of python paths for a package 
 @param pkg: the package to generate python paths for 
 @param rospack: the rospack instance 
 @return: the python paths for the package'","'Recursive subroutine for building dependency list and python path 
 :raises: :exc:`rospkg.ResourceNotFound` If an error occurs while attempting to load package or dependencies'"
"def follow_files(follow_paths, outstream, lastlines_dirpath=None, waitsecs=5): 
    (procs, pipes) = launch_tails(follow_paths, lastlines_dirpath) 
   while pipes: 
      (lines, bad_pipes) = poll_tail_pipes(pipes, lastlines_dirpath, waitsecs) 
      for bad in bad_pipes: 
         pipes.pop(bad) 
      try: 
         outstream.writelines((['\n'] + lines)) 
         outstream.flush() 
      except (IOError, OSError) as e: 
         break 
   snuff(procs.values())"," 'Follow files using tail, and write the lines to the given outstream. 
 :param follow_paths: a list of file paths to follow 
 :param outstream: a file-like object to write to 
 :param lastlines_dirpath: the directory to store the last lines of the 
 followed files in 
 :param waitsecs: the number of seconds to wait for each tail process to 
 finish before proceeding 
 :return: None'","'Launch tail on a set of files and merge their output into outstream. 
 Args: 
 follow_paths: list; Local paths to launch tail on. 
 outstream: file; Output stream to write aggregated lines to. 
 lastlines_dirpath: Local dirpath to record last lines seen in. 
 waitsecs: int; Timeout for poll_tail_pipes.'"
"def oo_select_keys_from_list(data, keys): 
    if (not isinstance(data, list)): 
      raise errors.AnsibleFilterError('|failed   expects   to   filter   on   a   list') 
   if (not isinstance(keys, list)): 
      raise errors.AnsibleFilterError('|failed   expects   first   param   is   a   list') 
   retval = [oo_select_keys(item, keys) for item in data] 
   return oo_flatten(retval)"," 'Return a list of the specified keys from the given list. 
 :param data: List to select keys from. 
 :param keys: List of keys to select. 
 :return: List of selected items.'","'This returns a list, which contains the value portions for the keys 
 Ex: data = { \'a\':1, \'b\':2, \'c\':3 } 
 keys = [\'a\', \'c\'] 
 returns [1, 3]'"
"def is_user_capable(user, api_name): 
    user = urllib.unquote(user) 
   sys.stderr.write((((('checking   permissions   for   user   ' + user) + '   on   api   ') + api_name) + '\n')) 
   secret_file = open('/etc/appscale/secret.key', 'r') 
   secret = secret_file.read() 
   secret = secret[0:(-1)] 
   secret_file.close() 
   uaserver_file = open('/etc/appscale/hypersoap', 'r') 
   uaserver = uaserver_file.read() 
   uaserver_file.close() 
   server = SOAPpy.SOAPProxy((('https://' + uaserver) + ':4343')) 
   capabilities = server.get_capabilities(user, secret) 
   if (not isinstance(capabilities, str)): 
      return False 
   capabilities = capabilities.split(':') 
   sys.stderr.write((((('user   ' + user) + '   has   the   following   capabilities:   ') + str(capabilities)) + '\n')) 
   if (api_name in capabilities): 
      return True 
   else: 
      return False"," 'Check if user has the given capability. 
 :param user: the user to check the capability for 
 :param api_name: the name of the API to check the capability for 
 :return: True if the user has the capability, False otherwise'","'Checks to see if the given user has access to user a particular API. 
 Args: 
 user: The current user email 
 api_name: The API we\'re checking to see if the user has permission 
 Returns: 
 True is capable, False otherwise'"
"def get_c_extract(r, name, sub): 
    if any([getattr(c.op, 'check_input', config.check_input) for (c, _) in r.clients if (not isinstance(c, string_types))]): 
      if any([getattr(c.op, 'check_broadcast', True) for (c, _) in r.clients if (not isinstance(c, string_types))]): 
         c_extract = r.type.c_extract(name, sub, True) 
      else: 
         try: 
            c_extract = r.type.c_extract(name, sub, True, check_broadcast=False) 
         except TypeError as e: 
            c_extract = r.type.c_extract(name, sub, True) 
   else: 
      c_extract = r.type.c_extract(name, sub, False) 
   pre = ('\n            py_%(name)s   =   PyList_GET_ITEM(storage_%(name)s,   0);\n            {Py_XINCREF(py_%(name)s);}\n            ' % locals()) 
   return (pre + c_extract)"," 'Returns the C code for extracting a value from the storage. 
 :param r: The result object. 
 :param name: The name of the storage. 
 :param sub: The name of the sub-storage. 
 :return: The C code for extracting a value from the storage.'",'Wrapper around c_extract that initializes py_name from storage.'
"def _join(value): 
    return '   '.join(map(_stringify, value))", 'Join a list of strings.','Internal function.'
"def produce(url, ident): 
    ctx = zmq.Context.instance() 
   s = ctx.socket(zmq.PUSH) 
   s.connect(url) 
   print(('Producing   %s' % ident)) 
   for i in range(MSGS): 
      s.send((u'%s:   %i' % (ident, time.time())).encode('utf8')) 
      time.sleep(1) 
   print(('Producer   %s   done' % ident)) 
   s.close()", 'Produce messages on a given URL.','Produce messages'
"def getLoopStartingClosest(extrusionHalfWidth, location, loop): 
    closestIndex = getClosestDistanceIndexToLine(location, loop).index 
   loop = getAroundLoop(closestIndex, closestIndex, loop) 
   closestPoint = getClosestPointOnSegment(loop[0], loop[1], location) 
   if ((abs((closestPoint - loop[0])) > extrusionHalfWidth) and (abs((closestPoint - loop[1])) > extrusionHalfWidth)): 
      loop = (([closestPoint] + loop[1:]) + [loop[0]]) 
   elif (abs((closestPoint - loop[0])) > abs((closestPoint - loop[1]))): 
      loop = (loop[1:] + [loop[0]]) 
   return loop"," 'Return the starting point of the loop. 
 :param extrusionHalfWidth: The extrusion width of the loop. 
 :param location: The location of the point on the loop. 
 :param loop: The loop to find the starting point of. 
 :return: The starting point of the loop. 
 :rtype: list'",'Add to threads from the last location from loop.'
"def var_count_error(is_independent, is_plotting): 
    if is_plotting: 
      v = 'Plotting' 
   else: 
      v = 'Registering   plot   modes' 
   if is_independent: 
      (n, s) = (PlotMode._i_var_max, 'independent') 
   else: 
      (n, s) = (PlotMode._d_var_max, 'dependent') 
   return ('%s   with   more   than   %i   %s   variables   is   not   supported.' % (v, n, s))"," 'Checks if a plot mode is supported. 
 Parameters 
 is_independent : bool 
 Whether the plot mode is independent. 
 is_plotting : bool 
 Whether the plot mode is plotting. 
 Returns 
 str : A string describing the problem. 
 Raises 
 ValueError : If the plot mode is not supported.'","'Used to format an error message which differs 
 slightly in 4 places.'"
"def train(xs, ys, update_fn=None, typecode=None): 
    if (len(xs) != len(ys)): 
      raise ValueError('xs   and   ys   should   be   the   same   length.') 
   classes = set(ys) 
   if (classes != set([0, 1])): 
      raise ValueError(""Classes   should   be   0's   and   1's"") 
   if (typecode is None): 
      typecode = 'd' 
   (N, ndims) = (len(xs), (len(xs[0]) + 1)) 
   if ((N == 0) or (ndims == 1)): 
      raise ValueError('No   observations   or   observation   of   0   dimension.') 
   X = numpy.ones((N, ndims), typecode) 
   X[:, 1:] = xs 
   Xt = numpy.transpose(X) 
   y = numpy.asarray(ys, typecode) 
   beta = numpy.zeros(ndims, typecode) 
   MAX_ITERATIONS = 500 
   CONVERGE_THRESHOLD = 0.01 
   stepsize = 1.0 
   i = 0 
   old_beta = old_llik = None 
   while (i < MAX_ITERATIONS): 
      ebetaX = numpy.exp(numpy.dot(beta, Xt)) 
      p = (ebetaX / (1 + ebetaX)) 
      logp = ((y * numpy.log(p)) + ((1 - y) * numpy.log((1 - p)))) 
      llik = sum(logp) 
      if (update_fn is not None): 
         update_fn(iter, llik) 
      if (old_llik is not None): 
         if (llik < old_llik): 
            stepsize /= 2.0 
            beta = old_beta 
         if (numpy.fabs((llik - old_llik)) <= CONVERGE_THRESHOLD): 
            break 
      (old_llik, old_beta) = (llik, beta) 
      i += 1 
      W = (numpy.identity(N) * p) 
      Xtyp = numpy.dot(Xt, (y - p)) 
      XtWX = numpy.dot(numpy.dot(Xt, W), X) 
      delta = numpy.linalg.solve(XtWX, Xtyp) 
      if (numpy.fabs((stepsize - 1.0)) > 0.001): 
         delta *= stepsize 
      beta += delta 
   else: 
      raise RuntimeError(""Didn't   converge."") 
   lr = LogisticRegression() 
   lr.beta = [float(x) for x in beta] 
   return lr"," 'Train a logistic regression model. 
 Parameters 
 xs : array-like 
 Training data. 
 ys : array-like 
 Training labels. 
 update_fn : function, optional 
 A function that is called at each iteration of the training process. 
 The function takes two arguments: the current iteration number and the 
 log-likelihood value. 
 typecode : str, optional 
 The typecode of the data. 
 Returns 
 lr : LogisticRegression 
 A LogisticRegression object. 
 Notes 
 This function assumes that the data is normalized. 
 References 
 .. [1] ""Logistic Regression"" in ""Machine Learning"" by H. Sch�tt, 
 Springer, 2002. 
 Examples 
 >>> from sklearn.svm import LogisticRegression 
 >>> from sklearn.datasets import make_classification 
 >>> X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=5, n_classes=2, sh","'train(xs, ys[, update_fn]) -> LogisticRegression 
 Train a logistic regression classifier on a training set.  xs is a 
 list of observations and ys is a list of the class assignments, 
 which should be 0 or 1.  xs and ys should contain the same number 
 of elements.  update_fn is an optional callback function that 
 takes as parameters that iteration number and log likelihood.'"
"@_api_version(1.21) 
 @_client_version('1.5.0') 
 def create_volume(name, driver=None, driver_opts=None): 
    response = _client_wrapper('create_volume', name, driver=driver, driver_opts=driver_opts) 
   _clear_context() 
   return response", 'Create a volume.',"'Create a new volume 
 .. versionadded:: 2015.8.4 
 name 
 name of volume 
 driver 
 Driver of the volume 
 driver_opts 
 Options for the driver volume 
 CLI Example: 
 .. code-block:: bash 
 salt myminion dockerng.create_volume my_volume driver=local'"
"def register(linter): 
    linter.register_reporter(HTMLReporter)", 'Register the linter with the HTMLReporter.','Register the reporter classes with the linter.'
"def test_scharr_v_vertical(): 
    (i, j) = np.mgrid[(-5):6, (-5):6] 
   image = (j >= 0).astype(float) 
   result = filters.scharr_v(image) 
   j[(np.abs(i) == 5)] = 10000 
   assert np.all((result[(j == 0)] == 1)) 
   assert np.all((result[(np.abs(j) > 1)] == 0))", 'Test vertical Scharr filter.','Vertical Scharr on an edge should be a vertical line.'
"def _get_lines_from_file(filename, lineno, context_lines): 
    try: 
      source = open(filename).readlines() 
      lower_bound = max(0, (lineno - context_lines)) 
      upper_bound = (lineno + context_lines) 
      pre_context = [line.strip('\n') for line in source[lower_bound:lineno]] 
      context_line = source[lineno].strip('\n') 
      post_context = [line.strip('\n') for line in source[(lineno + 1):upper_bound]] 
      return (lower_bound, pre_context, context_line, post_context) 
   except (OSError, IOError): 
      return (None, [], None, [])"," 'Returns a tuple of (lower_bound, pre_context, context_line, post_context) 
 if the file exists. 
 Otherwise returns (None, [], None, [])'","'Returns context_lines before and after lineno from file. 
 Returns (pre_context_lineno, pre_context, context_line, post_context).'"
"def parse_options(): 
    parser = OptionParser(usage=u'%prog   name   [options]', version=(u'Review   Board   ' + get_version_string())) 
   parser.add_option(u'--class-name', dest=u'class_name', default=None, help=u'class   name   of   extension   (capitalized   no   spaces)') 
   parser.add_option(u'--package-name', dest=u'package_name', default=None, help=u'package   name   of   extension   (lower   case   with   underscores)') 
   parser.add_option(u'--description', dest=u'description', default=None, help=u'description   of   extension') 
   parser.add_option(u'--author', dest=u'author', default=None, help=u'author   of   the   extension') 
   parser.add_option(u'--is-configurable', dest=u'is_configurable', action=u'store_true', default=False, help=u'whether   this   extension   is   configurable') 
   (globals()[u'options'], args) = parser.parse_args() 
   if (len(args) != 1): 
      print(u'Error:   incorrect   number   of   arguments') 
      parser.print_help() 
      exit((-1)) 
   options.extension_name = args[0] 
   autofill_unprovided_options()", 'Parse command line options.','Parses the options and stores them in the global options variable.'
"def download_libxml2(dest_dir, version=None): 
    version_re = re.compile('^LATEST_LIBXML2_IS_(.*)$') 
   filename = 'libxml2-%s.tar.gz' 
   return download_library(dest_dir, LIBXML2_LOCATION, 'libxml2', version_re, filename, version=version)", 'Download the libxml2 library and place it in the dest_dir.',"'Downloads libxml2, returning the filename where the library was downloaded'"
"def enqueue_push_course_update(update, course_key): 
    if (push_notification_enabled() and update.get('push_notification_selected')): 
      course = modulestore().get_course(course_key) 
      if course: 
         push_course_update_task.delay(unicode(course_key), course.clean_id(padding_char='_'), course.display_name)", 'Enqueue a push course update for the given course.',"'Enqueues a task for push notification for the given update for the given course if 
 (1) the feature is enabled and 
 (2) push_notification is selected for the update'"
"def fake_os_walk(paths): 
    paths_dict = dict(paths) 
   def os_walk(top, topdown=True): 
      (dirs, nondirs) = paths_dict[top] 
      (yield (top, dirs, nondirs)) 
      for name in dirs: 
         new_path = '/'.join([top, name]) 
         for x in os_walk(new_path, topdown): 
            (yield x) 
   return os_walk", 'Fake os.walk()',"'Helper function for mocking os.walk() where must test that manipulation 
 of the returned dirs variable works as expected'"
"def get_all_objects(start_obj=None): 
    output = [''] 
   widget_lines = _get_widgets() 
   widget_lines = [('            ' + e) for e in widget_lines] 
   widget_lines.insert(0, 'Qt   widgets   -   {}   objects:'.format(len(widget_lines))) 
   output += widget_lines 
   if (start_obj is None): 
      start_obj = QApplication.instance() 
   pyqt_lines = [] 
   _get_pyqt_objects(pyqt_lines, start_obj) 
   pyqt_lines = [('            ' + e) for e in pyqt_lines] 
   pyqt_lines.insert(0, 'Qt   objects   -   {}   objects:'.format(len(pyqt_lines))) 
   output += [''] 
   output += pyqt_lines 
   output += objreg.dump_objects() 
   return '\n'.join(output)"," 'Get all the objects in the system. 
 :param start_obj: 
 :type start_obj: 
 :return: 
 :rtype:'",'Get all children of an object recursively as a string.'
"def send_mail_to_student(student, param_dict, language=None): 
    if ('display_name' in param_dict): 
      param_dict['course_name'] = param_dict['display_name'] 
   param_dict['site_name'] = configuration_helpers.get_value('SITE_NAME', param_dict['site_name']) 
   subject = None 
   message = None 
   message_type = param_dict['message'] 
   email_template_dict = {'allowed_enroll': ('emails/enroll_email_allowedsubject.txt', 'emails/enroll_email_allowedmessage.txt'), 'enrolled_enroll': ('emails/enroll_email_enrolledsubject.txt', 'emails/enroll_email_enrolledmessage.txt'), 'allowed_unenroll': ('emails/unenroll_email_subject.txt', 'emails/unenroll_email_allowedmessage.txt'), 'enrolled_unenroll': ('emails/unenroll_email_subject.txt', 'emails/unenroll_email_enrolledmessage.txt'), 'add_beta_tester': ('emails/add_beta_tester_email_subject.txt', 'emails/add_beta_tester_email_message.txt'), 'remove_beta_tester': ('emails/remove_beta_tester_email_subject.txt', 'emails/remove_beta_tester_email_message.txt'), 'account_creation_and_enrollment': ('emails/enroll_email_enrolledsubject.txt', 'emails/account_creation_and_enroll_emailMessage.txt')} 
   (subject_template, message_template) = email_template_dict.get(message_type, (None, None)) 
   if ((subject_template is not None) and (message_template is not None)): 
      (subject, message) = render_message_to_string(subject_template, message_template, param_dict, language=language) 
   if (subject and message): 
      message = message.strip() 
      subject = ''.join(subject.splitlines()) 
      from_address = configuration_helpers.get_value('email_from_address', settings.DEFAULT_FROM_EMAIL) 
      send_mail(subject, message, from_address, [student], fail_silently=False)"," 'Send email to student. 
 :param student: Student to send email to. 
 :param param_dict: Dictionary of parameters to pass to email template. 
 :param language: Language to use for email template. 
 :return: None'","'Construct the email using templates and then send it. 
 `student` is the student\'s email address (a `str`), 
 `param_dict` is a `dict` with keys 
 `site_name`: name given to edX instance (a `str`) 
 `registration_url`: url for registration (a `str`) 
 `display_name` : display name of a course (a `str`) 
 `course_id`: id of course (a `str`) 
 `auto_enroll`: user input option (a `str`) 
 `course_url`: url of course (a `str`) 
 `email_address`: email of student (a `str`) 
 `full_name`: student full name (a `str`) 
 `message`: type of email to send and template to use (a `str`) 
 `is_shib_course`: (a `boolean`) 
 `language` is the language used to render the email. If None the language 
 of the currently-logged in user (that is, the user sending the email) will 
 be used. 
 Returns a boolean indicating whether the email was sent successfully.'"
"def get_profiler_log_path(autodir): 
    return os.path.join(autodir, 'results', 'default', 'debug', 'client.DEBUG')"," 'Returns the path to the profiler log file. 
 :param autodir: The directory where the profiler logs are stored. 
 :type autodir: str 
 :return: The path to the profiler log file.'","'Given the directory of a profiler client, find the client log path.'"
"def sh(cmd): 
    return check_call(cmd, shell=True)", 'Run a shell command',"'Execute command in a subshell, return status code.'"
"def raise_http_conflict_for_instance_invalid_state(exc, action, server_id): 
    attr = exc.kwargs.get('attr') 
   state = exc.kwargs.get('state') 
   if ((attr is not None) and (state is not None)): 
      msg = (_(""Cannot   '%(action)s'   instance   %(server_id)s   while   it   is   in   %(attr)s   %(state)s"") % {'action': action, 'attr': attr, 'state': state, 'server_id': server_id}) 
   else: 
      msg = (_(""Instance   %(server_id)s   is   in   an   invalid   state   for   '%(action)s'"") % {'action': action, 'server_id': server_id}) 
   raise webob.exc.HTTPConflict(explanation=msg)"," 'Raise a HTTPConflict with a message explaining why an instance is 
 in an invalid state for the given action.'","'Raises a webob.exc.HTTPConflict instance containing a message 
 appropriate to return via the API based on the original 
 InstanceInvalidState exception.'"
"def random_selection(a, start, end, i): 
    if (start < end): 
      p = choosePivot(start, end) 
      (a[start], a[p]) = (a[p], a[start]) 
      j = partition(a, start, end) 
      if (j == i): 
         return a[i] 
      if (j < i): 
         return random_selection(a, (j + 1), end, i) 
      else: 
         return random_selection(a, start, (j - 1), i) 
   else: 
      return a[start]"," 'Returns a random element from the range [start, end) 
 :param a: The list to be sorted 
 :param start: The first element to be sorted 
 :param end: The last element to be sorted 
 :param i: The index of the element to be returned'","'returns the ith order statistic 
 in the array a in linear time 
 >>> from random import sample 
 >>> test_cases = [sample(range(20), 10) for i in range(10)] 
 >>> orders = [randint(0, 9) for i in range(10)] 
 >>> results = [sorted(test_cases[i])[orders[i]] == random_selection(test_cases[i], 0, len(test_cases[i])-1, orders[i]) for i in range(10)] 
 >>> print sum(results) 
 10'"
"def main(): 
    parser = OptionParser() 
   parser.add_option('-d', '--days', dest='days', action='store', type='int', help='number   of   days   (14)', default=14) 
   parser.add_option('-i', '--info_only', action='store_true', dest='info_only', help='info   about   the   requested   action', default=False) 
   parser.add_option('-v', '--verbose', action='store_true', dest='verbose', help='verbose   mode,   print   the   name   of   each   repository', default=False) 
   (options, args) = parser.parse_args() 
   try: 
      ini_file = args[0] 
   except IndexError: 
      sys.exit(('Usage:   python   %s   <tool   shed   .ini   file>   [options]' % sys.argv[0])) 
   config_parser = ConfigParser.ConfigParser({'here': os.getcwd()}) 
   config_parser.read(ini_file) 
   config_dict = {} 
   for (key, value) in config_parser.items('app:main'): 
      config_dict[key] = value 
   config = tool_shed_config.Configuration(**config_dict) 
   app = DeprecateRepositoriesApplication(config) 
   cutoff_time = (datetime.utcnow() - timedelta(days=options.days)) 
   now = strftime('%Y-%m-%d   %H:%M:%S') 
   print '\n####################################################################################' 
   print ('#   %s   -   Handling   stuff   older   than   %i   days' % (now, options.days)) 
   if options.info_only: 
      print '#   Displaying   info   only   (   --info_only   )' 
   deprecate_repositories(app, cutoff_time, days=options.days, info_only=options.info_only, verbose=options.verbose)", 'Deprecate repositories that are older than the specified number of days.',"'Script to deprecate any repositories that are older than n days, and have been empty since creation.'"
"def collect_data_files(package, include_py_files=False, subdir=None): 
    if (not isinstance(package, string_types)): 
      raise ValueError 
   (pkg_base, pkg_dir) = get_package_paths(package) 
   if subdir: 
      pkg_dir = os.path.join(pkg_dir, subdir) 
   datas = [] 
   for (dirpath, dirnames, files) in os.walk(pkg_dir): 
      for f in files: 
         extension = os.path.splitext(f)[1] 
         if (include_py_files or (extension not in PY_IGNORE_EXTENSIONS)): 
            source = os.path.join(dirpath, f) 
            dest = remove_prefix(dirpath, (os.path.dirname(pkg_base) + os.sep)) 
            datas.append((source, dest)) 
   return datas"," 'Collect all the data files in a package. 
 :param package: a package name 
 :type package: string 
 :param include_py_files: include Python files 
 :type include_py_files: bool 
 :param subdir: subdirectory to search in 
 :type subdir: string 
 :return: list of (source, destination) pairs 
 :rtype: list'","'This routine produces a list of (source, dest) non-Python (i.e. data) 
 files which reside in package. Its results can be directly assigned to 
 ``datas`` in a hook script; see, for example, hook-sphinx.py. The 
 package parameter must be a string which names the package. 
 By default, all Python executable files (those ending in .py, .pyc, 
 and so on) will NOT be collected; setting the include_py_files 
 argument to True collects these files as well. This is typically used 
 with Python routines (such as those in pkgutil) that search a given 
 directory for Python executable files then load them as extensions or 
 plugins. The optional subdir give a subdirectory relative to package to 
 search, which is helpful when submodules are imported at run-time from a 
 directory lacking __init__.py 
 This function does not work on zipped Python eggs. 
 This function is used only for hook scripts, but not by the body of 
 PyInstaller.'"
"def encrypt(plaintext): 
    salt = _make_salt() 
   return _encrypt(salt, plaintext, g.tracking_secret)"," 'Encrypts the given plaintext using the given salt and secret. 
 The secret is the tracking secret from the :py:class:`~cryptography.hazmat.primitives.asymmetric.rsa.RSA` object. 
 :param plaintext: The plaintext to encrypt. 
 :type plaintext: str 
 :return: The encrypted data. 
 :rtype: str'","'Return the message `plaintext` encrypted. 
 The encrypted message will have its salt prepended and will be URL encoded 
 to make it suitable for use in URLs and Cookies. 
 NOTE: this function is here for backwards compatibility. Please do not 
 use it for new code.'"
"def parse_acl(acl_string): 
    referrers = [] 
   groups = [] 
   if acl_string: 
      for value in acl_string.split(','): 
         if value.startswith('.r:'): 
            referrers.append(value[len('.r:'):]) 
         else: 
            groups.append(value) 
   return (referrers, groups)"," 'Parse the ACL string into a list of referrers and a list of groups. 
 The referrers are the domain names that are allowed to access the 
 file.  The groups are the groups that are allowed to access the file.'","'Parses a standard Swift ACL string into a referrers list and groups list. 
 See :func:`clean_acl` for documentation of the standard Swift ACL format. 
 :param acl_string: The standard Swift ACL string to parse. 
 :returns: A tuple of (referrers, groups) where referrers is a list of 
 referrer designations (without the leading .r:) and groups is a 
 list of groups to allow access.'"
"def popen_nonblock(*args, **kwargs): 
    proc = popen_sp(*args, **kwargs) 
   if proc.stdin: 
      proc.stdin = pipebuf.NonBlockBufferedWriter(proc.stdin) 
   if proc.stdout: 
      proc.stdout = pipebuf.NonBlockBufferedReader(proc.stdout) 
   if proc.stderr: 
      proc.stderr = pipebuf.NonBlockBufferedReader(proc.stderr) 
   return proc"," 'Non-blocking version of popen_sp. 
 :param args: 
 :param kwargs: 
 :return: 
 :rtype: :class:`subprocess.Popen` 
 :raises subprocess.CalledProcessError: 
 :raises OSError: 
 :raises IOError: 
 :raises TypeError: 
 :raises ValueError: 
 :raises subprocess.TimeoutExpired: 
 :raises subprocess.Timeouted: 
 :raises subprocess.Timeout: 
 :raises subprocess.TimeoutError: 
 :raises subprocess.TimeoutExceeded: 
 :raises subprocess.TimeoutExceededError: 
 :raises subprocess.TimeoutExceeded: 
 :raises subprocess.TimeoutExceededError: 
 :raises subprocess.TimeoutExceededError: 
 :raises subprocess.TimeoutExceeded: 
 :raises subprocess.TimeoutExceededError: 
 :raises subprocess.TimeoutExceededError: 
 :raises subprocess.TimeoutExceededError:","'Create a process in the same way as popen_sp, but patch the file 
 descriptors so they can be accessed from Python/gevent 
 in a non-blocking manner.'"
"def dup_transform(f, p, q, K): 
    if (not f): 
      return [] 
   n = (len(f) - 1) 
   (h, Q) = ([f[0]], [[K.one]]) 
   for i in range(0, n): 
      Q.append(dup_mul(Q[(-1)], q, K)) 
   for (c, q) in zip(f[1:], Q[1:]): 
      h = dup_mul(h, p, K) 
      q = dup_mul_ground(q, c, K) 
      h = dup_add(h, q, K) 
   return h"," 'Transform a polynomial ``f`` into ``p`` and ``q`` using the 
 ``dup_mul`` and ``dup_mul_ground`` functions. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.densebasic import dup_transform 
 >>> f = ZZ.map([[1, 1, 2, 3, 5], [0, 2, 1, 3, 4], [0, 0, 1, 2, 3], [0, 0, 0, 1, 2], [0, 0, 0, 0, 1]]) 
 >>> dup_transform(f, 2, 3, ZZ) 
 [[1, 1, 2, 3, 5], [0, 0, 0, 1, 2], [0, 0, 0, 0, 1]]'","'Evaluate functional transformation ``q**n * f(p/q)`` in ``K[x]``. 
 Examples 
 >>> from sympy.polys import ring, ZZ 
 >>> R, x = ring(""x"", ZZ) 
 >>> R.dup_transform(x**2 - 2*x + 1, x**2 + 1, x - 1) 
 x**4 - 2*x**3 + 5*x**2 - 4*x + 4'"
"def serialize_item(collection, item): 
    __connect() 
   collection = mongodb[collection.collection_type()] 
   data = collection.find_one({'name': item.name}) 
   if data: 
      collection.update({'name': item.name}, item.to_dict()) 
   else: 
      collection.insert(item.to_dict())"," 'Serialize and persist item to MongoDB. 
 :param collection: Collection name 
 :param item: Item to serialize and persist 
 :type item: :class:`~.item.Item`'","'Save a collection item to database 
 @param Collection collection collection 
 @param Item item collection item'"
"def create_instance(c_instance): 
    return Serato(c_instance)", 'Create an instance of the Serato class','Creates and returns the Serato script'
"def req_item(): 
    if (request.function != 'fema'): 
      s3.filter = (FS('req_id$is_template') == False) 
   def prep(r): 
      if (r.interactive or (r.representation == 'aadata')): 
         list_fields = s3db.get_config('req_req_item', 'list_fields') 
         list_fields.insert(1, 'req_id$site_id') 
         levels = gis.get_relevant_hierarchy_levels() 
         levels.reverse() 
         for level in levels: 
            lfield = ('req_id$site_id$location_id$%s' % level) 
            list_fields.insert(1, lfield) 
         s3db.configure('req_req_item', insertable=False, list_fields=list_fields) 
         s3.crud_strings['req_req_item'].title_list = T('Requested   Items') 
         if ((r.method != None) and (r.method != 'update') and (r.method != 'read')): 
            s3db.req_hide_quantities(r.table) 
      return True 
   s3.prep = prep 
   output = s3_rest_controller('req', 'req_item') 
   if settings.get_req_prompt_match(): 
      req_item_inv_item_btn = dict(url=URL(c='req', f='req_item_inv_item', args=['[id]']), _class='action-btn', label=str(T('Request   from   Facility'))) 
      if s3.actions: 
         s3.actions.append(req_item_inv_item_btn) 
      else: 
         s3.actions = [req_item_inv_item_btn] 
   return output", 'Request item controller',"'REST Controller 
 @ToDo: Filter out fulfilled Items?'"
"def service_status(hostname=None, service=None, **kwargs): 
    if (not hostname): 
      raise CommandExecutionError('Missing   hostname   parameter') 
   if (not service): 
      raise CommandExecutionError('Missing   service   parameter') 
   target = 'service' 
   numeric = kwargs.get('numeric') 
   data = _status_query(target, hostname, service=service, enumerate=numeric) 
   ret = {'result': data['result']} 
   if ret['result']: 
      ret['status'] = data.get('json_data', {}).get('data', {}).get(target, {}).get('status', (((not numeric) and 'Unknown') or 2)) 
   else: 
      ret['error'] = data['error'] 
   return ret"," 'Returns the status of the specified service on the specified host. 
 :param hostname: The hostname of the host to query. 
 :param service: The service to query. 
 :param numeric: If True, return a numeric status code. 
 :returns: A dictionary containing the service status and error. 
 :rtype: dict'","'Check status of a particular service on a host on it in Nagios. 
 By default statuses are returned in a numeric format. 
 Parameters: 
 hostname 
 The hostname to check the status of the service in Nagios. 
 service 
 The service to check the status of in Nagios. 
 numeric 
 Turn to false in order to return status in text format 
 (\'OK\' instead of 0, \'Warning\' instead of 1 etc) 
 :return: status:     \'OK\', \'Warning\', \'Critical\' or \'Unknown\' 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' nagios_rpc.service_status hostname=webserver.domain.com service=\'HTTP\' 
 salt \'*\' nagios_rpc.service_status hostname=webserver.domain.com service=\'HTTP\' numeric=False'"
"def command_show(problem): 
    print problem.get_html()", 'Show a problem.','Display the text for this problem'
"def current_route_url(request, *elements, **kw): 
    return request.current_route_url(*elements, **kw)"," 'Returns the current URL, with the given elements replaced with the 
 given values. 
 :param request: The request. 
 :param elements: The elements to replace. 
 :param kw: The values to replace the elements with. 
 :return: The URL.'","'This is a backwards compatibility function.  Its result is the same as 
 calling:: 
 request.current_route_url(*elements, **kw) 
 See :meth:`pyramid.request.Request.current_route_url` for more 
 information.'"
"def obfuscatePowershellScript(code): 
    import re 
   newCode = code 
   newCode = remove_comments(newCode) 
   if ('function   Invoke-ReflectivePEInjection' in newCode): 
      newCode = newCode.replace(""$TypeBuilder.DefineLiteral('IMAGE_DLL_CHARACTERISTICS_DYNAMIC_BASE',   [UInt16]   0x0040)   |   Out-Null"", ""$TypeBuilder.DefineLiteral('IMAGE_DLL_CHARACTERIS'+'TICS_DYNAMIC_BASE',   [UInt16]   0x0040)   |   Out-Null"") 
   return newCode"," 'This function will obfuscate a powershell script. 
 :param code: A powershell script. 
 :return: A obfuscated powershell script.'","'Try to clean powershell script (perhaps in the future \'obfuscation\'...). 
 Comments are deteleted and some strings are replaced in some powershell functions to bypass AV detection'"
"def add_metadata_type(ir): 
    buf = [] 
   for line in ir.splitlines(): 
      if re_metadata_def.match(line): 
         if (None is re_metadata_correct_usage.search(line)): 
            line = line.replace('!{', 'metadata   !{') 
            line = line.replace('!""', 'metadata   !""') 
            def sub_metadata(m): 
               return 'metadata   {0}'.format(m.group(0)) 
            line = re_metadata_ref.sub(sub_metadata, line) 
            line = line.lstrip('metadata   ') 
      buf.append(line) 
   return '\n'.join(buf)", 'Add metadata type to the given IR',"'Rewrite metadata since llvm3.6 dropped the ""metadata"" type prefix.'"
"def factory(type): 
    return ArrowFactory(type)", 'Returns an ArrowFactory for the given type.',"'Returns an :class:`.ArrowFactory` for the specified :class:`Arrow <arrow.arrow.Arrow>` 
 or derived type. 
 :param type: the type, :class:`Arrow <arrow.arrow.Arrow>` or derived.'"
"def get_dl_data(song, mediatype='any'): 
    def mbsize(x): 
      '   Return   size   in   MB.   ' 
      return str(int((x / (1024 ** 2)))) 
   p = util.get_pafy(song) 
   dldata = [] 
   text = '   [Fetching   stream   info]   >' 
   streamlist = [x for x in p.allstreams] 
   if (mediatype == 'audio'): 
      streamlist = [x for x in p.audiostreams] 
   l = len(streamlist) 
   for (n, stream) in enumerate(streamlist): 
      sys.stdout.write(((((text + ('-' * n)) + '>') + ('   ' * ((l - n) - 1))) + '<\r')) 
      sys.stdout.flush() 
      try: 
         size = mbsize(stream.get_filesize()) 
      except TypeError: 
         util.dbg(((c.r + '---Error   getting   stream   size') + c.w)) 
         size = 0 
      item = {'mediatype': stream.mediatype, 'size': size, 'ext': stream.extension, 'quality': stream.quality, 'notes': stream.notes, 'url': stream.url} 
      dldata.append(item) 
   screen.writestatus('') 
   return (dldata, p)"," 'Get the dl data for a song. 
 :param song: The song to get the data for 
 :param mediatype: The type of media to get data for. 
 :return: The dl data for the song'","'Get filesize and metadata for all streams, return dict.'"
"def sign(wire, keyname, secret, time, fudge, original_id, error, other_data, request_mac, ctx=None, multi=False, first=True, algorithm=default_algorithm): 
    (algorithm_name, digestmod) = get_algorithm(algorithm) 
   if first: 
      ctx = hmac.new(secret, digestmod=digestmod) 
      ml = len(request_mac) 
      if (ml > 0): 
         ctx.update(struct.pack('!H', ml)) 
         ctx.update(request_mac) 
   id = struct.pack('!H', original_id) 
   ctx.update(id) 
   ctx.update(wire[2:]) 
   if first: 
      ctx.update(keyname.to_digestable()) 
      ctx.update(struct.pack('!H', dns.rdataclass.ANY)) 
      ctx.update(struct.pack('!I', 0)) 
   long_time = (time + 0L) 
   upper_time = ((long_time >> 32) & 65535L) 
   lower_time = (long_time & 4294967295L) 
   time_mac = struct.pack('!HIH', upper_time, lower_time, fudge) 
   pre_mac = (algorithm_name + time_mac) 
   ol = len(other_data) 
   if (ol > 65535): 
      raise ValueError('TSIG   Other   Data   is   >   65535   bytes') 
   post_mac = (struct.pack('!HH', error, ol) + other_data) 
   if first: 
      ctx.update(pre_mac) 
      ctx.update(post_mac) 
   else: 
      ctx.update(time_mac) 
   mac = ctx.digest() 
   mpack = struct.pack('!H', len(mac)) 
   tsig_rdata = ((((pre_mac + mpack) + mac) + id) + post_mac) 
   if multi: 
      ctx = hmac.new(secret, digestmod=digestmod) 
      ml = len(mac) 
      ctx.update(struct.pack('!H', ml)) 
      ctx.update(mac) 
   else: 
      ctx = None 
   return (tsig_rdata, mac, ctx)"," 'Generate a TSIG RDATA and MAC. 
 The TSIG RDATA is a string that can be used to verify the signature. 
 The MAC is a string that can be used to verify the TSIG RDATA. 
 The TSIG RDATA and MAC are generated by combining the TSIG parameters, 
 the TSIG RDATA, and the TSIG MAC. 
 The TSIG RDATA is a string that is constructed from the following: 
 * The TSIG parameters. 
 * The TSIG RDATA. 
 * The TSIG MAC. 
 The TSIG RDATA is a string of the form: 
 <pre> 
   <TSIG parameters> 
   <TSIG RDATA> 
   <TSIG MAC> 
 </pre> 
 The TSIG parameters are a string of the form: 
 <pre> 
   <TSIG algorithm> 
   <TSIG key> 
   <TSIG time> 
   <TSIG fudge> 
   <TSIG error> 
  ","'Return a (tsig_rdata, mac, ctx) tuple containing the HMAC TSIG rdata 
 for the input parameters, the HMAC MAC calculated by applying the 
 TSIG signature algorithm, and the TSIG digest context. 
 @rtype: (string, string, hmac.HMAC object) 
 @raises ValueError: I{other_data} is too long 
 @raises NotImplementedError: I{algorithm} is not supported'"
"def errors_response(status, errors): 
    document = {'errors': errors, 'jsonapi': {'version': JSONAPI_VERSION}} 
   return (jsonpify(document), status)", 'Return a JSON-API response with a list of errors.',"'Return an error response with multiple errors. 
 `status` is an integer representing an HTTP status code corresponding to an 
 error response. 
 `errors` is a list of error dictionaries, each of which must satisfy the 
 requirements of the JSON API specification. 
 This function returns a two-tuple whose left element is a dictionary 
 representing a JSON API response document and whose right element is 
 simply `status`. 
 The keys within each error object are described in the `Errors`_ 
 section of the JSON API specification. 
 .. _Errors: http://jsonapi.org/format/#errors'"
"def dmp_integrate_in(f, m, j, u, K): 
    if ((j < 0) or (j > u)): 
      raise IndexError(('0   <=   j   <=   u   expected,   got   %s' % (u, j))) 
   return _rec_integrate_in(f, m, u, 0, j, K)"," 'Integrate ``f`` in ``[m, j]``. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.densebasic import dmp_integrate_in 
 >>> f = ZZ.map([[1, 1], [1, 0]]) 
 >>> dmp_integrate_in(f, 2, 1) 
 1'","'Computes the indefinite integral of ``f`` in ``x_j`` in ``K[X]``. 
 Examples 
 >>> from sympy.polys import ring, QQ 
 >>> R, x,y = ring(""x,y"", QQ) 
 >>> R.dmp_integrate_in(x + 2*y, 1, 0) 
 1/2*x**2 + 2*x*y 
 >>> R.dmp_integrate_in(x + 2*y, 1, 1) 
 x*y + y**2'"
"def dump_thread_stack(): 
    threads = threading.enumerate() 
   output_file = (PROFILING_OUTPUT_FMT % get_filename_fmt()) 
   data = {} 
   for (thread, frame) in sys._current_frames().items(): 
      trace = traceback.format_stack(frame) 
      data[('%x' % thread)] = {'traceback': trace, 'name': get_thread_name(threads, thread)} 
   json.dump(data, file(output_file, 'w'), indent=4)", 'Dumps a thread-specific stack trace to a file.','Dumps all thread stacks to a file'
"@endpoint(u'/ajax/books/{library_id=None}', postprocess=json) 
 def books(ctx, rd, library_id): 
    db = get_db(ctx, rd, library_id) 
   with db.safe_read_lock: 
      id_is_uuid = rd.query.get(u'id_is_uuid', u'false') 
      ids = rd.query.get(u'ids') 
      if ((ids is None) or (ids == u'all')): 
         ids = db.all_book_ids() 
      else: 
         ids = ids.split(u',') 
         if (id_is_uuid == u'true'): 
            ids = {db.lookup_by_uuid(x) for x in ids} 
            ids.discard(None) 
         else: 
            try: 
               ids = {int(x) for x in ids} 
            except Exception: 
               raise HTTPNotFound(u'ids   must   a   comma   separated   list   of   integers') 
      last_modified = None 
      category_urls = (rd.query.get(u'category_urls', u'true').lower() == u'true') 
      device_compatible = (rd.query.get(u'device_compatible', u'false').lower() == u'true') 
      device_for_template = rd.query.get(u'device_for_template', None) 
      ans = {} 
      restricted_to = ctx.allowed_book_ids(rd, db) 
      for book_id in ids: 
         if (book_id not in restricted_to): 
            ans[book_id] = None 
            continue 
         (data, lm) = book_to_json(ctx, rd, db, book_id, get_category_urls=category_urls, device_compatible=device_compatible, device_for_template=device_for_template) 
         last_modified = (lm if (last_modified is None) else max(lm, last_modified)) 
         ans[book_id] = data 
   if (last_modified is not None): 
      rd.outheaders[u'Last-Modified'] = http_date(timestampfromdt(last_modified)) 
   return ans", 'Returns a JSON representation of the given book ids.',"'Return the metadata for the books as a JSON dictionary. 
 Query parameters: ?ids=all&category_urls=true&id_is_uuid=false&device_for_template=None 
 If category_urls is true the returned dictionary also contains a 
 mapping of category (field) names to URLs that return the list of books in the 
 given category. 
 If id_is_uuid is true then the book_id is assumed to be a book uuid instead.'"
"def get_injected_network_template(network_info, use_ipv6=CONF.use_ipv6, template=CONF.injected_network_template): 
    if (network_info is None): 
      return None 
   if hasattr(network_info, 'legacy'): 
      network_info = network_info.legacy() 
   nets = [] 
   ifc_num = (-1) 
   have_injected_networks = False 
   for (network_ref, mapping) in network_info: 
      ifc_num += 1 
      if (not network_ref['injected']): 
         continue 
      have_injected_networks = True 
      address = mapping['ips'][0]['ip'] 
      netmask = mapping['ips'][0]['netmask'] 
      address_v6 = None 
      gateway_v6 = None 
      netmask_v6 = None 
      if use_ipv6: 
         address_v6 = mapping['ip6s'][0]['ip'] 
         netmask_v6 = mapping['ip6s'][0]['netmask'] 
         gateway_v6 = mapping['gateway_v6'] 
      net_info = {'name': ('eth%d' % ifc_num), 'address': address, 'netmask': netmask, 'gateway': mapping['gateway'], 'broadcast': mapping['broadcast'], 'dns': '   '.join(mapping['dns']), 'address_v6': address_v6, 'gateway_v6': gateway_v6, 'netmask_v6': netmask_v6} 
      nets.append(net_info) 
   if (have_injected_networks is False): 
      return None 
   if (not template): 
      return None 
   _late_load_cheetah() 
   ifc_template = open(template).read() 
   return str(Template(ifc_template, searchList=[{'interfaces': nets, 'use_ipv6': use_ipv6}]))", 'Returns the injected network template for the given network info.',"'return a rendered network template for the given network_info 
 :param network_info: 
 :py:meth:`~nova.network.manager.NetworkManager.get_instance_nw_info` 
 Note: this code actually depends on the legacy network_info, but will 
 convert the type itself if necessary.'"
"@pytest.fixture 
 def reset_standarddir(no_cachedir_tag): 
    standarddir.init(None) 
   (yield) 
   standarddir.init(None)", 'Clear the standarddir','Clean up standarddir arguments before and after each test.'
"def refresh_db(full=False): 
    if full: 
      return (__salt__['cmd.retcode']('/bin/pkg   refresh   --full') == 0) 
   else: 
      return (__salt__['cmd.retcode']('/bin/pkg   refresh') == 0)"," 'Refresh all packages. 
 full: Refresh all packages in the system. 
 If the command fails, it will return False, otherwise True.'","'Updates the remote repos database. 
 full : False 
 Set to ``True`` to force a refresh of the pkg DB from all publishers, 
 regardless of the last refresh time. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pkg.refresh_db 
 salt \'*\' pkg.refresh_db full=True'"
"def from_current_timezone(value): 
    if (settings.USE_TZ and (value is not None) and timezone.is_naive(value)): 
      current_timezone = timezone.get_current_timezone() 
      try: 
         return timezone.make_aware(value, current_timezone) 
      except Exception: 
         raise ValidationError((_(u""%(datetime)s   couldn't   be   interpreted   in   time   zone   %(current_timezone)s;   it   may   be   ambiguous   or   it   may   not   exist."") % {u'datetime': value, u'current_timezone': current_timezone})) 
   return value"," 'Return a value in the current time zone. 
 If the value is a naive datetime, return it in the current time zone. 
 If the value is a timezone-aware datetime, return it unchanged.'","'When time zone support is enabled, convert naive datetimes 
 entered in the current time zone to aware datetimes.'"
"def DEFINE_boolean(name, default, help): 
    CONFIG.AddOption(type_info.Bool(name=name, default=default, description=help))"," 'Define a boolean option. 
 :param name: Option name 
 :param default: Default value 
 :param help: Option help 
 :return: None'",'A helper for defining boolean options.'
"def load_lang_conf(): 
    if osp.isfile(LANG_FILE): 
      with open(LANG_FILE, 'r') as f: 
         lang = f.read() 
   else: 
      lang = get_interface_language() 
      save_lang_conf(lang) 
   if (lang.strip('\n') in DISABLED_LANGUAGES): 
      lang = DEFAULT_LANGUAGE 
      save_lang_conf(lang) 
   return lang"," 'Loads language configuration from file. 
 :returns: language code'","'Load language setting from language config file if it exists, otherwise 
 try to use the local settings if Spyder provides a translation, or 
 return the default if no translation provided.'"
"def badDecorator(fn): 
    def nameCollision(*args, **kwargs): 
      return fn(*args, **kwargs) 
   return nameCollision"," 'A decorator that makes sure a function name doesn\'t collide with 
 another function. 
 :param fn: The function to decorate 
 :return: The decorated function'","'Decorate a function without preserving the name of the original function. 
 Always return a function with the same name.'"
"def find_file(path, saltenv='base', **kwargs): 
    if ('env' in kwargs): 
      salt.utils.warn_until('Oxygen', ""Parameter   'env'   has   been   detected   in   the   argument   list.      This   parameter   is   no   longer   used   and   has   been   replaced   by   'saltenv'   as   of   Salt   2016.11.0.      This   warning   will   be   removed   in   Salt   Oxygen."") 
      kwargs.pop('env') 
   path = os.path.normpath(path) 
   fnd = {'path': '', 'rel': ''} 
   if os.path.isabs(path): 
      return fnd 
   if (saltenv not in __opts__['file_roots']): 
      return fnd 
   def _add_file_stat(fnd): 
      '\n                        Stat   the   file   and,   assuming   no   errors   were   found,   convert   the   stat\n                        result   to   a   list   of   values   and   add   to   the   return   dict.\n\n                        Converting   the   stat   result   to   a   list,   the   elements   of   the   list\n                        correspond   to   the   following   stat_result   params:\n\n                        0   =>   st_mode=33188\n                        1   =>   st_ino=10227377\n                        2   =>   st_dev=65026\n                        3   =>   st_nlink=1\n                        4   =>   st_uid=1000\n                        5   =>   st_gid=1000\n                        6   =>   st_size=1056233\n                        7   =>   st_atime=1468284229\n                        8   =>   st_mtime=1456338235\n                        9   =>   st_ctime=1456338235\n                        ' 
      try: 
         fnd['stat'] = list(os.stat(fnd['path'])) 
      except Exception: 
         pass 
      return fnd 
   if ('index' in kwargs): 
      try: 
         root = __opts__['file_roots'][saltenv][int(kwargs['index'])] 
      except IndexError: 
         return fnd 
      except ValueError: 
         return fnd 
      full = os.path.join(root, path) 
      if (os.path.isfile(full) and (not salt.fileserver.is_file_ignored(__opts__, full))): 
         fnd['path'] = full 
         fnd['rel'] = path 
         return _add_file_stat(fnd) 
      return fnd 
   for root in __opts__['file_roots'][saltenv]: 
      full = os.path.join(root, path) 
      if (os.path.isfile(full) and (not salt.fileserver.is_file_ignored(__opts__, full))): 
         fnd['path'] = full 
         fnd['rel'] = path 
         return _add_file_stat(fnd) 
   return fnd"," 'Find a file in the salt file server. 
 The file server is configured in the \'file_roots\' option of the 
 salt master. 
 :param path: The path to the file to find. 
 :param saltenv: The salt environment. 
 :param kwargs: Additional arguments to pass to the salt.fileserver.find function. 
 :return: A dictionary of the file found, or None if the file could not be found.'",'Search the environment for the relative path.'
"def load_pandas(): 
    filepath = os.path.dirname(os.path.abspath(__file__)) 
   data = pd.read_csv(os.path.join((filepath + '/china_smoking.csv')), index_col='Location') 
   return utils.Dataset(data=data, title='Smoking   and   lung   cancer   in   Chinese   regions')", 'Loads the pandas dataset',"'Load the China smoking/lung cancer data and return a Dataset class. 
 Returns 
 Dataset instance: 
 See DATASET_PROPOSAL.txt for more information.'"
"def flatten(seq, scalarp=is_scalar_or_string): 
    for item in seq: 
      if scalarp(item): 
         (yield item) 
      else: 
         for subitem in flatten(item, scalarp): 
            (yield subitem)"," 'Flatten a sequence of iterables. 
 This function is a generalization of the `flatten` function from 
 `itertools`. 
 Parameters 
 seq : iterable 
 An iterable containing one or more iterables. 
 scalarp : callable 
 A callable that takes one argument and returns True if the argument 
 is a scalar, and False otherwise. 
 Examples 
 >>> from sympy import flatten, sin, cos 
 >>> flatten([[1, [2, 3], [4, 5, 6]]]) 
 [1, 2, 3, 4, 5, 6] 
 >>> flatten([1, [2, 3], [4, 5, 6]]) 
 [1, 2, 3, 4, 5, 6] 
 >>> flatten([1, [2, 3], [4, 5, 6]], scalarp=lambda x: isinstance(x, (int, float))) 
 [1, 2, 3, 4, 5, 6] 
 >>> flatten([","'Returns a generator of flattened nested containers 
 For example: 
 >>> from matplotlib.cbook import flatten 
 >>> l = ((\'John\', [\'Hunter\']), (1, 23), [[([42, (5, 23)], )]]) 
 >>> print(list(flatten(l))) 
 [\'John\', \'Hunter\', 1, 23, 42, 5, 23] 
 By: Composite of Holger Krekel and Luther Blissett 
 From: https://code.activestate.com/recipes/121294/ 
 and Recipe 1.12 in cookbook'"
"def isLineIntersectingLoops(loops, pointBegin, pointEnd): 
    normalizedSegment = (pointEnd - pointBegin) 
   normalizedSegmentLength = abs(normalizedSegment) 
   if (normalizedSegmentLength > 0.0): 
      normalizedSegment /= normalizedSegmentLength 
      segmentYMirror = complex(normalizedSegment.real, (- normalizedSegment.imag)) 
      pointBeginRotated = (segmentYMirror * pointBegin) 
      pointEndRotated = (segmentYMirror * pointEnd) 
      if isLoopListIntersectingInsideXSegment(loops, pointBeginRotated.real, pointEndRotated.real, segmentYMirror, pointBeginRotated.imag): 
         return True 
   return False"," 'Return True if point is inside the line segment, False otherwise.'",'Determine if the line is intersecting loops.'
"def report_expected_diffs(diffs, colorize=False): 
    if (not diffs): 
      return 'No   differences' 
   diffs = diffs.items() 
   diffs.sort() 
   s = [] 
   last = '' 
   for (path, desc) in diffs: 
      t = _space_prefix(last, path, indent=4, include_sep=False) 
      if colorize: 
         t = color_line(t, 11) 
      last = path 
      if (len(desc.splitlines()) > 1): 
         cur_indent = len(re.search('^[   ]*', t).group(0)) 
         desc = indent((cur_indent + 2), desc) 
         if colorize: 
            t += '\n' 
            for line in desc.splitlines(): 
               if line.strip().startswith('+'): 
                  line = color_line(line, 10) 
               elif line.strip().startswith('-'): 
                  line = color_line(line, 9) 
               else: 
                  line = color_line(line, 14) 
               t += (line + '\n') 
         else: 
            t += ('\n' + desc) 
      else: 
         t += ('   ' + desc) 
      s.append(t) 
   s.append(('Files   with   differences:   %s' % len(diffs))) 
   return '\n'.join(s)"," 'Return a formatted string with the list of expected diffs. 
 :param diffs: List of (path, desc) tuples. 
 :param colorize: Whether to colorize the output. 
 :return: A formatted string.'","'Takes the output of compare_expected, and returns a string 
 description of the differences.'"
"@treeio_login_required 
 @handle_response_format 
 def tax_view(request, tax_id, response_format='html'): 
    tax = get_object_or_404(Tax, pk=tax_id) 
   if ((not request.user.profile.has_permission(tax, mode='r')) and (not request.user.profile.is_admin('treeio_finance'))): 
      return user_denied(request, ""You   don't   have   access   to   this   Tax"", response_format) 
   return render_to_response('finance/tax_view', {'tax': tax}, context_instance=RequestContext(request), response_format=response_format)"," 'View taxes. 
 :param request: The request. 
 :type request: django.http.HttpRequest 
 :param tax_id: The tax ID. 
 :type tax_id: int 
 :param response_format: The format to render the response. 
 :type response_format: str 
 :return: The response. 
 :rtype: django.http.HttpResponse'",'View a tax'
"def timeit(func): 
    before = time.time() 
   res = func() 
   return ((time.time() - before), res)", 'Decorator to time function calls',"'Run some function, and return (RunTimeInSeconds,Result)'"
"def http_date_to_dt(http_date, obs_date=False): 
    if (not obs_date): 
      return strptime(http_date, '%a,   %d   %b   %Y   %H:%M:%S   %Z') 
   time_formats = ('%a,   %d   %b   %Y   %H:%M:%S   %Z', '%a,   %d-%b-%Y   %H:%M:%S   %Z', '%A,   %d-%b-%y   %H:%M:%S   %Z', '%a   %b   %d   %H:%M:%S   %Y') 
   for time_format in time_formats: 
      try: 
         return strptime(http_date, time_format) 
      except ValueError: 
         continue 
   raise ValueError(('time   data   %r   does   not   match   known   formats' % http_date))", 'Convert http date to a datetime object',"'Converts an HTTP date string to a datetime instance. 
 Args: 
 http_date (str): An RFC 1123 date string, e.g.: 
 ""Tue, 15 Nov 1994 12:45:26 GMT"". 
 obs_date (bool, optional): Support obs-date formats according to 
 RFC 7231, e.g.: 
 ""Sunday, 06-Nov-94 08:49:37 GMT"" (default ``False``). 
 Returns: 
 datetime: A UTC datetime instance corresponding to the given 
 HTTP date. 
 Raises: 
 ValueError: http_date doesn\'t match any of the available time formats'"
"def setup_platform(hass, config, add_devices, discovery_info=None): 
    import myusps 
   try: 
      cookie = hass.config.path(COOKIE) 
      session = myusps.get_session(config.get(CONF_USERNAME), config.get(CONF_PASSWORD), cookie_path=cookie) 
   except myusps.USPSError: 
      _LOGGER.exception('Could   not   connect   to   My   USPS') 
      return False 
   add_devices([USPSSensor(session, config.get(CONF_UPDATE_INTERVAL))])", 'Set up the USPS sensor.','Setup the USPS platform.'
"def task_enable_flocker_control(distribution, action='start'): 
    validate_start_action(action) 
   if is_systemd_distribution(distribution): 
      return sequence([run_from_args(['systemctl', 'enable', 'flocker-control']), run_from_args(['systemctl', action.lower(), 'flocker-control'])]) 
   elif is_ubuntu(distribution): 
      return sequence([put(path='/etc/init/flocker-control.override', content=dedent('                                                            start   on   runlevel   [2345]\n                                                            stop   on   runlevel   [016]\n                                                            ')), run(""echo   'flocker-control-api DCTB 4523/tcp DCTB  DCTB  DCTB #   Flocker   Control   API   port'   >>   /etc/services""), run(""echo   'flocker-control-agent DCTB 4524/tcp DCTB  DCTB  DCTB #   Flocker   Control   Agent   port'   >>   /etc/services""), run_from_args(['service', 'flocker-control', action.lower()])]) 
   else: 
      raise DistributionNotSupported(distribution=distribution)", 'Enables the flocker-control service.',"'Enable flocker-control service. We need to be able to indicate whether 
 we want to start the service, when we are deploying a new cluster, 
 or if we want to restart it, when we are using an existent cluster in 
 managed mode. 
 :param bytes distribution: name of the distribution where the flocker 
 controls currently runs. The supported distros are: 
 - ubuntu-14.04 
 - ubuntu-16.04 
 - centos-<centos version> 
 :param bytes action: action to perform with the flocker control service. 
 Currently, we support: 
 -start 
 -stop 
 :raises ``DistributionNotSupported`` if the ``distribution`` is not 
 currently supported 
 ``UnknownAction`` if the action passed is not a valid one'"
"def get_conn(service='SoftLayer_Virtual_Guest'): 
    client = SoftLayer.Client(username=config.get_cloud_config_value('user', get_configured_provider(), __opts__, search_global=False), api_key=config.get_cloud_config_value('apikey', get_configured_provider(), __opts__, search_global=False)) 
   return client[service]"," 'Returns an instance of the SoftLayer API client for the given service. 
 :param service: The service to get a client for. 
 :type service: str 
 :return: An instance of the SoftLayer API client. 
 :rtype: SoftLayer.api.SoftLayer_Client'",'Return a conn object for the passed VM data'
"def autocorr(s, axis=(-1)): 
    N = s.shape[axis] 
   S = np.fft.fft(s, n=((2 * N) - 1), axis=axis) 
   sxx = np.fft.ifft((S * S.conjugate()), axis=axis).real[:N] 
   return (sxx / N)"," 'Return the autocorrelation of a signal. 
 Parameters 
 s : array_like 
 Signal to be autocorrelated. 
 axis : int, optional 
 Axis along which to compute the autocorrelation. 
 Returns 
 autocorr : array 
 Autocorrelation of the signal. 
 Notes 
 The autocorrelation of a signal is the cross-correlation of the signal 
 with its shifted copy. 
 Examples 
 >>> from scipy.signal import autocorr 
 >>> from scipy import ndimage 
 >>> x = ndimage.uniform_random((20, 20), size=((20, 20), (20, 20))) 
 >>> autocorr(x, axis=(-1)) 
 array([[ 0.28424314,  0.14212157], 
 [ 0.14212157,  0.07106079], 
 [ 0.07106079,  0.","'Returns the autocorrelation of signal s at all lags. Adheres to the 
 definition r(k) = E{s(n)s*(n-k)} where E{} is the expectation operator.'"
"def _has_access_course(user, action, courselike): 
    def can_load(): 
      '\n                        Can   this   user   load   this   course?\n\n                        NOTE:   this   is   not   checking   whether   user   is   actually   enrolled   in   the   course.\n                        ' 
      response = (_visible_to_nonstaff_users(courselike) and _can_access_descriptor_with_start_date(user, courselike, courselike.id)) 
      return (ACCESS_GRANTED if (response or _has_staff_access_to_descriptor(user, courselike, courselike.id)) else response) 
   def can_enroll(): 
      '\n                        Returns   whether   the   user   can   enroll   in   the   course.\n                        ' 
      return _can_enroll_courselike(user, courselike) 
   def see_exists(): 
      ""\n                        Can   see   if   can   enroll,   but   also   if   can   load   it:   if   user   enrolled   in   a   course   and   now\n                        it's   past   the   enrollment   period,   they   should   still   see   it.\n                        "" 
      return (ACCESS_GRANTED if (can_load() or can_enroll()) else ACCESS_DENIED) 
   def can_see_in_catalog(): 
      '\n                        Implements   the   ""can   see   course   in   catalog""   logic   if   a   course   should   be   visible   in   the   main   course   catalog\n                        In   this   case   we   use   the   catalog_visibility   property   on   the   course   descriptor\n                        but   also   allow   course   staff   to   see   this.\n                        ' 
      return (_has_catalog_visibility(courselike, CATALOG_VISIBILITY_CATALOG_AND_ABOUT) or _has_staff_access_to_descriptor(user, courselike, courselike.id)) 
   def can_see_about_page(): 
      '\n                        Implements   the   ""can   see   course   about   page""   logic   if   a   course   about   page   should   be   visible\n                        In   this   case   we   use   the   catalog_visibility   property   on   the   course   descriptor\n                        but   also   allow   course   staff   to   see   this.\n                        ' 
      return (_has_catalog_visibility(courselike, CATALOG_VISIBILITY_CATALOG_AND_ABOUT) or _has_catalog_visibility(courselike, CATALOG_VISIBILITY_ABOUT) or _has_staff_access_to_descriptor(user, courselike, courselike.id)) 
   checkers = {'load': can_load, 'view_courseware_with_prerequisites': (lambda : _can_view_courseware_with_prerequisites(user, courselike)), 'load_mobile': (lambda : (can_load() and _can_load_course_on_mobile(user, courselike))), 'enroll': can_enroll, 'see_exists': see_exists, 'staff': (lambda : _has_staff_access_to_descriptor(user, courselike, courselike.id)), 'instructor': (lambda : _has_instructor_access_to_descriptor(user, courselike, courselike.id)), 'see_in_catalog': can_see_in_catalog, 'see_about_page': can_see_about_page} 
   return _dispatch(checkers, action, user, courselike)"," 'Check whether a user can perform an action on a course. 
 :param user: the user to check 
 :param action: the action to check 
 :param courselike: the course to check 
 :returns: ACCESS_GRANTED, ACCESS_DENIED, or None 
 :rtype: Union[None, str, bool, int, float, list, tuple, dict, NoneType, set, frozenset, GeneratorType, CallableType, GeneratorType, AnyType, AnyType, NoneType, DictType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, AnyType, Any","'Check if user has access to a course. 
 Arguments: 
 user (User): the user whose course access we are checking. 
 action (string): The action that is being checked. 
 courselike (CourseDescriptor or CourseOverview): The object 
 representing the course that the user wants to access. 
 Valid actions: 
 \'load\' -- load the courseware, see inside the course 
 \'load_forum\' -- can load and contribute to the forums (one access level for now) 
 \'load_mobile\' -- can load from a mobile context 
 \'enroll\' -- enroll.  Checks for enrollment window. 
 \'see_exists\' -- can see that the course exists. 
 \'staff\' -- staff access to course. 
 \'see_in_catalog\' -- user is able to see the course listed in the course catalog. 
 \'see_about_page\' -- user is able to see the course about page.'"
"def sequences_add_start_id(sequences, start_id=0, remove_last=False): 
    sequences_out = [[] for _ in range(len(sequences))] 
   for i in range(len(sequences)): 
      if remove_last: 
         sequences_out[i] = ([start_id] + sequences[i][:(-1)]) 
      else: 
         sequences_out[i] = ([start_id] + sequences[i]) 
   return sequences_out"," 'Add a start_id to all sequences in the sequences list. 
 :param sequences: List of sequences 
 :type sequences: list 
 :param start_id: The start id to add to all sequences 
 :type start_id: int 
 :param remove_last: If true, remove the last element from each sequence 
 :type remove_last: bool 
 :return: List of sequences with start_id added 
 :rtype: list'","'Add special start token(id) in the beginning of each sequence. 
 Examples 
 >>> sentences_ids = [[4,3,5,3,2,2,2,2], [5,3,9,4,9,2,2,3]] 
 >>> sentences_ids = sequences_add_start_id(sentences_ids, start_id=2) 
 ... [[2, 4, 3, 5, 3, 2, 2, 2, 2], [2, 5, 3, 9, 4, 9, 2, 2, 3]] 
 >>> sentences_ids = sequences_add_start_id(sentences_ids, start_id=2, remove_last=True) 
 ... [[2, 4, 3, 5, 3, 2, 2, 2], [2, 5, 3, 9, 4, 9, 2, 2]] 
 - For Seq2seq 
 >>> input = [a, b, c] 
 >>> target = [x, y, z] 
 >>> decode_seq = [start_id, a, b] <-- sequences_add_start_id(input, start_id, True)'"
"def test_cnn_sample_wt_fit(): 
    cnn = CondensedNearestNeighbour(random_state=RND_SEED) 
   assert_raises(RuntimeError, cnn.sample, X, Y)", 'Test that sample is not implemented for cnn',"'Test either if an error is raised when sample is called before 
 fitting'"
"def div_proxy(x, y): 
    f = int_or_true_div((as_scalar(x).type in discrete_types), (as_scalar(y).type in discrete_types)) 
   return f(x, y)"," 'Returns a function that returns the result of dividing x and y. 
 The function will return True if x and y are both integers, False if 
 they are both bools, and the result of dividing them if they are 
 both floats.'","'Proxy for either true_div or int_div, depending on types of x, y.'"
"def seteuid(uid): 
    uid = parse_uid(uid) 
   if (uid != os.getuid()): 
      os.seteuid(uid)"," 'Set the effective user id to the given user id. 
 :param uid: the user id to set the effective user id to 
 :type uid: int'",'Set effective user id.'
"def _native_set_to_python_list(typ, payload, c): 
    nitems = payload.used 
   listobj = c.pyapi.list_new(nitems) 
   ok = cgutils.is_not_null(c.builder, listobj) 
   with c.builder.if_then(ok, likely=True): 
      index = cgutils.alloca_once_value(c.builder, ir.Constant(nitems.type, 0)) 
      with payload._iterate() as loop: 
         i = c.builder.load(index) 
         item = loop.entry.key 
         itemobj = c.box(typ.dtype, item) 
         c.pyapi.list_setitem(listobj, i, itemobj) 
         i = c.builder.add(i, ir.Constant(i.type, 1)) 
         c.builder.store(i, index) 
   return (ok, listobj)"," 'Convert a native list to a python list. 
 Parameters 
 typ : type 
 The type of the list. 
 payload : payload.Payload 
 The payload. 
 c : Context 
 The current context. 
 Returns 
 ok : bool 
 True if the conversion succeeded. 
 listobj : object 
 The Python list object. 
 Examples 
 >>> from sympy.utilities.cgutils import _native_set_to_python_list 
 >>> from sympy.utilities.cgutils import _native_set_to_python_list 
 >>> from sympy.utilities.cgutils import _native_set_to_python_list 
 >>> from sympy.utilities.cgutils import _native_set_to_python_list 
 >>> from sympy.utilities.cgutils import _native_set_to_python_list 
 >>> from sympy.utilities.cgutils import _native_set_to_python_list 
 >>> from sympy.utilities.cgutils import _native_set_to_python_list 
 >>> from",'Create a Python list from a native set\'s items.'
"def synopsis(filename, cache={}): 
    mtime = os.stat(filename).st_mtime 
   (lastupdate, result) = cache.get(filename, (0, None)) 
   if (lastupdate < mtime): 
      info = inspect.getmoduleinfo(filename) 
      try: 
         file = open(filename) 
      except IOError: 
         return None 
      if (info and ('b' in info[2])): 
         try: 
            module = imp.load_module('__temp__', file, filename, info[1:]) 
         except: 
            return None 
         result = (module.__doc__ or '').splitlines()[0] 
         del sys.modules['__temp__'] 
      else: 
         result = source_synopsis(file) 
         file.close() 
      cache[filename] = (mtime, result) 
   return result"," 'Return the documentation string for the given module or source file. 
 If the module is a source file, the documentation string is returned 
 as a single string. If the module is a module object, the documentation 
 string is returned as a list of strings.'",'Get the one-line summary out of a module file.'
"def delete_rax_scaling_group(args): 
    print (""---   Cleaning   Autoscale   Groups   matching   '%s'"" % args.match_re) 
   for region in pyrax.identity.services.autoscale.regions: 
      asg = pyrax.connect_to_autoscale(region=region) 
      for group in rax_list_iterator(asg): 
         if re.search(args.match_re, group.name): 
            group.manager._delete = _force_delete_rax_scaling_group(group.manager) 
            prompt_and_delete(group, ('Delete   matching   %s?   [y/n]:   ' % group), args.assumeyes)", 'Delete all autoscale groups matching the given regex','Function for deleting Autoscale Groups'
"def get_default_ddir(): 
    user_home = os.path.expanduser('~') 
   (join, exists) = (os.path.join, os.path.exists) 
   if mswin: 
      return join(user_home, 'Downloads', 'mps') 
   USER_DIRS = join(user_home, '.config', 'user-dirs.dirs') 
   DOWNLOAD_HOME = join(user_home, 'Downloads') 
   if ('XDG_DOWNLOAD_DIR' in os.environ): 
      ddir = os.environ['XDG_DOWNLOAD_DIR'] 
   elif exists(USER_DIRS): 
      lines = open(USER_DIRS).readlines() 
      defn = [x for x in lines if x.startswith('XDG_DOWNLOAD_DIR')] 
      if (len(defn) == 1): 
         ddir = defn[0].split('=')[1].replace('""', '') 
         ddir = ddir.replace('$HOME', user_home).strip() 
      else: 
         ddir = (DOWNLOAD_HOME if exists(DOWNLOAD_HOME) else user_home) 
   else: 
      ddir = (DOWNLOAD_HOME if exists(DOWNLOAD_HOME) else user_home) 
   ddir = ddir 
   return os.path.join(ddir, 'mps')", 'Get the default download directory for mps',"'Get system default Download directory, append mps dir.'"
"def floating_ip_pool_list(call=None): 
    if (call != 'function'): 
      raise SaltCloudSystemExit('The   floating_ip_pool_list   action   must   be   called   with   -f   or   --function') 
   conn = get_conn() 
   return conn.floating_ip_pool_list()", 'List floating IP pools.',"'List all floating IP pools 
 .. versionadded:: 2016.3.0'"
"def delete(table_name, region=None, key=None, keyid=None, profile=None): 
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
   table = Table(table_name, connection=conn) 
   table.delete() 
   MAX_ATTEMPTS = 30 
   for i in range(MAX_ATTEMPTS): 
      if (not exists(table_name, region, key, keyid, profile)): 
         return True 
      else: 
         time.sleep(1) 
   return False"," 'Deletes the table. 
 :param table_name: The name of the table. 
 :param region: The region to use. 
 :param key: The key to use. 
 :param keyid: The key ID to use. 
 :param profile: The profile to use. 
 :return: True if the table was deleted, False otherwise.'","'Delete a DynamoDB table. 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_dynamodb.delete table_name region=us-east-1'"
"def _qualNameWalker(qualName): 
    (yield (qualName, [])) 
   qualParts = qualName.split('.') 
   for index in range(1, len(qualParts)): 
      (yield ('.'.join(qualParts[:(- index)]), qualParts[(- index):]))"," 'Return a generator that walks the given qualified name. 
 :param qualName: The qualified name to walk. 
 :return: A generator that yields the parts of the qualified name. 
 :rtype: generator[tuple[str, list[str]]]'","'Given a Python qualified name, this function yields a 2-tuple of the most 
 specific qualified name first, followed by the next-most-specific qualified 
 name, and so on, paired with the remainder of the qualified name. 
 @param qualName: A Python qualified name. 
 @type qualName: L{str}'"
"@pytest.mark.parametrize('specialchars,   count_char', [('   abcde               ', '   '), ('   aaaaaaaaaa', 'a'), ('\xc4\x81\xc3\xa9\xc4\xa9\xc3\xb8\xc3\xb8\xc3\xb8\xc3\xb8\xc3\xb8\xc3\xb8\xc3\xbc', u'\xf8')]) 
 @pytest.mark.django_db 
 def test_clean_specialchars_unique(specialchars, count_char): 
    form_data = {'code': 'foo', 'fullname': 'Foo', 'checkstyle': 'foo', 'nplurals': '2', 'specialchars': specialchars} 
   form = LanguageForm(form_data) 
   assert form.is_valid() 
   assert (form.cleaned_data['specialchars'].count(count_char) == 1)", 'Make sure specialchars is unique','Tests special characters are unique.'
"def present(name, attributes=None, region=None, key=None, keyid=None, profile=None): 
    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}} 
   is_present = __salt__['boto_sqs.exists'](name, region, key, keyid, profile) 
   if (not is_present): 
      if __opts__['test']: 
         msg = 'AWS   SQS   queue   {0}   is   set   to   be   created.'.format(name) 
         ret['comment'] = msg 
         ret['result'] = None 
         return ret 
      created = __salt__['boto_sqs.create'](name, region, key, keyid, profile) 
      if created: 
         ret['changes']['old'] = None 
         ret['changes']['new'] = {'queue': name} 
      else: 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   create   {0}   AWS   queue'.format(name) 
         return ret 
   else: 
      ret['comment'] = '{0}   present.'.format(name) 
   attrs_to_set = {} 
   _attributes = __salt__['boto_sqs.get_attributes'](name, region, key, keyid, profile) 
   if attributes: 
      for (attr, val) in six.iteritems(attributes): 
         _val = _attributes.get(attr, None) 
         if (attr == 'Policy'): 
            if isinstance(_val, six.string_types): 
               _val = json.loads(_val) 
            if isinstance(val, six.string_types): 
               val = json.loads(val) 
            if (_val != val): 
               log.debug('Policies   differ:\n{0}\n{1}'.format(_val, val)) 
               attrs_to_set[attr] = json.dumps(val, sort_keys=True) 
         elif (str(_val) != str(val)): 
            log.debug('Attributes   differ:\n{0}\n{1}'.format(_val, val)) 
            attrs_to_set[attr] = val 
   attr_names = ','.join(attrs_to_set) 
   if attrs_to_set: 
      if __opts__['test']: 
         ret['comment'] = 'Attribute(s)   {0}   to   be   set   on   {1}.'.format(attr_names, name) 
         ret['result'] = None 
         return ret 
      msg = '   Setting   {0}   attribute(s).'.format(attr_names) 
      ret['comment'] = (ret['comment'] + msg) 
      if ('new' in ret['changes']): 
         ret['changes']['new']['attributes_set'] = [] 
      else: 
         ret['changes']['new'] = {'attributes_set': []} 
      for (attr, val) in six.iteritems(attrs_to_set): 
         set_attr = __salt__['boto_sqs.set_attributes'](name, {attr: val}, region, key, keyid, profile) 
         if (not set_attr): 
            ret['result'] = False 
         msg = 'Set   attribute   {0}.'.format(attr) 
         ret['changes']['new']['attributes_set'].append(attr) 
   else: 
      ret['comment'] = (ret['comment'] + '   Attributes   set.') 
   return ret"," 'Set attributes on an SQS queue. 
 Returns a dictionary of the form: 
 { 
 \'name\': <queue name>, 
 \'result\': <True or False>, 
 \'comment\': <comment>, 
 \'changes\': { 
 \'old\': <old attributes>, 
 \'new\': { 
 \'queue\': <queue name>, 
 \'attributes_set\': <list of attribute names> 
 } 
 } 
 \'old\' and \'new\' are only present if the queue has been changed. 
 \'attributes_set\' is only present if the queue has been changed and 
 attributes have been set. 
 If \'test\' is set, then the function will return a dictionary with 
 \'result\' set to None, and \'comment\' set to a message indicating 
 whether or not the queue was created. 
 If \'test\' is not set, then the function will return a dictionary with 
 \'result\' set to True or False, and \'comment\' set to a message 
 indicating whether or not the queue","'Ensure the SQS queue exists. 
 name 
 Name of the SQS queue. 
 attributes 
 A dict of key/value SQS attributes. 
 region 
 Region to connect to. 
 key 
 Secret key to be used. 
 keyid 
 Access key to be used. 
 profile 
 A dict with region, key and keyid, or a pillar key (string) 
 that contains a dict with region, key and keyid.'"
"def _sqrt_match(p): 
    from sympy.simplify.radsimp import split_surds 
   p = _mexpand(p) 
   if p.is_Number: 
      res = (p, S.Zero, S.Zero) 
   elif p.is_Add: 
      pargs = sorted(p.args, key=default_sort_key) 
      if all(((x ** 2).is_Rational for x in pargs)): 
         (r, b, a) = split_surds(p) 
         res = (a, b, r) 
         return list(res) 
      v = [(sqrt_depth(x), x, i) for (i, x) in enumerate(pargs)] 
      nmax = max(v, key=default_sort_key) 
      if (nmax[0] == 0): 
         res = [] 
      else: 
         (depth, _, i) = nmax 
         r = pargs.pop(i) 
         v.pop(i) 
         b = S.One 
         if r.is_Mul: 
            bv = [] 
            rv = [] 
            for x in r.args: 
               if (sqrt_depth(x) < depth): 
                  bv.append(x) 
               else: 
                  rv.append(x) 
            b = Mul._from_args(bv) 
            r = Mul._from_args(rv) 
         a1 = [] 
         b1 = [b] 
         for x in v: 
            if (x[0] < depth): 
               a1.append(x[1]) 
            else: 
               x1 = x[1] 
               if (x1 == r): 
                  b1.append(1) 
               elif x1.is_Mul: 
                  x1args = list(x1.args) 
                  if (r in x1args): 
                     x1args.remove(r) 
                     b1.append(Mul(*x1args)) 
                  else: 
                     a1.append(x[1]) 
               else: 
                  a1.append(x[1]) 
         a = Add(*a1) 
         b = Add(*b1) 
         res = (a, b, (r ** 2)) 
   else: 
      (b, r) = p.as_coeff_Mul() 
      if is_sqrt(r): 
         res = (S.Zero, b, (r ** 2)) 
      else: 
         res = [] 
   return list(res)"," 'Return the roots of a polynomial, in the form (a, b, r). 
 Examples 
 >>> from sympy.polys.domains import QQ 
 >>> from sympy.polys.rationaltools import _sqrt_match 
 >>> _sqrt_match(QQ(1)) 
 (1, 0, 0) 
 >>> _sqrt_match(QQ(1) + QQ(2)) 
 (1, 1, 1) 
 >>> _sqrt_match(QQ(1) - QQ(2)) 
 (1, -1, 1) 
 >>> _sqrt_match(QQ(1) + QQ(2) + QQ(3)) 
 (1, 0, 1) 
 >>> _sqrt_match(QQ(1) - QQ(2) - QQ(3)) 
 (1, 0, 1) 
 >>> _sqrt_match(QQ(1) + QQ(2) - QQ(3)) 
 (1, 0, 1) 
 >>> _sqrt_match(Q","'Return [a, b, r] for p.match(a + b*sqrt(r)) where, in addition to 
 matching, sqrt(r) also has then maximal sqrt_depth among addends of p. 
 Examples 
 >>> from sympy.functions.elementary.miscellaneous import sqrt 
 >>> from sympy.simplify.sqrtdenest import _sqrt_match 
 >>> _sqrt_match(1 + sqrt(2) + sqrt(2)*sqrt(3) +  2*sqrt(1+sqrt(5))) 
 [1 + sqrt(2) + sqrt(6), 2, 1 + sqrt(5)]'"
"@treeio_login_required 
 @handle_response_format 
 def event_edit(request, event_id, response_format='html'): 
    event = get_object_or_404(Event, pk=event_id) 
   if (not request.user.profile.has_permission(event, mode='w')): 
      return user_denied(request, message=""You   don't   have   access   to   this   Event"") 
   if request.POST: 
      if ('cancel' not in request.POST): 
         form = EventForm(request.user.profile, None, None, request.POST, instance=event) 
         if form.is_valid(): 
            event = form.save() 
            return HttpResponseRedirect(reverse('events_event_view', args=[event.id])) 
      else: 
         return HttpResponseRedirect(reverse('events')) 
   else: 
      form = EventForm(request.user.profile, instance=event) 
   return render_to_response('events/event_edit', {'event': event, 'form': form}, context_instance=RequestContext(request), response_format=response_format)", 'Edit an event','Event edit'
"def is_media_request(request): 
    parsed_media_url = urlparse(settings.MEDIA_URL) 
   if request.path_info.startswith(parsed_media_url.path): 
      if parsed_media_url.netloc: 
         if (request.get_host() == parsed_media_url.netloc): 
            return True 
      else: 
         return True 
   return False", 'Check if the given request is a media request.','Check if a request is a media request.'
"def find_subnets(vpc_conn, vpc_id, identified_subnets): 
    subnet_ids = [] 
   subnet_names = [] 
   subnet_cidrs = [] 
   for subnet in (identified_subnets or []): 
      if re.match(SUBNET_RE, subnet): 
         subnet_ids.append(subnet) 
      elif re.match(CIDR_RE, subnet): 
         subnet_cidrs.append(subnet) 
      else: 
         subnet_names.append(subnet) 
   subnets_by_id = [] 
   if subnet_ids: 
      subnets_by_id = vpc_conn.get_all_subnets(subnet_ids, filters={'vpc_id': vpc_id}) 
      for subnet_id in subnet_ids: 
         if (not any(((s.id == subnet_id) for s in subnets_by_id))): 
            raise AnsibleSubnetSearchException('Subnet   ID   ""{0}""   does   not   exist'.format(subnet_id)) 
   subnets_by_cidr = [] 
   if subnet_cidrs: 
      subnets_by_cidr = vpc_conn.get_all_subnets(filters={'vpc_id': vpc_id, 'cidr': subnet_cidrs}) 
      for cidr in subnet_cidrs: 
         if (not any(((s.cidr_block == cidr) for s in subnets_by_cidr))): 
            raise AnsibleSubnetSearchException('Subnet   CIDR   ""{0}""   does   not   exist'.format(cidr)) 
   subnets_by_name = [] 
   if subnet_names: 
      subnets_by_name = vpc_conn.get_all_subnets(filters={'vpc_id': vpc_id, 'tag:Name': subnet_names}) 
      for name in subnet_names: 
         matching_count = len([1 for s in subnets_by_name if (s.tags.get('Name') == name)]) 
         if (matching_count == 0): 
            raise AnsibleSubnetSearchException('Subnet   named   ""{0}""   does   not   exist'.format(name)) 
         elif (matching_count > 1): 
            raise AnsibleSubnetSearchException('Multiple   subnets   named   ""{0}""'.format(name)) 
   return ((subnets_by_id + subnets_by_cidr) + subnets_by_name)"," 'Find subnets by their IDs, CIDRs or names. 
 :param vpc_conn: The connection to the VPC API. 
 :param vpc_id: The VPC ID to search for subnets in. 
 :param identified_subnets: A list of subnets to search for. 
 :return: A list of subnet IDs, CIDRs, and names.'","'Finds a list of subnets, each identified either by a raw ID, a unique 
 \'Name\' tag, or a CIDR such as 10.0.0.0/8. 
 Note that this function is duplicated in other ec2 modules, and should 
 potentially be moved into potentially be moved into a shared module_utils'"
"def test_sigma_clipped_stats(): 
    data = [0, 1] 
   mask = np.array([True, False]) 
   result = sigma_clipped_stats(data, mask=mask) 
   assert isinstance(result[1], float) 
   assert (result == (1.0, 1.0, 0.0)) 
   result = sigma_clipped_stats(data, mask_value=0.0) 
   assert isinstance(result[1], float) 
   assert (result == (1.0, 1.0, 0.0)) 
   data = [0, 2] 
   result = sigma_clipped_stats(data) 
   assert isinstance(result[1], float) 
   assert (result == (1.0, 1.0, 1.0)) 
   _data = np.arange(10) 
   data = np.ma.MaskedArray([_data, _data, (10 * _data)]) 
   mean = sigma_clip(data, axis=0, sigma=1).mean(axis=0) 
   assert_equal(mean, _data) 
   (mean, median, stddev) = sigma_clipped_stats(data, axis=0, sigma=1) 
   assert_equal(mean, _data) 
   assert_equal(median, _data) 
   assert_equal(stddev, np.zeros_like(_data))", 'Test sigma clipped stats','Test list data with input mask or mask_value (#3268).'
"def test_read_bin_lush_matrix_ubyte_scalar(): 
    path = (example_bin_lush_path + 'ubyte_scalar.lushbin') 
   result = read_bin_lush_matrix(path) 
   assert (str(result.dtype) == 'uint8') 
   assert (len(result.shape) == 3) 
   assert (result.shape[0] == 1) 
   assert (result.shape[1] == 1) 
   assert (result.shape[1] == 1) 
   assert (result[(0, 0)] == 12)", 'Read a binary lush matrix with ubyte scalar',"'Read data from a lush file with uint8 data (scalar). 
 Note: When you write a scalar from Koray\'s matlab code it always makes 
 everything 3D. Writing it straight from lush you might be able to get 
 a true scalar'"
"def data_path(path): 
    return (path if isabs(path) else join(project_data_dir(), path))"," 'Returns the absolute path of a file in the project data directory. 
 If path is an absolute path, it is returned unchanged. 
 If path is a relative path, it is resolved to the project data directory. 
 If path is an empty string, the current directory is returned.'","'If path is relative, return the given path inside the project data dir, 
 otherwise return the path unmodified'"
"def add_interface(zone, interface, permanent=True): 
    if (interface in get_interfaces(zone, permanent)): 
      log.info('Interface   is   already   bound   to   zone.') 
   cmd = '--zone={0}   --add-interface={1}'.format(zone, interface) 
   if permanent: 
      cmd += '   --permanent' 
   return __firewall_cmd(cmd)"," 'Add an interface to a zone. 
 :param zone: Zone to add the interface to. 
 :param interface: Interface to add. 
 :param permanent: Whether to add the interface permanently. 
 :returns: A tuple containing the return code and output of the 
 command.'","'Bind an interface to a zone 
 .. versionadded:: 2016.3.0 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' firewalld.add_interface zone eth0'"
"def dylib_info(filename): 
    is_dylib = DYLIB_RE.match(filename) 
   if (not is_dylib): 
      return None 
   return is_dylib.groupdict()"," 'Returns the information about a dylib file. 
 :param filename: Name of the dylib file. 
 :type filename: str 
 :return: Information about the dylib file. 
 :rtype: dict'","'A dylib name can take one of the following four forms: 
 Location/Name.SomeVersion_Suffix.dylib 
 Location/Name.SomeVersion.dylib 
 Location/Name_Suffix.dylib 
 Location/Name.dylib 
 returns None if not found or a mapping equivalent to: 
 dict( 
 location=\'Location\', 
 name=\'Name.SomeVersion_Suffix.dylib\', 
 shortname=\'Name\', 
 version=\'SomeVersion\', 
 suffix=\'Suffix\', 
 Note that SomeVersion and Suffix are optional and may be None 
 if not present.'"
"def finite_diff_kauers(sum): 
    function = sum.function 
   for l in sum.limits: 
      function = function.subs(l[0], (l[(-1)] + 1)) 
   return function"," 'Returns the finite difference of the sum. 
 The finite difference of the sum is defined as 
 .. math:: 
 f(x) - f(x+h) 
 where :math:`h` is a small positive step size. 
 The function is evaluated at the endpoints of the sum\'s limits. 
 Examples 
 >>> from sympy.abc import x, y 
 >>> from sympy import Sum, finite_diff_kauers 
 >>> Sum(x*y).subs(x, 2).subs(y, 3).doit() 
 12 
 >>> finite_diff_kauers(Sum(x*y).subs(x, 2).subs(y, 3)) 
 6 
 >>> Sum(x*y).subs(x, 2).subs(y, 3).doit() 
 12 
 >>> finite_diff_kauers(Sum(x*y).subs(x, 2).subs(y, 3)) 
 6'","'Takes as input a Sum instance and returns the difference between the sum 
 with the upper index incremented by 1 and the original sum. For example, 
 if S(n) is a sum, then finite_diff_kauers will return S(n + 1) - S(n). 
 Examples 
 >>> from sympy.series.kauers import finite_diff_kauers 
 >>> from sympy import Sum 
 >>> from sympy.abc import x, y, m, n, k 
 >>> finite_diff_kauers(Sum(k, (k, 1, n))) 
 n + 1 
 >>> finite_diff_kauers(Sum(1/k, (k, 1, n))) 
 1/(n + 1) 
 >>> finite_diff_kauers(Sum((x*y**2), (x, 1, n), (y, 1, m))) 
 (m + 1)**2*(n + 1) 
 >>> finite_diff_kauers(Sum((x*y), (x, 1, m), (y, 1, n))) 
 (m + 1)*(n + 1)'"
"def get_disk_list(std_mounts_only=True, get_all_disks=False): 
    mounts = utils.system_output('mount').splitlines() 
   hd_list = [] 
   hd_regexp = re.compile('([hsv]d[a-z]+3)$') 
   partfile = open(_DISKPART_FILE) 
   for partline in partfile: 
      parts = partline.strip().split() 
      if ((len(parts) != 4) or partline.startswith('major')): 
         continue 
      partname = parts[3] 
      if (not get_all_disks): 
         if (not partname[(-1):].isdigit()): 
            continue 
      if (not fd_mgr.use_partition(partname)): 
         continue 
      tunepath = fd_mgr.map_drive_name(partname) 
      mstat = 0 
      fstype = '' 
      fsopts = '' 
      fsmkfs = '?' 
      chkdev = ('/dev/' + partname) 
      mountpt = None 
      for mln in mounts: 
         splt = mln.split() 
         if (splt[0].strip() == chkdev.strip()): 
            mountpt = fd_mgr.check_mount_point(partname, splt[2]) 
            if (not mountpt): 
               mstat = (-1) 
               break 
            fstype = splt[4] 
            fsopts = splt[5] 
            if (fsopts[:3] != '(rw'): 
               mstat = (-1) 
               break 
            mstat = 1 
      if (std_mounts_only and (mstat < 0)): 
         continue 
      if (not get_all_disks): 
         if (not mountpt): 
            mountpt = fd_mgr.check_mount_point(partname, None) 
            if (not mountpt): 
               continue 
      hd_list.append({'device': partname, 'mountpt': mountpt, 'tunable': tunepath, 'fs_type': fstype, 'fs_opts': fsopts, 'fs_mkfs': fsmkfs, 'mounted': mstat}) 
   return hd_list"," 'Return a list of disk devices that are mounted and their mount points. 
 :param std_mounts_only: only return mounted disks 
 :param get_all_disks: return all disks (mounted or not) 
 :return: a list of disk devices and their mount points'","'Get a list of dictionaries with information about disks on this system. 
 :param std_mounts_only: Whether the function should return only disks that 
 have a mount point defined (True) or even devices that doesn\'t 
 (False). 
 :param get_all_disks: Whether the function should return only partitioned 
 disks (False) or return every disk, regardless of being partitioned 
 or not (True). 
 :return: List of dictionaries with disk information (see more below). 
 The \'disk_list\' array returned by get_disk_list() has an entry for each 
 disk drive we find on the box. Each of these entries is a map with the 
 following 3 string values: 
 \'device\'      disk device name (i.e. the part after /dev/) 
 \'mountpt\'     disk mount path 
 \'tunable\'     disk name for setting scheduler tunables (/sys/block/sd??) 
 The last value is an integer that indicates the current mount status 
 of the drive: 
 \'mounted\'     0 = not currently mounted 
 1 = mounted r/w on the expected path 
 -1 = mounted readonly or at an unexpected path 
 When the \'std_mounts_only\' argument is True we don\'t include drives 
 mounted on \'unusual\' mount points in the result. If a given device is 
 partitioned, it will return all partitions that exist on it. If it\'s not, 
 it will return the device itself (ie, if there are /dev/sdb1 and /dev/sdb2, 
 those will be returned but not /dev/sdb. if there is only a /dev/sdc, that 
 one will be returned).'"
"def no_afni(): 
    if (Info.version() is None): 
      return True 
   return False", 'Returns true if afni is not installed.','Checks if AFNI is available'
"def initialize(cli): 
    cli.register('building-command-table.main', change_name) 
   cli.register('building-command-table.deploy', inject_commands) 
   cli.register('building-argument-table.deploy.get-application-revision', modify_revision_arguments) 
   cli.register('building-argument-table.deploy.register-application-revision', modify_revision_arguments) 
   cli.register('building-argument-table.deploy.create-deployment', modify_revision_arguments)", 'Adds the deploy commands to the CLI.','The entry point for CodeDeploy high level commands.'
"def seed(seed=None): 
    get_random_state().seed(seed)"," 'Sets the random state. 
 Parameters 
 seed : int or None 
 Seed for the random number generator. 
 Returns 
 None'","'Resets the state of the random number generator with a seed. 
 This function resets the state of the global random number generator for 
 the current device. Be careful that generators for other devices are not 
 affected. 
 Args: 
 seed (None or int): Seed for the random number generator. If ``None``, 
 it uses :func:`os.urandom` if available or :func:`time.clock` 
 otherwise. Note that this function does not support seeding by an 
 integer array.'"
"def check_paths(): 
    if (not hasattr(settings, 'PEP_REPO_PATH')): 
      raise ImproperlyConfigured('No   PEP_REPO_PATH   in   settings') 
   if (not os.path.exists(settings.PEP_REPO_PATH)): 
      raise ImproperlyConfigured('PEP_REPO_PATH   in   settings   does   not   exist')", 'Check that PEP_REPO_PATH exists and is a directory.','Checks to ensure our PEP_REPO_PATH is setup correctly'
"def confirm(message=u'Confirm   (y   or   n)   '): 
    assert isinstance(message, text_type) 
   app = create_confirm_application(message) 
   return run_application(app)"," 'Confirm a message with a yes/no question. 
 :param message: The message to confirm. 
 :type message: str 
 :return: True if the user confirms, False otherwise.'",'Display a confirmation prompt.'
"def dictfind(dictionary, element): 
    for (key, value) in dictionary.iteritems(): 
      if (element is value): 
         return key"," 'Finds an element in a dictionary by value. 
 :param dictionary: Dictionary to search. 
 :param element: Element to find. 
 :returns: Key of the element if found, otherwise None.'","'Returns a key whose value in `dictionary` is `element` 
 or, if none exists, None. 
 >>> d = {1:2, 3:4} 
 >>> dictfind(d, 4) 
 3 
 >>> dictfind(d, 5)'"
"def main_loop(options, modules, sender, tags): 
    next_heartbeat = int((time.time() + 600)) 
   while ALIVE: 
      populate_collectors(options.cdir) 
      reload_changed_config_modules(modules, options, sender, tags) 
      reap_children() 
      check_children(options) 
      spawn_children() 
      time.sleep(15) 
      now = int(time.time()) 
      if (now >= next_heartbeat): 
         LOG.info(('Heartbeat   (%d   collectors   running)' % sum((1 for col in all_living_collectors())))) 
         next_heartbeat = (now + 600)"," 'Main loop for collectd. 
 This function is called once per second.  It is responsible for 
 populating the collectd configuration, reloading any changed 
 configuration modules, reaping and spawning children, and checking 
 and reloading children. 
 :param options: Options object. 
 :param modules: Modules to reload. 
 :param sender: Sender to use for sending events. 
 :param tags: Tags to use for sending events. 
 :return: None. 
 :rtype: None'",'The main loop of the program that runs when we\'re not in stdin mode.'
"def uninstall(): 
    global _task 
   _task.stop() 
   _task = None", 'Uninstall the task.',"'Remove the root Tk widget from the reactor. 
 Call this before destroy()ing the root widget.'"
"def _compute_mi(x, y, x_discrete, y_discrete, n_neighbors=3): 
    if (x_discrete and y_discrete): 
      return mutual_info_score(x, y) 
   elif (x_discrete and (not y_discrete)): 
      return _compute_mi_cd(y, x, n_neighbors) 
   elif ((not x_discrete) and y_discrete): 
      return _compute_mi_cd(x, y, n_neighbors) 
   else: 
      return _compute_mi_cc(x, y, n_neighbors)"," 'Computes the mutual information between two variables. 
 Parameters 
 x : numpy.ndarray 
 The first variable. 
 y : numpy.ndarray 
 The second variable. 
 x_discrete : bool 
 If True, x is a discrete variable. 
 y_discrete : bool 
 If True, y is a discrete variable. 
 n_neighbors : int 
 The number of neighbors used for the computation. 
 Returns 
 The mutual information between x and y. 
 Notes 
 This function uses the implementation from the scikit-learn 
 library. 
 References 
 .. [1] https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/pairwise.py#L430 
 Examples 
 >>> from sklearn.metrics import pairwise_distances 
 >>> from sklearn.metrics import _compute_mi 
 >>> x = np.array([[0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2,","'Compute mutual information between two variables. 
 This is a simple wrapper which selects a proper function to call based on 
 whether `x` and `y` are discrete or not.'"
"def _check_bem_size(surfs): 
    if (surfs[0]['np'] > 10000): 
      msg = ('The   bem   surface   has   %s   data   points.   5120   (ico   grade=4)   should   be   enough.' % surfs[0]['np']) 
      if (len(surfs) == 3): 
         msg += '   Dense   3-layer   bems   may   not   save   properly.' 
      warn(msg)"," 'Check if the BEM surface has too many data points. 
 If the surface has too many points, warn the user. 
 Parameters 
 surfs : list 
 The surfaces to check. 
 Returns 
 None 
 Notes 
 This is a heuristic check, and may not be very accurate.'",'Helper for checking bem surface sizes.'
"def url_name_for_block(block): 
    return block.location.name", 'Returns the name of the block that is being edited.',"'Given a block, returns the block\'s URL name. 
 Arguments: 
 block (XModuleMixin|CourseOverview|BlockStructureBlockData): 
 Block that is being accessed'"
"def storify(f, *requireds, **defaults): 
    stor = Storage() 
   for k in (requireds + tuple(f.keys())): 
      v = f[k] 
      if isinstance(v, list): 
         v = v[(-1)] 
      if hasattr(v, 'value'): 
         v = v.value 
      setattr(stor, k, v) 
   for (k, v) in defaults.iteritems(): 
      result = v 
      if hasattr(stor, k): 
         result = stor[k] 
      if ((v == ()) and (not isinstance(result, tuple))): 
         result = (result,) 
      setattr(stor, k, result) 
   return stor"," 'A function that takes a function that takes a Storage instance 
 and returns a Storage instance. 
 The function must be a function that takes a Storage instance and 
 returns a Storage instance. 
 The returned Storage instance will have all of the keys and values 
 that were in the original Storage instance, plus any values that 
 were added via the function. 
 The function may also add new keys and values to the Storage instance. 
 The function may also change the values of the keys it adds. 
 This is useful for creating a Storage instance that has some values 
 that are specific to the function, but also has some values that 
 are common to all Storage instances created by the function. 
 :param f: A function that takes a Storage instance and returns a 
 Storage instance 
 :param requireds: A list of keys that must be present in the Storage 
 instance returned by f 
 :param defaults: A dictionary of keys that must be present in the 
 Storage instance returned by f, with default values for the keys 
 :return: A Storage instance that has all of the keys and values that 
 were in the original Storage instance, plus any values that were ","'Creates a `storage` object from dictionary d, raising `KeyError` if 
 d doesn\'t have all of the keys in `requireds` and using the default 
 values for keys found in `defaults`. 
 For example, `storify({\'a\':1, \'c\':3}, b=2, c=0)` will return the equivalent of 
 `storage({\'a\':1, \'b\':2, \'c\':3})`.'"
"def login(session, *args, **kwargs): 
    if ((not session.logged_in) and ('name' in kwargs) and ('password' in kwargs)): 
      from evennia.commands.default.unloggedin import create_normal_player 
      player = create_normal_player(session, kwargs['name'], kwargs['password']) 
      if player: 
         session.sessionhandler.login(session, player)", 'Logs a player in to the game.',"'Peform a login. This only works if session is currently not logged 
 in. This will also automatically throttle too quick attempts. 
 Kwargs: 
 name (str): Player name 
 password (str): Plain-text password'"
"def double_coset_can_rep(dummies, sym, b_S, sgens, S_transversals, g): 
    size = g.size 
   g = g.array_form 
   num_dummies = (size - 2) 
   indices = list(range(num_dummies)) 
   all_metrics_with_sym = all([(_ is not None) for _ in sym]) 
   num_types = len(sym) 
   dumx = dummies[:] 
   dumx_flat = [] 
   for dx in dumx: 
      dumx_flat.extend(dx) 
   b_S = b_S[:] 
   sgensx = [h._array_form for h in sgens] 
   if b_S: 
      S_transversals = transversal2coset(size, b_S, S_transversals) 
   dsgsx = [] 
   for i in range(num_types): 
      dsgsx.extend(dummy_sgs(dumx[i], sym[i], num_dummies)) 
   ginv = _af_invert(g) 
   idn = list(range(size)) 
   TAB = [(idn, idn, g)] 
   for i in range((size - 2)): 
      b = i 
      testb = ((b in b_S) and sgensx) 
      if testb: 
         sgensx1 = [_af_new(_) for _ in sgensx] 
         deltab = _orbit(size, sgensx1, b) 
      else: 
         deltab = {b} 
      if all_metrics_with_sym: 
         md = _min_dummies(dumx, sym, indices) 
      else: 
         md = [min(_orbit(size, [_af_new(ddx) for ddx in dsgsx], ii)) for ii in range((size - 2))] 
      p_i = min([min([md[h[x]] for x in deltab]) for (s, d, h) in TAB]) 
      dsgsx1 = [_af_new(_) for _ in dsgsx] 
      Dxtrav = (_orbit_transversal(size, dsgsx1, p_i, False, af=True) if dsgsx else None) 
      if Dxtrav: 
         Dxtrav = [_af_invert(x) for x in Dxtrav] 
      for ii in range(num_types): 
         if (p_i in dumx[ii]): 
            if (sym[ii] is not None): 
               deltap = dumx[ii] 
            else: 
               p_i_index = (dumx[ii].index(p_i) % 2) 
               deltap = dumx[ii][p_i_index::2] 
            break 
      else: 
         deltap = [p_i] 
      TAB1 = [] 
      nTAB = len(TAB) 
      while TAB: 
         (s, d, h) = TAB.pop() 
         if (min([md[h[x]] for x in deltab]) != p_i): 
            continue 
         deltab1 = [x for x in deltab if (md[h[x]] == p_i)] 
         dg = _af_rmul(d, g) 
         dginv = _af_invert(dg) 
         sdeltab = [s[x] for x in deltab1] 
         gdeltap = [dginv[x] for x in deltap] 
         NEXT = [x for x in sdeltab if (x in gdeltap)] 
         for j in NEXT: 
            if testb: 
               s1 = _trace_S(s, j, b, S_transversals) 
               if (not s1): 
                  continue 
               else: 
                  s1 = [s[ix] for ix in s1] 
            else: 
               s1 = s 
            if Dxtrav: 
               d1 = _trace_D(dg[j], p_i, Dxtrav) 
               if (not d1): 
                  continue 
            else: 
               if (p_i != dg[j]): 
                  continue 
               d1 = idn 
            assert (d1[dg[j]] == p_i) 
            d1 = [d1[ix] for ix in d] 
            h1 = [d1[g[ix]] for ix in s1] 
            TAB1.append((s1, d1, h1)) 
      TAB1.sort(key=(lambda x: x[(-1)])) 
      nTAB1 = len(TAB1) 
      prev = ([0] * size) 
      while TAB1: 
         (s, d, h) = TAB1.pop() 
         if (h[:(-2)] == prev[:(-2)]): 
            if (h[(-1)] != prev[(-1)]): 
               return 0 
         else: 
            TAB.append((s, d, h)) 
         prev = h 
      sgensx = [h for h in sgensx if (h[b] == b)] 
      if (b in b_S): 
         b_S.remove(b) 
      _dumx_remove(dumx, dumx_flat, p_i) 
      dsgsx = [] 
      for i in range(num_types): 
         dsgsx.extend(dummy_sgs(dumx[i], sym[i], num_dummies)) 
   return TAB[0][(-1)]"," 'Check if the double coset can be represented as a product of 
 transversals and s-generators. 
 Parameters 
 dummies : list of list 
 A list of lists of dummies, each list representing a different type. 
 sym : list 
 A list of symbols. 
 b_S : list 
 A list of elements of the basis. 
 sgens : list 
 A list of s-generators. 
 S_transversals : list 
 A list of transversals. 
 g : list 
 A list of generators. 
 Returns 
 A list of transversals and s-generators. 
 Notes 
 The list of transversals and s-generators is sorted by the order of 
 the elements of the list of dummies. 
 Examples 
 >>> from sympy.combinatorics.cosets import double_coset_can_rep 
 >>> from sympy.combinatorics.transversal import transversal2coset 
 >>> from sympy.combinatorics.s_generator import dummy_sgs 
 >>> d","'Butler-Portugal algorithm for tensor canonicalization with dummy indices 
 dummies 
 list of lists of dummy indices, 
 one list for each type of index; 
 the dummy indices are put in order contravariant, covariant 
 [d0, -d0, d1, -d1, ...]. 
 sym 
 list of the symmetries of the index metric for each type. 
 possible symmetries of the metrics 
 * 0     symmetric 
 * 1     antisymmetric 
 * None  no symmetry 
 b_S 
 base of a minimal slot symmetry BSGS. 
 sgens 
 generators of the slot symmetry BSGS. 
 S_transversals 
 transversals for the slot BSGS. 
 g 
 permutation representing the tensor. 
 Return 0 if the tensor is zero, else return the array form of 
 the permutation representing the canonical form of the tensor. 
 A tensor with dummy indices can be represented in a number 
 of equivalent ways which typically grows exponentially with 
 the number of indices. To be able to establish if two tensors 
 with many indices are equal becomes computationally very slow 
 in absence of an efficient algorithm. 
 The Butler-Portugal algorithm [3] is an efficient algorithm to 
 put tensors in canonical form, solving the above problem. 
 Portugal observed that a tensor can be represented by a permutation, 
 and that the class of tensors equivalent to it under slot and dummy 
 symmetries is equivalent to the double coset `D*g*S` 
 (Note: in this documentation we use the conventions for multiplication 
 of permutations p, q with (p*q)(i) = p[q[i]] which is opposite 
 to the one used in the Permutation class) 
 Using the algorithm by Butler to find a representative of the 
 double coset one can find a canonical form for the tensor. 
 To see this correspondence, 
 let `g` be a permutation in array form; a tensor with indices `ind` 
 (the indices including both the contravariant and the covariant ones) 
 can be written as 
 `t = T(ind[g[0],..., ind[g[n-1]])`, 
 where `n= len(ind)`; 
 `g` has size `n + 2`, the last two indices for the sign of the tensor 
 (trick introduced in [4]). 
 A slot symmetry transformation `s` is a permutation acting on the slots 
 `t -> T(ind[(g*s)[0]],..., ind[(g*s)[n-1]])` 
 A dummy symmetry transformation acts on `ind` 
 `t -> T(ind[(d*g)[0]],..., ind[(d*g)[n-1]])` 
 Being interested only in the transformations of the tensor under 
 these symmetries, one can represent the tensor by `g`, which transforms 
 as 
 `g -> d*g*s`, so it belongs to the coset `D*g*S`. 
 Let us explain the conventions by an example. 
 Given a tensor `T^{d3 d2 d1}{}_{d1 d2 d3}` with the slot symmetries 
 `T^{a0 a1 a2 a3 a4 a5} = -T^{a2 a1 a0 a3 a4 a5}` 
 `T^{a0 a1 a2 a3 a4 a5} = -T^{a4 a1 a2 a3 a0 a5}` 
 and symmetric metric, find the tensor equivalent to it which 
 is the lowest under the ordering of indices: 
 lexicographic ordering `d1, d2, d3` then and contravariant index 
 before covariant index; that is the canonical form of the tensor. 
 The canonical form is `-T^{d1 d2 d3}{}_{d1 d2 d3}` 
 obtained using `T^{a0 a1 a2 a3 a4 a5} = -T^{a2 a1 a0 a3 a4 a5}`. 
 To convert this problem in the input for this function, 
 use the following labelling of the index names 
 (- for covariant for short) `d1, -d1, d2, -d2, d3, -d3` 
 `T^{d3 d2 d1}{}_{d1 d2 d3}` corresponds to `g = [4, 2, 0, 1, 3, 5, 6, 7]` 
 where the last two indices are for the sign 
 `sgens = [Permutation(0, 2)(6, 7), Permutation(0, 4)(6, 7)]` 
 sgens[0] is the slot symmetry `-(0, 2)` 
 `T^{a0 a1 a2 a3 a4 a5} = -T^{a2 a1 a0 a3 a4 a5}` 
 sgens[1] is the slot symmetry `-(0, 4)` 
 `T^{a0 a1 a2 a3 a4 a5} = -T^{a4 a1 a2 a3 a0 a5}` 
 The dummy symmetry group D is generated by the strong base generators 
 `[(0, 1), (2, 3), (4, 5), (0, 1)(2, 3),(2, 3)(4, 5)]` 
 The dummy symmetry acts from the left 
 `d = [1, 0, 2, 3, 4, 5, 6, 7]`  exchange `d1 -> -d1` 
 `T^{d3 d2 d1}{}_{d1 d2 d3} == T^{d3 d2}{}_{d1}{}^{d1}{}_{d2 d3}` 
 `g=[4, 2, 0, 1, 3, 5, 6, 7]  -> [4, 2, 1, 0, 3, 5, 6, 7] = _af_rmul(d, g)` 
 which differs from `_af_rmul(g, d)`. 
 The slot symmetry acts from the right 
 `s = [2, 1, 0, 3, 4, 5, 7, 6]`  exchanges slots 0 and 2 and changes sign 
 `T^{d3 d2 d1}{}_{d1 d2 d3} == -T^{d1 d2 d3}{}_{d1 d2 d3}` 
 `g=[4,2,0,1,3,5,6,7]  -> [0, 2, 4, 1, 3, 5, 7, 6] = _af_rmul(g, s)` 
 Example in which the tensor is zero, same slot symmetries as above: 
 `T^{d3}{}_{d1,d2}{}^{d1}{}_{d3}{}^{d2}` 
 `= -T^{d3}{}_{d1,d3}{}^{d1}{}_{d2}{}^{d2}`   under slot symmetry `-(2,4)`; 
 `= T_{d3 d1}{}^{d3}{}^{d1}{}_{d2}{}^{d2}`    under slot symmetry `-(0,2)`; 
 `= T^{d3}{}_{d1 d3}{}^{d1}{}_{d2}{}^{d2}`    symmetric metric; 
 `= 0`  since two of these lines have tensors differ only for the sign. 
 The double coset D*g*S consists of permutations `h = d*g*s` corresponding 
 to equivalent tensors; if there are two `h` which are the same apart 
 from the sign, return zero; otherwise 
 choose as representative the tensor with indices 
 ordered lexicographically according to `[d1, -d1, d2, -d2, d3, -d3]` 
 that is `rep = min(D*g*S) = min([d*g*s for d in D for s in S])` 
 The indices are fixed one by one; first choose the lowest index 
 for slot 0, then the lowest remaining index for slot 1, etc. 
 Doing this one obtains a chain of stabilizers 
 `S -> S_{b0} -> S_{b0,b1} -> ...` and 
 `D -> D_{p0} -> D_{p0,p1} -> ...` 
 where `[b0, b1, ...] = range(b)` is a base of the symmetric group; 
 the strong base `b_S` of S is an ordered sublist of it; 
 therefore it is sufficient to compute once the 
 strong base generators of S using the Schreier-Sims algorithm; 
 the stabilizers of the strong base generators are the 
 strong base generators of the stabilizer subgroup. 
 `dbase = [p0, p1, ...]` is not in general in lexicographic order, 
 so that one must recompute the strong base generators each time; 
 however this is trivial, there is no need to use the Schreier-Sims 
 algorithm for D. 
 The algorithm keeps a TAB of elements `(s_i, d_i, h_i)` 
 where `h_i = d_i*g*s_i` satisfying `h_i[j] = p_j` for `0 <= j < i` 
 starting from `s_0 = id, d_0 = id, h_0 = g`. 
 The equations `h_0[0] = p_0, h_1[1] = p_1,...` are solved in this order, 
 choosing each time the lowest possible value of p_i 
 For `j < i` 
 `d_i*g*s_i*S_{b_0,...,b_{i-1}}*b_j = D_{p_0,...,p_{i-1}}*p_j` 
 so that for dx in `D_{p_0,...,p_{i-1}}` and sx in 
 `S_{base[0],...,base[i-1]}` one has `dx*d_i*g*s_i*sx*b_j = p_j` 
 Search for dx, sx such that this equation holds for `j = i`; 
 it can be written as `s_i*sx*b_j = J, dx*d_i*g*J = p_j` 
 `sx*b_j = s_i**-1*J; sx = trace(s_i**-1, S_{b_0,...,b_{i-1}})` 
 `dx**-1*p_j = d_i*g*J; dx = trace(d_i*g*J, D_{p_0,...,p_{i-1}})` 
 `s_{i+1} = s_i*trace(s_i**-1*J, S_{b_0,...,b_{i-1}})` 
 `d_{i+1} = trace(d_i*g*J, D_{p_0,...,p_{i-1}})**-1*d_i` 
 `h_{i+1}*b_i = d_{i+1}*g*s_{i+1}*b_i = p_i` 
 `h_n*b_j = p_j` for all j, so that `h_n` is the solution. 
 Add the found `(s, d, h)` to TAB1. 
 At the end of the iteration sort TAB1 with respect to the `h`; 
 if there are two consecutive `h` in TAB1 which differ only for the 
 sign, the tensor is zero, so return 0; 
 if there are two consecutive `h` which are equal, keep only one. 
 Then stabilize the slot generators under `i` and the dummy generators 
 under `p_i`. 
 Assign `TAB = TAB1` at the end of the iteration step. 
 At the end `TAB` contains a unique `(s, d, h)`, since all the slots 
 of the tensor `h` have been fixed to have the minimum value according 
 to the symmetries. The algorithm returns `h`. 
 It is important that the slot BSGS has lexicographic minimal base, 
 otherwise there is an `i` which does not belong to the slot base 
 for which `p_i` is fixed by the dummy symmetry only, while `i` 
 is not invariant from the slot stabilizer, so `p_i` is not in 
 general the minimal value. 
 This algorithm differs slightly from the original algorithm [3]: 
 the canonical form is minimal lexicographically, and 
 the BSGS has minimal base under lexicographic order. 
 Equal tensors `h` are eliminated from TAB. 
 Examples 
 >>> from sympy.combinatorics.permutations import Permutation 
 >>> from sympy.combinatorics.perm_groups import PermutationGroup 
 >>> from sympy.combinatorics.tensor_can import double_coset_can_rep, get_transversals 
 >>> gens = [Permutation(x) for x in [[2, 1, 0, 3, 4, 5, 7, 6], [4, 1, 2, 3, 0, 5, 7, 6]]] 
 >>> base = [0, 2] 
 >>> g = Permutation([4, 2, 0, 1, 3, 5, 6, 7]) 
 >>> transversals = get_transversals(base, gens) 
 >>> double_coset_can_rep([list(range(6))], [0], base, gens, transversals, g) 
 [0, 1, 2, 3, 4, 5, 7, 6] 
 >>> g = Permutation([4, 1, 3, 0, 5, 2, 6, 7]) 
 >>> double_coset_can_rep([list(range(6))], [0], base, gens, transversals, g) 
 0'"
"def setup(app, status, warning): 
    logger = logging.getLogger() 
   logger.setLevel(logging.NOTSET) 
   for handler in logger.handlers[:]: 
      logger.removeHandler(handler) 
   info_handler = NewLineStreamHandler(SafeEncodingWriter(status)) 
   info_handler.addFilter(InfoFilter()) 
   info_handler.setLevel(VERBOSITY_MAP[app.verbosity]) 
   info_handler.setFormatter(ColorizeFormatter()) 
   warning_handler = WarningStreamHandler(SafeEncodingWriter(warning)) 
   warning_handler.addFilter(WarningSuppressor(app)) 
   warning_handler.addFilter(WarningIsErrorFilter(app)) 
   warning_handler.addFilter(WarningLogRecordTranslator(app)) 
   warning_handler.setLevel(logging.WARNING) 
   warning_handler.setFormatter(ColorizeFormatter()) 
   logger.addHandler(info_handler) 
   logger.addHandler(warning_handler)"," 'Setup logging. 
 :param app: the application 
 :type app: :class:`~.application.Application` 
 :param status: the status stream 
 :type status: :class:`~.status.StatusStream` 
 :param warning: the warning stream 
 :type warning: :class:`~.status.WarningStream` 
 :rtype: None 
 :return: None'",'Setup root logger for Sphinx'
"@task(queue='web') 
 def move_files(version_pk, hostname, html=False, localmedia=False, search=False, pdf=False, epub=False): 
    version = Version.objects.get(pk=version_pk) 
   if html: 
      from_path = version.project.artifact_path(version=version.slug, type_=version.project.documentation_type) 
      target = version.project.rtd_build_path(version.slug) 
      Syncer.copy(from_path, target, host=hostname) 
   if ('sphinx' in version.project.documentation_type): 
      if localmedia: 
         from_path = version.project.artifact_path(version=version.slug, type_='sphinx_localmedia') 
         to_path = version.project.get_production_media_path(type_='htmlzip', version_slug=version.slug, include_file=False) 
         Syncer.copy(from_path, to_path, host=hostname) 
      if search: 
         from_path = version.project.artifact_path(version=version.slug, type_='sphinx_search') 
         to_path = version.project.get_production_media_path(type_='json', version_slug=version.slug, include_file=False) 
         Syncer.copy(from_path, to_path, host=hostname) 
      if pdf: 
         from_path = version.project.artifact_path(version=version.slug, type_='sphinx_pdf') 
         to_path = version.project.get_production_media_path(type_='pdf', version_slug=version.slug, include_file=False) 
         Syncer.copy(from_path, to_path, host=hostname) 
      if epub: 
         from_path = version.project.artifact_path(version=version.slug, type_='sphinx_epub') 
         to_path = version.project.get_production_media_path(type_='epub', version_slug=version.slug, include_file=False) 
         Syncer.copy(from_path, to_path, host=hostname) 
   if ('mkdocs' in version.project.documentation_type): 
      if search: 
         from_path = version.project.artifact_path(version=version.slug, type_='mkdocs_json') 
         to_path = version.project.get_production_media_path(type_='json', version_slug=version.slug, include_file=False) 
         Syncer.copy(from_path, to_path, host=hostname)", 'Copies files from the local version to the production version.',"'Task to move built documentation to web servers 
 :param version_pk: Version id to sync files for 
 :param hostname: Hostname to sync to 
 :param html: Sync HTML 
 :type html: bool 
 :param localmedia: Sync local media files 
 :type localmedia: bool 
 :param search: Sync search files 
 :type search: bool 
 :param pdf: Sync PDF files 
 :type pdf: bool 
 :param epub: Sync ePub files 
 :type epub: bool'"
"def generate_reset_password_token(user): 
    password_hash = (md5(user.password) if user.password else None) 
   data = [str(user.id), password_hash] 
   return _security.reset_serializer.dumps(data)", 'Generates a reset password token for the user.',"'Generates a unique reset password token for the specified user. 
 :param user: The user to work with'"
"def rrggbb_to_triplet(color): 
    rgbtuple = _namedict.get(color) 
   if (rgbtuple is None): 
      if (color[0] != '#'): 
         raise BadColor(color) 
      red = color[1:3] 
      green = color[3:5] 
      blue = color[5:7] 
      rgbtuple = (int(red, 16), int(green, 16), int(blue, 16)) 
      _namedict[color] = rgbtuple 
   return rgbtuple", 'Convert a RGB color to a triplet.',"'Converts a #rrggbb color to the tuple (red, green, blue).'"
"@commands(u'suggest') 
 def suggest(bot, trigger): 
    if (not trigger.group(2)): 
      return bot.reply(u'No   query   term.') 
   query = trigger.group(2) 
   uri = u'http://websitedev.de/temp-bin/suggest.pl?q=' 
   answer = web.get((uri + query.replace(u'+', u'%2B'))) 
   if answer: 
      bot.say(answer) 
   else: 
      bot.reply(u'Sorry,   no   result.')"," 'Suggest a query term. 
 Examples: 
 !suggest +foo 
 !suggest +foo bar'",'Suggest terms starting with given input'
"def assign_role_for_exploration(committer_id, exploration_id, assignee_id, new_role): 
    _assign_role(committer_id, assignee_id, new_role, exploration_id, feconf.ACTIVITY_TYPE_EXPLORATION) 
   if (new_role in [ROLE_OWNER, ROLE_EDITOR]): 
      subscription_services.subscribe_to_exploration(assignee_id, exploration_id)"," 'Assigns a role to an exploration and subscribes the user to the exploration. 
 :param committer_id: ID of the user who is committing the change. 
 :param exploration_id: ID of the exploration to be assigned a role. 
 :param assignee_id: ID of the user to be assigned a role. 
 :param new_role: New role to be assigned to the user. 
 :returns: None.'","'Assign `assignee_id` to the given role and subscribes the assignee 
 to future exploration updates. 
 The caller should ensure that assignee_id corresponds to a valid user in 
 the system. 
 Args: 
 - committer_id: str. The user_id of the user who is performing the action. 
 - exploration_id: str. The exploration id. 
 - assignee_id: str. The user_id of the user whose role is being changed. 
 - new_role: str. The name of the new role: either \'owner\', \'editor\' or 
 \'viewer\'.'"
"def _queue_management_worker(executor_reference, processes, pending_work_items, work_ids_queue, call_queue, result_queue): 
    executor = None 
   def shutting_down(): 
      return (_shutdown or (executor is None) or executor._shutdown_thread) 
   def shutdown_worker(): 
      nb_children_alive = sum((p.is_alive() for p in processes.values())) 
      for i in range(0, nb_children_alive): 
         call_queue.put_nowait(None) 
      call_queue.close() 
      for p in processes.values(): 
         p.join() 
   reader = result_queue._reader 
   while True: 
      _add_call_item_to_queue(pending_work_items, work_ids_queue, call_queue) 
      sentinels = [p.sentinel for p in processes.values()] 
      assert sentinels 
      ready = wait(([reader] + sentinels)) 
      if (reader in ready): 
         result_item = reader.recv() 
      else: 
         executor = executor_reference() 
         if (executor is not None): 
            executor._broken = True 
            executor._shutdown_thread = True 
            executor = None 
         for (work_id, work_item) in pending_work_items.items(): 
            work_item.future.set_exception(BrokenProcessPool('A   process   in   the   process   pool   was   terminated   abruptly   while   the   future   was   running   or   pending.')) 
            del work_item 
         pending_work_items.clear() 
         for p in processes.values(): 
            p.terminate() 
         shutdown_worker() 
         return 
      if isinstance(result_item, int): 
         assert shutting_down() 
         p = processes.pop(result_item) 
         p.join() 
         if (not processes): 
            shutdown_worker() 
            return 
      elif (result_item is not None): 
         work_item = pending_work_items.pop(result_item.work_id, None) 
         if (work_item is not None): 
            if result_item.exception: 
               work_item.future.set_exception(result_item.exception) 
            else: 
               work_item.future.set_result(result_item.result) 
            del work_item 
      executor = executor_reference() 
      if shutting_down(): 
         try: 
            if (not pending_work_items): 
               shutdown_worker() 
               return 
         except Full: 
            pass 
      executor = None"," 'This worker is responsible for managing the queue of work items and 
 executing them. 
 :param executor_reference: The reference to the executor that will be used to 
 execute the work items. 
 :param processes: A dictionary mapping work_id to a Process instance. 
 :param pending_work_items: A dictionary mapping work_id to a WorkItem instance. 
 :param work_ids_queue: A queue that will be used to send work items to the 
 worker. 
 :param call_queue: A queue that will be used to send calls to the worker. 
 :param result_queue: A queue that will be used to send results to the worker. 
 :return: None 
 :rtype: None 
 :raises Full: If the queue is full. 
 :raises BrokenProcessPool: If a process in the pool was terminated abruptly 
 while the future was running or pending.'","'Manages the communication between this process and the worker processes. 
 This function is run in a local thread. 
 Args: 
 executor_reference: A weakref.ref to the ProcessPoolExecutor that owns 
 this thread. Used to determine if the ProcessPoolExecutor has been 
 garbage collected and that this function can exit. 
 process: A list of the multiprocessing.Process instances used as 
 workers. 
 pending_work_items: A dict mapping work ids to _WorkItems e.g. 
 {5: <_WorkItem...>, 6: <_WorkItem...>, ...} 
 work_ids_queue: A queue.Queue of work ids e.g. Queue([5, 6, ...]). 
 call_queue: A multiprocessing.Queue that will be filled with _CallItems 
 derived from _WorkItems for processing by the process workers. 
 result_queue: A multiprocessing.Queue of _ResultItems generated by the 
 process workers.'"
"def timefunc(correct, s, func, *args, **kwargs): 
    print(s.ljust(20), end='   ') 
   res = func(*args, **kwargs) 
   if (correct is not None): 
      assert np.allclose(res, correct), (res, correct) 
   print('{:>5.0f}   ms'.format((min(repeat((lambda : func(*args, **kwargs)), number=5, repeat=2)) * 1000))) 
   return res", 'A decorator to time functions.','Benchmark *func* and print out its runtime.'
"def demo(): 
    from nltk import Tree, ProbabilisticTree 
   s = u'(S   (NP   (DT   the)   (NN   cat))   (VP   (VBD   ate)   (NP   (DT   a)   (NN   cookie))))' 
   t = Tree.fromstring(s) 
   print(u'Convert   bracketed   string   into   tree:') 
   print(t) 
   print(t.__repr__()) 
   print(u'Display   tree   properties:') 
   print(t.label()) 
   print(t[0]) 
   print(t[1]) 
   print(t.height()) 
   print(t.leaves()) 
   print(t[1]) 
   print(t[(1, 1)]) 
   print(t[(1, 1, 0)]) 
   the_cat = t[0] 
   the_cat.insert(1, Tree.fromstring(u'(JJ   big)')) 
   print(u'Tree   modification:') 
   print(t) 
   t[(1, 1, 1)] = Tree.fromstring(u'(NN   cake)') 
   print(t) 
   print() 
   print(u'Collapse   unary:') 
   t.collapse_unary() 
   print(t) 
   print(u'Chomsky   normal   form:') 
   t.chomsky_normal_form() 
   print(t) 
   print() 
   pt = ProbabilisticTree(u'x', [u'y', u'z'], prob=0.5) 
   print(u'Probabilistic   Tree:') 
   print(pt) 
   print() 
   t = Tree.fromstring(t.pformat()) 
   print(u'Convert   tree   to   bracketed   string   and   back   again:') 
   print(t) 
   print() 
   print(u'LaTeX   output:') 
   print(t.pformat_latex_qtree()) 
   print() 
   print(u'Production   output:') 
   print(t.productions()) 
   print() 
   t.set_label((u'test', 3)) 
   print(t)", 'Demo of some tree manipulation functions',"'A demonstration showing how Trees and Trees can be 
 used.  This demonstration creates a Tree, and loads a 
 Tree from the Treebank corpus, 
 and shows the results of calling several of their methods.'"
"@register_specialize 
 @register_canonicalize 
 @gof.local_optimizer([T.mul]) 
 def local_mul_switch_sink(node): 
    if (node.op != T.mul): 
      return False 
   for (idx, i) in enumerate(node.inputs): 
      if (i.owner and (i.owner.op == T.switch)): 
         switch = i.owner 
         try: 
            if (get_scalar_constant_value(switch.inputs[1], only_process_constants=True) == 0.0): 
               listmul = (node.inputs[:idx] + node.inputs[(idx + 1):]) 
               fmul = T.mul(*(listmul + [switch.inputs[2]])) 
               copy_stack_trace(node.outputs, fmul) 
               fct = [T.switch(switch.inputs[0], 0, fmul)] 
               fct[0].tag.values_eq_approx = values_eq_approx_remove_nan 
               copy_stack_trace((node.outputs + switch.outputs), fct) 
               return fct 
         except NotScalarConstantError: 
            pass 
         try: 
            if (get_scalar_constant_value(switch.inputs[2], only_process_constants=True) == 0.0): 
               listmul = (node.inputs[:idx] + node.inputs[(idx + 1):]) 
               fmul = T.mul(*(listmul + [switch.inputs[1]])) 
               copy_stack_trace(node.outputs, fmul) 
               fct = [T.switch(switch.inputs[0], fmul, 0)] 
               fct[0].tag.values_eq_approx = values_eq_approx_remove_nan 
               copy_stack_trace((node.outputs + switch.outputs), fct) 
               return fct 
         except NotScalarConstantError: 
            pass 
   return False"," 'Switch on the inner mul, and switch on the outer mul. 
 This is only valid when the inner mul is a constant.'","'This optimization makes the folowing changes in the graph: 
 T.mul(A,T.switch(cond,0,iff),B) -->  T.switch(cond,0,T.mul(A,B,iff)) 
 T.mul(A,T.switch(cond,ift,0),B) -->  T.switch(cond,T.mul(A,B,ift),0) 
 A and B being several (or none) symbolic variables. 
 This is useful because A and B may not be numerically stable and give 
 NaN or inf values for cases where the switch returns 0. 
 With this optimization T.grad(T.switch(...)) has the right behavior. 
 Examples 
 x -> f(x) 
 x -> g(x) 
 y = T.switch(cond,f(x),g(x)) 
 **without the optimization 
 T.grad(y,x) -> grad(f(x),x) * grad(y,f(x)) +  grad(g(x),x) * grad(y,g(x)) 
 **with the optimization 
 T.grad(y,x) -> switch(cond,grad(f(x),x), 0) + switch(cond,0,grad(g(x),x)) 
 This will be particularly useful for the lazyif because we skip 
 an entire part of the graph.'"
"def po_due_followups(): 
    query = ((FS('followup_date') <= datetime.datetime.utcnow().date()) & (FS('completed') != True)) 
   resource = current.s3db.resource('po_household_followup', filter=query) 
   return resource.count()", 'Number of followups that are not completed','Number of due follow-ups'
"def lstrips(text, remove): 
    return _strips('l', text, remove)"," 'Strip leading and trailing whitespace from a string. 
 :param text: String to strip whitespace from. 
 :param remove: Whether to remove whitespace from the beginning 
 and/or end of the string. 
 :return: String with leading and/or trailing whitespace removed. 
 :rtype: str'",'removes the string `remove` from the right of `text`'
"def writeOutput(fileName): 
    repository = getNewRepository() 
   repository.fileNameInput.value = fileName 
   settings.startMainLoopFromWindow(repository.execute())", 'Write the output to a file',"'Craft a file, display dialog.'"
"def move(src, dst): 
    real_dst = dst 
   if os.path.isdir(dst): 
      if _samefile(src, dst): 
         os.rename(src, dst) 
         return 
      real_dst = os.path.join(dst, _basename(src)) 
      if os.path.exists(real_dst): 
         raise Error, (""Destination   path   '%s'   already   exists"" % real_dst) 
   try: 
      os.rename(src, real_dst) 
   except OSError: 
      if os.path.isdir(src): 
         if _destinsrc(src, dst): 
            raise Error, (""Cannot   move   a   directory   '%s'   into   itself   '%s'."" % (src, dst)) 
         copytree(src, real_dst, symlinks=True) 
         rmtree(src) 
      else: 
         copy2(src, real_dst) 
         os.unlink(src)"," 'Move a file or directory to a new location. 
 The destination is determined by the rules in 
 `move_destinsrc`.'","'Recursively move a file or directory to another location. This is 
 similar to the Unix ""mv"" command. 
 If the destination is a directory or a symlink to a directory, the source 
 is moved inside the directory. The destination path must not already 
 exist. 
 If the destination already exists but is not a directory, it may be 
 overwritten depending on os.rename() semantics. 
 If the destination is on our current filesystem, then rename() is used. 
 Otherwise, src is copied to the destination and then removed. 
 A lot more could be done here...  A look at a mv.c shows a lot of 
 the issues this implementation glosses over.'"
"def iri_to_uri(iri): 
    if (iri is None): 
      return iri 
   return quote(force_bytes(iri), safe=""/#%[]=:;$&()+,!?*@'~"")"," 'Convert an IRI to a URI. 
 :param iri: IRI to convert. 
 :type iri: str or unicode 
 :return: URI string. 
 :rtype: str'","'Convert an Internationalized Resource Identifier (IRI) portion to a URI 
 portion that is suitable for inclusion in a URL. 
 This is the algorithm from section 3.1 of RFC 3987.  However, since we are 
 assuming input is either UTF-8 or unicode already, we can simplify things a 
 little from the full method. 
 Returns an ASCII string containing the encoded result.'"
"def perform_check(prerelease=current_version.is_prerelease): 
    pypi = current_version 
   try: 
      pypi = available_on_pypi(prerelease) 
   except Exception: 
      log.warning('An   issue   occurred   while   checking   PyPI') 
   best = max(pypi, current_version) 
   where = None 
   command = None 
   cache = cache_file() 
   if cache: 
      os.utime(cache, None) 
   if (best == current_version): 
      log.info(('You   have   the   latest   version   of   Pwntools   (%s)' % best)) 
      return 
   command = ['pip', 'install', '-U'] 
   if (best == pypi): 
      where = 'pypi' 
      pypi_package = package_name 
      if best.is_prerelease: 
         pypi_package += ('==%s' % best) 
      command += [pypi_package] 
   command_str = '   '.join(command) 
   log.info((('A   newer   version   of   %s   is   available   on   %s   (%s   -->   %s).\n' % (package_name, where, current_version, best)) + ('Update   with:   $   %s' % command_str))) 
   return command", 'Returns a command to update to the latest version.',"'Perform the update check, and report to the user. 
 Arguments: 
 prerelease(bool): Whether or not to include pre-release versions. 
 Returns: 
 A list of arguments to the update command. 
 >>> from packaging.version import Version 
 >>> pwnlib.update.current_version = Version(""999.0.0"") 
 >>> print perform_check() 
 None 
 >>> pwnlib.update.current_version = Version(""0.0.0"") 
 >>> perform_check() # doctest: +ELLIPSIS 
 [\'pip\', \'install\', \'-U\', ...] 
 >>> def bail(*a): raise Exception() 
 >>> pypi   = pwnlib.update.available_on_pypi 
 >>> perform_check(prerelease=False) 
 [\'pip\', \'install\', \'-U\', \'pwntools\'] 
 >>> perform_check(prerelease=True)  # doctest: +ELLIPSIS 
 [\'pip\', \'install\', \'-U\', \'pwntools...\']'"
"def virtual_root(resource, request): 
    try: 
      reg = request.registry 
   except AttributeError: 
      reg = get_current_registry() 
   url_adapter = reg.queryMultiAdapter((resource, request), IResourceURL) 
   if (url_adapter is None): 
      url_adapter = ResourceURL(resource, request) 
   (vpath, rpath) = (url_adapter.virtual_path, url_adapter.physical_path) 
   if ((rpath != vpath) and rpath.endswith(vpath)): 
      vroot_path = rpath[:(- len(vpath))] 
      return find_resource(resource, vroot_path) 
   try: 
      return request.root 
   except AttributeError: 
      return find_root(resource)"," 'Return the virtual root of the resource. 
 :param resource: The resource to get the virtual root of 
 :param request: The request to get the virtual root of 
 :return: The virtual root of the resource'","'Provided any :term:`resource` and a :term:`request` object, return 
 the resource object representing the :term:`virtual root` of the 
 current :term:`request`.  Using a virtual root in a 
 :term:`traversal` -based :app:`Pyramid` application permits 
 rooting. For example, the resource at the traversal path ``/cms`` will 
 be found at ``http://example.com/`` instead of rooting it at 
 ``http://example.com/cms/``. 
 If the ``resource`` passed in is a context obtained via 
 :term:`traversal`, and if the ``HTTP_X_VHM_ROOT`` key is in the 
 WSGI environment, the value of this key will be treated as a 
 \'virtual root path\': the :func:`pyramid.traversal.find_resource` 
 API will be used to find the virtual root resource using this path; 
 if the resource is found, it will be returned.  If the 
 ``HTTP_X_VHM_ROOT`` key is not present in the WSGI environment, 
 the physical :term:`root` of the resource tree will be returned instead. 
 Virtual roots are not useful at all in applications that use 
 :term:`URL dispatch`. Contexts obtained via URL dispatch don\'t 
 really support being virtually rooted (each URL dispatch context 
 is both its own physical and virtual root).  However if this API 
 is called with a ``resource`` argument which is a context obtained 
 via URL dispatch, the resource passed in will be returned 
 unconditionally.'"
"def config(conf_path='/etc/rsyncd.conf'): 
    ret = '' 
   try: 
      with salt.utils.fopen(conf_path, 'r') as fp_: 
         for line in fp_: 
            ret += line 
   except IOError as exc: 
      if (exc.errno == errno.ENOENT): 
         raise CommandExecutionError('{0}   does   not   exist'.format(conf_path)) 
      elif (exc.errno == errno.EACCES): 
         raise CommandExecutionError('Unable   to   read   {0},   access   denied'.format(conf_path)) 
      elif (exc.errno == errno.EISDIR): 
         raise CommandExecutionError('Unable   to   read   {0},   path   is   a   directory'.format(conf_path)) 
      else: 
         raise CommandExecutionError('Error   {0}:   {1}'.format(exc.errno, exc.strerror)) 
   else: 
      return ret"," 'Configure rsync daemon 
 .. versionadded:: 0.17.0 
 :param conf_path: Path to rsync configuration file 
 :type conf_path: str 
 :return: Content of rsync configuration file 
 :rtype: str'","'.. versionchanged:: 2016.3.0 
 Return data now contains just the contents of the rsyncd.conf as a 
 string, instead of a dictionary as returned from :py:func:`cmd.run_all 
 <salt.modules.cmdmod.run_all>`. 
 Returns the contents of the rsync config file 
 conf_path : /etc/rsyncd.conf 
 Path to the config file 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' rsync.config'"
"def intTime(scale=1): 
    return int((time.time() * scale))"," 'Returns a time in seconds as an integer. 
 :param scale: Scale factor. 
 :type scale: float 
 :return: An integer representing the time in seconds.'",'The time in integer seconds. Pass scale=1000 to get milliseconds.'
"def _make_compound_key(table, key): 
    if (not isinstance(key, (list, tuple))): 
      key = [key] 
   return [table.columns[name] for name in key]"," 'Make a compound key from a list of column names. 
 If the key is not a list or tuple, it is converted to a list. 
 :param table: Table to make a compound key from 
 :param key: List of column names to make a compound key from 
 :return: List of columns to make the compound key from'","'Returns a list of columns from `column_key` for `table` representing 
 potentially a compound key. The `column_key` can be a name of a single 
 column or list of column names.'"
"def DNSServiceResolve(flags=0, interfaceIndex=_NO_DEFAULT, name=_NO_DEFAULT, regtype=_NO_DEFAULT, domain=_NO_DEFAULT, callBack=None): 
    _NO_DEFAULT.check(interfaceIndex) 
   _NO_DEFAULT.check(name) 
   _NO_DEFAULT.check(regtype) 
   _NO_DEFAULT.check(domain) 
   @_DNSServiceResolveReply 
   def _callback(sdRef, flags, interfaceIndex, errorCode, fullname, hosttarget, port, txtLen, txtRecord, context): 
      if (callBack is not None): 
         port = socket.ntohs(port) 
         txtRecord = _length_and_void_p_to_string(txtLen, txtRecord) 
         callBack(sdRef, flags, interfaceIndex, errorCode, fullname.decode(), hosttarget.decode(), port, txtRecord) 
   _global_lock.acquire() 
   try: 
      sdRef = _DNSServiceResolve(flags, interfaceIndex, name, regtype, domain, _callback, None) 
   finally: 
      _global_lock.release() 
   sdRef._add_callback(_callback) 
   return sdRef"," 'DNSServiceResolve() 
 This function is used to perform a DNS service query. 
 :param flags: 
 A bitmask of the following flags: 
 * DS_FLAG_RECURSION_IGNORE 
 * DS_FLAG_RECURSION_CACHE 
 * DS_FLAG_RECURSION_IGNORE_UNKNOWN 
 * DS_FLAG_RECURSION_IGNORE_NXDOMAIN 
 * DS_FLAG_RECURSION_IGNORE_NXRRSET 
 * DS_FLAG_RECURSION_IGNORE_NXRRSET_NXDOMAIN 
 * DS_FLAG_RECURSION_IGNORE_NXRRSET_NXRRTYPE 
 * DS_FLAG_RECURSION_IGNORE_NXRRTYPE_NXDOMAIN 
 * DS_FLAG_RECURSION_IGNORE_NXRRTYPE_NXRRSET 
 * DS_FLAG_RECURSION_IGNORE_NXRRTYPE_N","'Resolve a service name discovered via DNSServiceBrowse() to a 
 target host name, port number, and txt record. 
 Note: Applications should NOT use DNSServiceResolve() solely for 
 txt record monitoring; use DNSServiceQueryRecord() instead, as it 
 is more efficient for this task. 
 Note: When the desired results have been returned, the client MUST 
 terminate the resolve by closing the returned DNSServiceRef. 
 Note: DNSServiceResolve() behaves correctly for typical services 
 that have a single SRV record and a single TXT record.  To resolve 
 non-standard services with multiple SRV or TXT records, 
 DNSServiceQueryRecord() should be used. 
 flags: 
 Currently ignored, reserved for future use. 
 interfaceIndex: 
 The interface on which to resolve the service.  If this 
 resolve call is as a result of a currently active 
 DNSServiceBrowse() operation, then the interfaceIndex should 
 be the index reported in the browse callback.  If this resolve 
 call is using information previously saved (e.g. in a 
 preference file) for later use, then use 
 kDNSServiceInterfaceIndexAny (0), because the desired service 
 may now be reachable via a different physical interface. 
 name: 
 The name of the service instance to be resolved, as reported 
 to the DNSServiceBrowse() callback. 
 regtype: 
 The type of the service instance to be resolved, as reported 
 to the DNSServiceBrowse() callback. 
 domain: 
 The domain of the service instance to be resolved, as reported 
 to the DNSServiceBrowse() callback. 
 callBack: 
 The function to be called when a result is found, or if the 
 call asynchronously fails.  Its signature should be 
 callBack(sdRef, flags, interfaceIndex, errorCode, fullname, 
 hosttarget, port, txtRecord). 
 return value: 
 A DNSServiceRef instance.  The resolve operation will run 
 indefinitely until the client terminates it by closing the 
 DNSServiceRef. 
 Callback Parameters: 
 sdRef: 
 The DNSServiceRef returned by DNSServiceResolve(). 
 flags: 
 Currently unused, reserved for future use. 
 interfaceIndex: 
 The interface on which the service was resolved. 
 errorCode: 
 Will be kDNSServiceErr_NoError (0) on success, otherwise will 
 indicate the failure that occurred.  Other parameters are 
 undefined if an error occurred. 
 fullname: 
 The full service domain name, in the form 
 <servicename>.<protocol>.<domain>. 
 hosttarget: 
 The target hostname of the machine providing the service. 
 port: 
 The port, in host (not network) byte order, on which 
 connections are accepted for this service. 
 txtRecord: 
 A string containing the service\'s primary txt record, in 
 standard txt record format.'"
"def FindRegisterPackage(packageName, knownFile, searchPaths, registryAppName=None): 
    import regutil, string 
   if (not packageName): 
      raise error('A   package   name   must   be   supplied') 
   corePaths = regutil.GetRegisteredNamedPath(None).split(';') 
   if (not searchPaths): 
      searchPaths = corePaths 
   registryAppName = (registryAppName or packageName) 
   try: 
      (pathLook, pathAdd) = FindPackagePath(packageName, knownFile, searchPaths) 
      if (pathAdd is not None): 
         if (pathAdd in corePaths): 
            pathAdd = '' 
         regutil.RegisterNamedPath(registryAppName, pathAdd) 
      return pathLook 
   except error as details: 
      print ('***   The   %s   package   could   not   be   registered   -   %s' % (packageName, details)) 
      print '***   Please   ensure   you   have   passed   the   correct   paths   on   the   command   line.' 
      print '***   -   For   packages,   you   should   pass   a   path   to   the   packages   parent   directory,' 
      print '***   -   and   not   the   package   directory   itself...'"," 'Find the package path for a package. 
 :param packageName: The package name to look for. 
 :param knownFile: The known file for the package. 
 :param searchPaths: The search paths for the package. 
 :param registryAppName: The name of the registry application to register the 
 package under. 
 :return: The path to the package or None if the package could not be found.'","'Find and Register a package. 
 Assumes the core registry setup correctly. 
 In addition, if the location located by the package is already 
 in the **core** path, then an entry is registered, but no path. 
 (no other paths are checked, as the application whose path was used 
 may later be uninstalled.  This should not happen with the core)'"
"def kernel_info(attrs=None, where=None): 
    return _osquery_cmd(table='kernel_info', attrs=attrs, where=where)", 'Returns information about the kernel.',"'Return kernel_info information from osquery 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' osquery.kernel_info'"
"def vsepr_build_correct_answer(geometry, atoms): 
    return {'geometry': geometry, 'atoms': atoms}", 'Build a vsepr_correct_answer object from the given geometry and atoms.',"'geometry is string. 
 atoms is dict of atoms with proper positions. 
 Example: 
 correct_answer = vsepr_build_correct_answer(geometry=""AX4E0"", atoms={""c0"": ""N"", ""p0"": ""H"", ""p1"": ""(ep)"", ""p2"": ""H"", ""p3"": ""H""}) 
 returns a dictionary composed from input values: 
 {\'geometry\': geometry, \'atoms\': atoms}'"
"def _get_enabled_tax_rules(taxing_context, tax_class): 
    tax_rules = TaxRule.objects.may_match_postal_code(taxing_context.postal_code).filter(enabled=True, tax_classes=tax_class) 
   if taxing_context.customer_tax_group: 
      tax_rules = tax_rules.filter((Q(customer_tax_groups=taxing_context.customer_tax_group) | Q(customer_tax_groups=None))) 
   tax_rules = tax_rules.order_by('-override_group', 'priority') 
   return tax_rules"," 'Returns a list of tax rules that match the taxing context. 
 :param taxing_context: Taxing context to match against. 
 :param tax_class: Tax class to match against. 
 :return: A list of tax rules that match the taxing context. 
 :rtype: list[TaxRule]'","'Get enabled tax rules from the db for given parameters. 
 Returned rules are ordered desceding by override group and then 
 ascending by priority (as required by `_filter_and_group_rules`). 
 :type taxing_context: shuup.core.taxing.TaxingContext 
 :type tax_class: shuup.core.models.TaxClass'"
"def hilbert(n): 
    values = (1.0 / (1.0 + np.arange(((2 * n) - 1)))) 
   h = hankel(values[:n], r=values[(n - 1):]) 
   return h"," 'Returns the hilbert transform of a sequence of values. 
 Parameters 
 n : int 
 The length of the sequence. 
 Returns 
 h : array 
 The hilbert transform of the sequence. 
 Examples 
 >>> from sympy.core import hilbert 
 >>> hilbert(3) 
 array([ 0.5,  0.25,  0.125,  0.0625,  0.03125,  0.015625,  0.0078125,  0.00390625])'","'Create a Hilbert matrix of order `n`. 
 Returns the `n` by `n` array with entries `h[i,j] = 1 / (i + j + 1)`. 
 Parameters 
 n : int 
 The size of the array to create. 
 Returns 
 h : (n, n) ndarray 
 The Hilbert matrix. 
 See Also 
 invhilbert : Compute the inverse of a Hilbert matrix. 
 Notes 
 .. versionadded:: 0.10.0 
 Examples 
 >>> from scipy.linalg import hilbert 
 >>> hilbert(3) 
 array([[ 1.        ,  0.5       ,  0.33333333], 
 [ 0.5       ,  0.33333333,  0.25      ], 
 [ 0.33333333,  0.25      ,  0.2       ]])'"
"def float_sum(iterable): 
    return float(sum(iterable))"," 'Returns the sum of the elements of iterable. 
 If the elements are not all numbers, they are converted to 
 floating point numbers before the sum is calculated. 
 The sum is returned as a floating point number. 
 Examples 
 >>> float_sum([1, 2, 3, 4]) 
 10.0'","'Sum the elements of the iterable, and return the result as a float.'"
"def json2csv_entities(tweets_file, outfile, main_fields, entity_type, entity_fields, encoding='utf8', errors='replace', gzip_compress=False): 
    (writer, outf) = outf_writer_compat(outfile, encoding, errors, gzip_compress) 
   header = get_header_field_list(main_fields, entity_type, entity_fields) 
   writer.writerow(header) 
   for line in tweets_file: 
      tweet = json.loads(line) 
      if _is_composed_key(entity_type): 
         (key, value) = _get_key_value_composed(entity_type) 
         object_json = _get_entity_recursive(tweet, key) 
         if (not object_json): 
            continue 
         object_fields = extract_fields(object_json, main_fields) 
         items = _get_entity_recursive(object_json, value) 
         _write_to_file(object_fields, items, entity_fields, writer) 
      else: 
         tweet_fields = extract_fields(tweet, main_fields) 
         items = _get_entity_recursive(tweet, entity_type) 
         _write_to_file(tweet_fields, items, entity_fields, writer) 
   outf.close()"," 'Write a tweet to a csv file with the entities. 
 :param tweets_file: A file with tweets. 
 :param outfile: A file path to write the csv file to. 
 :param main_fields: A list of fields to include in the main tweet. 
 :param entity_type: A type of entity to include in the csv file. 
 :param entity_fields: A list of fields to include in the entity. 
 :param encoding: The encoding of the csv file. 
 :param errors: The errors to use when reading the csv file. 
 :param gzip_compress: Whether to use gzip compression. 
 :return: A tuple of (writer, outf) where writer is the csv writer and outf is the file path of the csv file. 
 :rtype: (csv.writer, str)'","'Extract selected fields from a file of line-separated JSON tweets and 
 write to a file in CSV format. 
 This utility function allows a file of full Tweets to be easily converted 
 to a CSV file for easier processing of Twitter entities. For example, the 
 hashtags or media elements of a tweet can be extracted. 
 It returns one line per entity of a Tweet, e.g. if a tweet has two hashtags 
 there will be two lines in the output file, one per hashtag 
 :param tweets_file: the file-like object containing full Tweets 
 :param str outfile: The path of the text file where results should be    written 
 :param list main_fields: The list of fields to be extracted from the main    object, usually the tweet. Useful examples: \'id_str\' for the tweetID. See    <https://dev.twitter.com/overview/api/tweets> for a full list of fields. 
 e. g.: [\'id_str\'], [\'id\', \'text\', \'favorite_count\', \'retweet_count\'] 
 If `entity_type` is expressed with hierarchy, then it is the list of    fields of the object that corresponds to the key of the entity_type,    (e.g., for entity_type=\'user.urls\', the fields in the main_fields list    belong to the user object; for entity_type=\'place.bounding_box\', the    files in the main_field list belong to the place object of the tweet). 
 :param list entity_type: The name of the entity: \'hashtags\', \'media\',    \'urls\' and \'user_mentions\' for the tweet object. For a user object,    this needs to be expressed with a hierarchy: `\'user.urls\'`. For the    bounding box of the Tweet location, use `\'place.bounding_box\'`. 
 :param list entity_fields: The list of fields to be extracted from the    entity. E.g. `[\'text\']` (of the Tweet) 
 :param error: Behaviour for encoding errors, see    https://docs.python.org/3/library/codecs.html#codec-base-classes 
 :param gzip_compress: if `True`, ouput files are compressed with gzip'"
"def quota_allocated_update(context, project_id, resource, allocated): 
    return IMPL.quota_allocated_update(context, project_id, resource, allocated)"," 'Update the quota allocated for a project. 
 :param context: The context. 
 :param project_id: The project ID. 
 :param resource: The resource name. 
 :param allocated: The new quota allocated. 
 :returns: The updated quota allocated.'","'Update allocated quota to subprojects or raise if it does not exist. 
 :raises: cinder.exception.ProjectQuotaNotFound'"
"def AllocateIdsAsync(model_key, size=None, **kwargs): 
    max = kwargs.pop('max', None) 
   config = _GetConfigFromKwargs(kwargs) 
   if (getattr(config, 'read_policy', None) == EVENTUAL_CONSISTENCY): 
      raise datastore_errors.BadRequestError('read_policy   is   only   supported   on   read   operations.') 
   (keys, _) = NormalizeAndTypeCheckKeys(model_key) 
   if (len(keys) > 1): 
      raise datastore_errors.BadArgumentError('Cannot   allocate   IDs   for   more   than   one   model   key   at   a   time') 
   rpc = _GetConnection().async_allocate_ids(config, keys[0], size, max) 
   return rpc"," 'Allocate IDs for a model key. 
 :param model_key: The model key to allocate IDs for. 
 :param size: The number of IDs to allocate. 
 :param max: The maximum number of IDs to allocate. 
 :returns: A RPC to use to query for the allocated IDs. 
 :rtype: :class:`~google.datastore.v1.DatastoreAsyncRpc`'","'Asynchronously allocates a range of IDs. 
 Identical to datastore.AllocateIds() except returns an asynchronous object. 
 Call get_result() on the return value to block on the call and get the 
 results.'"
"def unpack(source): 
    (payload, symtab, radix, count) = _filterargs(source) 
   if (count != len(symtab)): 
      raise UnpackingError('Malformed   p.a.c.k.e.r.   symtab.') 
   try: 
      unbase = Unbaser(radix) 
   except TypeError: 
      raise UnpackingError('Unknown   p.a.c.k.e.r.   encoding.') 
   def lookup(match): 
      'Look   up   symbols   in   the   synthetic   symtab.' 
      word = match.group(0) 
      return (symtab[unbase(word)] or word) 
   source = re.sub('\\b\\w+\\b', lookup, payload) 
   return _replacestrings(source)"," 'Unpack a string into a list of strings. 
 :param source: The string to be unpacked. 
 :type source: str 
 :return: The unpacked list of strings. 
 :rtype: list(str)'",'Unpacks P.A.C.K.E.R. packed js code.'
"def encode_quopri(msg): 
    orig = msg.get_payload(decode=True) 
   encdata = _qencode(orig) 
   msg.set_payload(encdata) 
   msg['Content-Transfer-Encoding'] = 'quoted-printable'"," 'Encode a message using QOP=1 (Quoted-Printable) and QOPRI=1 (QOP Required). 
 :param msg: Message to encode. 
 :return: Encoded message.'","'Encode the message\'s payload in quoted-printable. 
 Also, add an appropriate Content-Transfer-Encoding header.'"
"def dump_metadata(filename, show_row_group_metadata, out=sys.stdout): 
    def println(value): 
      u'Write   a   new   line   containing   `value`   to   `out`.' 
      out.write((value + u'\n')) 
   footer = read_footer(filename) 
   println(u'File   Metadata:   {0}'.format(filename)) 
   println(u'      Version:   {0}'.format(footer.version)) 
   println(u'      Num   Rows:   {0}'.format(footer.num_rows)) 
   println(u'      k/v   metadata:   ') 
   if (footer.key_value_metadata and (len(footer.key_value_metadata) > 0)): 
      for item in footer.key_value_metadata: 
         println(u'            {0}={1}'.format(item.key, item.value)) 
   else: 
      println(u'            (none)') 
   println(u'      schema:   ') 
   for element in footer.schema: 
      println(u'            {name}   ({type}):   length={type_length},   repetition={repetition_type},   children={num_children},   converted_type={converted_type}'.format(name=element.name, type=(parquet_thrift.Type._VALUES_TO_NAMES[element.type] if element.type else None), type_length=element.type_length, repetition_type=_get_name(parquet_thrift.FieldRepetitionType, element.repetition_type), num_children=element.num_children, converted_type=element.converted_type)) 
   if show_row_group_metadata: 
      println(u'      row   groups:   ') 
      for row_group in footer.row_groups: 
         num_rows = row_group.num_rows 
         size_bytes = row_group.total_byte_size 
         println(u'      rows={num_rows},   bytes={bytes}'.format(num_rows=num_rows, bytes=size_bytes)) 
         println(u'            chunks:') 
         for col_group in row_group.columns: 
            cmd = col_group.meta_data 
            println(u'                  type={type}   file_offset={offset}   compression={codec}   encodings={encodings}   path_in_schema={path_in_schema}   num_values={num_values}   uncompressed_bytes={raw_bytes}   compressed_bytes={compressed_bytes}   data_page_offset={data_page_offset}   dictionary_page_offset={dictionary_page_offset}'.format(type=_get_name(parquet_thrift.Type, cmd.type), offset=col_group.file_offset, codec=_get_name(parquet_thrift.CompressionCodec, cmd.codec), encodings=u','.join([_get_name(parquet_thrift.Encoding, s) for s in cmd.encodings]), path_in_schema=cmd.path_in_schema, num_values=cmd.num_values, raw_bytes=cmd.total_uncompressed_size, compressed_bytes=cmd.total_compressed_size, data_page_offset=cmd.data_page_offset, dictionary_page_offset=cmd.dictionary_page_offset)) 
            with open(filename, u'rb') as file_obj: 
               offset = _get_offset(cmd) 
               file_obj.seek(offset, 0) 
               values_read = 0 
               println(u'                  pages:   ') 
               while (values_read < num_rows): 
                  page_header = _read_page_header(file_obj) 
                  file_obj.seek(page_header.compressed_page_size, 1) 
                  daph = page_header.data_page_header 
                  type_ = _get_name(parquet_thrift.PageType, page_header.type) 
                  raw_bytes = page_header.uncompressed_page_size 
                  num_values = None 
                  if (page_header.type == parquet_thrift.PageType.DATA_PAGE): 
                     num_values = daph.num_values 
                     values_read += num_values 
                  if (page_header.type == parquet_thrift.PageType.DICTIONARY_PAGE): 
                     pass 
                  encoding_type = None 
                  def_level_encoding = None 
                  rep_level_encoding = None 
                  if daph: 
                     encoding_type = _get_name(parquet_thrift.Encoding, daph.encoding) 
                     def_level_encoding = _get_name(parquet_thrift.Encoding, daph.definition_level_encoding) 
                     rep_level_encoding = _get_name(parquet_thrift.Encoding, daph.repetition_level_encoding) 
                  println(u'                        page   header:   type={type}   uncompressed_size={raw_bytes}   num_values={num_values}   encoding={encoding}   def_level_encoding={def_level_encoding}   rep_level_encoding={rep_level_encoding}'.format(type=type_, raw_bytes=raw_bytes, num_values=num_values, encoding=encoding_type, def_level_encoding=def_level_encoding, rep_level_encoding=rep_level_encoding))"," 'Dump the metadata for a Parquet file. 
 Parameters 
 filename : str 
 The name of the Parquet file to dump. 
 show_row_group_metadata : bool 
 Whether to dump the metadata for row groups. 
 out : file-like 
 A file-like object to write to.'","'Dump metadata about the parquet object with the given filename. 
 Dump human-readable metadata to specified `out`. Optionally dump the row group metadata as well.'"
"def make_query_from_filter(sample_filter, require_meter=True): 
    q = {} 
   if sample_filter.user: 
      q['user_id'] = sample_filter.user 
   if sample_filter.project: 
      q['project_id'] = sample_filter.project 
   if sample_filter.meter: 
      q['counter_name'] = sample_filter.meter 
   elif require_meter: 
      raise RuntimeError('Missing   required   meter   specifier') 
   ts_range = make_timestamp_range(sample_filter.start_timestamp, sample_filter.end_timestamp, sample_filter.start_timestamp_op, sample_filter.end_timestamp_op) 
   if ts_range: 
      q['timestamp'] = ts_range 
   if sample_filter.resource: 
      q['resource_id'] = sample_filter.resource 
   if sample_filter.source: 
      q['source'] = sample_filter.source 
   if sample_filter.message_id: 
      q['message_id'] = sample_filter.message_id 
   q.update(dict(((('resource_%s' % k), v) for (k, v) in six.iteritems(improve_keys(sample_filter.metaquery, metaquery=True))))) 
   return q"," 'Returns a dict of query parameters from the given filter. 
 :param sample_filter: Filter to convert to query parameters. 
 :param require_meter: If True, a meter is required if a timestamp range is specified. 
 :returns: A dict of query parameters.'","'Return a query dictionary based on the settings in the filter. 
 :param sample_filter: SampleFilter instance 
 :param require_meter: If true and the filter does not have a meter, 
 raise an error.'"
"def get_value_from_user(message, default_value='', hidden=False): 
    return _validate_user_input(InputDialog(message, default_value, is_truthy(hidden)))"," 'Returns a value from the user, prompting for input if necessary. 
 :param message: The message to display to the user. 
 :param default_value: The default value to use if the user doesn\'t respond. 
 :param hidden: If True, the value is not saved. 
 :return: The user input, or the default value if the user didn\'t respond.'","'Pauses test execution and asks user to input a value. 
 Value typed by the user, or the possible default value, is returned. 
 Returning an empty value is fine, but pressing ``Cancel`` fails the keyword. 
 ``message`` is the instruction shown in the dialog and ``default_value`` is 
 the possible default value shown in the input field. 
 If ``hidden`` is given a true value, the value typed by the user is hidden. 
 ``hidden`` is considered true if it is a non-empty string not equal to 
 ``false`` or ``no``, case-insensitively. If it is not a string, its truth 
 value is got directly using same 
 [http://docs.python.org/2/library/stdtypes.html#truth-value-testing|rules 
 as in Python]. 
 Example: 
 | ${username} = | Get Value From User | Input user name | default    | 
 | ${password} = | Get Value From User | Input password  | hidden=yes | 
 Possibility to hide the typed in value is new in Robot Framework 2.8.4. 
 Considering strings ``false`` and ``no`` to be false is new in 2.9.'"
"def is_private_subnet(ip): 
    priv_subnets = [{'subnet': '10.0.0.0', 'mask': '255.0.0.0'}, {'subnet': '172.16.0.0', 'mask': '255.240.0.0'}, {'subnet': '192.168.0.0', 'mask': '255.255.0.0'}] 
   ip = struct.unpack('I', socket.inet_aton(ip))[0] 
   for network in priv_subnets: 
      subnet = struct.unpack('I', socket.inet_aton(network['subnet']))[0] 
      mask = struct.unpack('I', socket.inet_aton(network['mask']))[0] 
      if ((ip & mask) == (subnet & mask)): 
         return True 
   return False", 'Returns true if the IP is in a private subnet',"'Utility function to check if an IP address is inside a private subnet. 
 :type ip: ``str`` 
 :param ip: IP address to check 
 :return: ``bool`` if the specified IP address is private.'"
"def emit_certificate_event(event_name, user, course_id, course=None, event_data=None): 
    event_name = '.'.join(['edx', 'certificate', event_name]) 
   if (course is None): 
      course = modulestore().get_course(course_id, depth=0) 
   context = {'org_id': course.org, 'course_id': unicode(course_id)} 
   data = {'user_id': user.id, 'course_id': unicode(course_id), 'certificate_url': get_certificate_url(user.id, course_id)} 
   event_data = (event_data or {}) 
   event_data.update(data) 
   with tracker.get_tracker().context(event_name, context): 
      tracker.emit(event_name, event_data)", 'Emit a certificate event for the given user and course.','Emits certificate event.'
"def write_checkpoint(current_key, ctr, cluster_mapping, ids, bestscores, order, out_fp): 
    checkpoint_dir = (out_fp + '/checkpoints/') 
   if (not exists(checkpoint_dir)): 
      create_dir(checkpoint_dir) 
   out_fp = (checkpoint_dir + ('/checkpoint%d.pickle' % ctr)) 
   out_fh = open(out_fp, 'w') 
   pickle.dump((current_key, ctr, cluster_mapping, ids, bestscores, order), out_fh) 
   return out_fp"," 'Write the current key, cluster mapping, ids, bestscores and order to the 
 checkpoint file.'","'write intermediate results to checkpoint file 
 current_key: the identifier of the current denoiser round 
 ctr: a uniq counter to label the checkpoint 
 cluster_mapping: an intermediate cluster mapping as dict 
 ids: the dict of active ids 
 order:  a list of ids, which defines the order of which flowgrams are clustered 
 bestscores: a dict of'"
"def get_nexusport_binding(port_id, vlan_id, switch_ip, instance_id): 
    LOG.debug(_('get_nexusport_binding()   called')) 
   session = db.get_session() 
   try: 
      binding = session.query(nexus_models_v2.NexusPortBinding).filter_by(vlan_id=vlan_id).filter_by(switch_ip=switch_ip).filter_by(port_id=port_id).filter_by(instance_id=instance_id).all() 
      return binding 
   except exc.NoResultFound: 
      raise c_exc.NexusPortBindingNotFound(vlan_id=vlan_id)"," 'Get the NexusPortBinding object for a given port, vlan, and switch. 
 :param port_id: The ID of the port to bind the vlan to. 
 :param vlan_id: The ID of the vlan to bind the port to. 
 :param switch_ip: The IP of the switch to bind the vlan to. 
 :param instance_id: The ID of the instance to bind the vlan to. 
 :returns: A list of NexusPortBinding objects. 
 :raises: NexusPortBindingNotFound if no NexusPortBinding object is found.'",'Lists a nexusport binding'
"def filter_user(user_ref): 
    if user_ref: 
      user_ref = user_ref.copy() 
      user_ref.pop('password', None) 
      user_ref.pop('tenants', None) 
      user_ref.pop('groups', None) 
      user_ref.pop('domains', None) 
      try: 
         user_ref['extra'].pop('password', None) 
         user_ref['extra'].pop('tenants', None) 
      except KeyError: 
         pass 
      if ('password_expires_at' not in user_ref): 
         user_ref['password_expires_at'] = None 
   return user_ref"," 'Filters out password, tenants, groups and domains from user_ref. 
 :param user_ref: User reference to filter. 
 :returns: Filtered user reference. 
 :rtype: dict'","'Filter out private items in a user dict. 
 \'password\', \'tenants\' and \'groups\' are never returned. 
 :returns: user_ref'"
"def BdbQuit_excepthook(et, ev, tb, excepthook=None): 
    warnings.warn('`BdbQuit_excepthook`   is   deprecated   since   version   5.1', DeprecationWarning, stacklevel=2) 
   if (et == bdb.BdbQuit): 
      print 'Exiting   Debugger.' 
   elif (excepthook is not None): 
      excepthook(et, ev, tb) 
   else: 
      BdbQuit_excepthook.excepthook_ori(et, ev, tb)", 'Called by the debugger when it quits.',"'Exception hook which handles `BdbQuit` exceptions. 
 All other exceptions are processed using the `excepthook` 
 parameter.'"
"def social_auth_login_redirect(request): 
    redirect_value = request.REQUEST.get(REDIRECT_FIELD_NAME) 
   if redirect_value: 
      redirect_querystring = ((REDIRECT_FIELD_NAME + '=') + redirect_value) 
   else: 
      redirect_querystring = '' 
   return {'REDIRECT_FIELD_NAME': REDIRECT_FIELD_NAME, 'REDIRECT_FIELD_VALUE': redirect_value, 'redirect_querystring': redirect_querystring}"," 'Returns a dictionary of the form: 
 {""REDIRECT_FIELD_NAME"": ""REDIRECT_FIELD_VALUE"", ""redirect_querystring"": ""REDIRECT_FIELD_VALUE=REDIRECT_FIELD_VALUE""}. 
 This is used to redirect the user after a successful authentication. 
 :param request: The current request. 
 :return: A dictionary of the form described above.'",'Load current redirect to context.'
"def get_user_model(): 
    from django.db.models import get_model 
   try: 
      (app_label, model_name) = settings.AUTH_USER_MODEL.split('.') 
   except ValueError: 
      raise ImproperlyConfigured(""AUTH_USER_MODEL   must   be   of   the   form   'app_label.model_name'"") 
   user_model = get_model(app_label, model_name) 
   if (user_model is None): 
      raise ImproperlyConfigured((""AUTH_USER_MODEL   refers   to   model   '%s'   that   has   not   been   installed"" % settings.AUTH_USER_MODEL)) 
   return user_model"," 'Returns the User model. 
 This is used to access the User model, which is used by the 
 authentication system. 
 If the AUTH_USER_MODEL setting is not configured, the default 
 implementation is to use the django.contrib.auth.models.User model. 
 If the AUTH_USER_MODEL setting is configured, it must be of the form 
 ""app_label.model_name"" where app_label is the app_label of the 
 application that defines the model and model_name is the name of the 
 model. 
 If the AUTH_USER_MODEL setting is not configured, or is configured 
 incorrectly, an ImproperlyConfigured exception will be raised.'",'Returns the User model that is active in this project.'
"def remove_useless_meta(meta): 
    if meta: 
      for name in USELESS_META: 
         if (name in meta): 
            del meta[name] 
   return meta"," 'Remove useless metadata. 
 The following metadata is considered useless: 
 * ``md5`` 
 * ``sha1`` 
 * ``sha256`` 
 * ``sha512`` 
 * ``size`` 
 * ``type`` 
 * ``url`` 
 * ``date`` 
 * ``creation_date`` 
 * ``modification_date`` 
 * ``modification_time`` 
 * ``access_date`` 
 * ``access_time`` 
 * ``access_time_utc`` 
 * ``access_time_local`` 
 * ``access_time_local_utc`` 
 * ``access_time_local_utc_tz`` 
 * ``access_time_local_tz`` 
 * ``access_time_local_tz_utc`` 
 * ``access_time_local_tz_utc_tz`` 
 * ``access_time_local_tz_utc_tz_tz`` 
 * ``access_time_local_tz_tz_tz_tz`` 
 * ``access_time_local_tz_tz",'Removes useless (for database) attributes from the table\'s meta.'
"def bidirectional_rnn(incoming, rnncell_fw, rnncell_bw, return_seq=False, return_states=False, initial_state_fw=None, initial_state_bw=None, dynamic=False, scope=None, name='BiRNN'): 
    assert (rnncell_fw._num_units == rnncell_bw._num_units), 'RNN   Cells   number   of   units   must   match!' 
   sequence_length = None 
   if dynamic: 
      sequence_length = retrieve_seq_length_op((incoming if isinstance(incoming, tf.Tensor) else tf.pack(incoming))) 
   input_shape = utils.get_incoming_shape(incoming) 
   with tf.variable_scope(scope, name, values=[incoming]) as scope: 
      name = scope.name 
      inference = incoming 
      if (type(inference) not in [list, np.array]): 
         ndim = len(input_shape) 
         assert (ndim >= 3), 'Input   dim   should   be   at   least   3.' 
         axes = ([1, 0] + list(range(2, ndim))) 
         inference = tf.transpose(inference, axes) 
         inference = tf.unpack(inference) 
      (outputs, states_fw, states_bw) = _brnn(rnncell_fw, rnncell_bw, inference, initial_state_fw=initial_state_fw, initial_state_bw=initial_state_bw, sequence_length=sequence_length, dtype=tf.float32) 
      c = ((tf.GraphKeys.LAYER_VARIABLES + '/') + scope.name) 
      for v in [rnncell_fw.W, rnncell_fw.b, rnncell_bw.W, rnncell_bw.b]: 
         if hasattr(v, '__len__'): 
            for var in v: 
               tf.add_to_collection(c, var) 
         else: 
            tf.add_to_collection(c, v) 
      tf.add_to_collection(tf.GraphKeys.ACTIVATIONS, outputs[(-1)]) 
   if dynamic: 
      if return_seq: 
         o = outputs 
      else: 
         outputs = tf.transpose(tf.pack(outputs), [1, 0, 2]) 
         o = advanced_indexing_op(outputs, sequence_length) 
   else: 
      o = (outputs if return_seq else outputs[(-1)]) 
   sfw = states_fw 
   sbw = states_bw 
   tf.add_to_collection(((tf.GraphKeys.LAYER_TENSOR + '/') + name), o) 
   return ((o, sfw, sbw) if return_states else o)"," 'Compute the output and states of a bidirectional RNN cell. 
 Parameters 
 incoming : Tensor or list of Tensors 
 The input to the RNN cell. 
 rnncell_fw : RNNCell 
 The forward RNNCell. 
 rnncell_bw : RNNCell 
 The backward RNNCell. 
 return_seq : bool 
 Whether to return the output sequence. 
 return_states : bool 
 Whether to return the states of the RNN. 
 initial_state_fw : Tensor 
 The initial state of the forward RNNCell. 
 initial_state_bw : Tensor 
 The initial state of the backward RNNCell. 
 dynamic : bool 
 Whether to return the output sequence or the final state. 
 scope : str 
 The name of the scope to use. 
 name : str 
 The name to give the returned tensor. 
 Returns 
 o : Tensor 
 The output sequence. 
 sfw : Tensor 
 The forward states. 
 sbw : Tensor 
 The backward states. 
 Ex","'Bidirectional RNN. 
 Build a bidirectional recurrent neural network, it requires 2 RNN Cells 
 to process sequence in forward and backward order. Any RNN Cell can be 
 used i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two 
 cells number of units must match. 
 Input: 
 3-D Tensor Layer [samples, timesteps, input dim]. 
 Output: 
 if `return_seq`: 3-D Tensor [samples, timesteps, output dim]. 
 else: 2-D Tensor Layer [samples, output dim]. 
 Arguments: 
 incoming: `Tensor`. The incoming Tensor. 
 rnncell_fw: `RNNCell`. The RNN Cell to use for foward computation. 
 rnncell_bw: `RNNCell`. The RNN Cell to use for backward computation. 
 return_seq: `bool`. If True, returns the full sequence instead of 
 last sequence output only. 
 return_states: `bool`. If True, returns a tuple with output and 
 states: (output, states). 
 initial_state_fw: `Tensor`. An initial state for the forward RNN. 
 This must be a tensor of appropriate type and shape [batch_size 
 x cell.state_size]. 
 initial_state_bw: `Tensor`. An initial state for the backward RNN. 
 This must be a tensor of appropriate type and shape [batch_size 
 x cell.state_size]. 
 dynamic: `bool`. If True, dynamic computation is performed. It will not 
 compute RNN steps above the sequence length. Note that because TF 
 requires to feed sequences of same length, 0 is used as a mask. 
 So a sequence padded with 0 at the end must be provided. When 
 computation is performed, it will stop when it meets a step with 
 a value of 0. 
 scope: `str`. Define this layer scope (optional). A scope can be 
 used to share variables between layers. Note that scope will 
 override name. 
 name: `str`. A name for this layer (optional).'"
"def test_scenario_may_own_outlines(): 
    scenario = Scenario.from_string(OUTLINED_SCENARIO) 
   assert_equals(len(scenario.steps), 4) 
   expected_sentences = ['Given   I   have   entered   <input_1>   into   the   calculator', 'And   I   have   entered   <input_2>   into   the   calculator', 'When   I   press   <button>', 'Then   the   result   should   be   <output>   on   the   screen'] 
   for (step, expected_sentence) in zip(scenario.steps, expected_sentences): 
      assert_equals(type(step), Step) 
      assert_equals(step.sentence, expected_sentence) 
   assert_equals(scenario.name, 'Add   two   numbers') 
   assert_equals(scenario.outlines, [{'input_1': '20', 'input_2': '30', 'button': 'add', 'output': '50'}, {'input_1': '2', 'input_2': '5', 'button': 'add', 'output': '7'}, {'input_1': '0', 'input_2': '40', 'button': 'add', 'output': '40'}])", 'Test that scenario may own outlines.','A scenario may own outlines'
"def xml_decode(string): 
    string = string.replace('&amp;', '&') 
   string = string.replace('&lt;', '<') 
   string = string.replace('&gt;', '>') 
   string = string.replace('&quot;', '""') 
   string = string.replace('/', SLASH) 
   return string"," 'Decodes XML entities. 
 :param string: 
 :return: 
 :rtype: 
 :raises: 
 :param string: 
 :type string: 
 :return: 
 :rtype: 
 :raises: 
 :param string: 
 :type string: 
 :return: 
 :rtype: 
 :raises: 
 :param string: 
 :type string: 
 :return: 
 :rtype: 
 :raises: 
 :param string: 
 :type string: 
 :return: 
 :rtype: 
 :raises: 
 :param string: 
 :type string: 
 :return: 
 :rtype: 
 :raises: 
 :param string: 
 :type string: 
 :return: 
 :rtype: 
 :raises: 
 :param string: 
 :type string: 
 :return: 
 :rtype: 
 :raises: 
 :param string: 
 :type string: 
 :return: 
 :rtype: ",'Returns the string with special characters decoded.'
"def ChiNoncentral(name, k, l): 
    return rv(name, ChiNoncentralDistribution, (k, l))"," 'Generate a Chi-noncentral random variable. 
 Parameters 
 name : str 
 A string giving the name of the random variable. 
 k : int 
 The degrees of freedom. 
 l : int 
 The noncentrality parameter. 
 Returns 
 A `RandomVariable` instance. 
 Examples 
 >>> from sympy.stats import ChiNoncentral, Normal 
 >>> from sympy.abc import x 
 >>> from sympy import Symbol, simplify 
 >>> x = Symbol(""x"") 
 >>> Normal(name=""x"", mu=0, sigma=1) 
 Normal(x, 0, 1) 
 >>> ChiNoncentral(name=""x"", k=2, l=2) 
 ChiNoncentral(x, 2, 2) 
 >>> simplify(ChiNoncentral(x, k=2, l=2).pdf(x)) 
 1/2*x**2*exp(-x**2/4)/sqrt(pi) 
 >>> simplify(ChiNoncentral(x, k=2, l=2).cdf(x)) 
 1/","'Create a continuous random variable with a non-central Chi distribution. 
 The density of the non-central Chi distribution is given by 
 .. math:: 
 f(x) := \frac{e^{-(x^2+\lambda^2)/2} x^k\lambda} 
 {(\lambda x)^{k/2}} I_{k/2-1}(\lambda x) 
 with `x \geq 0`. Here, `I_\nu (x)` is the 
 :ref:`modified Bessel function of the first kind <besseli>`. 
 Parameters 
 k : A positive Integer, `k > 0`, the number of degrees of freedom 
 l : Shift parameter 
 Returns 
 A RandomSymbol. 
 Examples 
 >>> from sympy.stats import ChiNoncentral, density, E, std 
 >>> from sympy import Symbol, simplify 
 >>> k = Symbol(""k"", integer=True) 
 >>> l = Symbol(""l"") 
 >>> z = Symbol(""z"") 
 >>> X = ChiNoncentral(""x"", k, l) 
 >>> density(X)(z) 
 l*z**k*(l*z)**(-k/2)*exp(-l**2/2 - z**2/2)*besseli(k/2 - 1, l*z) 
 References 
 .. [1] http://en.wikipedia.org/wiki/Noncentral_chi_distribution'"
"def name_for_collection_relationship(base, local_cls, referred_cls, constraint): 
    return (referred_cls.__name__.lower() + '_collection')"," 'Return a name for the collection relationship between two classes. 
 :param base: the class that owns the collection 
 :param local_cls: the class that is being collected 
 :param referred_cls: the class that is being collected 
 :param constraint: a constraint on the collection relationship'","'Return the attribute name that should be used to refer from one 
 class to another, for a collection reference. 
 The default implementation is:: 
 return referred_cls.__name__.lower() + ""_collection"" 
 Alternate implementations 
 can be specified using the 
 :paramref:`.AutomapBase.prepare.name_for_collection_relationship` 
 parameter. 
 :param base: the :class:`.AutomapBase` class doing the prepare. 
 :param local_cls: the class to be mapped on the local side. 
 :param referred_cls: the class to be mapped on the referring side. 
 :param constraint: the :class:`.ForeignKeyConstraint` that is being 
 inspected to produce this relationship.'"
"def demo_str_rule_format(): 
    postag(ruleformat='str')", 'Demo of the str rule format',"'Exemplify repr(Rule) (see also str(Rule) and Rule.format(""verbose""))'"
"def get_location(vm_=None): 
    return __opts__.get('location', config.get_cloud_config_value('location', (vm_ or get_configured_provider()), __opts__, default=DEFAULT_LOCATION, search_global=False))"," 'Returns the location for the cloud provider. 
 :param vm_: The vm to get the location for. 
 :returns: The location. 
 :rtype: str'","'Return the EC2 region to use, in this order: 
 - CLI parameter 
 - VM parameter 
 - Cloud profile setting'"
"def test_unicode_column(tmpdir): 
    t = Table([np.array([u'a', u'b', u'cd'])]) 
   t.write(str(tmpdir.join('test.fits')), overwrite=True) 
   with fits.open(str(tmpdir.join('test.fits'))) as hdul: 
      assert np.all((hdul[1].data['col0'] == ['a', 'b', 'cd'])) 
      assert (hdul[1].header['TFORM1'] == '2A') 
   t2 = Table([np.array([u'\u2603'])]) 
   with pytest.raises(UnicodeEncodeError): 
      t2.write(str(tmpdir.join('test.fits')), overwrite=True)", 'Test unicode column writing',"'Test that a column of unicode strings is still written as one 
 byte-per-character in the FITS table (so long as the column can be ASCII 
 encoded). 
 Regression test for one of the issues fixed in 
 https://github.com/astropy/astropy/pull/4228'"
"@skip_if_not_win32 
 @with_environment 
 def test_get_home_dir_1(): 
    unfrozen = path.get_home_dir() 
   sys.frozen = True 
   IPython.__file__ = abspath(join(HOME_TEST_DIR, 'Lib/IPython/__init__.py')) 
   home_dir = path.get_home_dir() 
   nt.assert_equal(home_dir, unfrozen)", 'Test that the home directory is not changed when IPython is frozen',"'Testcase for py2exe logic, un-compressed lib'"
"def pct_to_int(value, num_items, min_value=1): 
    if (isinstance(value, string_types) and value.endswith('%')): 
      value_pct = int(value.replace('%', '')) 
      return (int(((value_pct / 100.0) * num_items)) or min_value) 
   else: 
      return int(value)"," 'Convert a percentage to an integer, rounding down to the nearest 
 integer. 
 :param value: 
 The value to convert. 
 :type value: 
 :param num_items: 
 The number of items in the set. 
 :type num_items: 
 :param min_value: 
 The minimum value to return if the value is less than 1. 
 :type min_value: 
 :return: 
 The integer value.'","'Converts a given value to a percentage if specified as ""x%"", 
 otherwise converts the given value to an integer.'"
"def zip_timeseries(*series, **kwargs): 
    next_slice = (max if (kwargs.get('order', 'descending') == 'descending') else min) 
   iterators = [PeekableIterator(s) for s in series] 
   widths = [] 
   for w in iterators: 
      r = w.peek() 
      if r: 
         (date, values) = r 
         widths.append(len(values)) 
      else: 
         widths.append(0) 
   while True: 
      items = [it.peek() for it in iterators] 
      if (not any(items)): 
         return 
      current_slice = next_slice((item[0] for item in items if item)) 
      data = [] 
      for (i, item) in enumerate(items): 
         if (item and (item[0] == current_slice)): 
            data.extend(item[1]) 
            iterators[i].next() 
         else: 
            data.extend(([0] * widths[i])) 
      (yield (current_slice, tuple(data)))"," 'Returns a zip of timeseries. 
 Parameters 
 *series : list of :class:`~pandas.core.series.Series` 
 The series to be zipped. 
 **kwargs : dict 
 Keyword arguments to be passed to the :meth:`~pandas.core.series.Series.resample` method. 
 Returns 
 :class:`~pandas.core.series.Series` 
 A new series with the zipped values.'","'Zip timeseries data while gracefully handling gaps in the data. 
 Timeseries data is expected to be a sequence of two-tuples (date, values). 
 Values is expected itself to be a tuple. The width of the values tuples 
 should be the same across all elements in a timeseries sequence. The result 
 will be a single sequence in timeseries format. 
 Gaps in sequences are filled with an appropriate number of zeros based on 
 the size of the first value-tuple of that sequence.'"
"def html_body(input_string, source_path=None, destination_path=None, input_encoding='unicode', output_encoding='unicode', doctitle=True, initial_header_level=1): 
    parts = html_parts(input_string=input_string, source_path=source_path, destination_path=destination_path, input_encoding=input_encoding, doctitle=doctitle, initial_header_level=initial_header_level) 
   fragment = parts['html_body'] 
   if (output_encoding != 'unicode'): 
      fragment = fragment.encode(output_encoding) 
   return fragment", 'Convert the given HTML fragment into a string.',"'Given an input string, returns an HTML fragment as a string. 
 The return value is the contents of the <body> element. 
 Parameters (see `html_parts()` for the remainder): 
 - `output_encoding`: The desired encoding of the output.  If a Unicode 
 string is desired, use the default value of ""unicode"" .'"
"def wlPen(wl): 
    l1 = 400 
   l2 = 700 
   hue = np.clip(((((l2 - l1) - (wl - l1)) * 0.8) / (l2 - l1)), 0, 0.8) 
   val = 1.0 
   if (wl > 700): 
      val = (1.0 * (((700 - wl) / 700.0) + 1)) 
   elif (wl < 400): 
      val = ((wl * 1.0) / 400.0) 
   color = pg.hsvColor(hue, 1.0, val) 
   pen = pg.mkPen(color) 
   return pen"," 'Generate a pen with the given color and width. 
 Parameters 
 wl : float 
 The width of the pen. 
 Returns 
 pen : pg.pen.Pen 
 A pen with the given color and width.'",'Return a pen representing the given wavelength'
"@conf.commands.register 
 def defragment(plist): 
    frags = defaultdict((lambda : [])) 
   final = [] 
   pos = 0 
   for p in plist: 
      p._defrag_pos = pos 
      pos += 1 
      if (IP in p): 
         ip = p[IP] 
         if ((ip.frag != 0) or (ip.flags & 1)): 
            ip = p[IP] 
            uniq = (ip.id, ip.src, ip.dst, ip.proto) 
            frags[uniq].append(p) 
            continue 
      final.append(p) 
   defrag = [] 
   missfrag = [] 
   for lst in frags.itervalues(): 
      lst.sort(key=(lambda x: x.frag)) 
      p = lst[0] 
      lastp = lst[(-1)] 
      if ((p.frag > 0) or ((lastp.flags & 1) != 0)): 
         missfrag += lst 
         continue 
      p = p.copy() 
      if (conf.padding_layer in p): 
         del p[conf.padding_layer].underlayer.payload 
      ip = p[IP] 
      if ((ip.len is None) or (ip.ihl is None)): 
         clen = len(ip.payload) 
      else: 
         clen = (ip.len - (ip.ihl << 2)) 
      txt = conf.raw_layer() 
      for q in lst[1:]: 
         if (clen != (q.frag << 3)): 
            if (clen > (q.frag << 3)): 
               warning(('Fragment   overlap   (%i   >   %i)   %r   ||   %r   ||      %r' % (clen, (q.frag << 3), p, txt, q))) 
            missfrag += lst 
            break 
         if ((q[IP].len is None) or (q[IP].ihl is None)): 
            clen += len(q[IP].payload) 
         else: 
            clen += (q[IP].len - (q[IP].ihl << 2)) 
         if (conf.padding_layer in q): 
            del q[conf.padding_layer].underlayer.payload 
         txt.add_payload(q[IP].payload.copy()) 
      else: 
         ip.flags &= (~ 1) 
         del ip.chksum 
         del ip.len 
         p = (p / txt) 
         p._defrag_pos = max((x._defrag_pos for x in lst)) 
         defrag.append(p) 
   defrag2 = [] 
   for p in defrag: 
      q = p.__class__(str(p)) 
      q._defrag_pos = p._defrag_pos 
      defrag2.append(q) 
   final += defrag2 
   final += missfrag 
   final.sort(key=(lambda x: x._defrag_pos)) 
   for p in final: 
      del p._defrag_pos 
   if hasattr(plist, 'listname'): 
      name = ('Defragmented   %s' % plist.listname) 
   else: 
      name = 'Defragmented' 
   return PacketList(final, name=name)"," 'Defragment a packet list. 
 :param plist: Packet list to defragment. 
 :type plist: :class:`packet.PacketList` 
 :returns: A new packet list with defragmented packets. 
 :rtype: :class:`packet.PacketList` 
 :raises: :class:`packet.PacketListError` 
 :raises: :class:`packet.PacketListError.PaddingError` 
 :raises: :class:`packet.PacketListError.FragmentError` 
 :raises: :class:`packet.PacketListError.FragmentOverlapError` 
 :raises: :class:`packet.PacketListError.FragmentError` 
 :raises: :class:`packet.PacketListError.FragmentError` 
 :raises: :class:`packet.PacketListError.FragmentError` 
 :raises: :class:`packet.PacketListError.FragmentError` 
 :raises: :class:`packet.PacketListError.FragmentError` 
 :raises: :",'defrag(plist) -> plist defragmented as much as possible'
"def validate_positive_integer_or_none(option, value): 
    if (value is None): 
      return value 
   return validate_positive_integer(option, value)", 'Validate that the value is a positive integer or None','Validate that \'value\' is a positive integer or None.'
"def sixteen(data): 
    n = 0 
   for b in serial.iterbytes(data): 
      (yield ('{:02X}   '.format(ord(b)), (b.decode('ascii') if ('   ' <= b < '\x7f') else '.'))) 
      n += 1 
      if (n == 8): 
         (yield ('   ', '')) 
      elif (n >= 16): 
         (yield (None, None)) 
         n = 0 
   if (n > 0): 
      while (n < 16): 
         n += 1 
         if (n == 8): 
            (yield ('   ', '')) 
         (yield ('         ', '   ')) 
      (yield (None, None))"," 'Return a string of hexadecimal values representing the data. 
 The format is: 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex value> 
 <hex value> <hex value> <hex","'yield tuples of hex and ASCII display in multiples of 16. Includes a 
 space after 8 bytes and (None, None) after 16 bytes and at the end.'"
"def test_example2(): 
    vert_template = Function('\n            void   main(void)\n            {\n                        gl_Position   =   $position;\n            }\n            ') 
   transformScale = Function('\n            vec4   transform_scale(vec4   pos)\n            {\n                        pos.xyz   *=   $scale;\n                        return   pos;\n            }\n            ') 
   class Transform(object, ): 
      def __init__(self): 
         self.func = Function(transformScale) 
         self.func['scale'] = 'uniform   float' 
      def set_scale(self, scale): 
         self.func['scale'].value = scale 
   transforms = [Transform(), Transform(), Transform()] 
   code = Function(vert_template) 
   ob = Variable('attribute   vec3   a_position') 
   for trans in transforms: 
      ob = trans.func(ob) 
   code['position'] = ob 
   print code", 'Testing the example2.frag','Demonstrate how a transform would work.'
"def texts(i, e): 
    return pq(e).text()", 'Returns the text content of the element with index i in e.','Helper for getting the text of an element'
"def test_conditional_get_vae(): 
    mlp = MLP(layers=[Linear(layer_name='h', dim=5, irange=0.01)]) 
   conditional = DummyConditional(mlp=mlp, name='conditional') 
   vae = DummyVAE() 
   conditional.set_vae(vae) 
   testing.assert_same_object(conditional.get_vae(), vae)", 'Test conditional get_vae.','Conditional.get_vae returns its VAE'
"def task_create(context, values, session=None): 
    values = values.copy() 
   session = (session or get_session()) 
   with session.begin(): 
      task_info_values = _pop_task_info_values(values) 
      task_ref = models.Task() 
      _task_update(context, task_ref, values, session=session) 
      _task_info_create(context, task_ref.id, task_info_values, session=session) 
   return task_get(context, task_ref.id, session)"," 'Create a task. 
 :param values: The values to create the task with. 
 :param session: The database session to use.'",'Create a task object'
"def getEvaluatedIntDefault(defaultInt, key, xmlElement=None): 
    evaluatedInt = getEvaluatedInt(key, xmlElement) 
   if (evaluatedInt == None): 
      return defaultInt 
   return evaluatedInt"," 'Returns the value of the key, or the default value if it does not exist.'",'Get the evaluated value as an int.'
"def _ToBlobstoreError(error): 
    error_map = {blobstore_service_pb.BlobstoreServiceError.INTERNAL_ERROR: InternalError, blobstore_service_pb.BlobstoreServiceError.BLOB_NOT_FOUND: BlobNotFoundError, blobstore_service_pb.BlobstoreServiceError.DATA_INDEX_OUT_OF_RANGE: DataIndexOutOfRangeError, blobstore_service_pb.BlobstoreServiceError.BLOB_FETCH_SIZE_TOO_LARGE: BlobFetchSizeTooLargeError, blobstore_service_pb.BlobstoreServiceError.PERMISSION_DENIED: PermissionDeniedError} 
   desired_exc = error_map.get(error.application_error) 
   return (desired_exc(error.error_detail) if desired_exc else error)"," 'Convert a BlobstoreServiceError to a BlobstoreError. 
 Args: 
 error: A BlobstoreServiceError. 
 Returns: 
 A BlobstoreError.'","'Translate an application error to a datastore Error, if possible. 
 Args: 
 error: An ApplicationError to translate.'"
"def csolve_prime(f, p, e=1): 
    from sympy.polys.domains import ZZ 
   X1 = [i for i in range(p) if (gf_eval(f, i, p, ZZ) == 0)] 
   if (e == 1): 
      return X1 
   X = [] 
   S = list(zip(X1, ([1] * len(X1)))) 
   while S: 
      (x, s) = S.pop() 
      if (s == e): 
         X.append(x) 
      else: 
         s1 = (s + 1) 
         ps = (p ** s) 
         S.extend([((x + (v * ps)), s1) for v in _raise_mod_power(x, s, p, f)]) 
   return sorted(X)"," 'Solve for prime factors of a polynomial. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import csolve_prime 
 >>> f = ZZ.factor(10) 
 >>> csolve_prime(f, 10) 
 [2, 5]'","'Solutions of f(x) congruent 0 mod(p**e). 
 Examples 
 >>> from sympy.polys.galoistools import csolve_prime 
 >>> csolve_prime([1, 1, 7], 3, 1) 
 [1] 
 >>> csolve_prime([1, 1, 7], 3, 2) 
 [1, 4, 7] 
 Solutions [7, 4, 1] (mod 3**2) are generated by ``_raise_mod_power()`` 
 from solution [1] (mod 3).'"
"def _get_nets(vif, subnet, version, net_num, link_id): 
    if (subnet.get_meta('dhcp_server') is not None): 
      net_info = {'id': ('network%d' % net_num), 'type': ('ipv%d_dhcp' % version), 'link': link_id, 'network_id': vif['network']['id']} 
      return net_info 
   ip = subnet['ips'][0] 
   address = ip['address'] 
   if (version == 4): 
      netmask = model.get_netmask(ip, subnet) 
   elif (version == 6): 
      netmask = str(subnet.as_netaddr().netmask) 
   net_info = {'id': ('network%d' % net_num), 'type': ('ipv%d' % version), 'link': link_id, 'ip_address': address, 'netmask': netmask, 'routes': _get_default_route(version, subnet), 'network_id': vif['network']['id']} 
   for route in subnet['routes']: 
      route_addr = netaddr.IPNetwork(route['cidr']) 
      new_route = {'network': str(route_addr.network), 'netmask': str(route_addr.netmask), 'gateway': route['gateway']['address']} 
      net_info['routes'].append(new_route) 
   return net_info"," 'Returns a dictionary containing the required information to create a 
 network.'","'Get networks for the given VIF and subnet 
 :param vif: Neutron VIF 
 :param subnet: Neutron subnet 
 :param version: IP version as an int, either \'4\' or \'6\' 
 :param net_num: Network index for generating name of each network 
 :param link_id: Arbitrary identifier for the link the networks are 
 attached to'"
"def inv_item_quantity(): 
    try: 
      item_id = request.args[0] 
   except: 
      raise HTTP(400, current.xml.json_message(False, 400, 'No   value   provided!')) 
   table = s3db.inv_inv_item 
   ptable = db.supply_item_pack 
   query = ((table.id == item_id) & (table.item_pack_id == ptable.id)) 
   record = db(query).select(table.quantity, ptable.quantity, limitby=(0, 1)).first() 
   d = {'iquantity': record.inv_inv_item.quantity, 'pquantity': record.supply_item_pack.quantity} 
   output = json.dumps(d) 
   response.headers['Content-Type'] = 'application/json' 
   return output", 'Returns the quantity of the item in the pack',"'Access via the .json representation to avoid work rendering menus, etc'"
"def get_connection(service, module=None, region=None, key=None, keyid=None, profile=None): 
    module = (module or service) 
   (cxkey, region, key, keyid) = _get_profile(service, region, key, keyid, profile) 
   cxkey = (cxkey + ':conn3') 
   if (cxkey in __context__): 
      return __context__[cxkey] 
   try: 
      session = boto3.session.Session(aws_access_key_id=keyid, aws_secret_access_key=key, region_name=region) 
      if (session is None): 
         raise SaltInvocationError('Region   ""{0}""   is   not   valid.'.format(region)) 
      conn = session.client(module) 
      if (conn is None): 
         raise SaltInvocationError('Region   ""{0}""   is   not   valid.'.format(region)) 
   except boto.exception.NoAuthHandlerFound: 
      raise SaltInvocationError('No   authentication   credentials   found   when   attempting   to   make   boto   {0}   connection   to   region   ""{1}"".'.format(service, region)) 
   __context__[cxkey] = conn 
   return conn", 'Get a boto3 connection for the service module.',"'Return a boto connection for the service. 
 .. code-block:: python 
 conn = __utils__[\'boto.get_connection\'](\'ec2\', profile=\'custom_profile\')'"
"def trigger(registry, xml_parent, data): 
    tconfig = XML.SubElement(xml_parent, 'hudson.tasks.BuildTrigger') 
   childProjects = XML.SubElement(tconfig, 'childProjects') 
   childProjects.text = data['project'] 
   tthreshold = XML.SubElement(tconfig, 'threshold') 
   threshold = data.get('threshold', 'SUCCESS') 
   supported_thresholds = ['SUCCESS', 'UNSTABLE', 'FAILURE'] 
   if (threshold not in supported_thresholds): 
      raise JenkinsJobsException(('threshold   must   be   one   of   %s' % ',   '.join(supported_thresholds))) 
   tname = XML.SubElement(tthreshold, 'name') 
   tname.text = hudson_model.THRESHOLDS[threshold]['name'] 
   tordinal = XML.SubElement(tthreshold, 'ordinal') 
   tordinal.text = hudson_model.THRESHOLDS[threshold]['ordinal'] 
   tcolor = XML.SubElement(tthreshold, 'color') 
   tcolor.text = hudson_model.THRESHOLDS[threshold]['color']"," 'Trigger a build for a given project. 
 :param registry: Jenkins registry to use 
 :param xml_parent: parent XML element 
 :param data: dictionary with the following keys: 
 :code:`project` - name of the project to trigger 
 :code:`threshold` - threshold to trigger the build 
 :code:`trigger_type` - type of trigger to use 
 :code:`build_number` - build number to trigger 
 :code:`build_number_increment` - increment for the build number 
 :code:`build_number_increment_by` - how to increment the build number 
 :code:`build_number_prefix` - prefix for the build number 
 :code:`build_number_suffix` - suffix for the build number 
 :code:`build_number_format` - format for the build number 
 :code:`build_number_format_prefix` - prefix for the build number format 
 :code:`build_number_format_suffix` - suffix for the build number format 
 :code:`build_number_format_pattern` - pattern for the build number format 
 :code:`","'yaml: trigger 
 Trigger non-parametrised builds of other jobs. 
 :arg str project: name of the job to trigger 
 :arg str threshold: when to trigger the other job (default \'SUCCESS\'), 
 alternatives: SUCCESS, UNSTABLE, FAILURE 
 Example: 
 .. literalinclude:: /../../tests/publishers/fixtures/trigger_success.yaml 
 :language: yaml'"
"def logger(_modem, message_, type_): 
    pass"," 'Logs a message to the console. 
 Arguments: 
 modem -- The modem to log to. 
 message -- The message to log. 
 type_ -- The type of the message.'",'Supress all output from pySerial and gsmmodem'
"def read_double(fid): 
    return _unpack_simple(fid, '>f8', np.float64)"," 'Reads a double from the file. 
 Parameters 
 fid : file object 
 The file to read from. 
 Returns 
 array 
 The double from the file.'",'Read 64bit float from bti file.'
"def getToothProfileCylinder(derivation, pitchRadius, teeth): 
    toothProfileHalfCylinder = getToothProfileHalfCylinder(derivation, pitchRadius) 
   toothProfileHalfCylinder = getThicknessMultipliedPath(toothProfileHalfCylinder, derivation.toothThicknessMultiplier) 
   toothProfileHalf = [] 
   innerRadius = (pitchRadius - derivation.dedendum) 
   for point in toothProfileHalfCylinder: 
      if (abs(point) >= innerRadius): 
         toothProfileHalf.append(point) 
   return getToothProfileCylinderByProfile(derivation, pitchRadius, teeth, toothProfileHalf)"," 'Returns the tooth profile for a cylinder with the given teeth. 
 The tooth profile is a list of points on the tooth profile. 
 The first point is the pitch radius, the second point is the 
 dedendum, and the rest are the teeth. 
 Parameters 
 derivation : Derivation object 
 pitchRadius : float 
 teeth : list of floats 
 toothProfileHalfCylinder : list of floats 
 Returns 
 toothProfileHalf : list of floats 
 The tooth profile for the given teeth. 
 The first point is the pitch radius, the second point is the 
 dedendum, and the rest are the teeth. 
 Examples 
 >>> from camsim.geom import getToothProfileCylinder 
 >>> from camsim.derivation import Derivation 
 >>> pitchRadius = 10.0 
 >>> teeth = [0.25, 0.5, 0.75] 
 >>> derivation = Derivation(pitchRadius, teeth) 
 >>> toothProfileHalfCylinder = getTooth",'Get profile for one tooth of a cylindrical gear.'
"def run_simple(hostname, port, application, use_reloader=False, use_debugger=False, use_evalex=True, extra_files=None, reloader_interval=1, reloader_type='auto', threaded=False, processes=1, request_handler=None, static_files=None, passthrough_errors=False, ssl_context=None): 
    if use_debugger: 
      from werkzeug.debug import DebuggedApplication 
      application = DebuggedApplication(application, use_evalex) 
   if static_files: 
      from werkzeug.wsgi import SharedDataMiddleware 
      application = SharedDataMiddleware(application, static_files) 
   def log_startup(sock): 
      display_hostname = (((hostname not in ('', '*')) and hostname) or 'localhost') 
      if (':' in display_hostname): 
         display_hostname = ('[%s]' % display_hostname) 
      quit_msg = '(Press   CTRL+C   to   quit)' 
      port = sock.getsockname()[1] 
      _log('info', '   *   Running   on   %s://%s:%d/   %s', (((ssl_context is None) and 'http') or 'https'), display_hostname, port, quit_msg) 
   def inner(): 
      try: 
         fd = int(os.environ['WERKZEUG_SERVER_FD']) 
      except (LookupError, ValueError): 
         fd = None 
      srv = make_server(hostname, port, application, threaded, processes, request_handler, passthrough_errors, ssl_context, fd=fd) 
      if (fd is None): 
         log_startup(srv.socket) 
      srv.serve_forever() 
   if use_reloader: 
      if (os.environ.get('WERKZEUG_RUN_MAIN') != 'true'): 
         if ((port == 0) and (not can_open_by_fd)): 
            raise ValueError('Cannot   bind   to   a   random   port   with   enabled   reloader   if   the   Python   interpreter   does   not   support   socket   opening   by   fd.') 
         address_family = select_ip_version(hostname, port) 
         s = socket.socket(address_family, socket.SOCK_STREAM) 
         s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) 
         s.bind((hostname, port)) 
         if hasattr(s, 'set_inheritable'): 
            s.set_inheritable(True) 
         if can_open_by_fd: 
            os.environ['WERKZEUG_SERVER_FD'] = str(s.fileno()) 
            s.listen(LISTEN_QUEUE) 
            log_startup(s) 
         else: 
            s.close() 
      from ._reloader import run_with_reloader 
      run_with_reloader(inner, extra_files, reloader_interval, reloader_type) 
   else: 
      inner()"," 'Runs a Werkzeug application on a given port. 
 :param hostname: Hostname or IP address to bind to. 
 :param port: Port to bind to. 
 :param application: Werkzeug application to run. 
 :param use_reloader: Whether to use the reloader. 
 :param use_debugger: Whether to use the debugger. 
 :param use_evalex: Whether to use the evalex debugger. 
 :param extra_files: Files to be served by the application. 
 :param reloader_interval: Interval between reloads. 
 :param reloader_type: Type of reloader to use. 
 :param threaded: Whether to run in a threaded mode. 
 :param processes: Number of processes to run. 
 :param request_handler: Request handler to use. 
 :param static_files: Static files to serve. 
 :param passthrough_errors: Whether to pass errors to the main process. 
 :param ssl_context: SSL context to use. 
 :rtype: None'","'Start a WSGI application. Optional features include a reloader, 
 multithreading and fork support. 
 This function has a command-line interface too:: 
 python -m werkzeug.serving --help 
 .. versionadded:: 0.5 
 `static_files` was added to simplify serving of static files as well 
 as `passthrough_errors`. 
 .. versionadded:: 0.6 
 support for SSL was added. 
 .. versionadded:: 0.8 
 Added support for automatically loading a SSL context from certificate 
 file and private key. 
 .. versionadded:: 0.9 
 Added command-line interface. 
 .. versionadded:: 0.10 
 Improved the reloader and added support for changing the backend 
 through the `reloader_type` parameter.  See :ref:`reloader` 
 for more information. 
 :param hostname: The host for the application.  eg: ``\'localhost\'`` 
 :param port: The port for the server.  eg: ``8080`` 
 :param application: the WSGI application to execute 
 :param use_reloader: should the server automatically restart the python 
 process if modules were changed? 
 :param use_debugger: should the werkzeug debugging system be used? 
 :param use_evalex: should the exception evaluation feature be enabled? 
 :param extra_files: a list of files the reloader should watch 
 additionally to the modules.  For example configuration 
 files. 
 :param reloader_interval: the interval for the reloader in seconds. 
 :param reloader_type: the type of reloader to use.  The default is 
 auto detection.  Valid values are ``\'stat\'`` and 
 ``\'watchdog\'``. See :ref:`reloader` for more 
 information. 
 :param threaded: should the process handle each request in a separate 
 thread? 
 :param processes: if greater than 1 then handle each request in a new process 
 up to this maximum number of concurrent processes. 
 :param request_handler: optional parameter that can be used to replace 
 the default one.  You can use this to replace it 
 with a different 
 :class:`~BaseHTTPServer.BaseHTTPRequestHandler` 
 subclass. 
 :param static_files: a dict of paths for static files.  This works exactly 
 like :class:`SharedDataMiddleware`, it\'s actually 
 just wrapping the application in that middleware before 
 serving. 
 :param passthrough_errors: set this to `True` to disable the error catching. 
 This means that the server will die on errors but 
 it can be useful to hook debuggers in (pdb etc.) 
 :param ssl_context: an SSL context for the connection. Either an 
 :class:`ssl.SSLContext`, a tuple in the form 
 ``(cert_file, pkey_file)``, the string ``\'adhoc\'`` if 
 the server should automatically create one, or ``None`` 
 to disable SSL (which is the default).'"
"def summary_table(res, alpha=0.05): 
    from scipy import stats 
   from statsmodels.sandbox.regression.predstd import wls_prediction_std 
   infl = OLSInfluence(res) 
   predict_mean_se = np.sqrt((infl.hat_matrix_diag * res.mse_resid)) 
   tppf = stats.t.isf((alpha / 2.0), res.df_resid) 
   predict_mean_ci = np.column_stack([(res.fittedvalues - (tppf * predict_mean_se)), (res.fittedvalues + (tppf * predict_mean_se))]) 
   (predict_se, predict_ci_low, predict_ci_upp) = wls_prediction_std(res) 
   predict_ci = np.column_stack((predict_ci_low, predict_ci_upp)) 
   resid_se = np.sqrt((res.mse_resid * (1 - infl.hat_matrix_diag))) 
   table_sm = np.column_stack([(np.arange(res.nobs) + 1), res.model.endog, res.fittedvalues, predict_mean_se, predict_mean_ci[:, 0], predict_mean_ci[:, 1], predict_ci[:, 0], predict_ci[:, 1], res.resid, resid_se, infl.resid_studentized_internal, infl.cooks_distance[0]]) 
   data = table_sm 
   ss2 = ['Obs', 'Dep   Var\nPopulation', 'Predicted\nValue', 'Std   Error\nMean   Predict', 'Mean   ci\n95%   low', 'Mean   ci\n95%   upp', 'Predict   ci\n95%   low', 'Predict   ci\n95%   upp', 'Residual', 'Std   Error\nResidual', 'Student\nResidual', ""Cook's\nD""] 
   colnames = ss2 
   from statsmodels.iolib.table import SimpleTable, default_html_fmt 
   from statsmodels.iolib.tableformatting import fmt_base 
   from copy import deepcopy 
   fmt = deepcopy(fmt_base) 
   fmt_html = deepcopy(default_html_fmt) 
   fmt['data_fmts'] = (['%4d'] + (['%6.3f'] * (data.shape[1] - 1))) 
   st = SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html) 
   return (st, data, ss2)"," 'Generate summary table for the results. 
 Parameters 
 res : Result object 
 results object. 
 Returns 
 st : SimpleTable object 
 summary table 
 data : array 
 data used for the table 
 ss2 : array 
 summary statistics for the table'","'generate summary table of outlier and influence similar to SAS 
 Parameters 
 alpha : float 
 significance level for confidence interval 
 Returns 
 st : SimpleTable instance 
 table with results that can be printed 
 data : ndarray 
 calculated measures and statistics for the table 
 ss2 : list of strings 
 column_names for table (Note: rows of table are observations)'"
"def get_receptive_field(layers, img_size): 
    receptive_field = np.zeros((len(layers), 2)) 
   conv_mode = True 
   first_conv_layer = True 
   expon = np.ones((1, 2)) 
   for (i, layer) in enumerate(layers[1:]): 
      j = (i + 1) 
      if (not conv_mode): 
         receptive_field[j] = img_size 
         continue 
      if is_conv2d(layer): 
         if (not first_conv_layer): 
            last_field = receptive_field[i] 
            new_field = (last_field + (expon * (np.array(layer.filter_size) - 1))) 
            receptive_field[j] = new_field 
         else: 
            receptive_field[j] = layer.filter_size 
            first_conv_layer = False 
      elif is_maxpool2d(layer): 
         receptive_field[j] = receptive_field[i] 
         expon *= np.array(layer.pool_size) 
      else: 
         conv_mode = False 
         receptive_field[j] = img_size 
   receptive_field[0] = img_size 
   return receptive_field"," 'Get the receptive field of the last layer in the network. 
 The receptive field of a layer is the size of the output of the 
 layer, which is the size of the input to the next layer. 
 Parameters 
 layers : list 
 The list of layers in the network. 
 img_size : int 
 The size of the input image. 
 Returns 
 receptive_field : ndarray 
 The receptive field of the last layer in the network. 
 Notes 
 The receptive field of a layer is the size of the output of the 
 layer, which is the size of the input to the next layer. 
 The receptive field of a convolutional layer is the size of the 
 filter. 
 The receptive field of a max pooling layer is the size of the 
 pool. 
 Examples 
 >>> layers = [nn.Conv2d(3, 3, 5), nn.MaxPool2d(2, 2)] 
 >>> receptive_field = get_receptive_field(layers, 28) 
","'Get the real filter sizes of each layer involved in 
 convoluation. See Xudong Cao: 
 https://www.kaggle.com/c/datasciencebowl/forums/t/13166/happy-lantern-festival-report-and-code 
 This does not yet take into consideration feature pooling, 
 padding, striding and similar gimmicks.'"
"def clear_all_actions(): 
    global _populated 
   _all_actions.clear() 
   _top_level_ids.clear() 
   _populated = False"," 'Clear all actions. 
 This is a utility function for tests.'","'Clear all registered actions. 
 This method is really only intended to be used by unit tests. We might be 
 able to remove this hack once we convert to djblets.registries. 
 Warning: 
 This will clear **all** actions, even if they were registered in 
 separate extensions.'"
"def timesince(d=None, now=None, abbreviate=False, separator=','): 
    if abbreviate: 
      chunks = (((((60 * 60) * 24) * 365), (lambda n: 'y')), ((((60 * 60) * 24) * 30), (lambda n: 'm')), ((((60 * 60) * 24) * 7), (lambda n: 'w')), (((60 * 60) * 24), (lambda n: 'd')), ((60 * 60), (lambda n: 'h')), (60, (lambda n: 'm')), (1, (lambda n: 's'))) 
   else: 
      chunks = (((((60 * 60) * 24) * 365), (lambda n: ungettext('year', 'years', n))), ((((60 * 60) * 24) * 30), (lambda n: ungettext('month', 'months', n))), ((((60 * 60) * 24) * 7), (lambda n: ungettext('week', 'weeks', n))), (((60 * 60) * 24), (lambda n: ungettext('day', 'days', n))), ((60 * 60), (lambda n: ungettext('hour', 'hours', n))), (60, (lambda n: ungettext('minute', 'minutes', n))), (1, (lambda n: ungettext('second', 'seconds', n)))) 
   if (not isinstance(d, datetime.datetime)): 
      d = datetime.datetime(d.year, d.month, d.day) 
   if (now and (not isinstance(now, datetime.datetime))): 
      now = datetime.datetime(now.year, now.month, now.day) 
   if (not now): 
      if d.tzinfo: 
         now = datetime.datetime.now(LocalTimezone(d)) 
      else: 
         now = datetime.datetime.now() 
   delta = (now - (d - datetime.timedelta(0, 0, d.microsecond))) 
   since = ((((delta.days * 24) * 60) * 60) + delta.seconds) 
   if (since <= 0): 
      if abbreviate: 
         return (u'0' + ugettext('s')) 
      else: 
         return (u'0   ' + ugettext('seconds')) 
   for (i, (seconds, name)) in enumerate(chunks): 
      count = (since // seconds) 
      if (count != 0): 
         break 
   if abbreviate: 
      s = (ugettext('%(number)d%(type)s') % {'number': count, 'type': name(count)}) 
   else: 
      s = (ugettext('%(number)d   %(type)s') % {'number': count, 'type': name(count)}) 
   if ((i + 1) < len(chunks)): 
      (seconds2, name2) = chunks[(i + 1)] 
      count2 = ((since - (seconds * count)) // seconds2) 
      if (count2 != 0): 
         if abbreviate: 
            s += (ugettext('%(separator)s   %(number)d%(type)s') % {'separator': separator, 'number': count2, 'type': name2(count2)}) 
         else: 
            s += (ugettext('%(separator)s   %(number)d   %(type)s') % {'separator': separator, 'number': count2, 'type': name2(count2)}) 
   return s"," 'Returns a string representing the time since the given date. 
 If the given date is None, the current time is used. 
 If the given date is in the past, the string will be negative. 
 If the given date is in the future, the string will be positive. 
 If the given date is in the past, the string will be negative. 
 If the given date is in the future, the string will be positive. 
 If the given date is in the past, the string will be negative. 
 If the given date is in the future, the string will be positive. 
 If the given date is in the past, the string will be negative. 
 If the given date is in the future, the string will be positive. 
 If the given date is in the past, the string will be negative. 
 If the given date is in the future, the string will be positive. 
 If the given date is in the past, the string will be negative. 
 If the given date is in the future, the string will be positive.'","'Takes two datetime objects and returns the time between d and now 
 as a nicely formatted string, e.g. ""10 minutes"".  If d occurs after now, 
 then ""0 seconds"" is returned. If abbreviate is True, it truncates values to, 
 for example, ""10m"" or ""4m 30s"". Alternately it can take a second value 
 and return the proper count. 
 Units used are years, months, weeks, days, hours, minutes, and seconds. 
 Microseconds are ignored.  Up to two adjacent units will be 
 displayed.  For example, ""2 weeks, 3 days"" and ""1 year, 3 months"" are 
 possible outputs, but ""2 weeks, 3 hours"" and ""1 year, 5 days"" are not. 
 Adapted from the timesince filter in Django: 
 http://docs.djangoproject.com/en/dev/ref/templates/builtins/#timesince'"
"def _fill_cdata(cls): 
    funcs = {} 
   for (key, name) in [('b', 'char'), ('h', 'short'), ('i', 'int'), ('q', 'longlong')]: 
      for (echar, esuffix) in [('<', 'le'), ('>', 'be')]: 
         esuffix = ('_' + esuffix) 
         for unsigned in [True, False]: 
            s = struct.Struct((echar + (key.upper() if unsigned else key))) 
            get_wrapper = (lambda f: (lambda *a, **k: f(*a, **k)[0])) 
            unpack = get_wrapper(s.unpack) 
            unpack_from = get_wrapper(s.unpack_from) 
            def get_unpack_from(s): 
               def unpack_from(data, offset=0): 
                  return (s.unpack_from(data, offset)[0], (offset + s.size)) 
               return unpack_from 
            unpack_from = get_unpack_from(s) 
            pack = s.pack 
            prefix = ('u' if unsigned else '') 
            if (s.size == 1): 
               esuffix = '' 
            bits = str((s.size * 8)) 
            funcs[('%s%s%s' % (prefix, name, esuffix))] = unpack 
            funcs[('%sint%s%s' % (prefix, bits, esuffix))] = unpack 
            funcs[('%s%s%s_from' % (prefix, name, esuffix))] = unpack_from 
            funcs[('%sint%s%s_from' % (prefix, bits, esuffix))] = unpack_from 
            funcs[('to_%s%s%s' % (prefix, name, esuffix))] = pack 
            funcs[('to_%sint%s%s' % (prefix, bits, esuffix))] = pack 
   for (key, func) in iteritems(funcs): 
      setattr(cls, key, staticmethod(func))"," 'fill in the cdata attribute of the class with the appropriate unpack/pack 
 functions'",'Add struct pack/unpack functions'
"def get_user_unique_id_and_display_name(request, mapped_properties): 
    user = mapped_properties['user'] 
   user_id = user.get('id') 
   user_name = (user.get('name') or request.remote_user) 
   if (not any([user_id, user_name])): 
      msg = _('Could   not   map   user   while   setting   ephemeral   user   identity.   Either   mapping   rules   must   specify   user   id/name   or   REMOTE_USER   environment   variable   must   be   set.') 
      raise exception.Unauthorized(msg) 
   elif (not user_name): 
      user['name'] = user_id 
   elif (not user_id): 
      user_id = user_name 
   user['id'] = parse.quote(user_id) 
   return (user['id'], user['name'])"," 'Get the user id and display name from the request or the user object. 
 :param mapped_properties: A dictionary of the mapped properties. 
 :param request: The request object. 
 :param user: A dictionary of the user object. 
 :returns: A tuple of the user id and display name.'","'Setup federated username. 
 Function covers all the cases for properly setting user id, a primary 
 identifier for identity objects. Initial version of the mapping engine 
 assumed user is identified by ``name`` and his ``id`` is built from the 
 name. We, however need to be able to accept local rules that identify user 
 by either id or name/domain. 
 The following use-cases are covered: 
 1) If neither user_name nor user_id is set raise exception.Unauthorized 
 2) If user_id is set and user_name not, set user_name equal to user_id 
 3) If user_id is not set and user_name is, set user_id as url safe version 
 of user_name. 
 :param request: current request object 
 :param mapped_properties: Properties issued by a RuleProcessor. 
 :type: dictionary 
 :raises keystone.exception.Unauthorized: If neither `user_name` nor 
 `user_id` is set. 
 :returns: tuple with user identification 
 :rtype: tuple'"
"def disable(name, lbn, target, profile='default', tgt_type='glob', expr_form=None): 
    if (expr_form is not None): 
      salt.utils.warn_until('Fluorine', ""the   target   type   should   be   passed   using   the   'tgt_type'   argument   instead   of   'expr_form'.   Support   for   using   'expr_form'   will   be   removed   in   Salt   Fluorine."") 
      tgt_type = expr_form 
   return _talk2modjk(name, lbn, target, 'worker_disable', profile, tgt_type)"," 'Disable a worker by name, lb-name, or target. 
 :param name: The name of the worker to disable. 
 :param lbn: The lb name of the worker to disable. 
 :param target: The target to disable. 
 :param profile: The profile to use. 
 :param tgt_type: The target type to use. 
 :param expr_form: The target type to use. 
 :return: The return value from the mod_jk worker. 
 :rtype: str'","'.. versionchanged:: Nitrogen 
 The ``expr_form`` argument has been renamed to ``tgt_type``, earlier 
 releases must use ``expr_form``. 
 Disable the named worker from the lbn load balancers at the targeted 
 minions. The worker will get traffic only for current sessions and won\'t 
 get new ones. 
 Example: 
 .. code-block:: yaml 
 disable-before-deploy: 
 modjk_worker.disable: 
 - name: {{ grains[\'id\'] }} 
 - lbn: application 
 - target: \'roles:balancer\' 
 - tgt_type: grain'"
"def swap_inf_nan(val): 
    if isinstance(val, string_types): 
      return val 
   elif isinstance(val, collections.Sequence): 
      return [swap_inf_nan(v) for v in val] 
   elif isinstance(val, collections.Mapping): 
      return dict([(swap_inf_nan(k), swap_inf_nan(v)) for (k, v) in iteritems(val)]) 
   elif isinstance(val, float): 
      if math.isnan(val): 
         return '__NaN__' 
      elif (val == float('inf')): 
         return '__Infinity__' 
      elif (val == float('-inf')): 
         return '__-Infinity__' 
      else: 
         return val 
   else: 
      return val"," 'Swaps inf and nan values. 
 :param val: value to swap inf and nan values in 
 :returns: value with inf and nan swapped 
 :rtype: `Any` 
 :raises: TypeError if val is not a sequence, mapping, float, or string'","'This takes an arbitrary object and preps it for jsonifying safely, templating Inf/NaN.'"
"def get_job_count_by_state(request, username): 
    res = {'completed': 0, 'running': 0, 'failed': 0, 'killed': 0, 'all': 0} 
   jobcounts = request.jt.get_job_count_by_user(username) 
   res['completed'] = jobcounts.nSucceeded 
   res['running'] = (jobcounts.nPrep + jobcounts.nRunning) 
   res['failed'] = jobcounts.nFailed 
   res['killed'] = jobcounts.nKilled 
   res['all'] = (((res['completed'] + res['running']) + res['failed']) + res['killed']) 
   return res"," 'Returns the number of jobs in each state for a given user. 
 :param request: The request object. 
 :param username: The username to look for. 
 :returns: A dictionary of job counts by state. 
 :rtype: dict'","'Returns the number of comlpeted, running, and failed jobs for a user.'"
"def filer_file_from_upload(request, path, upload_data, sha1=None): 
    return _filer_file_from_upload(model=File, request=request, path=path, upload_data=upload_data, sha1=sha1)", 'Returns a File object based on the upload data.',"'Create a filer.models.filemodels.File from an upload (UploadedFile or such). 
 If the `sha1` parameter is passed and a file with said SHA1 is found, it will be returned instead. 
 :param request: Request, to figure out the owner for this file 
 :type request: django.http.request.HttpRequest|None 
 :param path: Pathname string (see `filer_folder_from_path`) or a Filer Folder. 
 :type path: basestring|filer.models.Folder 
 :param upload_data: Upload data 
 :type upload_data: django.core.files.base.File 
 :param sha1: SHA1 checksum. If given and a matching `model` with the SHA1 is found, it is returned instead. 
 :type sha1: basestring 
 :rtype: filer.models.filemodels.File'"
"def last_updated(document): 
    if (config.LAST_UPDATED in document): 
      return document[config.LAST_UPDATED].replace(tzinfo=None) 
   else: 
      return epoch()"," 'Returns the last updated date for a document. 
 :param document: a document 
 :type document: dict 
 :return: a datetime object 
 :rtype: datetime'","'Fixes document\'s LAST_UPDATED field value. Flask-PyMongo returns 
 timezone-aware values while stdlib datetime values are timezone-naive. 
 Comparisons between the two would fail. 
 If LAST_UPDATE is missing we assume that it has been created outside of the 
 API context and inject a default value, to allow for proper computing of 
 Last-Modified header tag. By design all documents return a LAST_UPDATED 
 (and we don\'t want to break existing clients). 
 :param document: the document to be processed. 
 .. versionchanged:: 0.1.0 
 Moved to common.py and renamed as public, so it can also be used by edit 
 methods (via get_document()). 
 .. versionadded:: 0.0.5'"
"def stub_out_registry_image_update(stubs): 
    test_stubs.stub_out_registry_server(stubs) 
   def fake_image_update(ctx, image_id, values, purge_props=False): 
      return {'properties': {}} 
   stubs.Set(db_api, 'image_update', fake_image_update)", 'Stub out image_update() to return a simple image properties object.',"'Stubs an image update on the registry. 
 :param stubs: Set of stubout stubs'"
"def RemoveFlags(flag_values=FLAGS): 
    for flag_name in NamesOfDefinedFlags(): 
      module_bar.RemoveOneFlag(flag_name, flag_values=flag_values) 
   module_bar.RemoveFlags(flag_values=flag_values)"," 'Remove all flags in the current module. 
 :param flag_values: a dictionary of flag values to remove 
 :return: None 
 :rtype: None'",'Deletes the flag definitions done by the above DefineFlags().'
"@require_context 
 def volume_get_all_by_group(context, group_id, filters=None): 
    query = _volume_get_query(context).filter_by(consistencygroup_id=group_id) 
   if filters: 
      query = _process_volume_filters(query, filters) 
      if (query is None): 
         return [] 
   return query.all()", 'Get all volumes for a given consistency group.',"'Retrieves all volumes associated with the group_id. 
 :param context: context to query under 
 :param group_id: consistency group ID for all volumes being retrieved 
 :param filters: dictionary of filters; values that are in lists, tuples, 
 or sets cause an \'IN\' operation, while exact matching 
 is used for other values, see _process_volume_filters 
 function for more information 
 :returns: list of matching volumes'"
"def XmlToString(content, encoding='utf-8', pretty=False): 
    xml_parts = [('<?xml   version=""1.0""   encoding=""%s""?>' % encoding)] 
   if pretty: 
      xml_parts.append('\n') 
   _ConstructContentList(xml_parts, content, pretty) 
   return ''.join(xml_parts)"," 'Converts a string to an XML string. 
 :param content: The string to convert. 
 :param encoding: The encoding to use. 
 :param pretty: If True, add a newline after the xml declaration. 
 :return: The string converted to XML. 
 :rtype: str'","'Writes the XML content to disk, touching the file only if it has changed. 
 Visual Studio files have a lot of pre-defined structures.  This function makes 
 it easy to represent these structures as Python data structures, instead of 
 having to create a lot of function calls. 
 Each XML element of the content is represented as a list composed of: 
 1. The name of the element, a string, 
 2. The attributes of the element, a dictionary (optional), and 
 3+. The content of the element, if any.  Strings are simple text nodes and 
 lists are child elements. 
 Example 1: 
 <test/> 
 becomes 
 [\'test\'] 
 Example 2: 
 <myelement a=\'value1\' b=\'value2\'> 
 <childtype>This is</childtype> 
 <childtype>it!</childtype> 
 </myelement> 
 becomes 
 [\'myelement\', {\'a\':\'value1\', \'b\':\'value2\'}, 
 [\'childtype\', \'This is\'], 
 [\'childtype\', \'it!\'], 
 Args: 
 content:  The structured content to be converted. 
 encoding: The encoding to report on the first XML line. 
 pretty: True if we want pretty printing with indents and new lines. 
 Returns: 
 The XML content as a string.'"
"def add_email_to_campaign(survey, email): 
    token = settings.SURVEYGIZMO_API_TOKEN 
   secret = settings.SURVEYGIZMO_API_TOKEN_SECRET 
   if ((token is None) or (secret is None)): 
      return 
   survey_id = SURVEYS[survey]['exit_survey_id'] 
   campaign_id = SURVEYS[survey]['exit_survey_campaign_id'] 
   try: 
      requests.put('https://restapi.surveygizmo.com/v2/survey/{survey}/surveycampaign/{campaign}/contact?semailaddress={email}&api_token={token}&api_token_secret={secret}'.format(survey=survey_id, campaign=campaign_id, email=email, token=token, secret=secret), timeout=30) 
   except requests.exceptions.Timeout: 
      print ('Timedout   adding:   %s' % email)"," 'Adds an email to the campaign. 
 :param survey: The name of the survey. 
 :param email: The email address to add to the campaign. 
 :return: None'",'Add email to the exit survey campaign.'
"def test_lda_empty_docs(): 
    Z = np.zeros((5, 4)) 
   for X in [Z, csr_matrix(Z)]: 
      lda = LatentDirichletAllocation(max_iter=750).fit(X) 
      assert_almost_equal(lda.components_.sum(axis=0), np.ones(lda.components_.shape[1]))", 'Test that the components are all zero when there are no documents.','Test LDA on empty document (all-zero rows).'
"def _diff(state_data, resource_object): 
    objects_differ = None 
   for (k, v) in state_data['service'].items(): 
      if (k == 'escalation_policy_id'): 
         resource_value = resource_object['escalation_policy']['id'] 
      elif (k == 'service_key'): 
         resource_value = resource_object['service_key'] 
         if ('@' in resource_value): 
            resource_value = resource_value[0:resource_value.find('@')] 
      else: 
         resource_value = resource_object[k] 
      if (v != resource_value): 
         objects_differ = '{0}   {1}   {2}'.format(k, v, resource_value) 
         break 
   if objects_differ: 
      return state_data 
   else: 
      return {}"," 'Returns a dictionary of the differences between the state data and the 
 resource object. 
 :param state_data: The state data. 
 :param resource_object: The resource object.'","'helper method to compare salt state info with the PagerDuty API json structure, 
 and determine if we need to update. 
 returns the dict to pass to the PD API to perform the update, or empty dict if no update.'"
"@utils.arg('server', metavar='<server>', help=_('Name   or   ID   of   server.')) 
 def do_resize_confirm(cs, args): 
    _find_server(cs, args.server).confirm_resize()", 'Confirm resize of server.','Confirm a previous resize.'
"def op_abs_tmul(lin_op, value): 
    if (lin_op.type is lo.NEG): 
      result = value 
   elif (lin_op.type is lo.MUL): 
      coeff = mul(lin_op.data, {}, True) 
      if np.isscalar(coeff): 
         result = (coeff * value) 
      else: 
         result = (coeff.T * value) 
   elif (lin_op.type is lo.DIV): 
      divisor = mul(lin_op.data, {}, True) 
      result = (value / divisor) 
   elif (lin_op.type is lo.CONV): 
      result = conv_mul(lin_op, value, True, True) 
   else: 
      result = op_tmul(lin_op, value) 
   return result"," 'Opens the given linear operator and multiplies it with the given value. 
 Returns the result of the multiplication. 
 Parameters 
 lin_op : LinearOperator 
 The linear operator to multiply with the given value. 
 value : number 
 The value to multiply with the given linear operator. 
 Returns 
 result : LinearOperator 
 The result of the multiplication of the given linear operator and 
 the given value. 
 Examples 
 >>> from sympy import Function, Symbol, Matrix, linop 
 >>> from sympy.abc import x, y 
 >>> from sympy.matrices.expressions import Matrix 
 >>> from sympy.matrices.dense import MatrixDense 
 >>> from sympy.matrices.dense.linear_operator import MatrixLinearOperator 
 >>> from sympy.matrices.dense.linear_operator import MatrixLinearOperator_tmul 
 >>> from sympy.matrices.dense.linear_operator import MatrixLinearOperator_abs_tmul 
 >>> A = Matrix([[x, 0, 0], [0, y, 0], [0, 0, 1]])","'Applies the linear operator |A.T| to the arguments. 
 Parameters 
 lin_op : LinOp 
 A linear operator. 
 value : NumPy matrix 
 A numeric value to apply the operator\'s transpose to. 
 Returns 
 NumPy matrix or SciPy sparse matrix. 
 The result of applying the linear operator.'"
"def html4annotation(htmlpage, baseurl=None, proxy_resources=None): 
    htmlpage = add_tagids(htmlpage) 
   cleaned_html = descriptify(htmlpage, baseurl, proxy=proxy_resources) 
   return cleaned_html"," 'Generate HTML4-style annotations for a given HTML page. 
 :param htmlpage: A string containing the HTML to be annotated. 
 :param baseurl: The base URL for the annotations. 
 :param proxy_resources: A list of resource URLs to be proxied.'","'Convert the given html document for the annotation UI 
 This adds tags, removes scripts and optionally adds a base url'"
"def run_as_contextmanager(ctx, fn, *arg, **kw): 
    obj = ctx.__enter__() 
   try: 
      result = fn(obj, *arg, **kw) 
      ctx.__exit__(None, None, None) 
      return result 
   except: 
      exc_info = sys.exc_info() 
      raise_ = ctx.__exit__(*exc_info) 
      if (raise_ is None): 
         raise 
      else: 
         return raise_"," 'Run a function as a context manager. 
 This function runs a function as a context manager. 
 It takes a context manager and a function to be run. 
 It returns the result of the function or the result of the 
 context manager. 
 :param ctx: The context manager to run. 
 :param fn: The function to run. 
 :return: The result of the function or the result of the 
 context manager.'","'Run the given function under the given contextmanager, 
 simulating the behavior of \'with\' to support older 
 Python versions. 
 This is not necessary anymore as we have placed 2.6 
 as minimum Python version, however some tests are still using 
 this structure.'"
"def continued_indentation(logical_line, tokens, indent_level, hang_closing, indent_char, noqa, verbose): 
    first_row = tokens[0][2][0] 
   nrows = ((1 + tokens[(-1)][2][0]) - first_row) 
   if (noqa or (nrows == 1)): 
      return 
   indent_next = logical_line.endswith(':') 
   row = depth = 0 
   valid_hangs = ((4,) if (indent_char != ' DCTB ') else (4, 8)) 
   parens = ([0] * nrows) 
   rel_indent = ([0] * nrows) 
   open_rows = [[0]] 
   hangs = [None] 
   indent_chances = {} 
   last_indent = tokens[0][2] 
   visual_indent = None 
   last_token_multiline = False 
   indent = [last_indent[1]] 
   if (verbose >= 3): 
      print ('>>>   ' + tokens[0][4].rstrip()) 
   for (token_type, text, start, end, line) in tokens: 
      newline = (row < (start[0] - first_row)) 
      if newline: 
         row = (start[0] - first_row) 
         newline = ((not last_token_multiline) and (token_type not in NEWLINE)) 
      if newline: 
         last_indent = start 
         if (verbose >= 3): 
            print ('...   ' + line.rstrip()) 
         rel_indent[row] = (expand_indent(line) - indent_level) 
         close_bracket = ((token_type == tokenize.OP) and (text in ']})')) 
         for open_row in reversed(open_rows[depth]): 
            hang = (rel_indent[row] - rel_indent[open_row]) 
            hanging_indent = (hang in valid_hangs) 
            if hanging_indent: 
               break 
         if hangs[depth]: 
            hanging_indent = (hang == hangs[depth]) 
         visual_indent = ((not close_bracket) and (hang > 0) and indent_chances.get(start[1])) 
         if (close_bracket and indent[depth]): 
            if (start[1] != indent[depth]): 
               (yield (start, 'E124   closing   bracket   does   not   match   visual   indentation')) 
         elif (close_bracket and (not hang)): 
            if hang_closing: 
               (yield (start, 'E133   closing   bracket   is   missing   indentation')) 
         elif (indent[depth] and (start[1] < indent[depth])): 
            if (visual_indent is not True): 
               (yield (start, 'E128   continuation   line   under-indented   for   visual   indent')) 
         elif (hanging_indent or (indent_next and (rel_indent[row] == 8))): 
            if (close_bracket and (not hang_closing)): 
               (yield (start, ""E123   closing   bracket   does   not   match   indentation   of   opening   bracket's   line"")) 
            hangs[depth] = hang 
         elif (visual_indent is True): 
            indent[depth] = start[1] 
         elif (visual_indent in (text, str)): 
            pass 
         else: 
            if (hang <= 0): 
               error = ('E122', 'missing   indentation   or   outdented') 
            elif indent[depth]: 
               error = ('E127', 'over-indented   for   visual   indent') 
            elif ((not close_bracket) and hangs[depth]): 
               error = ('E131', 'unaligned   for   hanging   indent') 
            else: 
               hangs[depth] = hang 
               if (hang > 4): 
                  error = ('E126', 'over-indented   for   hanging   indent') 
               else: 
                  error = ('E121', 'under-indented   for   hanging   indent') 
            (yield (start, ('%s   continuation   line   %s' % error))) 
      if (parens[row] and (token_type not in (tokenize.NL, tokenize.COMMENT)) and (not indent[depth])): 
         indent[depth] = start[1] 
         indent_chances[start[1]] = True 
         if (verbose >= 4): 
            print ('bracket   depth   %s   indent   to   %s' % (depth, start[1])) 
      elif ((token_type in (tokenize.STRING, tokenize.COMMENT)) or (text in ('u', 'ur', 'b', 'br'))): 
         indent_chances[start[1]] = str 
      elif ((not indent_chances) and (not row) and (not depth) and (text == 'if')): 
         indent_chances[(end[1] + 1)] = True 
      elif ((text == ':') and line[end[1]:].isspace()): 
         open_rows[depth].append(row) 
      if (token_type == tokenize.OP): 
         if (text in '([{'): 
            depth += 1 
            indent.append(0) 
            hangs.append(None) 
            if (len(open_rows) == depth): 
               open_rows.append([]) 
            open_rows[depth].append(row) 
            parens[row] += 1 
            if (verbose >= 4): 
               print ('bracket   depth   %s   seen,   col   %s,   visual   min   =   %s' % (depth, start[1], indent[depth])) 
         elif ((text in ')]}') and (depth > 0)): 
            prev_indent = (indent.pop() or last_indent[1]) 
            hangs.pop() 
            for d in range(depth): 
               if (indent[d] > prev_indent): 
                  indent[d] = 0 
            for ind in list(indent_chances): 
               if (ind >= prev_indent): 
                  del indent_chances[ind] 
            del open_rows[(depth + 1):] 
            depth -= 1 
            if depth: 
               indent_chances[indent[depth]] = True 
            for idx in range(row, (-1), (-1)): 
               if parens[idx]: 
                  parens[idx] -= 1 
                  break 
         assert (len(indent) == (depth + 1)) 
         if (start[1] not in indent_chances): 
            indent_chances[start[1]] = text 
      last_token_multiline = (start[0] != end[0]) 
      if last_token_multiline: 
         rel_indent[(end[0] - first_row)] = rel_indent[row] 
   if (indent_next and (expand_indent(line) == (indent_level + 4))): 
      pos = (start[0], (indent[0] + 4)) 
      if visual_indent: 
         code = 'E129   visually   indented   line' 
      else: 
         code = 'E125   continuation   line' 
      (yield (pos, ('%s   with   same   indent   as   next   logical   line' % code)))"," 'Checks for indentation issues on a logical line. 
 Args: 
 logical_line: 
 The logical line to check. 
 tokens: 
 The token list of the logical line. 
 indent_level: 
 The current indent level. 
 hang_closing: 
 Whether closing bracket hangs from the line. 
 indent_char: 
 The indent character. 
 noqa: 
 Whether to check for noqa issues. 
 verbose: 
 Whether to print debug information. 
 Returns: 
 A generator of errors. 
 See Also: 
 E125, E126, E127, E128, E129, E130, E131, E132, E133, E134, E135, E136, E137, E138, E139, E140, E141, E142, E143, E144, E145, E146, E147, E148, E1","'Continuation lines indentation. 
 Continuation lines should align wrapped elements either vertically 
 using Python\'s implicit line joining inside parentheses, brackets 
 and braces, or using a hanging indent. 
 When using a hanging indent these considerations should be applied: 
 - there should be no arguments on the first line, and 
 - further indentation should be used to clearly distinguish itself as a 
 continuation line. 
 Okay: a = (\n) 
 E123: a = (\n    ) 
 Okay: a = (\n    42) 
 E121: a = (\n   42) 
 E122: a = (\n42) 
 E123: a = (\n    42\n    ) 
 E124: a = (24,\n     42\n) 
 E125: if (\n    b):\n    pass 
 E126: a = (\n        42) 
 E127: a = (24,\n      42) 
 E128: a = (24,\n    42) 
 E129: if (a or\n    b):\n    pass 
 E131: a = (\n    42\n 24)'"
"def test_prompt_should_ask_and_rm_repo_dir(mocker, tmpdir): 
    mock_read_user = mocker.patch('cookiecutter.vcs.read_user_yes_no', return_value=True, autospec=True) 
   repo_dir = tmpdir.mkdir('repo') 
   vcs.prompt_and_delete_repo(str(repo_dir)) 
   assert mock_read_user.called 
   assert (not repo_dir.exists())", 'Test prompt_and_delete_repo()',"'In `prompt_and_delete_repo()`, if the user agrees to delete/reclone the 
 repo, the repo should be deleted.'"
"def ipart(x): 
    return np.modf(x)[1]"," 'Return the imaginary part of x. 
 Parameters 
 x : float 
 The number whose imaginary part is to be returned. 
 Returns 
 The imaginary part of x. 
 Examples 
 >>> from sympy.physics.electromagnetism.fields import ipart 
 >>> ipart(1 + 2*1j) 
 1.0'",'Return integer part of given number.'
"def nlmeans_proxy(in_file, settings, snr=None, smask=None, nmask=None, out_file=None): 
    from dipy.denoise.nlmeans import nlmeans 
   from scipy.ndimage.morphology import binary_erosion 
   from scipy import ndimage 
   if (out_file is None): 
      (fname, fext) = op.splitext(op.basename(in_file)) 
      if (fext == u'.gz'): 
         (fname, fext2) = op.splitext(fname) 
         fext = (fext2 + fext) 
      out_file = op.abspath((u'./%s_denoise%s' % (fname, fext))) 
   img = nb.load(in_file) 
   hdr = img.header 
   data = img.get_data() 
   aff = img.affine 
   if (data.ndim < 4): 
      data = data[..., np.newaxis] 
   data = np.nan_to_num(data) 
   if (data.max() < 0.0001): 
      raise RuntimeError(u'There   is   no   signal   in   the   image') 
   df = 1.0 
   if (data.max() < 1000.0): 
      df = (1000.0 / data.max()) 
      data *= df 
   b0 = data[..., 0] 
   if (smask is None): 
      smask = np.zeros_like(b0) 
      smask[(b0 > np.percentile(b0, 85.0))] = 1 
   smask = binary_erosion(smask.astype(np.uint8), iterations=2).astype(np.uint8) 
   if (nmask is None): 
      nmask = np.ones_like(b0, dtype=np.uint8) 
      bmask = settings[u'mask'] 
      if (bmask is None): 
         bmask = np.zeros_like(b0) 
         bmask[(b0 > np.percentile(b0[(b0 > 0)], 10))] = 1 
         (label_im, nb_labels) = ndimage.label(bmask) 
         sizes = ndimage.sum(bmask, label_im, range((nb_labels + 1))) 
         maxidx = np.argmax(sizes) 
         bmask = np.zeros_like(b0, dtype=np.uint8) 
         bmask[(label_im == maxidx)] = 1 
      nmask[(bmask > 0)] = 0 
   else: 
      nmask = np.squeeze(nmask) 
      nmask[(nmask > 0.0)] = 1 
      nmask[(nmask < 1)] = 0 
      nmask = nmask.astype(bool) 
   nmask = binary_erosion(nmask, iterations=1).astype(np.uint8) 
   den = np.zeros_like(data) 
   est_snr = True 
   if (snr is not None): 
      snr = ([snr] * data.shape[(-1)]) 
      est_snr = False 
   else: 
      snr = [] 
   for i in range(data.shape[(-1)]): 
      d = data[..., i] 
      if est_snr: 
         s = np.mean(d[(smask > 0)]) 
         n = np.std(d[(nmask > 0)]) 
         snr.append((s / n)) 
      den[..., i] = nlmeans(d, snr[i], **settings) 
   den = np.squeeze(den) 
   den /= df 
   nb.Nifti1Image(den.astype(hdr.get_data_dtype()), aff, hdr).to_filename(out_file) 
   return (out_file, snr)"," 'Apply the NL-means denoising algorithm to the input image. 
 Parameters 
 in_file : str 
 Name of input image file. 
 snr : float, optional 
 Signal-to-noise ratio. If None, estimated from the input data. 
 smask : bool, optional 
 Mask the input image with a smoothed version of the data. 
 nmask : bool, optional 
 Mask the input image with a mask that is the complement of the data. 
 out_file : str, optional 
 Name of output image file. 
 Returns 
 out_file : str 
 Name of output image file. 
 snr : list 
 Signal-to-noise ratio for each dimension. 
 Notes 
 This function is a proxy for the NL-means denoising algorithm. 
 It can be used to apply the NL-means denoising algorithm to an 
 image, but it does not estimate the signal-to-noise ratio. 
 This function is intended to be used as a wrapper for the 
 NL-means denoising algorithm.",'Uses non-local means to denoise 4D datasets'
"def get_zones(permanent=True): 
    cmd = '--get-zones' 
   if permanent: 
      cmd += '   --permanent' 
   return __firewall_cmd(cmd).split()", 'Return the list of zones for the firewall.',"'Print predefined zones 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' firewalld.get_zones'"
"def p_test(p): 
    p[0] = p[1]", 'p_test(p) -> p','test : comparison'
"def get_terminal_size(): 
    if (sys.version_info >= (3, 3)): 
      import shutil 
      shutil_get_terminal_size = getattr(shutil, 'get_terminal_size', None) 
      if shutil_get_terminal_size: 
         sz = shutil_get_terminal_size() 
         return (sz.columns, sz.lines) 
   if (get_winterm_size is not None): 
      return get_winterm_size() 
   def ioctl_gwinsz(fd): 
      try: 
         import fcntl 
         import termios 
         cr = struct.unpack('hh', fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234')) 
      except Exception: 
         return 
      return cr 
   cr = (ioctl_gwinsz(0) or ioctl_gwinsz(1) or ioctl_gwinsz(2)) 
   if (not cr): 
      try: 
         fd = os.open(os.ctermid(), os.O_RDONLY) 
         try: 
            cr = ioctl_gwinsz(fd) 
         finally: 
            os.close(fd) 
      except Exception: 
         pass 
   if ((not cr) or (not cr[0]) or (not cr[1])): 
      cr = (os.environ.get('LINES', 25), os.environ.get('COLUMNS', DEFAULT_COLUMNS)) 
   return (int(cr[1]), int(cr[0]))", 'Returns the terminal size in lines and columns.',"'Returns the current size of the terminal as tuple in the form 
 ``(width, height)`` in columns and rows.'"
"def _get_forward_relationships(opts): 
    forward_relations = OrderedDict() 
   for field in [field for field in opts.fields if (field.serialize and get_remote_field(field))]: 
      forward_relations[field.name] = RelationInfo(model_field=field, related_model=get_related_model(field), to_many=False, to_field=_get_to_field(field), has_through_model=False, reverse=False) 
   for field in [field for field in opts.many_to_many if field.serialize]: 
      forward_relations[field.name] = RelationInfo(model_field=field, related_model=get_related_model(field), to_many=True, to_field=None, has_through_model=(not get_remote_field(field).through._meta.auto_created), reverse=False) 
   return forward_relations", 'Returns a dict of forward relationship info.','Returns an `OrderedDict` of field names to `RelationInfo`.'
"def get_qos(tenant_id, qos_id): 
    LOG.debug(_('get_qos()   called')) 
   session = db.get_session() 
   try: 
      qos = session.query(l2network_models.QoS).filter_by(tenant_id=tenant_id).filter_by(qos_id=qos_id).one() 
      return qos 
   except exc.NoResultFound: 
      raise c_exc.QosNotFound(qos_id=qos_id, tenant_id=tenant_id)"," 'Return the qos by its id. 
 :param tenant_id: Tenant ID 
 :param qos_id: QoS ID 
 :returns: QoS or None'",'Lists the qos given a tenant_id and qos_id'
"def get_resources_dests(resources_root, rules): 
    def get_rel_path(base, path): 
      base = base.replace(os.path.sep, '/') 
      path = path.replace(os.path.sep, '/') 
      assert path.startswith(base) 
      return path[len(base):].lstrip('/') 
   destinations = {} 
   for (base, suffix, dest) in rules: 
      prefix = os.path.join(resources_root, base) 
      for abs_base in iglob(prefix): 
         abs_glob = os.path.join(abs_base, suffix) 
         for abs_path in iglob(abs_glob): 
            resource_file = get_rel_path(resources_root, abs_path) 
            if (dest is None): 
               destinations.pop(resource_file, None) 
            else: 
               rel_path = get_rel_path(abs_base, abs_path) 
               rel_dest = dest.replace(os.path.sep, '/').rstrip('/') 
               destinations[resource_file] = ((rel_dest + '/') + rel_path) 
   return destinations", 'Returns a dict of relative resource file paths and their destination paths.','Find destinations for resources files'
"def cap_alert_is_template(alert_id): 
    if (not alert_id): 
      return False 
   table = current.s3db.cap_alert 
   query = (table.id == alert_id) 
   r = current.db(query).select(table.is_template, limitby=(0, 1)).first() 
   return (r and r.is_template)"," 'Check if an alert is a template. 
 :param alert_id: alert id 
 :type alert_id: int 
 :return: True if the alert is a template 
 :rtype: bool'",'Tell whether an alert entry is a template'
"def add_close_action(widget): 
    return add_action(widget, N_(u'Close...'), widget.close, hotkeys.CLOSE, hotkeys.QUIT)", 'Add a close action to a widget.','Adds close action and shortcuts to a widget.'
"@task 
 @timed 
 def check_memcache(): 
    if (not is_memcache_running()): 
      msg = colorize('red', 'Memcache   is   not   running   locally.') 
      print msg 
      sys.exit(1)", 'Check if memcache is running.','Check that memcache is running'
"def abstractmethod(funcobj): 
    funcobj.__isabstractmethod__ = True 
   return funcobj"," 'Decorator that marks a function as abstract. 
 The decorated function cannot be instantiated.'","'A decorator indicating abstract methods. 
 Requires that the metaclass is ABCMeta or derived from it.  A 
 class that has a metaclass derived from ABCMeta cannot be 
 instantiated unless all of its abstract methods are overridden. 
 The abstract methods can be called using any of the normal 
 \'super\' call mechanisms. 
 Usage: 
 class C: 
 __metaclass__ = ABCMeta 
 @abstractmethod 
 def my_abstract_method(self, ...):'"
"def validate_filters(where, resource): 
    operators = getattr(app.data, 'operators', set()) 
   allowed = (config.DOMAIN[resource]['allowed_filters'] + list(operators)) 
   def validate_filter(filter): 
      for (key, value) in filter.items(): 
         if (('*' not in allowed) and (key not in allowed)): 
            return (""filter   on   '%s'   not   allowed"" % key) 
         if (key in ('$or', '$and', '$nor')): 
            if (not isinstance(value, list)): 
               return (""operator   '%s'   expects   a   list   of   sub-queries"" % key) 
            for v in value: 
               if (not isinstance(v, dict)): 
                  return (""operator   '%s'   expects   a   list   of   sub-queries"" % key) 
               r = validate_filter(v) 
               if r: 
                  return r 
         elif config.VALIDATE_FILTERS: 
            res_schema = config.DOMAIN[resource]['schema'] 
            if (key not in res_schema): 
               return ""filter   on   '%s'   is   invalid"" 
            else: 
               field_schema = res_schema.get(key) 
               v = Validator({key: field_schema}) 
               if (not v.validate({key: value})): 
                  return ""filter   on   '%s'   is   invalid"" 
               else: 
                  return None 
   if (('*' in allowed) and (not config.VALIDATE_FILTERS)): 
      return None 
   return validate_filter(where)"," 'Validate the filters in the WHERE clause. 
 :param where: the WHERE clause to validate 
 :param resource: the resource to validate for'","'Report any filter which is not allowed by  `allowed_filters` 
 :param where: the where clause, as a dict. 
 :param resource: the resource being inspected. 
 .. versionchanged: 0.5 
 If the data layer supports a list of allowed operators, take them 
 into consideration when validating the query string (#388). 
 Recursively validate the whole query string. 
 .. versionadded: 0.0.9'"
"@pytest.mark.network 
 def test_uptodate_editables_flag(script, data): 
    script.pip('install', '-f', data.find_links, '--no-index', 'simple==1.0') 
   result = script.pip('install', '-e', 'git+https://github.com/pypa/pip-test-package.git#egg=pip-test-package') 
   result = script.pip('list', '-f', data.find_links, '--no-index', '--editable', '--uptodate', '--format=legacy') 
   assert ('simple   (1.0)' not in result.stdout), str(result) 
   assert (os.path.join('src', 'pip-test-package') in result.stdout), str(result)", 'Test that --uptodate flag works with --editable','test the behavior of --editable --uptodate flag in the list command'
"def _du(real_path): 
    total_size = 0 
   if os.path.isdir(real_path): 
      for (dirpath, dirnames, filenames) in os.walk(real_path): 
         for filename in filenames: 
            total_size += os.path.getsize(os.path.join(dirpath, filename)) 
   else: 
      total_size += os.path.getsize(real_path) 
   return total_size"," 'Return the size of a directory or file. 
 :param real_path: The absolute path to the file or directory 
 :type real_path: str 
 :return: The size of the file or directory in bytes'",'Get total size of file or files in dir (recursive).'
"def pr_api_url_from_web_url(url): 
    path = '/'.join(map(partial(replace, 'pull', 'pulls'), url_path_parts(url))) 
   return ((API_BASE_URL + REPOS_API_PATH) + path)"," 'Returns the pull request API URL for the given web URL. 
 :param url: The web URL to convert. 
 :returns: The pull request API URL.'","'get the api url for a pull request from the web one. 
 :param unicode url: the web URL of the pull request. 
 :return unicode: the API URL of the same pull request.'"
"def Client(version=None, unstable=False, session=None, **kwargs): 
    if (not session): 
      session = client_session.Session._construct(kwargs) 
   d = discover.Discover(session=session, **kwargs) 
   return d.create_client(version=version, unstable=unstable)"," 'Create a new client for the given version. 
 :param version: The version of the client to create. 
 :type version: str 
 :param unstable: Create an unstable client. 
 :type unstable: bool 
 :param session: A :class:`Session` instance to use for the connection. 
 :type session: :class:`Session` 
 :param kwargs: Additional keyword arguments to pass to the :class:`Session` 
 :type kwargs: dict 
 :return: A :class:`Client` instance. 
 :rtype: :class:`Client` 
 :raises ValueError: If the version is not valid. 
 :raises TypeError: If the version is not a string.'","'Factory function to create a new identity service client. 
 The returned client will be either a V3 or V2 client. Check the version 
 using the :py:attr:`~keystoneclient.v3.client.Client.version` property or 
 the instance\'s class (with instanceof). 
 :param tuple version: The required version of the identity API. If 
 specified the client will be selected such that the 
 major version is equivalent and an endpoint provides 
 at least the specified minor version. For example to 
 specify the 3.1 API use ``(3, 1)``. (optional) 
 :param bool unstable: Accept endpoints not marked as \'stable\'. (optional) 
 :param session: A session object to be used for communication. If one is 
 not provided it will be constructed from the provided 
 kwargs. (optional) 
 :type session: keystoneclient.session.Session 
 :param kwargs: Additional arguments are passed through to the client 
 that is being created. 
 :returns: New keystone client object. 
 :rtype: :py:class:`keystoneclient.v3.client.Client` or 
 :py:class:`keystoneclient.v2_0.client.Client` 
 :raises keystoneclient.exceptions.DiscoveryFailure: if the server\'s 
 response is invalid. 
 :raises keystoneclient.exceptions.VersionNotAvailable: if a suitable client 
 cannot be found.'"
"def NullController(*_args, **_kwargs): 
    return None"," 'Returns the NullController class. 
 This class is used to control a group of widgets that have no 
 effect. 
 :param _args: 
 :param _kwargs: 
 :return: 
 NullController'",'Nonexistent controller - simply returns None'
"@pytest.mark.django_db 
 def test_social_media_plugin_ordering(): 
    context = get_jinja_context() 
   icon_classes = SocialMediaLinksPlugin.icon_classes 
   link_1_type = 'Facebook' 
   link_1 = {'url': 'http://www.facebook.com', 'ordering': 2} 
   link_2_type = 'Twitter' 
   link_2 = {'url': 'http://www.twitter.com', 'ordering': 1} 
   links = {link_1_type: link_1, link_2_type: link_2} 
   plugin = SocialMediaLinksPlugin({'links': links}) 
   assert (len(plugin.get_links()) == 2) 
   assert (plugin.get_links()[0][2] == link_2['url'])", 'Test that the links are ordered correctly.','Test that social media plugin ordering works as expected'
"def parse_backend_conf(backend, **kwargs): 
    conf = settings.CACHES.get(backend, None) 
   if (conf is not None): 
      args = conf.copy() 
      args.update(kwargs) 
      backend = args.pop('BACKEND') 
      location = args.pop('LOCATION', '') 
      return (backend, location, args) 
   else: 
      try: 
         (mod_path, cls_name) = backend.rsplit('.', 1) 
         mod = importlib.import_module(mod_path) 
         backend_cls = getattr(mod, cls_name) 
      except (AttributeError, ImportError, ValueError): 
         raise InvalidCacheBackendError((""Could   not   find   backend   '%s'"" % backend)) 
      location = kwargs.pop('LOCATION', '') 
      return (backend, location, kwargs) 
   raise InvalidCacheBackendError((""Couldn't   find   a   cache   backend   named   '%s'"" % backend))"," 'Parses the given backend and returns the backend, location, and 
 arguments to be passed to the backend\'s constructor. 
 :param backend: The backend to parse. 
 :type backend: str 
 :param kwargs: Additional arguments to pass to the backend\'s 
 constructor. 
 :type kwargs: dict 
 :returns: A 3-tuple of (backend, location, kwargs).'","'Helper function to parse the backend configuration 
 that doesn\'t use the URI notation.'"
"def shell(cmds, env=None, **kwds): 
    sys = kwds.get('sys', _sys) 
   assert (sys is not None) 
   p = shell_process(cmds, env, **kwds) 
   if redirecting_io(sys=sys): 
      redirect_aware_commmunicate(p, sys=sys) 
      exit = p.returncode 
      return exit 
   else: 
      return p.wait()"," 'Run a shell command. 
 :param cmds: a list of strings 
 :param env: a dictionary of environment variables to set 
 :param kwds: additional keyword arguments 
 :return: the exit code of the command'",'Run shell commands with `shell_process` and wait.'
"def normalize_timestamp(timestamp): 
    return Timestamp(timestamp).normal", 'Returns a normalized timestamp.',"'Format a timestamp (string or numeric) into a standardized 
 xxxxxxxxxx.xxxxx (10.5) format. 
 Note that timestamps using values greater than or equal to November 20th, 
 2286 at 17:46 UTC will use 11 digits to represent the number of 
 seconds. 
 :param timestamp: unix timestamp 
 :returns: normalized timestamp as a string'"
"def simple_parse_to_segments(formatted_text): 
    if ('message_parser' in dir(hangups)): 
      segments = hangups.ChatMessageSegment.from_str(formatted_text) 
   else: 
      segments = kludgy_html_parser.simple_parse_to_segments(formatted_text) 
   return segments", 'Parse a formatted text string into a list of segments.',"'send formatted chat message 
 legacy notice: identical function in kludgy_html_parser 
 the older function is ""overridden"" here for compatibility reasons'"
"def _write_proj(fid, projs): 
    if (len(projs) == 0): 
      return 
   start_block(fid, FIFF.FIFFB_PROJ) 
   for proj in projs: 
      start_block(fid, FIFF.FIFFB_PROJ_ITEM) 
      write_int(fid, FIFF.FIFF_NCHAN, proj['data']['ncol']) 
      write_name_list(fid, FIFF.FIFF_PROJ_ITEM_CH_NAME_LIST, proj['data']['col_names']) 
      write_string(fid, FIFF.FIFF_NAME, proj['desc']) 
      write_int(fid, FIFF.FIFF_PROJ_ITEM_KIND, proj['kind']) 
      if (proj['kind'] == FIFF.FIFFV_PROJ_ITEM_FIELD): 
         write_float(fid, FIFF.FIFF_PROJ_ITEM_TIME, 0.0) 
      write_int(fid, FIFF.FIFF_PROJ_ITEM_NVEC, proj['data']['nrow']) 
      write_int(fid, FIFF.FIFF_MNE_PROJ_ITEM_ACTIVE, proj['active']) 
      write_float_matrix(fid, FIFF.FIFF_PROJ_ITEM_VECTORS, proj['data']['data']) 
      if (proj['explained_var'] is not None): 
         write_float(fid, FIFF.FIFF_MNE_ICA_PCA_EXPLAINED_VAR, proj['explained_var']) 
      end_block(fid, FIFF.FIFFB_PROJ_ITEM) 
   end_block(fid, FIFF.FIFFB_PROJ)"," 'Write the projected data to the file. 
 Parameters 
 fid : file object 
 File descriptor. 
 projs : list of dicts 
 List of projected data. 
 Returns 
 None 
 Notes 
 This function is used to write projected data to the file. 
 The projected data is stored as a list of dicts. 
 Each dict contains the following fields: 
 - \'data\': dictionary of the projected data. 
 - \'desc\': description of the projected data. 
 - \'kind\': kind of the projected data. 
 - \'active\': whether the projected data is active or not. 
 - \'explained_var\': explained variance of the projected data. 
 - \'col_names\': name of the columns. 
 - \'nrow\': number of rows. 
 - \'ncol\': number of columns. 
 - \'data\': matrix of the projected data. 
 - \'desc\': description of the projected data. 
 - \'kind\': kind of the","'Write a projection operator to a file. 
 Parameters 
 fid : file 
 The file descriptor of the open file. 
 projs : dict 
 The projection operator.'"
"def clean_up_tables(db, tmp_prefix): 
    db.execute('BEGIN') 
   for table in ('point', 'line', 'roads', 'polygon'): 
      db.execute(('DROP   TABLE   %(tmp_prefix)s_%(table)s' % locals())) 
      db.execute((""DELETE   FROM   geometry_columns   WHERE   f_table_name   =   '%(tmp_prefix)s_%(table)s'"" % locals())) 
   db.execute('COMMIT')"," 'Clean up the tables in the temporary database. 
 :param db: database connection 
 :param tmp_prefix: prefix for temporary tables 
 :return: None'",'Drop all temporary tables created by prepare_data().'
"def custom_url(generator, metadata): 
    global global_siteurl 
   global_siteurl = generator.settings['SITEURL']"," 'Adds the current site URL to the generator\'s metadata. 
 This is necessary for the django-static-generator to know where to 
 serve static files. 
 This is not necessary for django-sphinx-generator, since it serves static 
 files from the same directory as the generated documentation.'",'Saves globally the value of SITEURL configuration parameter'
"def find_sockfiles(): 
    paths = [] 
   for dir in SEARCH_DIRS: 
      if ((not os.path.isdir(dir)) or (not os.access(dir, os.R_OK))): 
         continue 
      for name in os.listdir(dir): 
         subdir = os.path.join(dir, name) 
         if ((not os.path.isdir(subdir)) or (not os.access(subdir, os.R_OK))): 
            continue 
         for subname in os.listdir(subdir): 
            path = os.path.join(subdir, subname) 
            if utils.is_sockfile(path): 
               paths.append(path) 
               break 
   for sockfile in DEFAULT_SOCKFILES: 
      if (not utils.is_sockfile(sockfile)): 
         continue 
      paths.append(sockfile) 
   return paths"," 'Find all sockfiles in the given directories. 
 :return: a list of sockfiles'",'Returns a list of paths to socket files to monitor.'
"def local_extra_dirs(func): 
    def wraps(self, *args, **kwargs): 
      if (kwargs.get('base_dir', None) is None): 
         return func(self, *args, **kwargs) 
      else: 
         for c in self.__class__.__mro__: 
            if (c.__name__ == 'DiskObjectStore'): 
               return getattr(c, func.__name__)(self, *args, **kwargs) 
         raise Exception((""Could   not   call   DiskObjectStore's   %s   method,   does   your   Object   Store   plugin   inherit   from   DiskObjectStore?"" % func.__name__)) 
   return wraps"," 'A decorator to add extra directories to the local path. 
 This is useful for when you are using a remote object store 
 and want to add some extra directories to the local path. 
 This decorator will add the extra directories to the local path 
 before calling the wrapped method. 
 Example: 
 @local_extra_dirs 
 def get_object(self, object_id): 
 return self.get_object_by_id(object_id) 
 :param func: The function to decorate 
 :type func: function 
 :param self: The self argument of the wrapped function 
 :type self: object 
 :param args: The arguments of the wrapped function 
 :type args: tuple 
 :param kwargs: The keyword arguments of the wrapped function 
 :type kwargs: dict 
 :return: The wrapped function 
 :rtype: function'",'Non-local plugin decorator using local directories for the extra_dirs (job_work and temp).'
"def topological_sort(graph, key=None): 
    (V, E) = graph 
   L = [] 
   S = set(V) 
   E = list(E) 
   for (v, u) in E: 
      S.discard(u) 
   if (key is None): 
      key = (lambda value: value) 
   S = sorted(S, key=key, reverse=True) 
   while S: 
      node = S.pop() 
      L.append(node) 
      for (u, v) in list(E): 
         if (u == node): 
            E.remove((u, v)) 
            for (_u, _v) in E: 
               if (v == _v): 
                  break 
            else: 
               kv = key(v) 
               for (i, s) in enumerate(S): 
                  ks = key(s) 
                  if (kv > ks): 
                     S.insert(i, v) 
                     break 
               else: 
                  S.append(v) 
   if E: 
      raise ValueError('cycle   detected') 
   else: 
      return L"," 'Returns a list of the nodes in topological order. 
 If a key is specified, the nodes are sorted according to that key. 
 If no key is specified, the nodes are sorted in ascending order. 
 If a cycle is detected, a ValueError is raised. 
 >>> from sympy.combinatorics.permutations import permutations 
 >>> from sympy.combinatorics.graphs import topological_sort 
 >>> from sympy.combinatorics.permutations import permutation 
 >>> from sympy.combinatorics.graphs import Graph 
 >>> G = Graph([[0, 1], [0, 2], [1, 2], [2, 0], [2, 3]]) 
 >>> permutations(G.nodes()) 
 [0, 1, 2, 3] 
 >>> permutations(G.nodes(), 2) 
 [0, 1] 
 >>> permutations(G.nodes(), 1) 
 [0] 
 >>> permutations(G.nodes(), 0) 
 [0, 1, 2, 3","'Topological sort of graph\'s vertices. 
 Parameters 
 ``graph`` : ``tuple[list, list[tuple[T, T]]`` 
 A tuple consisting of a list of vertices and a list of edges of 
 a graph to be sorted topologically. 
 ``key`` : ``callable[T]`` (optional) 
 Ordering key for vertices on the same level. By default the natural 
 (e.g. lexicographic) ordering is used (in this case the base type 
 must implement ordering relations). 
 Examples 
 Consider a graph:: 
 | 7 |\    | 5 |     | 3 | 
 V  V           V V   | 
 | 11 |         | 8 |  | 
 V  \     V V   /  V  V 
 | 2 |  |  | 9 | |  | 10 | 
 where vertices are integers. This graph can be encoded using 
 elementary Python\'s data structures as follows:: 
 >>> V = [2, 3, 5, 7, 8, 9, 10, 11] 
 >>> E = [(7, 11), (7, 8), (5, 11), (3, 8), (3, 10), 
 ...      (11, 2), (11, 9), (11, 10), (8, 9)] 
 To compute a topological sort for graph ``(V, E)`` issue:: 
 >>> from sympy.utilities.iterables import topological_sort 
 >>> topological_sort((V, E)) 
 [3, 5, 7, 8, 11, 2, 9, 10] 
 If specific tie breaking approach is needed, use ``key`` parameter:: 
 >>> topological_sort((V, E), key=lambda v: -v) 
 [7, 5, 11, 3, 10, 8, 9, 2] 
 Only acyclic graphs can be sorted. If the input graph has a cycle, 
 then :py:exc:`ValueError` will be raised:: 
 >>> topological_sort((V, E + [(10, 7)])) 
 Traceback (most recent call last): 
 ValueError: cycle detected 
 .. seealso:: http://en.wikipedia.org/wiki/Topological_sorting'"
"def get_transcripts_from_youtube(youtube_id, settings, i18n, youtube_transcript_name=''): 
    _ = i18n.ugettext 
   utf8_parser = etree.XMLParser(encoding='utf-8') 
   youtube_text_api = copy.deepcopy(settings.YOUTUBE['TEXT_API']) 
   youtube_text_api['params']['v'] = youtube_id 
   youtube_transcript_name = youtube_video_transcript_name(youtube_text_api) 
   if youtube_transcript_name: 
      youtube_text_api['params']['name'] = youtube_transcript_name 
   data = requests.get(('http://' + youtube_text_api['url']), params=youtube_text_api['params']) 
   if ((data.status_code != 200) or (not data.text)): 
      msg = _(""Can't   receive   transcripts   from   Youtube   for   {youtube_id}.   Status   code:   {status_code}."").format(youtube_id=youtube_id, status_code=data.status_code) 
      raise GetTranscriptsFromYouTubeException(msg) 
   (sub_starts, sub_ends, sub_texts) = ([], [], []) 
   xmltree = etree.fromstring(data.content, parser=utf8_parser) 
   for element in xmltree: 
      if (element.tag == 'text'): 
         start = float(element.get('start')) 
         duration = float(element.get('dur', 0)) 
         text = element.text 
         end = (start + duration) 
         if text: 
            sub_starts.append(int((start * 1000))) 
            sub_ends.append(int(((end + 0.0001) * 1000))) 
            sub_texts.append(text.replace('\n', '   ')) 
   return {'start': sub_starts, 'end': sub_ends, 'text': sub_texts}"," 'Get transcripts from Youtube. 
 :param youtube_id: Youtube ID 
 :type youtube_id: str 
 :param settings: Django settings 
 :type settings: dict 
 :param i18n: i18n 
 :type i18n: i18n.Translator 
 :param youtube_transcript_name: Transcript name 
 :type youtube_transcript_name: str 
 :return: 
 :rtype: dict 
 :raise GetTranscriptsFromYouTubeException: 
 :raises: GetTranscriptsFromYouTubeException'","'Gets transcripts from youtube for youtube_id. 
 Parses only utf-8 encoded transcripts. 
 Other encodings are not supported at the moment. 
 Returns (status, transcripts): bool, dict.'"
"def coreproperties(title, subject, creator, keywords, lastmodifiedby=None): 
    coreprops = makeelement('coreProperties', nsprefix='cp') 
   coreprops.append(makeelement('title', tagtext=title, nsprefix='dc')) 
   coreprops.append(makeelement('subject', tagtext=subject, nsprefix='dc')) 
   coreprops.append(makeelement('creator', tagtext=creator, nsprefix='dc')) 
   coreprops.append(makeelement('keywords', tagtext=','.join(keywords), nsprefix='cp')) 
   if (not lastmodifiedby): 
      lastmodifiedby = creator 
   coreprops.append(makeelement('lastModifiedBy', tagtext=lastmodifiedby, nsprefix='cp')) 
   coreprops.append(makeelement('revision', tagtext='1', nsprefix='cp')) 
   coreprops.append(makeelement('category', tagtext='Examples', nsprefix='cp')) 
   coreprops.append(makeelement('description', tagtext='Examples', nsprefix='dc')) 
   currenttime = time.strftime('%Y-%m-%dT%H:%M:%SZ') 
   for doctime in ['created', 'modified']: 
      elm_str = ('<dcterms:%s   xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""   xmlns:dcterms=""http://purl.org/dc/terms/""   xsi:type=""dcterms:W3CDTF"">%s</dcterms:%s>' % (doctime, currenttime, doctime)) 
      coreprops.append(etree.fromstring(elm_str)) 
   return coreprops"," 'Returns a core properties element with the given properties. 
 title, subject, creator, keywords, lastmodifiedby are the properties to 
 add.'","'Create core properties (common document properties referred to in the 
 \'Dublin Core\' specification). See appproperties() for other stuff.'"
"def setup_request_bound_data(config): 
    def attach_bound_data(request): 
      parent = getattr(request, 'parent', None) 
      return (parent.bound_data if parent else {}) 
   config.add_request_method(attach_bound_data, name='bound_data', reify=True)"," 'Adds a method to the request class that returns the bound data. 
 The bound data is the data that is bound to the request object. 
 This is a dictionary that maps the URL path to the data that was 
 bound to the request object when it was created. 
 :param config: The configuration to add the method to.'","'Attach custom data on request object, and share it with parent 
 requests during batch.'"
"def has_no_time(at): 
    if isinstance(at, datetime): 
      return False 
   return ((at.hour is None) and (at.minute is None) and (at.second is None) and (at.microsecond is None))"," 'Returns True if the given time is in the format ""00:00:00""'","'Returns True if the given object is an ``adatetime`` where ``hour``, 
 ``minute``, ``second`` and ``microsecond`` are all None.'"
"def band_stop_obj(wp, ind, passb, stopb, gpass, gstop, type): 
    passbC = passb.copy() 
   passbC[ind] = wp 
   nat = ((stopb * (passbC[0] - passbC[1])) / ((stopb ** 2) - (passbC[0] * passbC[1]))) 
   nat = min(abs(nat)) 
   if (type == 'butter'): 
      GSTOP = (10 ** (0.1 * abs(gstop))) 
      GPASS = (10 ** (0.1 * abs(gpass))) 
      n = (log10(((GSTOP - 1.0) / (GPASS - 1.0))) / (2 * log10(nat))) 
   elif (type == 'cheby'): 
      GSTOP = (10 ** (0.1 * abs(gstop))) 
      GPASS = (10 ** (0.1 * abs(gpass))) 
      n = (arccosh(sqrt(((GSTOP - 1.0) / (GPASS - 1.0)))) / arccosh(nat)) 
   elif (type == 'ellip'): 
      GSTOP = (10 ** (0.1 * gstop)) 
      GPASS = (10 ** (0.1 * gpass)) 
      arg1 = sqrt(((GPASS - 1.0) / (GSTOP - 1.0))) 
      arg0 = (1.0 / nat) 
      d0 = special.ellipk([(arg0 ** 2), (1 - (arg0 ** 2))]) 
      d1 = special.ellipk([(arg1 ** 2), (1 - (arg1 ** 2))]) 
      n = ((d0[0] * d1[1]) / (d0[1] * d1[0])) 
   else: 
      raise ValueError(('Incorrect   type:   %s' % type)) 
   return n"," 'Returns the order of the band stop filter. 
 The order is defined as: 
 n = (log10(((GSTOP - 1.0) / (GPASS - 1.0))) / (2 * log10(nat))) 
 where nat is the natural logarithm of the ratio of the 
 passband and stopband transmission coefficients. 
 The order is given in decibels. 
 Parameters 
 wp : float 
 The passband transmission coefficient. 
 ind : int 
 The index of the filter. 
 passb : ndarray 
 The passband transmission coefficients. 
 stopb : ndarray 
 The stopband transmission coefficients. 
 gpass : float 
 The gain of the passband. 
 gstop : float 
 The gain of the stopband. 
 type : string 
 The type of the filter. 
 Valid values are: 
 ""butter"" 
 ""cheby"" 
 ""ellip"" 
 Examples 
 >>> from scipy.signal import band_stop_obj 
 >>> from scipy.signal import butter 
","'Band Stop Objective Function for order minimization. 
 Returns the non-integer order for an analog band stop filter. 
 Parameters 
 wp : scalar 
 Edge of passband `passb`. 
 ind : int, {0, 1} 
 Index specifying which `passb` edge to vary (0 or 1). 
 passb : ndarray 
 Two element sequence of fixed passband edges. 
 stopb : ndarray 
 Two element sequence of fixed stopband edges. 
 gstop : float 
 Amount of attenuation in stopband in dB. 
 gpass : float 
 Amount of ripple in the passband in dB. 
 type : {\'butter\', \'cheby\', \'ellip\'} 
 Type of filter. 
 Returns 
 n : scalar 
 Filter order (possibly non-integer).'"
"def view(tpl_name, **defaults): 
    def decorator(func): 
      @functools.wraps(func) 
      def wrapper(*args, **kwargs): 
         result = func(*args, **kwargs) 
         if isinstance(result, (dict, DictMixin)): 
            tplvars = defaults.copy() 
            tplvars.update(result) 
            return template(tpl_name, **tplvars) 
         elif (result is None): 
            return template(tpl_name, defaults) 
         return result 
      return wrapper 
   return decorator"," 'Decorator that returns a template view. 
 :param tpl_name: Name of the template to render. 
 :param defaults: Default values to pass to the template. 
 :returns: A decorator that renders a template with the given name. 
 :rtype: functools.wraps'","'Decorator: renders a template for a handler. 
 The handler can control its behavior like that: 
 - return a dict of template vars to fill out the template 
 - return something other than a dict and the view decorator will not 
 process the template, but return the handler result as is. 
 This includes returning a HTTPResponse(dict) to get, 
 for instance, JSON with autojson or other castfilters.'"
"def primes(n): 
    if (n == 2): 
      return [2] 
   elif (n < 2): 
      return [] 
   s = list(range(3, (n + 1), 2)) 
   mroot = (n ** 0.5) 
   half = (((n + 1) // 2) - 1) 
   i = 0 
   m = 3 
   while (m <= mroot): 
      if s[i]: 
         j = (((m * m) - 3) // 2) 
         s[j] = 0 
         while (j < half): 
            s[j] = 0 
            j += m 
      i = (i + 1) 
      m = ((2 * i) + 3) 
   return ([2] + [x for x in s if x])", 'Primes up to n',"'Simple test function 
 Taken from http://www.huyng.com/posts/python-performance-analysis/'"
"def subjunctive(sentence, classical=True, **kwargs): 
    S = sentence 
   if (not (hasattr(S, 'words') and hasattr(S, 'parse_token'))): 
      raise TypeError(('%s   object   is   not   a   parsed   Sentence' % repr(S.__class__.__name__))) 
   if question(S): 
      return False 
   for (i, w) in enumerate(S): 
      b = False 
      if w.type.startswith('VB'): 
         if s(w).startswith('wish'): 
            return True 
         if ((s(w) == 'hope') and (i > 0) and (s(S[(i - 1)]) in ('i', 'we'))): 
            return True 
         if ((s(w) == 'were') and (i > 0) and ((s(S[(i - 1)]) in ('i', 'it', 'he', 'she')) or (S[(i - 1)].type == 'NN'))): 
            return True 
         if (s(w) in subjunctive1): 
            b = True 
         elif ((s(w) == 'is') and (0 < i < (len(S) - 1)) and (s(S[(i - 1)]) == 'it') and (s(S[(i + 1)]) in subjunctive2)): 
            b = True 
         elif ((s(w) == 'is') and (0 < i < (len(S) - 3)) and (s(S[(i - 1)]) == 'it') and (s(S[(i + 2)]) in ('good', 'bad')) and (s(S[(i + 3)]) == 'idea')): 
            b = True 
      if b: 
         v = find((lambda w: w.type.startswith('VB')), S[(i + 1):]) 
         if (v and (classical is True) and v and (v.type == 'VB')): 
            return True 
         if (v and (classical is False)): 
            return True 
   return False"," 'Returns True if the sentence is in the subjunctive mood. 
 The subjunctive mood is used in sentences that express 
 wish, hope, or uncertainty. 
 The subjunctive mood is not used in questions, but 
 if the sentence is a question, it is not considered. 
 The subjunctive mood is also not used in sentences that 
 express certainty, but if the sentence is certain, it is 
 not considered. 
 Examples 
 >>> from nltk.sentiment.util import subjunctive 
 >>> subjunctive(""I wish I had a million dollars"") 
 True 
 >>> subjunctive(""I wish I had a million dollars"", classical=False) 
 True 
 >>> subjunctive(""I wish I had a million dollars"", classical=False, 
 ... positive=True) 
 False 
 >>> subjunctive(""I wish I had a million dollars"", classical=True) 
 False 
 >>> subjunctive(""I wish I had a million dollars"", classical=True, 
 ... positive=True) 
 False 
 >>>","'The subjunctive mood is a classical mood used to express a wish, judgment or opinion. 
 It is marked by the verb wish/were, or infinitive form of a verb 
 preceded by an ""it is""-statement: 
 ""It is recommended that he bring his own computer.""'"
"def test_MultipleLocator_set_params(): 
    mult = mticker.MultipleLocator(base=0.7) 
   mult.set_params(base=1.7) 
   assert (mult._base == 1.7)", 'Test that set_params sets the base parameter correctly.',"'Create multiple locator with 0.7 base, and change it to something else. 
 See if change was successful. 
 Should not exception.'"
"@contextmanager 
 def disable_overrides(): 
    prev = _OVERRIDES_DISABLED.disabled 
   _OVERRIDES_DISABLED.disabled += (True,) 
   (yield) 
   _OVERRIDES_DISABLED.disabled = prev"," 'Context manager for temporarily disabling the overrides. 
 The context manager returns the previous value of the 
 :attr:`~astroid.overrides.Overrides.disabled` attribute. 
 :rtype: bool'","'A context manager which disables field overrides inside the context of a 
 `with` statement, allowing code to get at the `original` value of a field.'"
"def main(args): 
    if (not find_sockfiles()): 
      return 13 
   if (MySQLdb is None): 
      utils.err(""error:   Python   module   `MySQLdb'   is   missing"") 
      return 1 
   last_db_refresh = now() 
   dbs = find_databases() 
   while True: 
      ts = now() 
      if ((ts - last_db_refresh) >= DB_REFRESH_INTERVAL): 
         find_databases(dbs) 
         last_db_refresh = ts 
      errs = [] 
      for (dbname, db) in dbs.iteritems(): 
         try: 
            collect(db) 
         except (EnvironmentError, EOFError, RuntimeError, socket.error, MySQLdb.MySQLError) as e: 
            if (isinstance(e, IOError) and (e[0] == errno.EPIPE)): 
               return 2 
            utils.err(('error:   failed   to   collect   data   from   %s:   %s' % (db, e))) 
            errs.append(dbname) 
      for dbname in errs: 
         del dbs[dbname] 
      sys.stdout.flush() 
      time.sleep(COLLECTION_INTERVAL)", 'Main function','Collects and dumps stats from a MySQL server.'
"def stack(xs, axis=0): 
    xs = [expand_dims.expand_dims(x, axis=axis) for x in xs] 
   return concat.concat(xs, axis=axis)"," 'Stacks the given input along the specified axis. 
 Parameters 
 xs : ndarray 
 The input arrays. 
 axis : int, optional 
 The axis along which to stack. 
 Returns 
 stacked : ndarray 
 The stacked array. 
 Examples 
 >>> x = np.array([[1, 2], [3, 4]]) 
 >>> stack(x) 
 array([[1, 2, 3, 4]])'","'Concatenate variables along a new axis. 
 Args: 
 xs (list of chainer.Variable): Variables to be concatenated. 
 axis (int): Axis of result along which variables are stacked. 
 Returns: 
 ~chainer.Variable: Output variable.'"
"def _execute(*cmd, **kwargs): 
    if CONF.fake_network: 
      LOG.debug('FAKE   NET:   %s', '   '.join(map(str, cmd))) 
      return ('fake', 0) 
   else: 
      return utils.execute(*cmd, **kwargs)"," 'Execute a command in a shell. 
 :param cmd: the command to execute. 
 :param kwargs: keyword arguments to pass to the shell. 
 :returns: (code, output)'",'Wrapper around utils._execute for fake_network.'
"def which_files(file, mode=(F_OK | X_OK), path=None, pathext=None): 
    (filepath, file) = split(file) 
   if filepath: 
      path = (filepath,) 
   elif (path is None): 
      path = defpath 
   elif isinstance(path, str): 
      path = path.split(pathsep) 
   if (pathext is None): 
      pathext = defpathext 
   elif isinstance(pathext, str): 
      pathext = pathext.split(pathsep) 
   if (not ('' in pathext)): 
      pathext.insert(0, '') 
   for dir in path: 
      basepath = join(dir, file) 
      for ext in pathext: 
         fullpath = (basepath + ext) 
         if (exists(fullpath) and access(fullpath, mode)): 
            (yield fullpath)"," 'Returns a generator of file names matching the given file, 
 with the given mode. 
 The file name is the base name of the file, with the given extension 
 appended. 
 If the file is in a directory, the directory name is also included in the 
 path. 
 If the file is a directory, it is not included in the path. 
 If the file is a symbolic link, the path is not modified. 
 If the file is a socket, the path is not modified. 
 If the file is a FIFO, the path is not modified. 
 If the file is a device, the path is not modified. 
 If the file is a named pipe, the path is not modified. 
 If the file is a socketpair, the path is not modified. 
 If the file is a character device, the path is not modified. 
 If the file is a block device, the path is not modified. 
 If the file is a FIFO, the path is not modified. 
 If the file is a socket, the path is not modified. 
 If the file is a named pipe, the path is not modified. ","'Locate a file in a path supplied as a part of the file name, 
 or the user\'s path, or a supplied path. 
 The function yields full paths (not necessarily absolute paths), 
 in which the given file name matches an existing file in a directory on the path. 
 >>> def test_which(expected, *args, **argd): 
 ...     result = list(which_files(*args, **argd)) 
 ...     assert result == expected, \'which_files: %s != %s\' % (result, expected) 
 ...     try: 
 ...         result = [ which(*args, **argd) ] 
 ...     except IOError: 
 ...         result = [] 
 ...     assert result[:1] == expected[:1], \'which: %s != %s\' % (result[:1], expected[:1]) 
 >>> if windows: cmd = environ[\'COMSPEC\'] 
 >>> if windows: test_which([cmd], \'cmd\') 
 >>> if windows: test_which([cmd], \'cmd.exe\') 
 >>> if windows: test_which([cmd], \'cmd\', path=dirname(cmd)) 
 >>> if windows: test_which([cmd], \'cmd\', pathext=\'.exe\') 
 >>> if windows: test_which([cmd], cmd) 
 >>> if windows: test_which([cmd], cmd, path=\'<nonexistent>\') 
 >>> if windows: test_which([cmd], cmd, pathext=\'<nonexistent>\') 
 >>> if windows: test_which([cmd], cmd[:-4]) 
 >>> if windows: test_which([cmd], cmd[:-4], path=\'<nonexistent>\') 
 >>> if windows: test_which([], \'cmd\', path=\'<nonexistent>\') 
 >>> if windows: test_which([], \'cmd\', pathext=\'<nonexistent>\') 
 >>> if windows: test_which([], \'<nonexistent>/cmd\') 
 >>> if windows: test_which([], cmd[:-4], pathext=\'<nonexistent>\') 
 >>> if not windows: sh = \'/bin/sh\' 
 >>> if not windows: test_which([sh], \'sh\') 
 >>> if not windows: test_which([sh], \'sh\', path=dirname(sh)) 
 >>> if not windows: test_which([sh], \'sh\', pathext=\'<nonexistent>\') 
 >>> if not windows: test_which([sh], sh) 
 >>> if not windows: test_which([sh], sh, path=\'<nonexistent>\') 
 >>> if not windows: test_which([sh], sh, pathext=\'<nonexistent>\') 
 >>> if not windows: test_which([], \'sh\', mode=W_OK)  # not running as root, are you? 
 >>> if not windows: test_which([], \'sh\', path=\'<nonexistent>\') 
 >>> if not windows: test_which([], \'<nonexistent>/sh\')'"
"def runner(): 
    client = salt.runner.RunnerClient(__opts__) 
   ret = client.get_docs() 
   return ret", 'Get the docs from salt-runner.',"'Return all inline documentation for runner modules 
 CLI Example: 
 .. code-block:: bash 
 salt-run doc.runner'"
"def customized_clean_str(string): 
    string = re.sub('\\n', '   ', string) 
   string = re.sub(""\\'s"", ""   's"", string) 
   string = re.sub('\\\xe2\x80\x99s', ""   's"", string) 
   string = re.sub(""\\'ve"", '   have', string) 
   string = re.sub('\\\xe2\x80\x99ve', '   have', string) 
   string = re.sub(""\\'t"", '   not', string) 
   string = re.sub('\\\xe2\x80\x99t', '   not', string) 
   string = re.sub(""\\'re"", '   are', string) 
   string = re.sub('\\\xe2\x80\x99re', '   are', string) 
   string = re.sub(""\\'d"", '', string) 
   string = re.sub('\\\xe2\x80\x99d', '', string) 
   string = re.sub(""\\'ll"", '   will', string) 
   string = re.sub('\\\xe2\x80\x99ll', '   will', string) 
   string = re.sub('\\\xe2\x80\x9c', '   \xe2\x80\x9c   ', string) 
   string = re.sub('\\\xe2\x80\x9d', '   \xe2\x80\x9d   ', string) 
   string = re.sub('\\""', '   \xe2\x80\x9c   ', string) 
   string = re.sub(""\\'"", ""   '   "", string) 
   string = re.sub('\\\xe2\x80\x99', ""   '   "", string) 
   string = re.sub('\\.', '   .   ', string) 
   string = re.sub('\\,', '   ,   ', string) 
   string = re.sub('\\-', '   ', string) 
   string = re.sub('\\(', '   (   ', string) 
   string = re.sub('\\)', '   )   ', string) 
   string = re.sub('\\!', '   !   ', string) 
   string = re.sub('\\]', '   ]   ', string) 
   string = re.sub('\\[', '   [   ', string) 
   string = re.sub('\\?', '   ?   ', string) 
   string = re.sub('\\>', '   >   ', string) 
   string = re.sub('\\<', '   <   ', string) 
   string = re.sub('\\=', '   =   ', string) 
   string = re.sub('\\;', '   ;   ', string) 
   string = re.sub('\\;', '   ;   ', string) 
   string = re.sub('\\:', '   :   ', string) 
   string = re.sub('\\""', '   ""   ', string) 
   string = re.sub('\\$', '   $   ', string) 
   string = re.sub('\\_', '   _   ', string) 
   string = re.sub('\\s{2,}', '   ', string) 
   return string.strip().lower()"," 'Clean a string by removing all non-letters, punctuation and 
 whitespace characters. 
 The following are examples of input and output: 
 >>> clean_str(\'The quick brown fox jumps over the lazy dog.\') 
 \'the quick brown fox jumps over the lazy dog.\' 
 >>> clean_str(\'The quick brown fox jumps over the lazy dog.\') 
 \'the quick brown fox jumps over the lazy dog.\' 
 >>> clean_str(\'The quick brown fox jumps over the lazy dog.\') 
 \'the quick brown fox jumps over the lazy dog.\' 
 >>> clean_str(\'The quick brown fox jumps over the lazy dog.\') 
 \'the quick brown fox jumps over the lazy dog.\' 
 >>> clean_str(\'The quick brown fox jumps over the lazy dog.\') 
 \'the quick brown fox jumps over the lazy dog.\' 
 >>> clean_str(\'The quick brown fox jumps over the lazy dog.\') 
 \'the quick brown fox jumps over",'Tokenization/string cleaning for a datasets.'
"def var_label(var, precision=3): 
    if (var.name is not None): 
      return var.name 
   elif isinstance(var, gof.Constant): 
      h = np.asarray(var.data) 
      is_const = False 
      if (h.ndim == 0): 
         is_const = True 
         h = np.array([h]) 
      dstr = np.array2string(h, precision=precision) 
      if ('\n' in dstr): 
         dstr = dstr[:dstr.index('\n')] 
      if is_const: 
         dstr = dstr.replace('[', '').replace(']', '') 
      return dstr 
   else: 
      return type_to_str(var.type)"," 'Return a string representation of the variable. 
 Parameters 
 var : gof.Variable 
 The variable. 
 precision : int 
 The number of decimal places to use in the string representation. 
 Returns 
 str 
 The string representation of the variable.'",'Return label of variable node.'
"def valid_android_zip(app_dir): 
    try: 
      print '[INFO]   Checking   for   ZIP   Validity   and   Mode' 
      man = os.path.isfile(os.path.join(app_dir, 'AndroidManifest.xml')) 
      src = os.path.exists(os.path.join(app_dir, 'src/')) 
      if (man and src): 
         return ('eclipse', True) 
      man = os.path.isfile(os.path.join(app_dir, 'app/src/main/AndroidManifest.xml')) 
      src = os.path.exists(os.path.join(app_dir, 'app/src/main/java/')) 
      if (man and src): 
         return ('studio', True) 
      xcode = [f for f in os.listdir(app_dir) if f.endswith('.xcodeproj')] 
      if xcode: 
         return ('ios', True) 
      return ('', False) 
   except: 
      PrintException('[ERROR]   Determining   Upload   type')"," 'Checks the app dir for the presence of an AndroidManifest.xml file 
 and a src/ directory. 
 Returns a tuple with the name of the IDE (studio, eclipse, ios) and a 
 boolean indicating whether or not the app is valid.'",'Test if this is an valid android zip.'
"def validate_required_iff(**kwargs): 
    def _validator(form, field): 
      all_conditions_met = True 
      for (key, value) in kwargs.iteritems(): 
         if (getattr(form, key).data != value): 
            all_conditions_met = False 
      if all_conditions_met: 
         if ((field.data is None) or (isinstance(field.data, (str, unicode)) and (not field.data.strip())) or (isinstance(field.data, FileStorage) and (not field.data.filename.strip()))): 
            raise validators.ValidationError('This   field   is   required.') 
      else: 
         field.errors[:] = [] 
         raise validators.StopValidation() 
   return _validator"," 'A validator that only allows the field to be validated if all required 
 conditions are met. 
 :param kwargs: 
 :type kwargs: dict 
 :param field: 
 :type field: Field 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
","'Used as a validator within a wtforms.Form 
 This implements a conditional DataRequired 
 Each of the kwargs is a condition that must be met in the form 
 Otherwise, no validation is done'"
"@pytest.mark.django_db 
 def test_format_registry_reregister(no_formats): 
    registry = FormatRegistry() 
   filetype = registry.register('foo', 'foo') 
   new_filetype = registry.register('foo', 'foo') 
   assert (new_filetype == filetype) 
   _test_formats(registry, ['foo']) 
   new_filetype = registry.register('foo', 'foo', title='Bar') 
   assert (new_filetype == filetype) 
   assert (new_filetype.title == 'Bar') 
   _test_formats(registry, ['foo']) 
   new_filetype = registry.register('foo', 'bar') 
   assert (new_filetype == filetype) 
   assert (new_filetype.title == 'Bar') 
   assert (str(new_filetype.extension) == 'bar') 
   _test_formats(registry, ['foo'])", 'Test that we can register the same filetype multiple times.','Tests the creation of a file extension'
"def import_no_virt_driver_import_deps(physical_line, filename): 
    thisdriver = _get_virt_name(virt_file_re, filename) 
   thatdriver = _get_virt_name(virt_import_re, physical_line) 
   if ((thatdriver is not None) and (thisdriver is not None) and (thisdriver != thatdriver)): 
      return (0, 'N311:   importing   code   from   other   virt   drivers   forbidden')", 'Check for importing other virt drivers.',"'Check virt drivers\' modules aren\'t imported by other drivers 
 Modules under each virt driver\'s directory are 
 considered private to that virt driver. Other drivers 
 in Nova must not access those drivers. Any code that 
 is to be shared should be refactored into a common 
 module 
 N311'"
"def _trim_text(text, max_width): 
    width = get_cwidth(text) 
   if (width > max_width): 
      if (len(text) == width): 
         trimmed_text = (text[:max(1, (max_width - 3))] + u'...')[:max_width] 
         return (trimmed_text, len(trimmed_text)) 
      else: 
         trimmed_text = u'' 
         for c in text: 
            if (get_cwidth((trimmed_text + c)) <= (max_width - 3)): 
               trimmed_text += c 
         trimmed_text += u'...' 
         return (trimmed_text, get_cwidth(trimmed_text)) 
   else: 
      return (text, width)"," 'Trim a text to a given width. 
 If the text is shorter than the given width, the text is returned. 
 If the text is longer than the given width, the text is trimmed to 
 the given width and \'...\' is appended. 
 :param text: Text to trim. 
 :param max_width: Maximum width to trim to. 
 :return: Trimmed text and the length of the trimmed text.'","'Trim the text to `max_width`, append dots when the text is too long. 
 Returns (text, width) tuple.'"
"def _labels_inertia_precompute_dense(X, x_squared_norms, centers, distances): 
    n_samples = X.shape[0] 
   (labels, mindist) = pairwise_distances_argmin_min(X=X, Y=centers, metric='euclidean', metric_kwargs={'squared': True}) 
   labels = labels.astype(np.int32) 
   if (n_samples == distances.shape[0]): 
      distances[:] = mindist 
   inertia = mindist.sum() 
   return (labels, inertia)"," 'Precompute distances and labels for inertia computation. 
 Parameters 
 X : ndarray 
 Points to compute distances to. 
 x_squared_norms : ndarray 
 Squared norms of X. 
 centers : ndarray 
 Centers of clusters. 
 distances : ndarray 
 Distances to centers. 
 Returns 
 labels : ndarray 
 Cluster labels. 
 inertia : float 
 Inertia of the clustering. 
 Notes 
 This function is used to precompute the distance matrix for inertia computation. 
 It is more efficient to compute the distances once, rather than for each cluster. 
 The precomputed distances are stored in distances. 
 This function assumes that the points are already normalized by their 
 squared norms. 
 This function is used by :func:`sklearn.cluster.DBSCAN`. 
 References 
 .. [1] ""A Fast Algorithm for Training Support Vector Machines"", 
 D. E. Gee, C. R. Rasmussen, and S. S. Williams, 19","'Compute labels and inertia using a full distance matrix. 
 This will overwrite the \'distances\' array in-place. 
 Parameters 
 X : numpy array, shape (n_sample, n_features) 
 Input data. 
 x_squared_norms : numpy array, shape (n_samples,) 
 Precomputed squared norms of X. 
 centers : numpy array, shape (n_clusters, n_features) 
 Cluster centers which data is assigned to. 
 distances : numpy array, shape (n_samples,) 
 Pre-allocated array in which distances are stored. 
 Returns 
 labels : numpy array, dtype=np.int, shape (n_samples,) 
 Indices of clusters that samples are assigned to. 
 inertia : float 
 Sum of distances of samples to their closest cluster center.'"
"def compute_node_statistics(context): 
    return IMPL.compute_node_statistics(context)", 'Compute the statistics for a node.',"'Get aggregate statistics over all compute nodes. 
 :param context: The security context 
 :returns: Dictionary containing compute node characteristics summed up 
 over all the compute nodes, e.g. \'vcpus\', \'free_ram_mb\' etc.'"
"def common_texification(text): 
    text = re_mathdefault.sub(repl_mathdefault, text) 
   parts = re_mathsep.split(text) 
   for (i, s) in enumerate(parts): 
      if (not (i % 2)): 
         s = re_escapetext.sub(repl_escapetext, s) 
      else: 
         s = (u'\\(\\displaystyle   %s\\)' % s) 
      parts[i] = s 
   return u''.join(parts)", 'Converts a LaTeX formula to a TeX formula.',"'Do some necessary and/or useful substitutions for texts to be included in 
 LaTeX documents.'"
"@register.filter(name='str_to_list') 
 def str_to_list(info): 
    print ast.literal_eval(info), type(ast.literal_eval(info)) 
   return ast.literal_eval(info)", 'Filter to convert string to list','str to list'
"def upload_problem_grade_report(_xmodule_instance_args, _entry_id, course_id, _task_input, action_name): 
    start_time = time() 
   start_date = datetime.now(UTC) 
   status_interval = 100 
   enrolled_students = CourseEnrollment.objects.users_enrolled_in(course_id) 
   task_progress = TaskProgress(action_name, enrolled_students.count(), start_time) 
   header_row = OrderedDict([('id', 'Student   ID'), ('email', 'Email'), ('username', 'Username')]) 
   graded_scorable_blocks = _graded_scorable_blocks_to_header(course_id) 
   rows = [((list(header_row.values()) + ['Grade']) + list(chain.from_iterable(graded_scorable_blocks.values())))] 
   error_rows = [(list(header_row.values()) + ['error_msg'])] 
   current_step = {'step': 'Calculating   Grades'} 
   course = get_course_by_id(course_id) 
   for (student, course_grade, err_msg) in CourseGradeFactory().iter(course, enrolled_students): 
      student_fields = [getattr(student, field_name) for field_name in header_row] 
      task_progress.attempted += 1 
      if (not course_grade): 
         if (not err_msg): 
            err_msg = u'Unknown   error' 
         error_rows.append((student_fields + [err_msg])) 
         task_progress.failed += 1 
         continue 
      earned_possible_values = [] 
      for block_location in graded_scorable_blocks: 
         try: 
            problem_score = course_grade.locations_to_scores[block_location] 
         except KeyError: 
            earned_possible_values.append([u'Not   Available', u'Not   Available']) 
         else: 
            if problem_score.attempted: 
               earned_possible_values.append([problem_score.earned, problem_score.possible]) 
            else: 
               earned_possible_values.append([u'Not   Attempted', problem_score.possible]) 
      rows.append(((student_fields + [course_grade.percent]) + list(chain.from_iterable(earned_possible_values)))) 
      task_progress.succeeded += 1 
      if ((task_progress.attempted % status_interval) == 0): 
         task_progress.update_task_state(extra_meta=current_step) 
   if (len(rows) > 1): 
      upload_csv_to_report_store(rows, 'problem_grade_report', course_id, start_date) 
   if (len(error_rows) > 1): 
      upload_csv_to_report_store(error_rows, 'problem_grade_report_err', course_id, start_date) 
   return task_progress.update_task_state(extra_meta={'step': 'Uploading   CSV'})"," 'Uploads a CSV file of problem grade reports to the report store. 
 This is a task that is run by the grade_report_uploader.'","'Generate a CSV containing all students\' problem grades within a given 
 `course_id`.'"
"def _ip_to_number(ipstr): 
    net = ([int(digit) for digit in ipstr.split('.')] + [0, 0, 0]) 
   net = net[:4] 
   return (((((((0L + net[0]) << 8) + net[1]) << 8) + net[2]) << 8) + net[3])"," 'Convert IP address to 32 bit integer. 
 :param ipstr: 
 :type ipstr: 
 :return: 
 :rtype:'","'Translate a string ip address to a packed number. 
 @param ipstr: the ip address to transform 
 @type ipstr: a string ""x.x.x.x"" 
 @return: an int32 number representing the ip address'"
"def _validate_dict_keys(dict_to_check, required_keys, optional_keys): 
    assert (set(required_keys) <= set(dict_to_check.keys())), ('Missing   keys:   %s' % dict_to_check) 
   assert (set(dict_to_check.keys()) <= set((required_keys + optional_keys))), ('Extra   keys:   %s' % dict_to_check)"," 'Checks that a dictionary has the required keys and that it does not have 
 any extra keys.'","'Checks that all of the required keys, and possibly some of the optional 
 keys, are in the given dict. 
 Raises: 
 AssertionError: if the validation fails.'"
"def _convert_to_array_of_opt_val(optvals): 
    array_of_optv = DataObject() 
   array_of_optv.OptionValue = optvals 
   return array_of_optv"," 'Convert a list of OptionValue objects to a list of OptionValue objects 
 that are in an array of OptionValue objects.'",'Wraps the given array into a DataObject.'
"def image_volume_cache_create(context, host, cluster_name, image_id, image_updated_at, volume_id, size): 
    return IMPL.image_volume_cache_create(context, host, cluster_name, image_id, image_updated_at, volume_id, size)", 'Create an image volume cache.','Create a new image volume cache entry.'
"def test_against_hor2eq(): 
    location = EarthLocation(lon=Angle(u'-111d36.0m'), lat=Angle(u'31d57.8m'), height=(2120.0 * u.m)) 
   obstime = Time(2451545.0, format=u'jd', scale=u'ut1') 
   altaz_frame = AltAz(obstime=obstime, location=location, temperature=(0 * u.deg_C), pressure=(0.781 * u.bar)) 
   altaz_frame_noatm = AltAz(obstime=obstime, location=location, temperature=(0 * u.deg_C), pressure=(0.0 * u.bar)) 
   altaz = SkyCoord(u'264d55m06s   37d54m41s', frame=altaz_frame) 
   altaz_noatm = SkyCoord(u'264d55m06s   37d54m41s', frame=altaz_frame_noatm) 
   radec_frame = u'icrs' 
   radec_actual = altaz.transform_to(radec_frame) 
   radec_actual_noatm = altaz_noatm.transform_to(radec_frame) 
   radec_expected = SkyCoord(u'07h36m55.2s   +15d25m08s', frame=radec_frame) 
   distance = radec_actual.separation(radec_expected).to(u'arcsec') 
   radec_expected_noatm = SkyCoord(u'07h36m58.9s   +15d25m37s', frame=radec_frame) 
   distance_noatm = radec_actual_noatm.separation(radec_expected_noatm).to(u'arcsec') 
   assert (distance < (5 * u.arcsec)) 
   assert (distance_noatm < (0.4 * u.arcsec))", 'Test against the hor2eq transformation.',"'Check that Astropy gives consistent results with an IDL hor2eq example. 
 See : http://idlastro.gsfc.nasa.gov/ftp/pro/astro/hor2eq.pro 
 Test is against these run outputs, run at 2000-01-01T12:00:00: 
 # NORMAL ATMOSPHERE CASE 
 IDL> hor2eq, ten(37,54,41), ten(264,55,06), 2451545.0d, ra, dec, /verb, obs=\'kpno\', pres=781.0, temp=273.0 
 Latitude = +31 57 48.0   Longitude = *** 36 00.0 
 Julian Date =  2451545.000000 
 Az, El =  17 39 40.4  +37 54 41   (Observer Coords) 
 Az, El =  17 39 40.4  +37 53 40   (Apparent Coords) 
 LMST = +11 15 26.5 
 LAST = +11 15 25.7 
 Hour Angle = +03 38 30.1  (hh:mm:ss) 
 Ra, Dec:  07 36 55.6  +15 25 02   (Apparent Coords) 
 Ra, Dec:  07 36 55.2  +15 25 08   (J2000.0000) 
 Ra, Dec:  07 36 55.2  +15 25 08   (J2000) 
 IDL> print, ra, dec 
 114.23004       15.418818 
 # NO PRESSURE CASE 
 IDL> hor2eq, ten(37,54,41), ten(264,55,06), 2451545.0d, ra, dec, /verb, obs=\'kpno\', pres=0.0, temp=273.0 
 Latitude = +31 57 48.0   Longitude = *** 36 00.0 
 Julian Date =  2451545.000000 
 Az, El =  17 39 40.4  +37 54 41   (Observer Coords) 
 Az, El =  17 39 40.4  +37 54 41   (Apparent Coords) 
 LMST = +11 15 26.5 
 LAST = +11 15 25.7 
 Hour Angle = +03 38 26.4  (hh:mm:ss) 
 Ra, Dec:  07 36 59.3  +15 25 31   (Apparent Coords) 
 Ra, Dec:  07 36 58.9  +15 25 37   (J2000.0000) 
 Ra, Dec:  07 36 58.9  +15 25 37   (J2000) 
 IDL> print, ra, dec 
 114.24554       15.427022'"
"def test_vector_to_conv_c01b_invertible(): 
    rng = np.random.RandomState([2013, 5, 1]) 
   batch_size = 3 
   rows = 4 
   cols = 5 
   channels = 2 
   conv = Conv2DSpace([rows, cols], channels=channels, axes=('c', 0, 1, 'b')) 
   vec = VectorSpace(conv.get_total_dimension()) 
   X = conv.make_batch_theano() 
   Y = conv.format_as(X, vec) 
   Z = vec.format_as(Y, conv) 
   A = vec.make_batch_theano() 
   B = vec.format_as(A, conv) 
   C = conv.format_as(B, vec) 
   f = function([X, A], [Z, C]) 
   X = rng.randn(*conv.get_origin_batch(batch_size).shape).astype(X.dtype) 
   A = rng.randn(*vec.get_origin_batch(batch_size).shape).astype(A.dtype) 
   (Z, C) = f(X, A) 
   np.testing.assert_allclose(Z, X) 
   np.testing.assert_allclose(C, A)", 'Test that a vector space can be converted to a conv2d and back.',"'Tests that the format_as methods between Conv2DSpace 
 and VectorSpace are invertible for the (\'c\', 0, 1, \'b\') 
 axis format.'"
"@when(u'we   connect   to   test   database') 
 def step_db_connect_test(context): 
    db_name = context.conf[u'dbname'] 
   context.cli.sendline(u'\\connect   {0}'.format(db_name))", 'Connect to test database','Send connect to database.'
"def setup_user_info(): 
    if (os.geteuid() != 0): 
      return 
   global g_user_uid, g_user_gid 
   (g_user_uid, g_user_gid) = desktop.lib.daemon_utils.get_uid_gid(SETUID_USER, SETGID_GROUP)"," 'Set up the user and group IDs for the daemon. 
 This is done by setting the uid and gid of the current process 
 to the values specified by SETUID_USER and SETGID_GROUP.'",'Translate the user/group info into uid/gid.'
"def copy_constr(constr, func): 
    expr = func(constr.expr) 
   return type(constr)(expr, constr.constr_id, constr.size)"," 'Copy a constraint, but replace the expr with the result of func.'","'Creates a copy of the constraint modified according to func. 
 Parameters 
 constr : LinConstraint 
 The constraint to modify. 
 func : function 
 Function to modify the constraint expression. 
 Returns 
 LinConstraint 
 A copy of the constraint with the specified changes.'"
"def __virtual__(): 
    if ('postgres.tablespace_exists' not in __salt__): 
      return (False, 'Unable   to   load   postgres   module.      Make   sure   `postgres.bins_dir`   is   set.') 
   return True", 'Checks if the tablespace exists','Only load if the postgres module is present and is new enough (has ts funcs)'
"def mul_elemwise(lh_op, rh_op): 
    return lo.LinOp(lo.MUL_ELEM, lh_op.size, [rh_op], lh_op)"," 'Multiply two LinOp\'s elementwise. 
 Parameters 
 lh_op : LinOp 
 rh_op : LinOp 
 Returns 
 lo.LinOp 
 Examples 
 >>> from sympy.linalg import mul_elemwise 
 >>> from sympy import Symbol 
 >>> from sympy.abc import x, y 
 >>> lh_op = lo.LinOp(lo.MUL_ELEM, [3, 4], [Symbol(x)], [Symbol(y)]) 
 >>> rh_op = lo.LinOp(lo.MUL_ELEM, [5, 6], [Symbol(x)], [Symbol(y)]) 
 >>> mul_elemwise(lh_op, rh_op) 
 LinOp(3*x + 4*y, 5*x + 6*y)'","'Multiply two linear operators elementwise. 
 Parameters 
 lh_op : LinOp 
 The left-hand operator in the product. 
 rh_op : LinOp 
 The right-hand operator in the product. 
 Returns 
 LinOp 
 A linear operator representing the product.'"
"def list_headers(general=None, request=None, response=None, entity=None): 
    if (not (general or request or response or entity)): 
      general = request = response = entity = True 
   search = [] 
   for (bool, strval) in ((general, 'general'), (request, 'request'), (response, 'response'), (entity, 'entity')): 
      if bool: 
         search.append(strval) 
   return [head for head in _headers.values() if (head.category in search)]"," 'List all headers that are present in general, request, response or entity. 
 :param general: True to include general headers 
 :param request: True to include request headers 
 :param response: True to include response headers 
 :param entity: True to include entity headers 
 :return: A list of headers'",'list all headers for a given category'
"def map_vera_device(vera_device, remap): 
    import pyvera as veraApi 
   if isinstance(vera_device, veraApi.VeraDimmer): 
      return 'light' 
   if isinstance(vera_device, veraApi.VeraBinarySensor): 
      return 'binary_sensor' 
   if isinstance(vera_device, veraApi.VeraSensor): 
      return 'sensor' 
   if isinstance(vera_device, veraApi.VeraArmableDevice): 
      return 'switch' 
   if isinstance(vera_device, veraApi.VeraLock): 
      return 'lock' 
   if isinstance(vera_device, veraApi.VeraThermostat): 
      return 'climate' 
   if isinstance(vera_device, veraApi.VeraCurtain): 
      return 'cover' 
   if isinstance(vera_device, veraApi.VeraSwitch): 
      if (vera_device.device_id in remap): 
         return 'light' 
      else: 
         return 'switch' 
   return None"," 'Returns a remap of the vera device type to the home-assistant type. 
 If the vera device type is not in the remap, then the device type is 
 returned as is.'",'Map vera  classes to HA types.'
"def auth(username, password): 
    _cred = __get_yubico_users(username) 
   client = Yubico(_cred['id'], _cred['key']) 
   try: 
      if client.verify(password): 
         return True 
      else: 
         return False 
   except yubico_exceptions.StatusCodeError as e: 
      log.info('Unable   to   verify   YubiKey   `{0}`'.format(e)) 
      return False", 'Authenticate a user with YubiKey','Authenticate against yubico server'
"def check_mapping_file(mapping_fp, output_dir='.', has_barcodes=True, char_replace='_', verbose=True, variable_len_barcodes=False, disable_primer_check=False, added_demultiplex_field=None, suppress_html=False): 
    (header, mapping_data, run_description, errors, warnings) = process_id_map(open(mapping_fp, 'U'), disable_primer_check, has_barcodes, char_replace, variable_len_barcodes, added_demultiplex_field, strip_quotes=False, suppress_stripping=True) 
   if (not suppress_html): 
      formatted_html = format_mapping_html_data(header, mapping_data, errors, warnings) 
      output_html = join(((output_dir + basename(mapping_fp).replace('.txt', '')) + '.html')) 
      html_f = open(output_html, 'w') 
      html_f.write(formatted_html) 
      qiime_dir = get_qiime_project_dir() 
      copyfile(join(qiime_dir, 'qiime', 'support_files', 'js/overlib.js'), join(output_dir, 'overlib.js')) 
   corrected_mapping_data = correct_mapping_data(mapping_data, header, char_replace) 
   output_corrected_fp = join(((output_dir + basename(mapping_fp).replace('.txt', '')) + '_corrected.txt')) 
   write_corrected_mapping(output_corrected_fp, header, run_description, corrected_mapping_data) 
   output_log_fp = join(((output_dir + basename(mapping_fp).replace('.txt', '')) + '.log')) 
   write_log_file(output_log_fp, errors, warnings) 
   if verbose: 
      if (errors or warnings): 
         print ('Errors   and/or   warnings   detected   in   mapping   file.      Please   ' + 'check   the   log   and   html   file   for   details.') 
      else: 
         print 'No   errors   or   warnings   were   found   in   mapping   file.'"," 'Checks the mapping file for errors and warnings. 
 :param mapping_fp: Path to the mapping file to check. 
 :param output_dir: Directory to output files to. 
 :param has_barcodes: True if the mapping file has barcodes. 
 :param char_replace: Character to replace in barcodes. 
 :param verbose: Whether to print warnings and errors to stdout. 
 :param variable_len_barcodes: True if barcodes are variable length. 
 :param disable_primer_check: Disable the primer check. 
 :param added_demultiplex_field: If True, the demultiplexing field will be added to the header. 
 :param suppress_html: If True, the html file will not be generated. 
 :return: None'","'Main program function for checking mapping file 
 Checks mapping file for errors, warnings, writes log file, html file, 
 and corrected mapping file. 
 mapping_fp:  path to metadata mapping file 
 output_dir:  output directory for log, html, corrected mapping file. 
 has_barcodes:  If True, will test for perform barcodes test (presence, 
 uniqueness, valid IUPAC DNA chars). 
 char_replace:  Character used to replace invalid characters in data 
 fields.  SampleIDs always use periods to be MIENS compliant. 
 verbose: If True, a message about warnings and/or errors will be printed 
 to stdout. 
 variable_len_barcodes:  If True, suppresses warnings about barcodes of 
 varying length. 
 disable_primer_check:  If True, disables tests for valid primer sequences. 
 added_demultiplex_field:  If specified, references a field in the mapping 
 file to use for demultiplexing.  These are to be read from fasta labels 
 during the actual demultiplexing step.  All combinations of barcodes, 
 primers, and the added_demultiplex_field must be unique.'"
"@with_device 
 def push(local_path, remote_path): 
    msg = ('Pushing   %r   to   %r' % (local_path, remote_path)) 
   remote_filename = os.path.basename(local_path) 
   if log.isEnabledFor(logging.DEBUG): 
      msg += ('   (%s)' % context.device) 
   with log.waitfor(msg) as w: 
      with AdbClient() as c: 
         stat_ = c.stat(remote_path) 
         if (not stat_): 
            remote_filename = os.path.basename(remote_path) 
            remote_path = os.path.dirname(remote_path) 
            stat_ = c.stat(remote_path) 
         if (not stat_): 
            log.error(('Could   not   stat   %r' % remote_path)) 
         mode = stat_['mode'] 
         if stat.S_ISDIR(mode): 
            remote_path = os.path.join(remote_path, remote_filename) 
         return c.write(remote_path, misc.read(local_path), callback=_create_adb_push_pull_callback(w))"," 'Push a file from the local device to the remote device. 
 :param local_path: Path to the local file. 
 :param remote_path: Path to the remote file. 
 :return: True if the file was successfully pushed, False otherwise.'","'Upload a file to the device. 
 Arguments: 
 local_path(str): Path to the local file to push. 
 remote_path(str): Path or directory to store the file on the device. 
 Example: 
 >>> write(\'./filename\', \'contents\') 
 >>> _=adb.push(\'./filename\', \'/data/local/tmp\') 
 >>> adb.read(\'/data/local/tmp/filename\') 
 \'contents\' 
 >>> adb.push(\'./filename\', \'/does/not/exist\') 
 Traceback (most recent call last): 
 PwnlibException: Could not stat \'/does/not/exist\''"
"def oo_ami_selector(data, image_name): 
    if (not isinstance(data, list)): 
      raise errors.AnsibleFilterError('|failed   expects   first   param   is   a   list') 
   if (not data): 
      return None 
   elif ((image_name is None) or (not image_name.endswith('_*'))): 
      ami = sorted(data, key=itemgetter('name'), reverse=True)[0] 
      return ami['ami_id'] 
   else: 
      ami_info = [(ami, ami['name'].split('_')[(-1)]) for ami in data] 
      ami = sorted(ami_info, key=itemgetter(1), reverse=True)[0][0] 
      return ami['ami_id']"," 'Return the id of the first AMI that matches the given name. 
 :param data: list of ami info 
 :param image_name: name of the AMI 
 :return: ami id'","'This takes a list of amis and an image name and attempts to return 
 the latest ami.'"
"def get_lib_extension(): 
    if (sys.platform in ['win32', 'cygwin']): 
      return 'pyd' 
   else: 
      return 'so'"," 'Returns the extension for the given platform. 
 This is used to determine the extension for a library. 
 Currently only supports Windows and Unix.'",'Return the platform-dependent extension for compiled modules.'
"def _sort_names(FQDNs): 
    return sorted(FQDNs, key=(lambda fqdn: fqdn.split('.')[::(-1)][1:]))"," 'Sort a list of FQDNs by name. 
 :param FQDNs: 
 :return: 
 :rtype: 
 :raises: 
 :example: 
 >>> _sort_names([""www.example.com"", ""example.com"", ""example.org""]) 
 [""example.com"", ""example.org"", ""www.example.com""]'","'Sort FQDNs by SLD (and if many, by their subdomains) 
 :param list FQDNs: list of domain names 
 :returns: Sorted list of domain names 
 :rtype: list'"
"def tostring(xml=None, xmlns=u'', stream=None, outbuffer=u'', top_level=False, open_only=False, namespaces=None): 
    output = [outbuffer] 
   tag_name = xml.tag.split(u'}', 1)[(-1)] 
   if (u'}' in xml.tag): 
      tag_xmlns = xml.tag.split(u'}', 1)[0][1:] 
   else: 
      tag_xmlns = u'' 
   default_ns = u'' 
   stream_ns = u'' 
   use_cdata = False 
   if stream: 
      default_ns = stream.default_ns 
      stream_ns = stream.stream_ns 
      use_cdata = stream.use_cdata 
   namespace = u'' 
   if tag_xmlns: 
      if ((top_level and (tag_xmlns not in [default_ns, xmlns, stream_ns])) or ((not top_level) and (tag_xmlns != xmlns))): 
         namespace = (u'   xmlns=""%s""' % tag_xmlns) 
   if (stream and (tag_xmlns in stream.namespace_map)): 
      mapped_namespace = stream.namespace_map[tag_xmlns] 
      if mapped_namespace: 
         tag_name = (u'%s:%s' % (mapped_namespace, tag_name)) 
   output.append((u'<%s' % tag_name)) 
   output.append(namespace) 
   new_namespaces = set() 
   for (attrib, value) in xml.attrib.items(): 
      value = escape(value, use_cdata) 
      if (u'}' not in attrib): 
         output.append((u'   %s=""%s""' % (attrib, value))) 
      else: 
         attrib_ns = attrib.split(u'}')[0][1:] 
         attrib = attrib.split(u'}')[1] 
         if (attrib_ns == XML_NS): 
            output.append((u'   xml:%s=""%s""' % (attrib, value))) 
         elif (stream and (attrib_ns in stream.namespace_map)): 
            mapped_ns = stream.namespace_map[attrib_ns] 
            if mapped_ns: 
               if (namespaces is None): 
                  namespaces = set() 
               if (attrib_ns not in namespaces): 
                  namespaces.add(attrib_ns) 
                  new_namespaces.add(attrib_ns) 
                  output.append((u'   xmlns:%s=""%s""' % (mapped_ns, attrib_ns))) 
               output.append((u'   %s:%s=""%s""' % (mapped_ns, attrib, value))) 
   if open_only: 
      output.append(u'>') 
      return u''.join(output) 
   if (len(xml) or xml.text): 
      output.append(u'>') 
      if xml.text: 
         output.append(escape(xml.text, use_cdata)) 
      if len(xml): 
         for child in xml: 
            output.append(tostring(child, tag_xmlns, stream, namespaces=namespaces)) 
      output.append((u'</%s>' % tag_name)) 
   elif xml.text: 
      output.append((u'>%s</%s>' % (escape(xml.text, use_cdata), tag_name))) 
   else: 
      output.append(u'   />') 
   if xml.tail: 
      output.append(escape(xml.tail, use_cdata)) 
   for ns in new_namespaces: 
      namespaces.remove(ns) 
   return u''.join(output)"," 'Converts an XML document to a string 
 :param xml: the XML document to convert 
 :param xmlns: the namespace to use for the root element 
 :param stream: the stream to use for namespaces 
 :param outbuffer: the string to append to 
 :param top_level: if True, the document will be converted to a top-level element 
 :param open_only: if True, only the opening tag will be returned 
 :param namespaces: a set of namespaces to use for namespacing 
 :return: the string representation of the document'
    ##############################################################################
    def tostring(xml, xmlns=None, stream=None, outbuffer=u'', top_level=False, open_only=False, namespaces=None): 
    if (xmlns is None): 
      xmlns = u'' 
   if (top_level and (xmlns not in [u'', stream.default_ns])): 
      xmlns = (u'   xmlns=""%s""' % xmlns) 
   if (stream and (xmlns in stream.namespace_map)): 
     ","'Serialize an XML object to a Unicode string. 
 If an outer xmlns is provided using ``xmlns``, then the current element\'s 
 namespace will not be included if it matches the outer namespace. An 
 exception is made for elements that have an attached stream, and appear 
 at the stream root. 
 :param XML xml: The XML object to serialize. 
 :param string xmlns: Optional namespace of an element wrapping the XML 
 object. 
 :param stream: The XML stream that generated the XML object. 
 :param string outbuffer: Optional buffer for storing serializations 
 during recursive calls. 
 :param bool top_level: Indicates that the element is the outermost 
 element. 
 :param set namespaces: Track which namespaces are in active use so 
 that new ones can be declared when needed. 
 :type xml: :py:class:`~xml.etree.ElementTree.Element` 
 :type stream: :class:`~sleekxmpp.xmlstream.xmlstream.XMLStream` 
 :rtype: Unicode string'"
"def safe_join(directory, *pathnames): 
    for filename in pathnames: 
      if (filename != ''): 
         filename = posixpath.normpath(filename) 
      for sep in _os_alt_seps: 
         if (sep in filename): 
            raise NotFound() 
      if (os.path.isabs(filename) or (filename == '..') or filename.startswith('../')): 
         raise NotFound() 
      directory = os.path.join(directory, filename) 
   return directory"," 'Join the given pathnames into a pathname. 
 :param directory: Directory to join the pathnames to. 
 :param pathnames: Pathnames to join. 
 :raises NotFound: If any of the pathnames is not found. 
 :returns: Joined pathname.'","'Safely join `directory` and zero or more untrusted `pathnames` 
 components. 
 Example usage:: 
 @app.route(\'/wiki/<path:filename>\') 
 def wiki_page(filename): 
 filename = safe_join(app.config[\'WIKI_FOLDER\'], filename) 
 with open(filename, \'rb\') as fd: 
 content = fd.read()  # Read and process the file content... 
 :param directory: the trusted base directory. 
 :param pathnames: the untrusted pathnames relative to that directory. 
 :raises: :class:`~werkzeug.exceptions.NotFound` if one or more passed 
 paths fall out of its boundaries.'"
"def urldecode(query): 
    if (query and (not (set(query) <= urlencoded))): 
      error = u""Error   trying   to   decode   a   non   urlencoded   string.   Found   invalid   characters:   %s   in   the   string:   '%s'.   Please   ensure   the   request/response   body   is   x-www-form-urlencoded."" 
      raise ValueError((error % ((set(query) - urlencoded), query))) 
   invalid_hex = u'%[^0-9A-Fa-f]|%[0-9A-Fa-f][^0-9A-Fa-f]' 
   if len(re.findall(invalid_hex, query)): 
      raise ValueError(u'Invalid   hex   encoding   in   query   string.') 
   query = (query.encode(u'utf-8') if ((not PY3) and isinstance(query, unicode_type)) else query) 
   params = urlparse.parse_qsl(query, keep_blank_values=True) 
   return decode_params_utf8(params)"," 'Decode a urlencoded string. 
 :param query: A string containing the urlencoded string. 
 :return: A dictionary containing the decoded parameters.'","'Decode a query string in x-www-form-urlencoded format into a sequence 
 of two-element tuples. 
 Unlike urlparse.parse_qsl(..., strict_parsing=True) urldecode will enforce 
 correct formatting of the query string by validation. If validation fails 
 a ValueError will be raised. urllib.parse_qsl will only raise errors if 
 any of name-value pairs omits the equals sign.'"
"@contextmanager 
 def context(grpc_context): 
    try: 
      (yield) 
   except KeyError as key_error: 
      grpc_context.code(status.Code.NOT_FOUND) 
      grpc_context.details('Unable   to   find   the   item   keyed   by   {}'.format(key_error))"," 'Context manager that raises a NotFound exception if the item 
 being accessed does not exist.'",'A context manager that automatically handles KeyError.'
"def delete_device(name, safety_on=True): 
    config = _get_vistara_configuration() 
   if (not config): 
      return False 
   access_token = _get_oath2_access_token(config['client_key'], config['client_secret']) 
   if (not access_token): 
      return 'Vistara   access   token   not   available' 
   query_string = 'dnsName:{0}'.format(name) 
   devices = _search_devices(query_string, config['client_id'], access_token) 
   if (not devices): 
      return 'No   devices   found' 
   device_count = len(devices) 
   if (safety_on and (device_count != 1)): 
      return 'Expected   to   delete   1   device   and   found   {0}.   Set   safety_on=False   to   override.'.format(device_count) 
   delete_responses = [] 
   for device in devices: 
      device_id = device['id'] 
      log.debug(device_id) 
      delete_response = _delete_resource(device_id, config['client_id'], access_token) 
      if (not delete_response): 
         return False 
      delete_responses.append(delete_response) 
   return delete_responses"," 'Delete a device by name. 
 :param name: The name of the device to delete. 
 :param safety_on: If true, then only delete the first device found. 
 :returns: A list of delete responses.'","'Deletes a device from Vistara based on DNS name or partial name. By default, 
 delete_device will only perform the delete if a single host is returned. Set 
 safety_on=False to delete all matches (up to default API search page size) 
 CLI Example: 
 .. code-block:: bash 
 salt-run vistara.delete_device \'hostname-101.mycompany.com\' 
 salt-run vistara.delete_device \'hostname-101\' 
 salt-run vistara.delete_device \'hostname-1\' safety_on=False'"
"def _write_3(fid, val): 
    f_bytes = np.zeros(3, dtype=np.uint8) 
   f_bytes[0] = ((val >> 16) & 255) 
   f_bytes[1] = ((val >> 8) & 255) 
   f_bytes[2] = (val & 255) 
   fid.write(f_bytes.tostring())", 'Write a 3-byte unsigned integer to the file.','Write 3 byte integer to file.'
"def make_avpr_object(json_data): 
    if (hasattr(json_data, 'get') and callable(json_data.get)): 
      name = json_data.get('protocol') 
      namespace = json_data.get('namespace') 
      types = json_data.get('types') 
      messages = json_data.get('messages') 
      return Protocol(name, namespace, types, messages) 
   else: 
      raise ProtocolParseException(('Not   a   JSON   object:   %s' % json_data))"," 'Make an avpr object from a json object. 
 :param json_data: JSON object. 
 :returns: An AVPR object.'",'Build Avro Protocol from data parsed out of JSON string.'
"@constructor 
 def shape_padaxis(t, axis): 
    _t = as_tensor_variable(t) 
   ndim = (_t.ndim + 1) 
   if (not ((- ndim) <= axis < ndim)): 
      msg = 'axis   {0}   is   out   of   bounds   [-{1},   {1})'.format(axis, ndim) 
      raise IndexError(msg) 
   if (axis < 0): 
      axis += ndim 
   pattern = [i for i in xrange(_t.type.ndim)] 
   pattern.insert(axis, 'x') 
   return DimShuffle(_t.broadcastable, pattern)(_t)"," 'Return a tensor with shape (x, ..., x, y, ..., y) 
 Parameters 
 t : tensor 
 A tensor with shape (x, ..., x, y, ..., y) 
 axis : int 
 The axis of the tensor to reshape 
 Returns 
 A tensor with shape (x, ..., x, y, ..., y) 
 Notes 
 This function is useful for reshaping tensors of shape (x, y) to 
 (x, y, x, y) or (x, y, x, y, x, y). 
 Examples 
 >>> from sympy.tensor import shape_padaxis 
 >>> x = tensor([[1, 2], [3, 4]]) 
 >>> shape_padaxis(x, 1) 
 tensor([[1, 2], [3, 4]]) 
 >>> shape_padaxis(x, 0) 
 tensor([[1, 2], [3, 4]]) 
 >>> shape_padaxis(x, 2) 
 tensor([[1, 2], [3,","'Reshape `t` by inserting 1 at the dimension `axis`. 
 Example 
 >>> tensor = theano.tensor.tensor3() 
 >>> theano.tensor.shape_padaxis(tensor, axis=0) 
 DimShuffle{x,0,1,2}.0 
 >>> theano.tensor.shape_padaxis(tensor, axis=1) 
 DimShuffle{0,x,1,2}.0 
 >>> theano.tensor.shape_padaxis(tensor, axis=3) 
 DimShuffle{0,1,2,x}.0 
 >>> theano.tensor.shape_padaxis(tensor, axis=-1) 
 DimShuffle{0,1,2,x}.0 
 See Also 
 shape_padleft 
 shape_padright 
 Dimshuffle'"
"def dumps_with_persistent_ids(obj, protocol=None): 
    file = BytesIO() 
   pickler = pickle.Pickler(file, protocol) 
   pickler.persistent_id = _persistent_id 
   pickler.dump(obj) 
   return file.getvalue()"," 'Dumps an object with a persistent ID. 
 :param obj: The object to dump 
 :param protocol: The pickle protocol to use. 
 :type protocol: int 
 :return: The pickled data 
 :rtype: str'","'Performs a pickle dumps on the given object, substituting all references to 
 a TradingEnvironment or AssetFinder with tokenized representations. 
 All arguments are passed to pickle.Pickler and are described therein.'"
"def cluster_remove(version, name='main', stop=False): 
    cmd = [salt.utils.which('pg_dropcluster')] 
   if stop: 
      cmd += ['--stop'] 
   cmd += [version, name] 
   cmdstr = '   '.join([pipes.quote(c) for c in cmd]) 
   ret = __salt__['cmd.run_all'](cmdstr, python_shell=False) 
   if (ret.get('retcode', 0) != 0): 
      log.error('Error   removing   a   Postgresql   cluster   {0}/{1}'.format(version, name)) 
   else: 
      ret['changes'] = 'Successfully   removed   cluster   {0}/{1}'.format(version, name) 
   return ret"," 'Remove a PostgreSQL cluster 
 :param version: Version of PostgreSQL to remove the cluster from 
 :param name: Name of the cluster to remove 
 :param stop: Whether to stop the cluster before removing it 
 :return: dict'","'Remove a cluster on a Postgres server. By default it doesn\'t try 
 to stop the cluster. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' postgres.cluster_remove \'9.3\' 
 salt \'*\' postgres.cluster_remove \'9.3\' \'main\' 
 salt \'*\' postgres.cluster_remove \'9.3\' \'main\' stop=True'"
"def returner(ret): 
    _options = _get_options(ret) 
   sid = _options.get('sid', None) 
   token = _options.get('token', None) 
   sender = _options.get('from', None) 
   receiver = _options.get('to', None) 
   if ((sid is None) or (token is None)): 
      log.error('Twilio   sid/authentication   token   missing') 
      return None 
   if ((sender is None) or (receiver is None)): 
      log.error('Twilio   to/from   fields   are   missing') 
      return None 
   client = TwilioRestClient(sid, token) 
   try: 
      message = client.messages.create(body='Minion:   {0}\nCmd:   {1}\nSuccess:   {2}\n\nJid:   {3}'.format(ret['id'], ret['fun'], ret['success'], ret['jid']), to=receiver, from_=sender) 
   except TwilioRestException as e: 
      log.error('Twilio   [https://www.twilio.com/docs/errors/{0}]'.format(e.code)) 
      return False 
   return True"," 'Send a Twilio message to the minion. 
 :param ret: 
 :type ret: dict 
 :return: True if message sent, False if not.'",'Return a response in an SMS message'
"def expanding_quantile(arg, quantile, min_periods=1, freq=None): 
    return ensure_compat('expanding', 'quantile', arg, freq=freq, min_periods=min_periods, func_kw=['quantile'], quantile=quantile)"," 'Returns the value of the specified quantile of the expanding 
 distribution. 
 Parameters 
 arg : scalar or array-like 
 A scalar or array-like of values. 
 quantile : float 
 The quantile of the expanding distribution. 
 min_periods : int, default 1 
 The minimum number of periods to use. 
 freq : DateOffset or string, default None 
 A frequency with which to compute the quantile. 
 Returns 
 A scalar or array-like of values. 
 Raises 
 ValueError 
 If `arg` is not a valid expanding distribution.'","'Expanding quantile. 
 Parameters 
 arg : Series, DataFrame 
 quantile : float 
 0 <= quantile <= 1 
 min_periods : int, default None 
 Minimum number of observations in window required to have a value 
 (otherwise result is NA). 
 freq : string or DateOffset object, optional (default None) 
 Frequency to conform the data to before computing the 
 statistic. Specified as a frequency string or DateOffset object. 
 Returns 
 y : type of input argument 
 Notes 
 The `freq` keyword is used to conform time series data to a specified 
 frequency by resampling the data. This is done with the default parameters 
 of :meth:`~pandas.Series.resample` (i.e. using the `mean`). 
 To learn more about the frequency strings, please see `this link 
 <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.'"
"def is_funcvar(expr): 
    assert isinstance(expr, string_types), (u'%s   is   not   a   string' % expr) 
   return (re.match(u'^[A-Z]\\d*$', expr) is not None)"," 'Check if a variable is a function variable. 
 :param expr: 
 :type expr: 
 :return: 
 :rtype: 
 :raise: 
 :raises: 
 :example: 
 >>> from sympy.abc import x, y, z 
 >>> is_funcvar(x) 
 True 
 >>> is_funcvar(y) 
 False 
 >>> is_funcvar(z) 
 False'","'A function variable must be a single uppercase character followed by 
 zero or more digits. 
 :param expr: str 
 :return: bool True if expr is of the correct form'"
"def getLocalAndroidPath(client, args): 
    localPath = os.path.join(args.localOutputFolder, '{0}-{1}'.format(client.conn.modules['pupydroid.utils'].getAndroidID(), client.desc['user'])) 
   if (not os.path.exists(localPath)): 
      logging.info('Creating   {0}   folder   locally'.format(localPath)) 
      os.makedirs(localPath) 
   return localPath", 'Return the local path for the given client and args',"'Returns the current local path for saving data locally 
 Create folder if not exists'"
"def main(): 
    parser = ArgumentParser(description=CMDLINE_HELP) 
   parser.add_argument('--noserver', action='store_true', dest='noserver', default=False, help='Do   not   start   Server   process') 
   parser.add_argument('--noportal', action='store_true', dest='noportal', default=False, help='Do   not   start   Portal   process') 
   parser.add_argument('--logserver', action='store_true', dest='logserver', default=False, help='Log   Server   output   to   logfile') 
   parser.add_argument('--iserver', action='store_true', dest='iserver', default=False, help='Server   in   interactive   mode') 
   parser.add_argument('--iportal', action='store_true', dest='iportal', default=False, help='Portal   in   interactive   mode') 
   parser.add_argument('--pserver', action='store_true', dest='pserver', default=False, help='Profile   Server') 
   parser.add_argument('--pportal', action='store_true', dest='pportal', default=False, help='Profile   Portal') 
   parser.add_argument('--nologcycle', action='store_false', dest='nologcycle', default=True, help='Do   not   cycle   log   files') 
   parser.add_argument('--doexit', action='store_true', dest='doexit', default=False, help='Immediately   exit   after   processes   have   started.') 
   parser.add_argument('gamedir', help='path   to   game   dir') 
   parser.add_argument('twistdbinary', help='path   to   twistd   binary') 
   parser.add_argument('slogfile', help='path   to   server   log   file') 
   parser.add_argument('plogfile', help='path   to   portal   log   file') 
   parser.add_argument('hlogfile', help='path   to   http   log   file') 
   args = parser.parse_args() 
   global GAMEDIR 
   global SERVER_LOGFILE, PORTAL_LOGFILE, HTTP_LOGFILE 
   global SERVER_PIDFILE, PORTAL_PIDFILE 
   global SERVER_RESTART, PORTAL_RESTART 
   global SPROFILER_LOGFILE, PPROFILER_LOGFILE 
   GAMEDIR = args.gamedir 
   sys.path.insert(1, os.path.join(GAMEDIR, SERVERDIR)) 
   SERVER_PIDFILE = os.path.join(GAMEDIR, SERVERDIR, 'server.pid') 
   PORTAL_PIDFILE = os.path.join(GAMEDIR, SERVERDIR, 'portal.pid') 
   SERVER_RESTART = os.path.join(GAMEDIR, SERVERDIR, 'server.restart') 
   PORTAL_RESTART = os.path.join(GAMEDIR, SERVERDIR, 'portal.restart') 
   SERVER_LOGFILE = args.slogfile 
   PORTAL_LOGFILE = args.plogfile 
   HTTP_LOGFILE = args.hlogfile 
   TWISTED_BINARY = args.twistdbinary 
   SPROFILER_LOGFILE = os.path.join(GAMEDIR, SERVERDIR, 'logs', 'server.prof') 
   PPROFILER_LOGFILE = os.path.join(GAMEDIR, SERVERDIR, 'logs', 'portal.prof') 
   server_argv = [TWISTED_BINARY, '--nodaemon', ('--logfile=%s' % SERVER_LOGFILE), ('--pidfile=%s' % SERVER_PIDFILE), ('--python=%s' % SERVER_PY_FILE)] 
   portal_argv = [TWISTED_BINARY, ('--logfile=%s' % PORTAL_LOGFILE), ('--pidfile=%s' % PORTAL_PIDFILE), ('--python=%s' % PORTAL_PY_FILE)] 
   pserver_argv = ['--savestats', '--profiler=cprofile', ('--profile=%s' % SPROFILER_LOGFILE)] 
   pportal_argv = ['--savestats', '--profiler=cprofile', ('--profile=%s' % PPROFILER_LOGFILE)] 
   pid = get_pid(SERVER_PIDFILE) 
   if (pid and (not args.noserver)): 
      print(('\nEvennia   Server   is   already   running   as   process   %(pid)s.   Not   restarted.' % {'pid': pid})) 
      args.noserver = True 
   if args.noserver: 
      server_argv = None 
   else: 
      set_restart_mode(SERVER_RESTART, 'shutdown') 
      if (not args.logserver): 
         del server_argv[2] 
         print('\nStarting   Evennia   Server   (output   to   stdout).') 
      else: 
         if (not args.nologcycle): 
            cycle_logfile(SERVER_LOGFILE) 
         print('\nStarting   Evennia   Server   (output   to   server   logfile).') 
      if args.pserver: 
         server_argv.extend(pserver_argv) 
         print('\nRunning   Evennia   Server   under   cProfile.') 
   pid = get_pid(PORTAL_PIDFILE) 
   if (pid and (not args.noportal)): 
      print(('\nEvennia   Portal   is   already   running   as   process   %(pid)s.   Not   restarted.' % {'pid': pid})) 
      args.noportal = True 
   if args.noportal: 
      portal_argv = None 
   else: 
      if args.iportal: 
         portal_argv[1] = '--nodaemon' 
         set_restart_mode(PORTAL_RESTART, True) 
         print('\nStarting   Evennia   Portal   in   non-Daemon   mode   (output   to   stdout).') 
      else: 
         if (not args.nologcycle): 
            cycle_logfile(PORTAL_LOGFILE) 
            cycle_logfile(HTTP_LOGFILE) 
         set_restart_mode(PORTAL_RESTART, False) 
         print('\nStarting   Evennia   Portal   in   Daemon   mode   (output   to   portal   logfile).') 
      if args.pportal: 
         portal_argv.extend(pportal_argv) 
         print('\nRunning   Evennia   Portal   under   cProfile.') 
   if args.doexit: 
      print(PROCESS_DOEXIT) 
   if (os.name == 'nt'): 
      if server_argv: 
         del server_argv[(-2)] 
      if portal_argv: 
         del portal_argv[(-2)] 
   start_services(server_argv, portal_argv, doexit=args.doexit)", 'Start the server and portal processes.',"'This handles the command line input of the runner, usually created by 
 the evennia launcher'"
"def mult(a, b): 
    try: 
      return (a * b) 
   except TypeError: 
      return (to_decimal(a) * to_decimal(b))"," 'Multiply two numbers. 
 :param a: first number 
 :type a: int or float 
 :param b: second number 
 :type b: int or float 
 :return: a * b 
 :rtype: int or float'","'If we get TypeError from * (possibly because one is float and the other is Decimal), then promote them both to Decimal.'"
"def test_check_dictionary(): 
    def CheckDictionary(C): 
      C.newClassAttr = 'xyz' 
      AreEqual(C.newClassAttr, 'xyz') 
      a = C() 
      a.__dict__[1] = '1' 
      if (object in C.__bases__): 
         try: 
            C.__dict__[2] = '2' 
            AssertUnreachable() 
         except TypeError: 
            pass 
         AreEqual(C.__dict__.has_key(2), False) 
      AreEqual(a.__dict__.has_key(1), True) 
      AreEqual(dir(a).__contains__(1), True) 
      AreEqual(repr(a.__dict__), ""{1:   '1'}"") 
      C.newTypeAttr = 1 
      AreEqual(hasattr(C, 'newTypeAttr'), True) 
      class OldClass: 
         pass 
      if isinstance(C, type(OldClass)): 
         C.__dict__ = dict(C.__dict__) 
         AreEqual(hasattr(C, 'newTypeAttr'), True) 
      else: 
         try: 
            C.__dict__ = {} 
            AssertUnreachable() 
         except AttributeError: 
            pass 
      a.newInstanceAttr = 1 
      AreEqual(hasattr(a, 'newInstanceAttr'), True) 
      a.__dict__ = dict(a.__dict__) 
      AreEqual(hasattr(a, 'newInstanceAttr'), True) 
      a.abc = 'xyz' 
      AreEqual(hasattr(a, 'abc'), True) 
      AreEqual(getattr(a, 'abc'), 'xyz') 
   class OldClass: 
      def __init__(self): 
         pass 
   class NewClass(object, ): 
      def __init__(self): 
         pass 
   CheckDictionary(OldClass) 
   CheckDictionary(NewClass)"," 'Test that __dict__ can be set to a dictionary with attributes. 
 This test should be run on Python 2.4 and above.'",'tests to verify that Symbol dictionaries do the right thing in dynamic scenarios'
"def create_generic_related_manager(superclass): 
    class GenericRelatedObjectManager(superclass, ): 
      def __init__(self, model=None, instance=None, symmetrical=None, source_col_name=None, target_col_name=None, content_type=None, content_type_field_name=None, object_id_field_name=None, prefetch_cache_name=None): 
         super(GenericRelatedObjectManager, self).__init__() 
         self.model = model 
         self.content_type = content_type 
         self.symmetrical = symmetrical 
         self.instance = instance 
         self.source_col_name = source_col_name 
         self.target_col_name = target_col_name 
         self.content_type_field_name = content_type_field_name 
         self.object_id_field_name = object_id_field_name 
         self.prefetch_cache_name = prefetch_cache_name 
         self.pk_val = self.instance._get_pk_val() 
         self.core_filters = {(u'%s__pk' % content_type_field_name): content_type.id, (u'%s__exact' % object_id_field_name): instance._get_pk_val()} 
      def get_query_set(self): 
         try: 
            return self.instance._prefetched_objects_cache[self.prefetch_cache_name] 
         except (AttributeError, KeyError): 
            db = (self._db or router.db_for_read(self.model, instance=self.instance)) 
            return super(GenericRelatedObjectManager, self).get_query_set().using(db).filter(**self.core_filters) 
      def get_prefetch_query_set(self, instances): 
         db = (self._db or router.db_for_read(self.model, instance=instances[0])) 
         query = {(u'%s__pk' % self.content_type_field_name): self.content_type.id, (u'%s__in' % self.object_id_field_name): set((obj._get_pk_val() for obj in instances))} 
         qs = super(GenericRelatedObjectManager, self).get_query_set().using(db).filter(**query) 
         object_id_converter = instances[0]._meta.pk.to_python 
         return (qs, (lambda relobj: object_id_converter(getattr(relobj, self.object_id_field_name))), (lambda obj: obj._get_pk_val()), False, self.prefetch_cache_name) 
      def add(self, *objs): 
         for obj in objs: 
            if (not isinstance(obj, self.model)): 
               raise TypeError((u""'%s'   instance   expected"" % self.model._meta.object_name)) 
            setattr(obj, self.content_type_field_name, self.content_type) 
            setattr(obj, self.object_id_field_name, self.pk_val) 
            obj.save() 
      add.alters_data = True 
      def remove(self, *objs): 
         db = router.db_for_write(self.model, instance=self.instance) 
         for obj in objs: 
            obj.delete(using=db) 
      remove.alters_data = True 
      def clear(self): 
         db = router.db_for_write(self.model, instance=self.instance) 
         for obj in self.all(): 
            obj.delete(using=db) 
      clear.alters_data = True 
      def create(self, **kwargs): 
         kwargs[self.content_type_field_name] = self.content_type 
         kwargs[self.object_id_field_name] = self.pk_val 
         db = router.db_for_write(self.model, instance=self.instance) 
         return super(GenericRelatedObjectManager, self).using(db).create(**kwargs) 
      create.alters_data = True 
   return GenericRelatedObjectManager"," 'Creates a generic related object manager. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used to manage any related objects, 
 regardless of their model. 
 This manager can be used","'Factory function for a manager that subclasses \'superclass\' (which is a 
 Manager) and adds behavior for generic related objects.'"
"def get_metadata(stream, extract_cover=True): 
    mi = MetaInformation(_('Unknown'), [_('Unknown')]) 
   stream.seek(0) 
   pml = '' 
   if stream.name.endswith('.pmlz'): 
      with TemporaryDirectory('_unpmlz') as tdir: 
         zf = ZipFile(stream) 
         zf.extractall(tdir) 
         pmls = glob.glob(os.path.join(tdir, '*.pml')) 
         for p in pmls: 
            with open(p, 'r+b') as p_stream: 
               pml += p_stream.read() 
         if extract_cover: 
            mi.cover_data = get_cover(os.path.splitext(os.path.basename(stream.name))[0], tdir, True) 
   else: 
      pml = stream.read() 
      if extract_cover: 
         mi.cover_data = get_cover(os.path.splitext(os.path.basename(stream.name))[0], os.path.abspath(os.path.dirname(stream.name))) 
   for comment in re.findall('(?mus)\\\\v.*?\\\\v', pml): 
      m = re.search('TITLE=""(.*?)""', comment) 
      if m: 
         mi.title = re.sub('[\x00-\x1f]', '', prepare_string_for_xml(m.group(1).strip().decode('cp1252', 'replace'))) 
      m = re.search('AUTHOR=""(.*?)""', comment) 
      if m: 
         if (mi.authors == [_('Unknown')]): 
            mi.authors = [] 
         mi.authors.append(re.sub('[\x00-\x1f]', '', prepare_string_for_xml(m.group(1).strip().decode('cp1252', 'replace')))) 
      m = re.search('PUBLISHER=""(.*?)""', comment) 
      if m: 
         mi.publisher = re.sub('[\x00-\x1f]', '', prepare_string_for_xml(m.group(1).strip().decode('cp1252', 'replace'))) 
      m = re.search('COPYRIGHT=""(.*?)""', comment) 
      if m: 
         mi.rights = re.sub('[\x00-\x1f]', '', prepare_string_for_xml(m.group(1).strip().decode('cp1252', 'replace'))) 
      m = re.search('ISBN=""(.*?)""', comment) 
      if m: 
         mi.isbn = re.sub('[\x00-\x1f]', '', prepare_string_for_xml(m.group(1).strip().decode('cp1252', 'replace'))) 
   return mi"," 'Read metadata from the stream. 
 :param stream: stream to read metadata from 
 :param extract_cover: whether to extract cover 
 :return: MetaInformation'",'Return metadata as a L{MetaInfo} object'
"def install_readline(hook): 
    global readline_hook, readline_ref 
   readline_hook = hook 
   PyOS_RFP = c_int.from_address(Console.GetProcAddress(sys.dllhandle, 'PyOS_ReadlineFunctionPointer')) 
   if (sys.version < '2.3'): 
      readline_ref = HOOKFUNC22(hook_wrapper) 
   else: 
      readline_ref = HOOKFUNC23(hook_wrapper_23) 
   func_start = c_int.from_address(addressof(readline_ref)).value 
   PyOS_RFP.value = func_start"," 'Install the readline hook. 
 This is a wrapper around PyOS_ReadlineFunctionPointer. 
 It is used to install the readline hook. 
 The hook is a function that is called when the user types a key 
 that is not a control character. 
 This function is used to intercept the keypress and to 
 intercept the readline.read() call. 
 The hook function should call readline.read() and then return 
 the key that was pressed. 
 If the key is a control character, the hook function should return 
 None. 
 The hook function should return a string that is the key that 
 was pressed. 
 The hook function should return None if the key is a control 
 character. 
 If the hook function returns None, the key will be interpreted 
 as a control character. 
 If the hook function returns a string, the key will be interpreted 
 as a normal keypress.'",'Set up things for the interpreter to call our function like GNU readline.'
"def _run_shell_command(cmd, tmpdir): 
    cmdsuf = '' 
   if (platform.system() == 'Windows'): 
      cmdsuf = '.bat' 
      cmd = ('@echo   off\r\n' + cmd) 
   (handle, path) = tempfile.mkstemp(text=True, dir=tmpdir, suffix=cmdsuf) 
   os.write(handle, cmd.encode('utf-8')) 
   os.close(handle) 
   os.chmod(path, stat.S_IRWXU) 
   proc = Popen(path, shell=True, stdout=PIPE, stderr=PIPE) 
   proc.wait() 
   (stdout, _) = proc.communicate() 
   os.unlink(path) 
   return _chomp(as_unicode(stdout))", 'Run a shell command and return its output.','Write the code to a temporary file.'
"def _run_composer(action, directory=None, composer=None, php=None, runas=None, prefer_source=None, prefer_dist=None, no_scripts=None, no_plugins=None, optimize=None, no_dev=None, quiet=False, composer_home='/root', extra_flags=None): 
    if (composer is not None): 
      if (php is None): 
         php = 'php' 
   else: 
      composer = 'composer' 
   if (not _valid_composer(composer)): 
      raise CommandNotFoundError(""'composer.{0}'   is   not   available.   Couldn't   find   '{1}'."".format(action, composer)) 
   if (action is None): 
      raise SaltInvocationError(""The   'action'   argument   is   required"") 
   if ((directory is None) and (action != 'selfupdate')): 
      raise SaltInvocationError(""The   'directory'   argument   is   required   for   composer.{0}"".format(action)) 
   cmd = [composer, action, '--no-interaction', '--no-ansi'] 
   if (extra_flags is not None): 
      cmd.extend(salt.utils.shlex_split(extra_flags)) 
   if (php is not None): 
      cmd = ([php] + cmd) 
   if (directory is not None): 
      cmd.extend(['--working-dir', directory]) 
   if (quiet is True): 
      cmd.append('--quiet') 
   if (no_dev is True): 
      cmd.append('--no-dev') 
   if (prefer_source is True): 
      cmd.append('--prefer-source') 
   if (prefer_dist is True): 
      cmd.append('--prefer-dist') 
   if (no_scripts is True): 
      cmd.append('--no-scripts') 
   if (no_plugins is True): 
      cmd.append('--no-plugins') 
   if (optimize is True): 
      cmd.append('--optimize-autoloader') 
   result = __salt__['cmd.run_all'](cmd, runas=runas, env={'COMPOSER_HOME': composer_home}, python_shell=False) 
   if (result['retcode'] != 0): 
      raise CommandExecutionError(result['stderr']) 
   if (quiet is True): 
      return True 
   return result"," 'Run composer with given arguments. 
 :param action: The action to run. 
 :param directory: The directory to run composer from. 
 :param composer: The composer binary to run. 
 :param php: The php binary to run. 
 :param runas: The user to run the command as. 
 :param prefer_source: Prefer source distribution over binary. 
 :param prefer_dist: Prefer distribution over source. 
 :param no_scripts: Do not download scripts. 
 :param no_plugins: Do not download plugins. 
 :param optimize: Optimize autoloader. 
 :param no_dev: Do not download dev packages. 
 :param quiet: Run silently. 
 :param composer_home: The composer home directory. 
 :return: The return code of the command. 
 :raises CommandNotFoundError: If composer is not found. 
 :raises CommandExecutionError: If the command fails. 
 :raises SaltInvocationError: If the action is not provided.'","'Run PHP\'s composer with a specific action. 
 If composer has not been installed globally making it available in the 
 system PATH & making it executable, the ``composer`` and ``php`` parameters 
 will need to be set to the location of the executables. 
 action 
 The action to pass to composer (\'install\', \'update\', \'selfupdate\', etc). 
 directory 
 Directory location of the composer.json file.  Required except when 
 action=\'selfupdate\' 
 composer 
 Location of the composer.phar file. If not set composer will 
 just execute ""composer"" as if it is installed globally. 
 (i.e. /path/to/composer.phar) 
 php 
 Location of the php executable to use with composer. 
 (i.e. /usr/bin/php) 
 runas 
 Which system user to run composer as. 
 prefer_source 
 --prefer-source option of composer. 
 prefer_dist 
 --prefer-dist option of composer. 
 no_scripts 
 --no-scripts option of composer. 
 no_plugins 
 --no-plugins option of composer. 
 optimize 
 --optimize-autoloader option of composer. Recommended for production. 
 no_dev 
 --no-dev option for composer. Recommended for production. 
 quiet 
 --quiet option for composer. Whether or not to return output from composer. 
 composer_home 
 $COMPOSER_HOME environment variable 
 extra_flags 
 None, or a string containing extra flags to pass to composer.'"
"def _setup_styles(conditions, style_dict, style, default): 
    tags = set([tag for cond in conditions for tag in cond.split('/')]) 
   msg = ""Can't   map   between   conditions   and   the   provided   {0}.   Make   sure   you   have   provided   keys   in   the   format   of   '/'-separated   tags,   and   that   these   correspond   to   '/'-separated   tags   for   the   condition   names   (e.g.,   conditions   like   'Visual/Right',   and   styles   like   'colors=dict(Visual='red'))'.   The   offending   tag   was   '{1}'."" 
   for key in style_dict: 
      for tag in key.split('/'): 
         if (tag not in tags): 
            raise ValueError(msg.format(style, tag)) 
   condition_warning = ('Condition   {0}   could   not   be   mapped   to   a   ' + style) 
   style_warning = '.   Using   the   default   of   {0}.'.format(default) 
   for condition in conditions: 
      if (condition not in style_dict): 
         if ('/' not in condition): 
            warn((condition_warning.format(condition) + style_warning)) 
            style_dict[condition] = default 
         for style_ in style_dict: 
            if (style_ in condition.split('/')): 
               style_dict[condition] = style_dict[style_] 
               break 
   return style_dict"," 'Sets up the styles for the given conditions. 
 Args: 
 conditions (list of Condition): The conditions that should be styled. 
 style_dict (dict): The mapping of conditions to styles. 
 style (str): The style to use if no condition matches. 
 default (str): The default style to use if no condition matches. 
 Returns: 
 style_dict: The mapping of conditions to styles. 
 Examples: 
 >>> from pylab import cm 
 >>> from matplotlib.colors import ListedColormap 
 >>> from mpl_toolkits.mplot3d import Axes3D 
 >>> from matplotlib.colors import LinearSegmentedColormap 
 >>> from matplotlib.colors import palette 
 >>> from matplotlib.colors import _setup_styles 
 >>> from matplotlib.colors import _make_color_map 
 >>> from matplotlib.colors import _color_map_from_style 
 >>> from matplotlib.colors import _color_map_from_style_dict 
 >>> from matplotlib.colors import _color_map_from_style_dict_from_style 
 >>> from matplotlib.",'Set linestyles and colors for plot_compare_evokeds.'
"def parse(json_string): 
    try: 
      json_data = json.loads(json_string) 
   except: 
      raise SchemaParseException(('Error   parsing   JSON:   %s' % json_string)) 
   names = Names() 
   return make_avsc_object(json_data, names)", 'Parse a JSON string into an Avro schema.','Constructs the Schema from the JSON text.'
"def _filter_configured_avoids(module): 
    run_app = False 
   if (hasattr(settings, 'LETTUCE_AVOID_APPS') and isinstance(settings.LETTUCE_AVOID_APPS, (list, tuple))): 
      for appname in settings.LETTUCE_AVOID_APPS: 
         if module.__name__.startswith(appname): 
            run_app = True 
   return (not run_app)"," 'Filter out modules that are configured to be avoided. 
 :param module: The module to test. 
 :return: True if the module should be skipped, False otherwise.'",'returns apps that are not within django.conf.settings.LETTUCE_AVOID_APPS'
"def _grow_nonoverlapping_labels(subject, seeds_, extents_, hemis, vertices_, graphs, names_): 
    labels = [] 
   for hemi in set(hemis): 
      hemi_index = (hemis == hemi) 
      seeds = seeds_[hemi_index] 
      extents = extents_[hemi_index] 
      names = names_[hemi_index] 
      graph = graphs[hemi] 
      n_vertices = len(vertices_[hemi]) 
      n_labels = len(seeds) 
      parc = np.empty(n_vertices, dtype='int32') 
      parc[:] = (-1) 
      sources = {} 
      edge = [] 
      for (label, seed) in enumerate(seeds): 
         if np.any((parc[seed] >= 0)): 
            raise ValueError('Overlapping   seeds') 
         parc[seed] = label 
         for s in np.atleast_1d(seed): 
            sources[s] = (label, 0.0) 
            edge.append(s) 
      while edge: 
         vert_from = edge.pop(0) 
         (label, old_dist) = sources[vert_from] 
         row = graph[vert_from, :] 
         for (vert_to, dist) in zip(row.indices, row.data): 
            new_dist = (old_dist + dist) 
            if (new_dist > extents[label]): 
               continue 
            vert_to_label = parc[vert_to] 
            if (vert_to_label >= 0): 
               (_, vert_to_dist) = sources[vert_to] 
               if (new_dist > vert_to_dist): 
                  continue 
               elif (vert_to in edge): 
                  edge.remove(vert_to) 
            parc[vert_to] = label 
            sources[vert_to] = (label, new_dist) 
            edge.append(vert_to) 
      for i in xrange(n_labels): 
         vertices = np.nonzero((parc == i))[0] 
         name = str(names[i]) 
         label_ = Label(vertices, hemi=hemi, name=name, subject=subject) 
         labels.append(label_) 
   return labels"," 'Grow a set of labels non-overlapping with the seeds. 
 Parameters 
 subject : str 
 Subject name. 
 seeds_ : array 
 Seed locations. 
 extents_ : array 
 Extents of each seed. 
 hemis : array 
 Hemisphere. 
 vertices_ : array 
 Vertices of the labels. 
 graphs : array 
 Graphs of the vertices. 
 names_ : array 
 Names of the labels. 
 Returns 
 labels : array 
 Array of labels. 
 Examples 
 >>> from nipype.workflows.dmri.label import _grow_nonoverlapping_labels 
 >>> from nipype.workflows.dmri.label import _grow_nonoverlapping_labels 
 >>> seeds = np.array([[200, 200, 200], [200, 200, 200], [200, 200, 200]]) 
 >>> extents = np.array([[100, 1",'Grow labels while ensuring that they don\'t overlap.'
"def _perform_pairwise_tests(labels, dists, tail_type, num_permutations): 
    result = [] 
   num_tests = 0 
   for (g1_idx, (g1_label, g1_dist)) in enumerate(zip(labels[:(-1)], dists[:(-1)])): 
      for (g2_label, g2_dist) in zip(labels[(g1_idx + 1):], dists[(g1_idx + 1):]): 
         if (((len(g1_dist) == 1) and (len(g2_dist) == 1)) or ((len(g1_dist) < 1) or (len(g2_dist) < 1))): 
            (obs_t, param_p_val, nonparam_p_val) = (nan, nan, nan) 
         else: 
            (obs_t, param_p_val, _, nonparam_p_val) = mc_t_two_sample(g1_dist, g2_dist, tails=tail_type, permutations=num_permutations) 
         result.append([g1_label, g2_label, obs_t, param_p_val, None, nonparam_p_val, None]) 
         if (not isnan(obs_t)): 
            num_tests += 1 
   for stat in result: 
      corr_param_p_val = stat[3] 
      if ((corr_param_p_val is not None) and (not isnan(corr_param_p_val))): 
         corr_param_p_val = min((corr_param_p_val * num_tests), 1) 
      stat[4] = corr_param_p_val 
      corr_nonparam_p_val = stat[5] 
      if ((corr_nonparam_p_val is not None) and (not isnan(corr_nonparam_p_val))): 
         corr_nonparam_p_val = min((corr_nonparam_p_val * num_tests), 1) 
      stat[6] = corr_nonparam_p_val 
   return result"," 'Perform pairwise tests for the given labels and distances. 
 Parameters 
 labels : list of str 
 The labels of the groups. 
 dists : list of float 
 The distances of the groups. 
 tail_type : str 
 The type of tail to use for the tests. 
 num_permutations : int 
 The number of permutations to perform for the tests. 
 Returns 
 result : list of tuples 
 The results of the pairwise tests. 
 The first element of each tuple is the label of the group with the smaller 
 distance. 
 The second element of each tuple is the label of the group with the larger 
 distance. 
 The third element of each tuple is the observed t-statistic for the 
 corresponding pairwise test. 
 The fourth element of each tuple is the p-value for the corresponding 
 pairwise test. 
 The fifth element of each tuple is the p-value for the corresponding 
 nonparametric test. 
 The sixth element of each tuple is the p-value for the corresponding 
 nonparametric test.'","'Perform t-test for all pairs of distributions. 
 Computes corrected p-values in addition to uncorrected.'"
"def get_num_instances(server=None, version=None): 
    req = servers_service_pb.GetNumInstancesRequest() 
   if server: 
      req.set_server(server) 
   if version: 
      req.set_version(version) 
   resp = servers_service_pb.GetNumInstancesResponse() 
   try: 
      apiproxy_stub_map.MakeSyncCall('servers', 'GetNumInstances', req, resp) 
   except apiproxy_errors.ApplicationError as e: 
      if (e.application_error == servers_service_pb.ServersServiceError.INVALID_VERSION): 
         raise InvalidVersionError() 
      else: 
         raise Error() 
   return resp.instances()"," 'Returns the number of instances for the server. 
 :param server: The server to query. 
 :type server: str 
 :param version: The version of the server to query. 
 :type version: str 
 :returns: A list of :class:`~.instances.Instance` objects.'","'Return the number of instances that are set for the given server version. 
 This is only valid for fixed servers, an error will be raised for 
 automatically-scheduled servers.  Support for automatically-servers may be 
 supported in the future. 
 Args: 
 server: String containing the name of the server to fetch this info for, if 
 None the server of the current instance will be used. 
 version: String containing the name of the version to fetch this info for, 
 if None the version of the current instance will be used.  If that version 
 does not exist in the other server, then an InvalidVersionError is raised. 
 Raises: 
 InvalidVersionError on invalid input.'"
"def raw(key=None): 
    if key: 
      ret = __pillar__.get(key, {}) 
   else: 
      ret = __pillar__ 
   return ret"," 'Returns a dictionary of raw pillar data. 
 :param key: key to return if present in pillar, otherwise return the pillar 
 dictionary. 
 :returns: A dictionary of raw pillar data.'","'Return the raw pillar data that is currently loaded into the minion. 
 Contrast with :py:func:`items` which calls the master to fetch the most 
 up-to-date Pillar. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pillar.raw 
 With the optional key argument, you can select a subtree of the 
 pillar raw data.:: 
 salt \'*\' pillar.raw key=\'roles\''"
"def smart_str(s, encoding='utf-8', strings_only=False, errors='strict'): 
    if (strings_only and isinstance(s, (types.NoneType, int))): 
      return s 
   if (not isinstance(s, basestring)): 
      try: 
         return str(s) 
      except UnicodeEncodeError: 
         if isinstance(s, Exception): 
            return '   '.join([smart_str(arg, encoding, strings_only, errors) for arg in s]) 
         return unicode(s).encode(encoding, errors) 
   elif isinstance(s, unicode): 
      return s.encode(encoding, errors) 
   elif (s and (encoding != 'utf-8')): 
      return s.decode('utf-8', errors).encode(encoding, errors) 
   else: 
      return s"," 'Smart str() method. 
 :param s: The string to convert. 
 :param encoding: The encoding to use. 
 :param strings_only: If True, return a string, otherwise return a unicode string. 
 :param errors: The encoding errors to use. 
 :return: The string.'","'Returns a bytestring version of \'s\', encoded as specified in \'encoding\'. 
 If strings_only is True, don\'t convert (some) non-string-like objects.'"
"def localize(value): 
    return force_unicode(formats.localize(value, use_l10n=True))"," 'Returns the value with localization applied. 
 :param value: The value to localize. 
 :return: The localized value. 
 :rtype: unicode'","'Forces a value to be rendered as a localized value, 
 regardless of the value of ``settings.USE_L10N``.'"
"def test__FileIO_tell(): 
    pass", 'Test the ``tell`` method.','TODO'
"@requires_application() 
 def test_functionality_desktop(): 
    _test_functionality('gl2')", 'Test functionality of the desktop.','Test desktop GL backend for full functionality.'
"def SearchDataAdapter(results, format, collection): 
    if (results and results['response'] and results['response']['docs']): 
      search_data = results['response']['docs'] 
      if collection['template']['fieldsSelected']: 
         headers = collection['template']['fieldsSelected'] 
      else: 
         headers = [field['name'] for field in collection['fields']] 
      rows = [] 
      for data in search_data: 
         row = [] 
         for column in headers: 
            if (column not in data): 
               row.append('') 
            elif (isinstance(data[column], basestring) or isinstance(data[column], (int, long, float, complex))): 
               row.append(data[column]) 
            elif isinstance(data[column], list): 
               row.append([smart_str(val, errors='replace') for val in data[column]]) 
            else: 
               row.append(smart_str(data[column])) 
         rows.append(row) 
   else: 
      rows = [[]] 
   (yield (headers, rows))"," 'Returns a list of tuples, where each tuple is a list of 
 header names and a list of row data. 
 :param results: 
 :type results: 
 :param format: 
 :type format: 
 :param collection: 
 :type collection: 
 :return: 
 :rtype: 
 :raises: 
 :class:`exceptions.RestException`'","'SearchDataAdapter(results, format, db) -> headers, 2D array of data.'"
"def delaxes(*args): 
    if (not len(args)): 
      ax = gca() 
   else: 
      ax = args[0] 
   ret = gcf().delaxes(ax) 
   return ret"," 'Remove all axes from the figure. 
 Parameters 
 ax : Axes 
 The axes to remove. 
 Returns 
 Axes 
 The axes to remove. 
 Examples 
 >>> import matplotlib.pyplot as plt 
 >>> plt.figure() 
 >>> ax = plt.gca() 
 >>> ax.plot(1) 
 >>> delaxes(ax) 
 >>> delaxes() 
 >>> delaxes(ax) 
 >>> delaxes(ax) 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>> delaxes() 
 >>>","'Remove an axes from the current figure.  If *ax* 
 doesn\'t exist, an error will be raised. 
 ``delaxes()``: delete the current axes'"
"def volume_get_all_by_group(context, group_id, filters=None): 
    return IMPL.volume_get_all_by_group(context, group_id, filters=filters)", 'Get all volumes in a group.','Get all volumes belonging to a consistency group.'
"def __virtual__(): 
    if (not salt.utils.is_darwin()): 
      return (False, 'The   mac_system   module   could   not   be   loaded:   module   only   works   on   MacOS   systems.') 
   if (not _atrun_enabled()): 
      if (not _enable_atrun()): 
         return (False, 'atrun   could   not   be   enabled   on   this   system') 
   return __virtualname__"," 'Returns the mac_system module if it is available and the user has 
 enabled atrun. 
 If the user has not enabled atrun, the module will not be loaded.'",'Only for MacOS with atrun enabled'
"def to_tornado_future(asyncio_future): 
    tf = tornado.concurrent.Future() 
   tornado.concurrent.chain_future(asyncio_future, tf) 
   return tf"," 'Convert asyncio future to tornado future. 
 :param asyncio_future: asyncio future 
 :return: tornado future'","'Convert an `asyncio.Future` to a `tornado.concurrent.Future`. 
 .. versionadded:: 4.1'"
"def has_open_quotes(s): 
    if (s.count('""') % 2): 
      return '""' 
   elif (s.count(""'"") % 2): 
      return ""'"" 
   else: 
      return False"," 'Return True if the string has an open quote, False otherwise.'","'Return whether a string has open quotes. 
 This simply counts whether the number of quote characters of either type in 
 the string is odd. 
 Returns 
 If there is an open quote, the quote character is returned.  Else, return 
 False.'"
"def get_bulk_archive(selected_submissions, zip_directory=''): 
    zip_file = tempfile.NamedTemporaryFile(prefix='tmp_securedrop_bulk_dl_', dir=config.TEMP_DIR, delete=False) 
   sources = set([i.source.journalist_designation for i in selected_submissions]) 
   with zipfile.ZipFile(zip_file, 'w') as zip: 
      for source in sources: 
         submissions = [s for s in selected_submissions if (s.source.journalist_designation == source)] 
         for submission in submissions: 
            filename = path(submission.source.filesystem_id, submission.filename) 
            verify(filename) 
            document_number = submission.filename.split('-')[0] 
            zip.write(filename, arcname=os.path.join(zip_directory, source, ('%s_%s' % (document_number, submission.source.last_updated.date())), os.path.basename(filename))) 
   return zip_file"," 'Returns a zip file of the selected submissions. 
 :param selected_submissions: A list of submissions to be included in the zip file 
 :param zip_directory: The directory where the zip file will be stored 
 :returns: A zip file with all the selected submissions'",'Generate a zip file from the selected submissions'
"def _frangi_hessian_common_filter(image, scale_range, scale_step, beta1, beta2): 
    from ..feature import hessian_matrix, hessian_matrix_eigvals 
   sigmas = np.arange(scale_range[0], scale_range[1], scale_step) 
   if np.any((np.asarray(sigmas) < 0.0)): 
      raise ValueError('Sigma   values   less   than   zero   are   not   valid') 
   beta1 = (2 * (beta1 ** 2)) 
   beta2 = (2 * (beta2 ** 2)) 
   filtered_array = np.zeros((sigmas.shape + image.shape)) 
   lambdas_array = np.zeros((sigmas.shape + image.shape)) 
   for (i, sigma) in enumerate(sigmas): 
      (Drr, Drc, Dcc) = hessian_matrix(image, sigma, order='rc') 
      Drr = ((sigma ** 2) * Drr) 
      Drc = ((sigma ** 2) * Drc) 
      Dcc = ((sigma ** 2) * Dcc) 
      (lambda1, lambda2) = hessian_matrix_eigvals(Drr, Drc, Dcc) 
      lambda1[(lambda1 == 0)] = 1e-10 
      rb = ((lambda2 / lambda1) ** 2) 
      s2 = ((lambda1 ** 2) + (lambda2 ** 2)) 
      filtered = (np.exp(((- rb) / beta1)) * (np.ones(np.shape(image)) - np.exp(((- s2) / beta2)))) 
      filtered_array[i] = filtered 
      lambdas_array[i] = lambda1 
   return (filtered_array, lambdas_array)"," 'Filter an image using the Frangi method. 
 Parameters 
 image : ndarray 
 The input image. 
 scale_range : tuple 
 The range of scales to use. 
 scale_step : int 
 The step size between scales. 
 beta1 : float 
 The first scale beta parameter. 
 beta2 : float 
 The second scale beta parameter. 
 Returns 
 filtered_array : ndarray 
 The filtered image. 
 lambdas_array : ndarray 
 The eigenvalues of the Hessian matrix at each scale.'","'This is an intermediate function for Frangi and Hessian filters. 
 Shares the common code for Frangi and Hessian functions. 
 Parameters 
 image : (N, M) ndarray 
 Array with input image data. 
 scale_range : 2-tuple of floats, optional 
 The range of sigmas used. 
 scale_step : float, optional 
 Step size between sigmas. 
 beta1 : float, optional 
 Frangi correction constant that adjusts the filter\'s 
 sensitivity to deviation from a blob-like structure. 
 beta2 : float, optional 
 Frangi correction constant that adjusts the filter\'s 
 sensitivity to areas of high variance/texture/structure. 
 Returns 
 filtered_list : list 
 List of pre-filtered images.'"
"def dns_dhcp(interface='Local   Area   Connection'): 
    cmd = ['netsh', 'interface', 'ip', 'set', 'dns', interface, 'source=dhcp'] 
   return (__salt__['cmd.retcode'](cmd, python_shell=False) == 0)"," 'Set DNS server to DHCP. 
 This function sets the DNS server to DHCP on the local machine. 
 This function is a wrapper for the command: 
 netsh interface ip set dns <interface> source=dhcp 
 This function will return True if the command is successful and 
 False if the command fails. 
 :param interface: 
 The interface to set the DNS server to DHCP on. 
 :return: 
 True if the command is successful and False if the command fails.'","'Configure the interface to get its DNS servers from the DHCP server 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' win_dns_client.dns_dhcp <interface>'"
"def dijkstra_predecessor_and_distance(G, source, cutoff=None, weight='weight'): 
    weight = _weight_function(G, weight) 
   pred = {source: []} 
   return (pred, _dijkstra(G, source, weight, pred=pred, cutoff=cutoff))"," 'Returns a dict of predecessors and distances for all vertices in the graph 
 for a given source vertex. 
 Parameters 
 G : graph 
 The graph. 
 source : int 
 The source vertex. 
 cutoff : int, optional 
 If given, only vertices with a distance less than or equal to cutoff are 
 included in the output. 
 Returns 
 pred : dict 
 A dict of predecessors and distances for all vertices in the graph. 
 The keys are the vertex indices, and the values are lists of the vertices 
 that are predecessors of the vertex. 
 distance : dict 
 A dict of distances from the source vertex. 
 Examples 
 >>> from networkx.algorithms.shortest_paths import dijkstra_predecessor_and_distance 
 >>> G = nx.Graph() 
 >>> G.add_edge(0, 1, weight=1) 
 >>> G.add_edge(0, 2, weight=1) 
 >>> G.add_edge(1, 2, weight=2) 
 >>> G.add_edge(","'Compute weighted shortest path length and predecessors. 
 Uses Dijkstra\'s Method to obtain the shortest weighted paths 
 and return dictionaries of predecessors for each node and 
 distance for each node from the `source`. 
 Parameters 
 G : NetworkX graph 
 source : node label 
 Starting node for path 
 cutoff : integer or float, optional 
 Depth to stop the search. Only return paths with length <= cutoff. 
 weight : string or function 
 If this is a string, then edge weights will be accessed via the 
 edge attribute with this key (that is, the weight of the edge 
 joining `u` to `v` will be ``G.edge[u][v][weight]``). If no 
 such edge attribute exists, the weight of the edge is assumed to 
 be one. 
 If this is a function, the weight of an edge is the value 
 returned by the function. The function must accept exactly three 
 positional arguments: the two endpoints of an edge and the 
 dictionary of edge attributes for that edge. The function must 
 return a number. 
 Returns 
 pred, distance : dictionaries 
 Returns two dictionaries representing a list of predecessors 
 of a node and the distance to each node. 
 Notes 
 Edge weight attributes must be numerical. 
 Distances are calculated as sums of weighted edges traversed. 
 The list of predecessors contains more than one element only when 
 there are more than one shortest paths to the key node.'"
"def internJID(jidstring): 
    if (jidstring in __internJIDs): 
      return __internJIDs[jidstring] 
   else: 
      j = JID(jidstring) 
      __internJIDs[jidstring] = j 
      return j"," 'Intern a JID string. 
 :param jidstring: The JID string to intern. 
 :type jidstring: str 
 :return: The interned JID.'","'Return interned JID. 
 @rtype: L{JID}'"
"def get_all_roles(exclude_system=False): 
    if exclude_system: 
      result = Role.query(system=False) 
   else: 
      result = Role.get_all() 
   return result", 'Returns a list of all roles.',"'Retrieve all the available roles. 
 :param exclude_system: True to exclude system roles. 
 :type exclude_system: ``bool`` 
 :rtype: ``list`` of :class:`RoleDB`'"
"def get_user_hash(request): 
    ip = request.META.get('REMOTE_ADDR', '') 
   ua = request.META.get('User-Agent', '') 
   session_key = (request.session.session_key or '') 
   return hashlib.sha1('-'.join(map(str, (ip, ua, session_key)))).hexdigest()"," 'Returns a hash of the user\'s session, IP, and User-Agent.'","'Get a hash identifying an user. 
 It\'s a hash of session key, ip and user agent'"
"def test_max_pool(): 
    X_sym = tensor.tensor4('X') 
   pool_it = max_pool(X_sym, pool_shape=(2, 2), pool_stride=(2, 2), image_shape=(6, 4)) 
   f = theano.function(inputs=[X_sym], outputs=pool_it) 
   X = np.array([[2, 1, 3, 4], [1, 1, 3, 3], [5, 5, 7, 7], [5, 6, 8, 7], [9, 10, 11, 12], [9, 10, 12, 12]], dtype=theano.config.floatX)[np.newaxis, np.newaxis, ...] 
   expected = np.array([[2, 4], [6, 8], [10, 12]], dtype=theano.config.floatX)[np.newaxis, np.newaxis, ...] 
   actual = f(X) 
   assert np.allclose(expected, actual)", 'Test max pooling','Test max pooling for known result.'
"def test_alknn_not_good_object(): 
    nn = 'rnd' 
   allknn = AllKNN(n_neighbors=nn, random_state=RND_SEED, kind_sel='mode') 
   assert_raises(ValueError, allknn.fit_sample, X, Y)", 'Test that allknn raises an error when given a non-good object','Test either if an error is raised while a wrong type of NN is given'
"def required_estimates_fields(columns): 
    return metadata_columns.union(viewvalues(columns))", 'Returns a set of fields that are required to be estimated.',"'Compute the set of resource columns required to serve 
 `columns`.'"
"@register.tag 
 def ssi(parser, token): 
    bits = token.split_contents() 
   parsed = False 
   if (len(bits) not in (2, 3)): 
      raise TemplateSyntaxError(u""'ssi'   tag   takes   one   argument:   the   path   to   the   file   to   be   included"") 
   if (len(bits) == 3): 
      if (bits[2] == u'parsed'): 
         parsed = True 
      else: 
         raise TemplateSyntaxError((u""Second   (optional)   argument   to   %s   tag   must   be   'parsed'"" % bits[0])) 
   filepath = parser.compile_filter(bits[1]) 
   return SsiNode(filepath, parsed)"," 'Includes a file. 
 The first argument is the path to the file to be included. 
 The second (optional) argument is a boolean value that indicates 
 whether the included file should be parsed as a template. 
 If the second argument is not present, the default is ``False``.'","'Outputs the contents of a given file into the page. 
 Like a simple ""include"" tag, the ``ssi`` tag includes the contents 
 of another file -- which must be specified using an absolute path -- 
 in the current page:: 
 {% ssi ""/home/html/ljworld.com/includes/right_generic.html"" %} 
 If the optional ""parsed"" parameter is given, the contents of the included 
 file are evaluated as template code, with the current context:: 
 {% ssi ""/home/html/ljworld.com/includes/right_generic.html"" parsed %}'"
"def real_path(path): 
    return os.path.normpath(os.path.normcase(os.path.realpath(path)))"," 'Return the real path of a path. 
 This function is useful for converting paths to the real path of the 
 current working directory. 
 :param path: The path to convert. 
 :returns: The real path of the given path. 
 :rtype: str'","'Returns: the canonicalized absolute pathname. The resulting path will have no symbolic link, \'/./\' or \'/../\' components.'"
"def main(): 
    try: 
      f_mounts = open('/proc/mounts', 'r') 
   except IOError as e: 
      utils.err((""error:   can't   open   /proc/mounts:   %s"" % e)) 
      return 13 
   utils.drop_privileges() 
   while True: 
      devices = [] 
      f_mounts.seek(0) 
      ts = int(time.time()) 
      for line in f_mounts: 
         try: 
            (fs_spec, fs_file, fs_vfstype, fs_mntops, fs_freq, fs_passno) = line.split(None) 
         except ValueError as e: 
            utils.err((""error:   can't   parse   line   at   /proc/mounts:   %s"" % e)) 
            continue 
         if (fs_spec == 'none'): 
            continue 
         elif ((fs_vfstype in FSTYPE_IGNORE) or fs_vfstype.startswith('fuse.')): 
            continue 
         elif (fs_file.startswith('/dev') or fs_file.startswith('/sys') or fs_file.startswith('/proc') or fs_file.startswith('/lib') or fs_file.startswith('net:')): 
            continue 
         device_found = False 
         if fs_spec.startswith('/dev'): 
            for device in devices: 
               if (fs_spec == device[0]): 
                  device_found = True 
                  if (len(fs_file) < len(device[1])): 
                     device[1] = fs_file 
                  break 
            if (not device_found): 
               devices.append([fs_spec, fs_file, fs_vfstype]) 
         else: 
            devices.append([fs_spec, fs_file, fs_vfstype]) 
      for device in devices: 
         (fs_spec, fs_file, fs_vfstype) = device 
         try: 
            r = os.statvfs(fs_file) 
         except OSError as e: 
            utils.err((""can't   get   info   for   mount   point:   %s:   %s"" % (fs_file, e))) 
            continue 
         used = (r.f_blocks - r.f_bfree) 
         if (r.f_blocks == 0): 
            percent_used = 100 
         else: 
            percent_used = ((used * 100.0) / r.f_blocks) 
         print ('df.bytes.total   %d   %s   mount=%s   fstype=%s' % (ts, (r.f_frsize * r.f_blocks), fs_file, fs_vfstype)) 
         print ('df.bytes.used   %d   %s   mount=%s   fstype=%s' % (ts, (r.f_frsize * used), fs_file, fs_vfstype)) 
         print ('df.bytes.percentused   %d   %s   mount=%s   fstype=%s' % (ts, percent_used, fs_file, fs_vfstype)) 
         print ('df.bytes.free   %d   %s   mount=%s   fstype=%s' % (ts, (r.f_frsize * r.f_bfree), fs_file, fs_vfstype)) 
         used = (r.f_files - r.f_ffree) 
         if (r.f_files == 0): 
            percent_used = 100 
         else: 
            percent_used = ((used * 100.0) / r.f_files) 
         print ('df.inodes.total   %d   %s   mount=%s   fstype=%s' % (ts, r.f_files, fs_file, fs_vfstype)) 
         print ('df.inodes.used   %d   %s   mount=%s   fstype=%s' % (ts, used, fs_file, fs_vfstype)) 
         print ('df.inodes.percentused   %d   %s   mount=%s   fstype=%s' % (ts, percent_used, fs_file, fs_vfstype)) 
         print ('df.inodes.free   %d   %s   mount=%s   fstype=%s' % (ts, r.f_ffree, fs_file, fs_vfstype)) 
      sys.stdout.flush() 
      time.sleep(COLLECTION_INTERVAL)"," 'Collects the size of all mounted filesystems and prints them to 
 standard output.'",'dfstats main loop'
"def _create_base_cipher(dict_parameters): 
    try: 
      key = dict_parameters.pop('key') 
   except KeyError: 
      raise TypeError(""Missing   'key'   parameter"") 
   expect_byte_string(key) 
   if (len(key) != key_size): 
      raise ValueError(('Incorrect   DES   key   length   (%d   bytes)' % len(key))) 
   start_operation = _raw_des_lib.DES_start_operation 
   stop_operation = _raw_des_lib.DES_stop_operation 
   cipher = VoidPointer() 
   result = start_operation(key, c_size_t(len(key)), cipher.address_of()) 
   if result: 
      raise ValueError(('Error   %X   while   instantiating   the   DES   cipher' % result)) 
   return SmartPointer(cipher.get(), stop_operation)"," 'Create a DES cipher. 
 This function is used to create a DES cipher object. 
 The cipher object is a pointer to a DES structure. 
 The key is a byte string. 
 The key size must be 8 bytes. 
 The key must be 8 bytes. 
 Parameters 
 dict_parameters : dictionary 
 A dictionary with the following keys: 
 * key: a byte string 
 * key_size: the length of the key 
 Raises 
 TypeError 
 ValueError 
 Examples 
 >>> from Crypto.Cipher import DES 
 >>> des = DES.create_cipher(""key"", 8) 
 >>> des.encrypt(""message"") 
 >>> des.decrypt(""encrypted_message"") 
 >>> des.encrypt(""message"", 2) 
 >>> des.decrypt(""encrypted_message"", 2) 
 >>> des.encrypt(""message"", 2, 2) 
 >>> des.decrypt(""encrypted_message"", 2, 2) 
 >>> des.encrypt(""message"", 2, 2, 2)","'This method instantiates and returns a handle to a low-level 
 base cipher. It will absorb named parameters in the process.'"
"@register_specialize 
 @register_canonicalize 
 @gof.local_optimizer([Subtensor]) 
 def local_subtensor_inc_subtensor(node): 
    if isinstance(node.op, Subtensor): 
      x = node.inputs[0] 
      if ((not x.owner) or (not isinstance(x.owner.op, IncSubtensor))): 
         return 
      if (not x.owner.op.set_instead_of_inc): 
         return 
      if ((x.owner.inputs[2:] == node.inputs[1:]) and (tuple(x.owner.op.idx_list) == tuple(node.op.idx_list))): 
         out = node.outputs[0] 
         y = x.owner.inputs[1] 
         if (x.dtype != y.dtype): 
            y = y.astype(x.dtype) 
         if (out.type == y.type): 
            return [y] 
         else: 
            assert (out.broadcastable != y.broadcastable) 
            x_subtensor = node.op(x.owner.inputs[0], *x.owner.inputs[2:]) 
            return [T.alloc(y, *x_subtensor.shape)] 
      else: 
         return", 'Subtensor is replaced by IncSubtensor',"'Subtensor(SetSubtensor(x, y, idx), idx) -> y'"
"def load_check(agentConfig, hostname, checkname): 
    from jmxfetch import JMX_CHECKS 
   agentConfig['checksd_hostname'] = hostname 
   osname = get_os() 
   checks_places = get_checks_places(osname, agentConfig) 
   for config_path in _file_configs_paths(osname, agentConfig): 
      check_name = _conf_path_to_check_name(config_path) 
      if ((check_name == checkname) and (check_name not in JMX_CHECKS)): 
         (conf_is_valid, check_config, invalid_check) = _load_file_config(config_path, check_name, agentConfig) 
         if (invalid_check and (not conf_is_valid)): 
            return invalid_check 
         (load_success, load_failure) = load_check_from_places(check_config, check_name, checks_places, agentConfig) 
         return (load_success.values()[0] or load_failure) 
   for (check_name, service_disco_check_config) in _service_disco_configs(agentConfig).iteritems(): 
      if (check_name == checkname): 
         (sd_init_config, sd_instances) = service_disco_check_config[1] 
         check_config = {'init_config': sd_init_config, 'instances': sd_instances} 
         (load_success, load_failure) = load_check_from_places(check_config, check_name, checks_places, agentConfig) 
         return (load_success.values()[0] or load_failure) 
   return None"," 'Load a check from file or service discovery. 
 :param agentConfig: Agent configuration. 
 :param hostname: Hostname. 
 :param checkname: Check name. 
 :returns: Success or failure. 
 :rtype: dict'",'Same logic as load_check_directory except it loads one specific check'
"def import_string(import_name, silent=False): 
    if isinstance(import_name, unicode): 
      import_name = str(import_name) 
   try: 
      if (':' in import_name): 
         (module, obj) = import_name.split(':', 1) 
      elif ('.' in import_name): 
         (module, obj) = import_name.rsplit('.', 1) 
      else: 
         return __import__(import_name) 
      if isinstance(obj, unicode): 
         obj = obj.encode('utf-8') 
      try: 
         return getattr(__import__(module, None, None, [obj]), obj) 
      except (ImportError, AttributeError): 
         modname = ((module + '.') + obj) 
         __import__(modname) 
         return sys.modules[modname] 
   except ImportError as e: 
      if (not silent): 
         raise ImportStringError(import_name, e), None, sys.exc_info()[2]"," 'Import an object by its name. 
 This function is similar to __import__, but it is more flexible. 
 It supports importing objects by name, by path, and by module. 
 The syntax is ``import_string(import_name[, path[, obj]])``. 
 The first argument is the name of the object to import. 
 If the object is a module, the second argument is the module name. 
 If the object is a function, the second argument is the function name. 
 If the object is a class, the second argument is the class name. 
 If the object is a module, the third argument is the object name. 
 If the object is a function, the third argument is the function name. 
 If the object is a class, the third argument is the class name. 
 If the object is a module, the fourth argument is the module name. 
 If the object is a function, the fourth argument is the function name. 
 If the object is a class, the fourth argument is the class name. 
 If the object is a module, the fifth argument is the object name. 
 If the object is a function, the fifth","'Imports an object based on a string.  This is useful if you want to 
 use import paths as endpoints or something similar.  An import path can 
 be specified either in dotted notation (``xml.sax.saxutils.escape``) 
 or with a colon as object delimiter (``xml.sax.saxutils:escape``). 
 If `silent` is True the return value will be `None` if the import fails. 
 For better debugging we recommend the new :func:`import_module` 
 function to be used instead. 
 :param import_name: the dotted name for the object to import. 
 :param silent: if set to `True` import errors are ignored and 
 `None` is returned instead. 
 :return: imported object'"
"def get(): 
    from frappe.desk.notifications import get_notification_info_for_boot, get_notifications 
   from frappe.boot import get_bootinfo, get_unseen_notes 
   from frappe.limits import get_limits, get_expiry_message 
   bootinfo = None 
   if (not getattr(frappe.conf, u'disable_session_cache', None)): 
      bootinfo = frappe.cache().hget(u'bootinfo', frappe.session.user) 
      if bootinfo: 
         bootinfo[u'from_cache'] = 1 
         bootinfo[u'notification_info'].update(get_notifications()) 
         bootinfo[u'user'][u'recent'] = json.dumps(frappe.cache().hget(u'user_recent', frappe.session.user)) 
   if (not bootinfo): 
      bootinfo = get_bootinfo() 
      bootinfo[u'notification_info'] = get_notification_info_for_boot() 
      frappe.cache().hset(u'bootinfo', frappe.session.user, bootinfo) 
      try: 
         frappe.cache().ping() 
      except redis.exceptions.ConnectionError: 
         message = _(u'Redis   cache   server   not   running.   Please   contact   Administrator   /   Tech   support') 
         if (u'messages' in bootinfo): 
            bootinfo[u'messages'].append(message) 
         else: 
            bootinfo[u'messages'] = [message] 
      if frappe.local.request: 
         bootinfo[u'change_log'] = get_change_log() 
   bootinfo[u'metadata_version'] = frappe.cache().get_value(u'metadata_version') 
   if (not bootinfo[u'metadata_version']): 
      bootinfo[u'metadata_version'] = frappe.reset_metadata_version() 
   bootinfo.notes = get_unseen_notes() 
   for hook in frappe.get_hooks(u'extend_bootinfo'): 
      frappe.get_attr(hook)(bootinfo=bootinfo) 
   bootinfo[u'lang'] = frappe.translate.get_user_lang() 
   bootinfo[u'disable_async'] = frappe.conf.disable_async 
   bootinfo.limits = get_limits() 
   bootinfo.expiry_message = get_expiry_message() 
   return bootinfo", 'Returns the bootinfo dictionary','get session boot info'
"def english_sort(x, y): 
    return cmp(_spat.sub('', x), _spat.sub('', y))"," 'Sorts two strings in English alphabetical order. 
 The strings are assumed to be in the form of a ""spatial"" 
 address. 
 Examples 
 >>> from sympy import Symbol, english_sort 
 >>> english_sort(Symbol(""a""), Symbol(""b"")) 
 -1 
 >>> english_sort(Symbol(""b""), Symbol(""a"")) 
 1 
 >>> english_sort(Symbol(""b""), Symbol(""b"")) 
 0 
 >>> english_sort(Symbol(""a""), Symbol(""a"")) 
 0 
 >>> english_sort(Symbol(""a""), Symbol(""c"")) 
 1 
 >>> english_sort(Symbol(""c""), Symbol(""a"")) 
 -1 
 >>> english_sort(Symbol(""a""), Symbol(""d"")) 
 1 
 >>> english_sort(Symbol(""d""), Symbol(""a"")) 
 -1 
 >>> english_sort(Symbol(""a""), Symbol(""d"")) 
 0 
 >>> english_sort(Symbol(""a""), Symbol(""d"")) 
 0 
 >>> english_sort",'Comapare two english phrases ignoring starting prepositions.'
"@register.filter(is_safe=True) 
 def length(value): 
    try: 
      return len(value) 
   except (ValueError, TypeError): 
      return u''"," 'Returns the length of a string. 
 If the value is a unicode string, then the length of the unicode string 
 is returned. 
 If the value is a non-unicode string, then the length of the string is 
 returned. 
 If the value is not a string, then an empty string is returned.'",'Returns the length of the value - useful for lists.'
"def get_event_loop(): 
    ev = _state.event_loop 
   if ((not os.getenv(_EVENT_LOOP_KEY)) and (ev is not None)): 
      ev.clear() 
      _state.event_loop = None 
      ev = None 
   if (ev is None): 
      ev = EventLoop() 
      _state.event_loop = ev 
      os.environ[_EVENT_LOOP_KEY] = '1' 
   return ev"," 'Returns the current event loop. 
 If the event loop is not running, creates one and starts it. 
 :rtype: :class:`eventloop.EventLoop`'","'Return a EventLoop instance. 
 A new instance is created for each new HTTP request.  We determine 
 that we\'re in a new request by inspecting os.environ, which is reset 
 at the start of each request.  Also, each thread gets its own loop.'"
"def get_visibility(name): 
    if SPECIAL.match(name): 
      visibility = 'special' 
   elif PRIVATE.match(name): 
      visibility = 'private' 
   elif PROTECTED.match(name): 
      visibility = 'protected' 
   else: 
      visibility = 'public' 
   return visibility"," 'Return the visibility of a name. 
 Arguments: 
 name: Name to check 
 Returns: 
 Visibility of the name. 
 Raises: 
 TypeError: If name is not a string'","'return the visibility from a name: public, protected, private or special'"
"def _extractCommon(string): 
    elements = string.split(None, 2) 
   if (len(elements) != 3): 
      raise InvalidEntry() 
   (hostnames, keyType, keyAndComment) = elements 
   splitkey = keyAndComment.split(None, 1) 
   if (len(splitkey) == 2): 
      (keyString, comment) = splitkey 
      comment = comment.rstrip('\n') 
   else: 
      keyString = splitkey[0] 
      comment = None 
   key = Key.fromString(keyString.decode('base64')) 
   return (hostnames, keyType, key, comment)"," 'Extracts the common parts of a key-hostname-comment triplet. 
 This is used to determine the key type and the key itself.'","'Extract common elements of base64 keys from an entry in a hosts file. 
 @return: a 4-tuple of hostname data (L{str}), ssh key type (L{str}), key 
 (L{Key}), and comment (L{str} or L{None}).  The hostname data is simply the 
 beginning of the line up to the first occurrence of whitespace.'"
"def test_np_rng(): 
    rngs = [make_np_rng(rng_or_seed=42, which_method='uniform'), make_np_rng(rng_or_seed=numpy.random.RandomState(42), which_method='uniform'), make_np_rng(default_seed=42), make_np_rng()] 
   random_numbers = rngs[0].uniform(size=(100,)) 
   equals = numpy.ones((100,)) 
   for rng in rngs[1:]: 
      equal = (random_numbers == rng.uniform(size=(100,))) 
      equals *= equal 
   assert equals.all()", 'Test that np.random.RandomState is used for np.random.RandomState rngs.',"'Tests that the four possible ways of creating 
 a numpy RNG give the same results with the same seed'"
"def _translate_str(sequence, table, stop_symbol='*', to_stop=False, cds=False, pos_stop='X', gap=None): 
    sequence = sequence.upper() 
   amino_acids = [] 
   forward_table = table.forward_table 
   stop_codons = table.stop_codons 
   if (table.nucleotide_alphabet.letters is not None): 
      valid_letters = set(table.nucleotide_alphabet.letters.upper()) 
   else: 
      valid_letters = set((IUPAC.ambiguous_dna.letters.upper() + IUPAC.ambiguous_rna.letters.upper())) 
   n = len(sequence) 
   if cds: 
      if (str(sequence[:3]).upper() not in table.start_codons): 
         raise CodonTable.TranslationError(""First   codon   '{0}'   is   not   a   start   codon"".format(sequence[:3])) 
      if ((n % 3) != 0): 
         raise CodonTable.TranslationError('Sequence   length   {0}   is   not   a   multiple   of   three'.format(n)) 
      if (str(sequence[(-3):]).upper() not in stop_codons): 
         raise CodonTable.TranslationError(""Final   codon   '{0}'   is   not   a   stop   codon"".format(sequence[(-3):])) 
      sequence = sequence[3:(-3)] 
      n -= 6 
      amino_acids = ['M'] 
   elif ((n % 3) != 0): 
      warnings.warn('Partial   codon,   len(sequence)   not   a   multiple   of   three.   Explicitly   trim   the   sequence   or   add   trailing   N   before   translation.   This   may   become   an   error   in   future.', BiopythonWarning) 
   if (gap is not None): 
      if (not isinstance(gap, basestring)): 
         raise TypeError('Gap   character   should   be   a   single   character   string.') 
      elif (len(gap) > 1): 
         raise ValueError('Gap   character   should   be   a   single   character   string.') 
   for i in range(0, (n - (n % 3)), 3): 
      codon = sequence[i:(i + 3)] 
      try: 
         amino_acids.append(forward_table[codon]) 
      except (KeyError, CodonTable.TranslationError): 
         if (codon in table.stop_codons): 
            if cds: 
               raise CodonTable.TranslationError('Extra   in   frame   stop   codon   found.') 
            if to_stop: 
               break 
            amino_acids.append(stop_symbol) 
         elif valid_letters.issuperset(set(codon)): 
            amino_acids.append(pos_stop) 
         elif ((gap is not None) and (codon == (gap * 3))): 
            amino_acids.append(gap) 
         else: 
            raise CodonTable.TranslationError(""Codon   '{0}'   is   invalid"".format(codon)) 
   return ''.join(amino_acids)"," 'Translate a sequence of codons into an amino acid sequence. 
 This function is the same as the translate() function in the 
 CodonTable class, except that it does not require an instance of 
 CodonTable. 
 Parameters 
 sequence : str 
 A string of codons. 
 table : CodonTable 
 The codon table to use. 
 stop_symbol : str, optional 
 The symbol to use for a stop codon. 
 to_stop : bool, optional 
 If True, stop codons are included in the amino acid sequence. 
 cds : bool, optional 
 If True, codons are treated as if they were in a CDS. 
 pos_stop : str, optional 
 The symbol to use for a positive stop codon. 
 gap : str, optional 
 A gap character to use. 
 Returns 
 amino_acids : list of str 
 A list of amino acids. 
 Raises 
 CodonTable.TranslationError 
 If the first or last codon is not a start or stop codon. 
 BiopythonWarning","'Helper function to translate a nucleotide string (PRIVATE). 
 Arguments: 
 - sequence - a string 
 - table - a CodonTable object (NOT a table name or id number) 
 - stop_symbol - a single character string, what to use for terminators. 
 - to_stop - boolean, should translation terminate at the first 
 in frame stop codon?  If there is no in-frame stop codon 
 then translation continues to the end. 
 - pos_stop - a single character string for a possible stop codon 
 (e.g. TAN or NNN) 
 - cds - Boolean, indicates this is a complete CDS.  If True, this 
 checks the sequence starts with a valid alternative start 
 codon (which will be translated as methionine, M), that the 
 sequence length is a multiple of three, and that there is a 
 single in frame stop codon at the end (this will be excluded 
 from the protein sequence, regardless of the to_stop option). 
 If these tests fail, an exception is raised. 
 - gap - Single character string to denote symbol used for gaps. 
 Defaults to None. 
 Returns a string. 
 e.g. 
 >>> from Bio.Data import CodonTable 
 >>> table = CodonTable.ambiguous_dna_by_id[1] 
 >>> _translate_str(""AAA"", table) 
 \'K\' 
 >>> _translate_str(""TAR"", table) 
 >>> _translate_str(""TAN"", table) 
 \'X\' 
 >>> _translate_str(""TAN"", table, pos_stop=""@"") 
 >>> _translate_str(""TA?"", table) 
 Traceback (most recent call last): 
 TranslationError: Codon \'TA?\' is invalid 
 In a change to older versions of Biopython, partial codons are now 
 always regarded as an error (previously only checked if cds=True) 
 and will trigger a warning (likely to become an exception in a 
 future release). 
 If **cds=True**, the start and stop codons are checked, and the start 
 codon will be translated at methionine. The sequence must be an 
 while number of codons. 
 >>> _translate_str(""ATGCCCTAG"", table, cds=True) 
 \'MP\' 
 >>> _translate_str(""AAACCCTAG"", table, cds=True) 
 Traceback (most recent call last): 
 TranslationError: First codon \'AAA\' is not a start codon 
 >>> _translate_str(""ATGCCCTAGCCCTAG"", table, cds=True) 
 Traceback (most recent call last): 
 TranslationError: Extra in frame stop codon found.'"
"def track_time_change(year=None, month=None, day=None, hour=None, minute=None, second=None): 
    def track_time_change_decorator(action): 
      'Decorator   to   track   time   changes.' 
      event.track_time_change(HASS, functools.partial(action, HASS), year, month, day, hour, minute, second) 
      return action 
   return track_time_change_decorator"," 'Decorator to track time changes. 
 :param year: Year. 
 :param month: Month. 
 :param day: Day. 
 :param hour: Hour. 
 :param minute: Minute. 
 :param second: Second. 
 :return: Function decorator. 
 :rtype: Function decorator'",'Decorator factory to track time changes.'
"def lz4_encode(payload): 
    return lz4f.compressFrame(payload)"," 'Encode a payload as LZ4 compressed data. 
 :param payload: The payload to encode. 
 :type payload: bytes 
 :return: The encoded payload. 
 :rtype: bytes'",'Encode payload using interoperable LZ4 framing. Requires Kafka >= 0.10'
"def rc(group, **kwargs): 
    aliases = {u'lw': u'linewidth', u'ls': u'linestyle', u'c': u'color', u'fc': u'facecolor', u'ec': u'edgecolor', u'mew': u'markeredgewidth', u'aa': u'antialiased'} 
   if is_string_like(group): 
      group = (group,) 
   for g in group: 
      for (k, v) in six.iteritems(kwargs): 
         name = (aliases.get(k) or k) 
         key = (u'%s.%s' % (g, name)) 
         try: 
            rcParams[key] = v 
         except KeyError: 
            raise KeyError((u'Unrecognized   key   ""%s""   for   group   ""%s""   and   name   ""%s""' % (key, g, name)))"," 'Set rc parameters for a given group. 
 Parameters 
 group : string or sequence of strings 
 A string or sequence of strings describing the group to set rc parameters for. 
 See :ref:`rc_groups` for a list of possible groups. 
 kwargs : dict 
 A dictionary of keyword arguments to pass to rcParams. 
 Examples 
 >>> from matplotlib import rc 
 >>> rc(""lw"", 2) 
 >>> rc(""font"", size=12) 
 >>> rc(""axes"", facecolor=""w"") 
 >>> rc(""axes"", edgecolor=""r"") 
 >>> rc(""axes"", edgecolor=""r"", facecolor=""w"") 
 >>> rc(""axes"", edgecolor=""r"", facecolor=""w"", linestyle=""-"") 
 >>> rc(""axes"", edgecolor=""r"", facecolor=""w"", linestyle=""-"", linewidth=2) 
 >>> rc(""axes"", edgecolor=""r"", facecolor=""w"", linestyle=""-"", linewidth=2, fontsize=12) 
 >>>","'Set the current rc params.  Group is the grouping for the rc, e.g., 
 for ``lines.linewidth`` the group is ``lines``, for 
 ``axes.facecolor``, the group is ``axes``, and so on.  Group may 
 also be a list or tuple of group names, e.g., (*xtick*, *ytick*). 
 *kwargs* is a dictionary attribute name/value pairs, e.g.,:: 
 rc(\'lines\', linewidth=2, color=\'r\') 
 sets the current rc params and is equivalent to:: 
 rcParams[\'lines.linewidth\'] = 2 
 rcParams[\'lines.color\'] = \'r\' 
 The following aliases are available to save typing for interactive 
 users: 
 Alias   Property 
 \'lw\'    \'linewidth\' 
 \'ls\'    \'linestyle\' 
 \'c\'     \'color\' 
 \'fc\'    \'facecolor\' 
 \'ec\'    \'edgecolor\' 
 \'mew\'   \'markeredgewidth\' 
 \'aa\'    \'antialiased\' 
 Thus you could abbreviate the above rc command as:: 
 rc(\'lines\', lw=2, c=\'r\') 
 Note you can use python\'s kwargs dictionary facility to store 
 dictionaries of default parameters.  e.g., you can customize the 
 font rc as follows:: 
 font = {\'family\' : \'monospace\', 
 \'weight\' : \'bold\', 
 \'size\'   : \'larger\'} 
 rc(\'font\', **font)  # pass in the font dict as kwargs 
 This enables you to easily switch between several configurations. 
 Use :func:`~matplotlib.pyplot.rcdefaults` to restore the default 
 rc params after changes.'"
"def _comp_method_SERIES(op, name, str_rep, masker=False): 
    def na_op(x, y): 
      if is_categorical_dtype(x): 
         return op(x, y) 
      elif (is_categorical_dtype(y) and (not isscalar(y))): 
         return op(y, x) 
      if is_object_dtype(x.dtype): 
         result = _comp_method_OBJECT_ARRAY(op, x, y) 
      else: 
         if is_datetimelike_v_numeric(x, y): 
            raise TypeError('invalid   type   comparison') 
         if (isscalar(y) and isnull(y)): 
            if (name == '__ne__'): 
               return np.ones(len(x), dtype=bool) 
            else: 
               return np.zeros(len(x), dtype=bool) 
         mask = None 
         if (needs_i8_conversion(x) or ((not isscalar(y)) and needs_i8_conversion(y))): 
            if isscalar(y): 
               mask = isnull(x) 
               y = _index.convert_scalar(x, _values_from_object(y)) 
            else: 
               mask = (isnull(x) | isnull(y)) 
               y = y.view('i8') 
            x = x.view('i8') 
         try: 
            with np.errstate(all='ignore'): 
               result = getattr(x, name)(y) 
            if (result is NotImplemented): 
               raise TypeError('invalid   type   comparison') 
         except AttributeError: 
            result = op(x, y) 
         if ((mask is not None) and mask.any()): 
            result[mask] = masker 
      return result 
   def wrapper(self, other, axis=None): 
      if (axis is not None): 
         self._get_axis_number(axis) 
      if isinstance(other, ABCSeries): 
         name = _maybe_match_name(self, other) 
         if (not self._indexed_same(other)): 
            msg = 'Can   only   compare   identically-labeled   Series   objects' 
            raise ValueError(msg) 
         return self._constructor(na_op(self.values, other.values), index=self.index, name=name) 
      elif isinstance(other, pd.DataFrame): 
         return NotImplemented 
      elif isinstance(other, (np.ndarray, pd.Index)): 
         if ((not lib.isscalar(lib.item_from_zerodim(other))) and (len(self) != len(other))): 
            raise ValueError('Lengths   must   match   to   compare') 
         if isinstance(other, ABCPeriodIndex): 
            return self._constructor(na_op(self.values, other.asobject.values), index=self.index) 
         return self._constructor(na_op(self.values, np.asarray(other)), index=self.index).__finalize__(self) 
      elif isinstance(other, pd.Categorical): 
         if (not is_categorical_dtype(self)): 
            msg = ""Cannot   compare   a   Categorical   for   op   {op}   with   Series   of   dtype   {typ}.\nIf   you   want   to   compare   values,   use   'series   <op>   np.asarray(other)'."" 
            raise TypeError(msg.format(op=op, typ=self.dtype)) 
      if is_categorical_dtype(self): 
         with np.errstate(all='ignore'): 
            res = op(self.values, other) 
      else: 
         values = self.get_values() 
         if isinstance(other, (list, np.ndarray)): 
            other = np.asarray(other) 
         with np.errstate(all='ignore'): 
            res = na_op(values, other) 
         if isscalar(res): 
            raise TypeError(('Could   not   compare   %s   type   with   Series' % type(other))) 
         res = _values_from_object(res) 
      res = pd.Series(res, index=self.index, name=self.name, dtype='bool') 
      return res 
   return wrapper"," 'Compares the Series to another object. 
 Parameters 
 other : object 
 The object to compare to the Series. 
 axis : {None, \'index\', \'columns\'} 
 The axis to compare. 
 Returns 
 Series 
 A Series containing the results of the comparison.'","'Wrapper function for Series arithmetic operations, to avoid 
 code duplication.'"
"@allow_public 
 def contribute_view(request): 
    return serve(request, 'contribute.json', document_root=settings.ROOT)", 'View for the Contribute page.','Generate a contribute.json'
"def load_from_folder(app): 
    blueprints_path = app.config.get('BLUEPRINTS_PATH', 'modules') 
   path = os.path.join(app.config.get('PROJECT_ROOT', '..'), blueprints_path) 
   base_module_name = '.'.join([app.name, blueprints_path]) 
   dir_list = os.listdir(path) 
   mods = {} 
   object_name = app.config.get('BLUEPRINTS_OBJECT_NAME', 'module') 
   module_file = app.config.get('BLUEPRINTS_MODULE_NAME', 'main') 
   blueprint_module = (module_file + '.py') 
   for fname in dir_list: 
      if ((not os.path.exists(os.path.join(path, fname, 'DISABLED'))) and os.path.isdir(os.path.join(path, fname)) and os.path.exists(os.path.join(path, fname, blueprint_module))): 
         module_root = '.'.join([base_module_name, fname]) 
         module_name = '.'.join([module_root, module_file]) 
         mods[fname] = importlib.import_module(module_name) 
         blueprint = getattr(mods[fname], object_name) 
         app.logger.info(('registering   blueprint:   %s' % blueprint.name)) 
         app.register_blueprint(blueprint) 
         try: 
            importlib.import_module('.'.join([module_root, 'admin'])) 
         except ImportError as e: 
            app.logger.info('%s   module   does   not   define   admin   or   error:   %s', fname, e) 
   app.logger.info('%s   modules   loaded', mods.keys())", 'Load all blueprints from the given folder',"'This code looks for any modules or packages in the given 
 directory, loads them 
 and then registers a blueprint 
 - blueprints must be created with the name \'module\' 
 Implemented directory scan 
 Bulk of the code taken from: 
 https://github.com/smartboyathome/ 
 Cheshire-Engine/blob/master/ScoringServer/utils.py'"
"def encipher_bifid(msg, key, symbols=None): 
    (msg, key, A) = _prep(msg, key, symbols, bifid10) 
   long_key = (''.join(uniq(key)) or A) 
   n = (len(A) ** 0.5) 
   if (n != int(n)): 
      raise ValueError(('Length   of   alphabet   (%s)   is   not   a   square   number.' % len(A))) 
   N = int(n) 
   if (len(long_key) < (N ** 2)): 
      long_key = (list(long_key) + [x for x in A if (x not in long_key)]) 
   row_col = dict([(ch, divmod(i, N)) for (i, ch) in enumerate(long_key)]) 
   (r, c) = zip(*[row_col[x] for x in msg]) 
   rc = (r + c) 
   ch = {i: ch for (ch, i) in row_col.items()} 
   rv = ''.join((ch[i] for i in zip(rc[::2], rc[1::2]))) 
   return rv"," 'Encrypts a message using the Bifid cipher. 
 The Bifid cipher is a simple substitution cipher. 
 The key is a string of length ``N``, where ``N`` is the square root of 
 the length of the alphabet. 
 The alphabet is the set of symbols in the message. 
 The message is a string of length ``N``. 
 The symbols argument is a list of symbols to use in the alphabet. 
 If symbols is not given, then the symbols in the message are used. 
 Examples 
 >>> from sympy import encipher_bifid 
 >>> encipher_bifid(""Hello, world!"", ""abcdefghijklmnopqrstuvwxyz"") 
 \'mwlk\''","'Performs the Bifid cipher encryption on plaintext ``msg``, and 
 returns the ciphertext. 
 This is the version of the Bifid cipher that uses an `n \times n` 
 Polybius square. 
 INPUT: 
 ``msg``: plaintext string 
 ``key``: short string for key; duplicate characters are 
 ignored and then it is padded with the characters in 
 ``symbols`` that were not in the short key 
 ``symbols``: `n \times n` characters defining the alphabet 
 (default is string.printable) 
 OUTPUT: 
 ciphertext (using Bifid5 cipher without spaces) 
 See Also 
 decipher_bifid, encipher_bifid5, encipher_bifid6'"
"def _get_view_to_display_matrix(scene): 
    from mayavi.core.ui.mayavi_scene import MayaviScene 
   from tvtk.pyface.tvtk_scene import TVTKScene 
   if (not isinstance(scene, (MayaviScene, TVTKScene))): 
      raise TypeError(('scene   must   be   an   instance   of   TVTKScene/MayaviScene,   found   type   %s' % type(scene))) 
   (x, y) = tuple(scene.get_size()) 
   view_to_disp_mat = np.array([[(x / 2.0), 0.0, 0.0, (x / 2.0)], [0.0, ((- y) / 2.0), 0.0, (y / 2.0)], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]) 
   return view_to_disp_mat"," 'Returns a 4x4 view-to-display matrix for a given scene. 
 :param scene: Mayavi or TVTK scene. 
 :return: 4x4 view-to-display matrix.'","'Return the 4x4 matrix to convert view coordinates to display coordinates. 
 It\'s assumed that the view should take up the entire window and that the 
 origin of the window is in the upper left corner.'"
"def debug_msg(text, prefix='Debug'): 
    msg(colorize(('%s:   %s' % (prefix, str(text))), 'cyan'))"," 'Prints a message to the debug stream. 
 :param text: The text to print. 
 :param prefix: A prefix to print before the message.'",'Colorize debug message with prefix'
"@pytest.mark.cmd 
 @pytest.mark.django_db 
 def test_list_projects_modified_since(capfd): 
    call_command('list_projects', '--modified-since=5') 
   (out, err) = capfd.readouterr() 
   assert ('project0' in out) 
   assert ('project1' in out)"," 'Test that projects are listed that have been modified since 5 seconds 
 ago.'",'Projects modified since a revision'
"def get_modules_containing_name(evaluator, mods, name): 
    def check_python_file(path): 
      try: 
         return parser_cache[path].parser.module 
      except KeyError: 
         try: 
            return check_fs(path) 
         except IOError: 
            return None 
   def check_fs(path): 
      with open(path, 'rb') as f: 
         source = source_to_unicode(f.read()) 
         if (name in source): 
            module_name = os.path.basename(path)[:(-3)] 
            module = _load_module(evaluator, path, source) 
            add_module(evaluator, module_name, module) 
            return module 
   mods = set((m for m in mods if (not isinstance(m, compiled.CompiledObject)))) 
   mod_paths = set() 
   for m in mods: 
      mod_paths.add(m.path) 
      (yield m) 
   if settings.dynamic_params_for_other_modules: 
      paths = set(settings.additional_dynamic_modules) 
      for p in mod_paths: 
         if (p is not None): 
            d = os.path.dirname(os.path.abspath(p)) 
            for entry in os.listdir(d): 
               if (entry not in mod_paths): 
                  if entry.endswith('.py'): 
                     paths.add(((d + os.path.sep) + entry)) 
      for p in sorted(paths): 
         c = check_python_file(p) 
         if ((c is not None) and (c not in mods) and (not isinstance(c, compiled.CompiledObject))): 
            (yield c)"," 'Returns all modules containing a given name. 
 If the module is dynamically loaded, it will be returned. 
 If the module is not dynamically loaded, it will be added to the 
 set of modules to be dynamically loaded. 
 :param evaluator: Evaluator 
 :param mods: Modules to search in 
 :param name: Name to search for 
 :rtype: set of Module objects'",'Search a name in the directories of modules.'
"def get_next_disk_info(mapping, disk_bus, device_type='disk', boot_index=None, assigned_devices=None): 
    disk_dev = find_disk_dev_for_disk_bus(mapping, disk_bus, assigned_devices) 
   info = {'bus': disk_bus, 'dev': disk_dev, 'type': device_type} 
   if ((boot_index is not None) and (boot_index >= 0)): 
      info['boot_index'] = str(boot_index) 
   return info"," 'Returns information about the next disk on the given bus. 
 :param mapping: The mapping of disk devices to their buses. 
 :param disk_bus: The bus to query. 
 :param device_type: The device type to query. 
 :param boot_index: The boot index to query. 
 :param assigned_devices: The list of devices to query. 
 :return: A dictionary containing the information about the next disk. 
 :rtype: dict'","'Determine the disk info for the next device on disk_bus. 
 Considering the disks already listed in the disk mapping, 
 determine the next available disk dev that can be assigned 
 for the disk bus. 
 Returns the disk_info for the next available disk.'"
"def atleast_2d(*arys): 
    res = [] 
   for a in arys: 
      if (not isinstance(a, cupy.ndarray)): 
         raise TypeError('Only   cupy   arrays   can   be   atleast_2d') 
      if (a.ndim == 0): 
         a = a.reshape(1, 1) 
      elif (a.ndim == 1): 
         a = a[None, :] 
      res.append(a) 
   if (len(res) == 1): 
      res = res[0] 
   return res", 'Return a 2D array containing at least 2 dimensions.',"'Converts arrays to arrays with dimensions >= 2. 
 If an input array has dimensions less than two, then this function inserts 
 new axes at the head of dimensions to make it have two dimensions. 
 Args: 
 arys (tuple of arrays): Arrays to be converted. All arguments must be 
 :class:`cupy.ndarray` objects. 
 Returns: 
 If there are only one input, then it returns its converted version. 
 Otherwise, it returns a list of converted arrays. 
 .. seealso:: :func:`numpy.atleast_2d`'"
"def last_month_day(tm): 
    (year, month) = tm[:2] 
   day = _DAYS[month] 
   if ((day == 28) and ((year % 4) == 0)): 
      day = 29 
   return day"," 'Returns the last day of the given month. 
 This function is used to calculate the last day of the month for 
 the year 2000 and 2004. 
 >>> last_month_day(datetime(2000, 1, 1)) 
 31 
 >>> last_month_day(datetime(2004, 1, 1)) 
 29'",'Return last day of this month'
"def set_block_scalar_indent(TokenClass): 
    def callback(lexer, match, context): 
      text = match.group() 
      context.block_scalar_indent = None 
      if (not text): 
         return 
      increment = match.group(1) 
      if increment: 
         current_indent = max(context.indent, 0) 
         increment = int(increment) 
         context.block_scalar_indent = (current_indent + increment) 
      if text: 
         (yield (match.start(), TokenClass, text)) 
         context.pos = match.end() 
   return callback"," 'Set the block scalar indent of the lexer. 
 This callback is used to set the block scalar indent of the lexer. 
 The block scalar indent is the number of spaces that should be added 
 to the start of the current block. 
 :param TokenClass: the token class that should be modified 
 :param callback: a callback that sets the block scalar indent 
 :return: the callback 
 :rtype: Callback'",'Set an explicit indentation level for a block scalar.'
"@require_context 
 def vol_get_usage_by_time(context, begin): 
    return model_query(context, models.VolumeUsage, read_deleted='yes').filter(or_((models.VolumeUsage.tot_last_refreshed == None), (models.VolumeUsage.tot_last_refreshed > begin), (models.VolumeUsage.curr_last_refreshed == None), (models.VolumeUsage.curr_last_refreshed > begin))).all()"," 'Returns the usage of volumes by time, since the last refresh.'",'Return volumes usage that have been updated after a specified time.'
"def role_list(request): 
    return keystoneclient(request, admin=True).roles.list()", 'List all roles in the tenant.','Returns a global list of available roles.'
"def run_discovery(entry_points_iter, cached=False): 
    reg_cache = {} 
   if cached: 
      reg_cache = cache.registry_cache() 
   discovery = QtWidgetDiscovery(cached_descriptions=reg_cache) 
   registry = QtWidgetRegistry() 
   discovery.found_category.connect(registry.register_category) 
   discovery.found_widget.connect(registry.register_widget) 
   discovery.run() 
   if cached: 
      cache.save_registry_cache(reg_cache) 
   return registry"," 'Returns a registry containing all discovered widgets. 
 :param entry_points_iter: An iterable of entry points 
 :param cached: If True, the registry will be cached, otherwise it 
 will be discarded after the function returns. 
 :return: A registry containing all discovered widgets'","'Run the default discovery and return an instance of 
 :class:`QtWidgetRegistry`.'"
"def _reduce_function(func, globs): 
    if func.__closure__: 
      cells = [cell.cell_contents for cell in func.__closure__] 
   else: 
      cells = None 
   return (_reduce_code(func.__code__), globs, func.__name__, cells)", 'Helper function to reduce a function',"'Reduce a Python function and its globals to picklable components. 
 If there are cell variables (i.e. references to a closure), their 
 values will be frozen.'"
"def replace(s, old, new, maxsplit=0): 
    return s.replace(old, new, maxsplit)"," 'Replace all occurences of `old` with `new` in `s`. 
 :param s: 
 :param old: 
 :param new: 
 :param maxsplit: 
 :return: 
 :rtype: 
 :raise ValueError: 
 :raise TypeError: 
 :raise KeyError: 
 :raise IndexError: 
 :raise UnicodeDecodeError: 
 :raise UnicodeEncodeError: 
 :raise AttributeError: 
 :raise LookupError: 
 :raise OverflowError: 
 :raise IOError: 
 :raise ValueError: 
 :raise TypeError: 
 :raise KeyError: 
 :raise IndexError: 
 :raise UnicodeDecodeError: 
 :raise UnicodeEncodeError: 
 :raise AttributeError: 
 :raise LookupError: 
 :raise OverflowError: 
 :raise IOError: 
 :raise ValueError: 
 :raise TypeError: 
 :raise KeyError: 
 :raise IndexError: 
 :raise UnicodeDecodeError: 
 :raise UnicodeEncodeError: 
 :raise AttributeError: 
 :","'replace (str, old, new[, maxsplit]) -> string 
 Return a copy of string str with all occurrences of substring 
 old replaced by new. If the optional argument maxsplit is 
 given, only the first maxsplit occurrences are replaced.'"
"@inlineCallbacks 
 def main(reactor, args, base_path, top_level): 
    options = RunOptions(top_level=top_level) 
   configure_eliot_logging_for_acceptance() 
   try: 
      options.parseOptions(args) 
   except UsageError as e: 
      sys.stderr.write(('%s:   %s\n' % (base_path.basename(), e))) 
      raise SystemExit(1) 
   runner = options.runner 
   def cluster_cleanup(): 
      print 'stopping   cluster' 
      return runner.stop_cluster(reactor) 
   cleanup_trigger_id = reactor.addSystemEventTrigger('before', 'shutdown', cluster_cleanup) 
   from flocker.common.script import eliot_logging_service 
   log_writer = eliot_logging_service(destination=FileDestination(file=open(('%s.log' % (base_path.basename(),)), 'a')), reactor=reactor, capture_stdout=False) 
   log_writer.startService() 
   reactor.addSystemEventTrigger('before', 'shutdown', log_writer.stopService) 
   (yield runner.ensure_keys(reactor)) 
   cluster = (yield runner.start_cluster(reactor)) 
   save_managed_config(options['cert-directory'], options['config'], cluster) 
   managed_config_file = options['cert-directory'].child('managed.yaml') 
   managed_config = create_managed_config(options['config'], cluster) 
   managed_config_file.setContent(yaml.safe_dump(managed_config, default_flow_style=False)) 
   if (options['distribution'] in ('centos-7',)): 
      remote_logs_file = open('remote_logs.log', 'a') 
      for node in cluster.all_nodes: 
         capture_journal(reactor, node.address, remote_logs_file).addErrback(write_failure) 
   elif (options['distribution'] in ('ubuntu-14.04',)): 
      remote_logs_file = open('remote_logs.log', 'a') 
      for node in cluster.all_nodes: 
         capture_upstart(reactor, node.address, remote_logs_file).addErrback(write_failure) 
   flocker_client = make_client(reactor, cluster) 
   (yield wait_for_nodes(reactor, flocker_client, len(cluster.agent_nodes))) 
   if options['no-keep']: 
      print 'not   keeping   cluster' 
   else: 
      save_environment(options['cert-directory'], cluster, options.package_source()) 
      reactor.removeSystemEventTrigger(cleanup_trigger_id)", 'Run the cluster.',"':param reactor: Reactor to use. 
 :param list args: The arguments passed to the script. 
 :param FilePath base_path: The executable being run. 
 :param FilePath top_level: The top-level of the Flocker repository.'"
"def hilbert(x, N=None, axis=(-1)): 
    x = asarray(x) 
   if iscomplexobj(x): 
      raise ValueError('x   must   be   real.') 
   if (N is None): 
      N = x.shape[axis] 
   if (N <= 0): 
      raise ValueError('N   must   be   positive.') 
   Xf = fftpack.fft(x, N, axis=axis) 
   h = zeros(N) 
   if ((N % 2) == 0): 
      h[0] = h[(N // 2)] = 1 
      h[1:(N // 2)] = 2 
   else: 
      h[0] = 1 
      h[1:((N + 1) // 2)] = 2 
   if (x.ndim > 1): 
      ind = ([newaxis] * x.ndim) 
      ind[axis] = slice(None) 
      h = h[ind] 
   x = fftpack.ifft((Xf * h), axis=axis) 
   return x"," 'Hilbert transform. 
 Parameters 
 x : 1D array 
 Input signal. 
 N : int, optional 
 Number of points in the signal. 
 axis : int, optional 
 Axis along which to apply the transform. 
 Returns 
 x : 1D array 
 Hilbert transform of x. 
 See Also 
 fftpack.hilbert'","'Compute the analytic signal, using the Hilbert transform. 
 The transformation is done along the last axis by default. 
 Parameters 
 x : array_like 
 Signal data.  Must be real. 
 N : int, optional 
 Number of Fourier components.  Default: ``x.shape[axis]`` 
 axis : int, optional 
 Axis along which to do the transformation.  Default: -1. 
 Returns 
 xa : ndarray 
 Analytic signal of `x`, of each 1-D array along `axis` 
 See Also 
 scipy.fftpack.hilbert : Return Hilbert transform of a periodic sequence x. 
 Notes 
 The analytic signal ``x_a(t)`` of signal ``x(t)`` is: 
 .. math:: x_a = F^{-1}(F(x) 2U) = x + i y 
 where `F` is the Fourier transform, `U` the unit step function, 
 and `y` the Hilbert transform of `x`. [1]_ 
 In other words, the negative half of the frequency spectrum is zeroed 
 out, turning the real-valued signal into a complex signal.  The Hilbert 
 transformed signal can be obtained from ``np.imag(hilbert(x))``, and the 
 original signal from ``np.real(hilbert(x))``. 
 Examples 
 In this example we use the Hilbert transform to determine the amplitude 
 envelope and instantaneous frequency of an amplitude-modulated signal. 
 >>> import numpy as np 
 >>> import matplotlib.pyplot as plt 
 >>> from scipy.signal import hilbert, chirp 
 >>> duration = 1.0 
 >>> fs = 400.0 
 >>> samples = int(fs*duration) 
 >>> t = np.arange(samples) / fs 
 We create a chirp of which the frequency increases from 20 Hz to 100 Hz and 
 apply an amplitude modulation. 
 >>> signal = chirp(t, 20.0, t[-1], 100.0) 
 >>> signal *= (1.0 + 0.5 * np.sin(2.0*np.pi*3.0*t) ) 
 The amplitude envelope is given by magnitude of the analytic signal. The 
 instantaneous frequency can be obtained by differentiating the 
 instantaneous phase in respect to time. The instantaneous phase corresponds 
 to the phase angle of the analytic signal. 
 >>> analytic_signal = hilbert(signal) 
 >>> amplitude_envelope = np.abs(analytic_signal) 
 >>> instantaneous_phase = np.unwrap(np.angle(analytic_signal)) 
 >>> instantaneous_frequency = (np.diff(instantaneous_phase) / 
 ...                            (2.0*np.pi) * fs) 
 >>> fig = plt.figure() 
 >>> ax0 = fig.add_subplot(211) 
 >>> ax0.plot(t, signal, label=\'signal\') 
 >>> ax0.plot(t, amplitude_envelope, label=\'envelope\') 
 >>> ax0.set_xlabel(""time in seconds"") 
 >>> ax0.legend() 
 >>> ax1 = fig.add_subplot(212) 
 >>> ax1.plot(t[1:], instantaneous_frequency) 
 >>> ax1.set_xlabel(""time in seconds"") 
 >>> ax1.set_ylim(0.0, 120.0) 
 References 
 .. [1] Wikipedia, ""Analytic signal"". 
 http://en.wikipedia.org/wiki/Analytic_signal 
 .. [2] Leon Cohen, ""Time-Frequency Analysis"", 1995. Chapter 2. 
 .. [3] Alan V. Oppenheim, Ronald W. Schafer. Discrete-Time Signal 
 Processing, Third Edition, 2009. Chapter 12. 
 ISBN 13: 978-1292-02572-8'"
"def unpickleStringO(val, sek): 
    x = _cStringIO() 
   x.write(val) 
   x.seek(sek) 
   return x"," 'unpickleStringO(val, sek) -> StringIO 
 Return a StringIO object containing the string value of val at the 
 given seek position in the pickled stream.'","'Convert the output of L{pickleStringO} into an appropriate type for the 
 current python version.  This may be called on Python 3 and will convert a 
 cStringIO into an L{io.StringIO}. 
 @param val: The content of the file. 
 @type val: L{bytes} 
 @param sek: The seek position of the file. 
 @type sek: L{int} 
 @return: a file-like object which you can write bytes to. 
 @rtype: L{cStringIO.OutputType} on Python 2, L{io.StringIO} on Python 3.'"
"@FileSystem.in_directory(current_directory, 'django', 'alfaces') 
 def test_django_background_server_running_in_background(): 
    import tornado.ioloop 
   import tornado.web 
   class MainHandler(tornado.web.RequestHandler, ): 
      def get(self): 
         self.write('Hello,   world') 
         raise SystemExit() 
   def runserver(): 
      application = tornado.web.Application([('/', MainHandler)]) 
      application.listen(8000) 
      tornado.ioloop.IOLoop.instance().start() 
   server = multiprocessing.Process(target=runserver) 
   server.start() 
   time.sleep(1) 
   e = 'Lettuce   could   not   run   the   builtin   Django   server   at   0.0.0.0:8000""\nmaybe   you   forgot   a   ""runserver""   instance   running   ?\n\nwell   if   you   really   do   not   want   lettuce   to   run   the   server   for   you,   then   just   run:\n\npython   manage.py   --no-server' 
   try: 
      (status, out) = commands.getstatusoutput('python   manage.py   harvest   --verbosity=3   --no-color') 
      assert_equals(out, e) 
      assert_not_equals(status, 0) 
   finally: 
      os.kill(server.pid, 9)", 'Test that the django background server is running','the django builtin server fails if the HTTP port is not available'
"def do_lower(s): 
    return soft_unicode(s).lower()"," 'Returns a lowercased version of the string. 
 This is a soft unicode version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the string. 
 This is a lowercased version of the",'Convert a value to lowercase.'
"def human_readable_to_bytes(value): 
    value = value.lower() 
   if (value[(-2):] == 'ib'): 
      suffix = value[(-3):].lower() 
   else: 
      suffix = value[(-2):].lower() 
   has_size_identifier = ((len(value) >= 2) and (suffix in SIZE_SUFFIX)) 
   if (not has_size_identifier): 
      try: 
         return int(value) 
      except ValueError: 
         raise ValueError(('Invalid   size   value:   %s' % value)) 
   else: 
      multiplier = SIZE_SUFFIX[suffix] 
      return (int(value[:(- len(suffix))]) * multiplier)"," 'Convert a human-readable size to bytes. 
 :param value: Human-readable size in bytes. 
 :return: Integer in bytes.'","'Converts a human readable size to bytes. 
 :param value: A string such as ""10MB"".  If a suffix is not included, 
 then the value is assumed to be an integer representing the size 
 in bytes. 
 :returns: The converted value in bytes as an integer'"
"@mock_streams('stdout') 
 def test_global_parallel_honors_runs_once(): 
    @decorators.runs_once 
   def mytask(): 
      print 'yolo' 
   with settings(hide('everything'), parallel=True): 
      execute(mytask, hosts=['localhost', '127.0.0.1']) 
   result = sys.stdout.getvalue() 
   eq_(result, 'yolo\n') 
   assert (result != 'yolo\nyolo\n')", 'Test that parallel tasks are run once per host','fab -P (or env.parallel) should honor @runs_once'
"def request_latency(results, limit): 
    scenario_results = [r['scenario'] for r in results if (r['scenario'].get('metrics') and r['scenario']['metrics'].get('call_durations'))] 
   if (len(scenario_results) > 0): 
      unique_metrics = [] 
      for result in scenario_results: 
         if (result['metrics'] not in unique_metrics): 
            unique_metrics.append(result['metrics']) 
      total_requests = 0 
      requests_under_limit = 0 
      for metric in unique_metrics: 
         for (duration, num_requests) in metric['call_durations'].iteritems(): 
            if (float(duration) <= limit): 
               requests_under_limit += num_requests 
         total_requests += (metric['ok_count'] + metric['err_count']) 
      return (float(requests_under_limit) / total_requests) 
   return None"," 'Returns the percentage of requests that took less than the given 
 limit in time. 
 :param results: The results from the test 
 :param limit: The maximum allowed time for a request to take 
 :returns: A float in the range of (0,1]'","'Calculate the percentage of scenario requests have a latency under the 
 specified time limit. 
 :param results: Results to extract values from. 
 :param limit: Request latency limit in seconds.'"
"@with_setup(prepare_stdout, registry.clear) 
 def test_jsonreport_output_with_no_steps(): 
    with check_jsonreport(u'missing_steps'): 
      runner = Runner(feature_name(u'missing_steps'), enable_jsonreport=True) 
      runner.run()", 'Verify that the output is the same as the jsonreport.','Test jsonreport output with no steps'
"def is_full_slice(obj, l): 
    return (isinstance(obj, slice) and (obj.start == 0) and (obj.stop == l) and (obj.step is None))", 'Returns True if obj is a slice and it is a full slice.','we have a full length slice'
"def second(seq): 
    return next(itertools.islice(seq, 1, None))", 'Return the second element of a sequence.',"'The second element in a sequence 
 >>> second(\'ABC\') 
 \'B\''"
"@contextmanager 
 def collect_profile(file_prefix): 
    import cProfile 
   import uuid 
   profiler = cProfile.Profile() 
   profiler.enable() 
   try: 
      (yield) 
   finally: 
      profiler.disable() 
      profiler.dump_stats('{0}_{1}_master.profile'.format(file_prefix, uuid.uuid4()))"," 'Context manager that collects a profile of the code block. 
 This is a convenient way to profile the code that is executed 
 within a context manager. 
 :param file_prefix: prefix to use for the file name. 
 :type file_prefix: str 
 :returns: 
 :rtype: context manager'",'Context manager to collect profile information.'
"def guard_null(context, builder, value, exc_tuple): 
    with builder.if_then(is_scalar_zero(builder, value), likely=False): 
      exc = exc_tuple[0] 
      exc_args = (exc_tuple[1:] or None) 
      context.call_conv.return_user_exc(builder, exc, exc_args)"," 'Guard against null values. 
 If the value is null, raise an exception. 
 :param builder: The builder to use. 
 :param value: The value to check. 
 :param exc_tuple: The tuple to use for the exception. 
 :param context: The context to use. 
 :return: The builder.'","'Guard against *value* being null or zero. 
 *exc_tuple* should be a (exception type, arguments...) tuple.'"
"def ode_separable(eq, func, order, match): 
    x = func.args[0] 
   f = func.func 
   C1 = get_numbered_constants(eq, num=1) 
   r = match 
   u = r.get('hint', f(x)) 
   return Eq(Integral(((r['m2']['coeff'] * r['m2'][r['y']]) / r['m1'][r['y']]), (r['y'], None, u)), (Integral((((- r['m1']['coeff']) * r['m1'][x]) / r['m2'][x]), x) + C1))"," 'ODE separable: 
 Given an ODE in the form of 
 .. math:: f(x,y) = \sum_{i=1}^{n} \frac{\partial f}{\partial y_i}(x,y) y_i + \sum_{i=1}^{n} \frac{\partial f}{\partial x_i}(x,y) x_i = 0 
 return the ODE in the form of 
 .. math:: \int \frac{d y_i}{f(x,y)} = \int \frac{d x_i}{f(x,y)} 
 where ``n`` is the order of the ODE. 
 :param eq: 
 :param func: 
 :param order: 
 :param match: 
 :return: 
 :rtype: 
 :example: 
 >>> from sympy import ode, diff, integrate, sin, cos, Function, ode_separable 
 >>> from sympy.abc import x, y 
 >>> from sympy.abc import a, b 
 >>> f = Function(""f"") 
 >>> g = Function(""g"") ","'Solves separable 1st order differential equations. 
 This is any differential equation that can be written as `P(y) 
 \tfrac{dy}{dx} = Q(x)`.  The solution can then just be found by 
 rearranging terms and integrating: `\int P(y) \,dy = \int Q(x) \,dx`. 
 This hint uses :py:meth:`sympy.simplify.simplify.separatevars` as its back 
 end, so if a separable equation is not caught by this solver, it is most 
 likely the fault of that function. 
 :py:meth:`~sympy.simplify.simplify.separatevars` is 
 smart enough to do most expansion and factoring necessary to convert a 
 separable equation `F(x, y)` into the proper form `P(x)\cdot{}Q(y)`.  The 
 general solution is:: 
 >>> from sympy import Function, dsolve, Eq, pprint 
 >>> from sympy.abc import x 
 >>> a, b, c, d, f = map(Function, [\'a\', \'b\', \'c\', \'d\', \'f\']) 
 >>> genform = Eq(a(x)*b(f(x))*f(x).diff(x), c(x)*d(f(x))) 
 >>> pprint(genform) 
 d 
 a(x)*b(f(x))*--(f(x)) = c(x)*d(f(x)) 
 dx 
 >>> pprint(dsolve(genform, f(x), hint=\'separable_Integral\')) 
 f(x) 
 |  b(y)            | c(x) 
 |  ---- dy = C1 +  | ---- dx 
 |  d(y)            | a(x) 
 Examples 
 >>> from sympy import Function, dsolve, Eq 
 >>> from sympy.abc import x 
 >>> f = Function(\'f\') 
 >>> pprint(dsolve(Eq(f(x)*f(x).diff(x) + x, 3*x*f(x)**2), f(x), 
 ... hint=\'separable\', simplify=False)) 
 /   2       \         2 
 log\3*f (x) - 1/        x 
 ---------------- = C1 + -- 
 6                2 
 References 
 - M. Tenenbaum & H. Pollard, ""Ordinary Differential Equations"", 
 Dover 1963, pp. 52 
 # indirect doctest'"
"def _update_rs_from_primary(sds, replica_set_name, server_description, max_set_version, max_election_id): 
    if (replica_set_name is None): 
      replica_set_name = server_description.replica_set_name 
   elif (replica_set_name != server_description.replica_set_name): 
      sds.pop(server_description.address) 
      return (_check_has_primary(sds), replica_set_name, max_set_version, max_election_id) 
   max_election_tuple = (max_set_version, max_election_id) 
   if (None not in server_description.election_tuple): 
      if ((None not in max_election_tuple) and (max_election_tuple > server_description.election_tuple)): 
         address = server_description.address 
         sds[address] = ServerDescription(address) 
         return (_check_has_primary(sds), replica_set_name, max_set_version, max_election_id) 
      max_election_id = server_description.election_id 
   if ((server_description.set_version is not None) and ((max_set_version is None) or (server_description.set_version > max_set_version))): 
      max_set_version = server_description.set_version 
   for server in sds.values(): 
      if ((server.server_type is SERVER_TYPE.RSPrimary) and (server.address != server_description.address)): 
         sds[server.address] = ServerDescription(server.address) 
         break 
   for new_address in server_description.all_hosts: 
      if (new_address not in sds): 
         sds[new_address] = ServerDescription(new_address) 
   for addr in (set(sds) - server_description.all_hosts): 
      sds.pop(addr) 
   return (_check_has_primary(sds), replica_set_name, max_set_version, max_election_id)"," 'Update the replica set description from the primary description 
 :param sds: ServerDescription list 
 :param replica_set_name: Replica set name 
 :param server_description: Primary description 
 :param max_set_version: Maximum version of the replica set 
 :param max_election_id: Maximum election id of the replica set 
 :return: True if the primary description is updated, False otherwise 
 :rtype: bool'","'Update topology description from a primary\'s ismaster response. 
 Pass in a dict of ServerDescriptions, current replica set name, the 
 ServerDescription we are processing, and the TopologyDescription\'s 
 max_set_version and max_election_id if any. 
 Returns (new topology type, new replica_set_name, new max_set_version, 
 new max_election_id).'"
"def langnames_to_langcodes(names): 
    iso639 = _load_iso639() 
   translate = _ 
   ans = {} 
   names = set(names) 
   for (k, v) in iso639['by_3t'].iteritems(): 
      tv = translate(v) 
      if (tv in names): 
         names.remove(tv) 
         ans[tv] = k 
      if (not names): 
         break 
   for x in names: 
      ans[x] = None 
   return ans", 'Return a dict from language names to ISO-639 codes.',"'Given a list of localized language names return a mapping of the names to 3 
 letter ISO 639 language codes. If a name is not recognized, it is mapped to 
 None.'"
"@pytest.fixture() 
 def celery_app(request, celery_config, celery_parameters, celery_enable_logging, use_celery_app_trap): 
    mark = request.node.get_marker(u'celery') 
   config = dict(celery_config, **(mark.kwargs if mark else {})) 
   with _create_app(request, enable_logging=celery_enable_logging, use_trap=use_celery_app_trap, parameters=celery_parameters, **config) as app: 
      (yield app)"," 'A fixture that returns a Celery application. 
 This fixture can be used to run tests in a Celery application context. 
 The fixture is automatically configured based on the `celery` marker 
 of the test function. 
 :param request: The current request. 
 :param celery_config: The Celery configuration dictionary. 
 :param celery_parameters: The Celery application parameters. 
 :param celery_enable_logging: Whether to enable logging. 
 :param use_celery_app_trap: Whether to use the Celery application trap. 
 :returns: A Celery application. 
 :rtype: :class:`celery.app.Celery'",'Fixture creating a Celery application instance.'
"def checkMatch(input, prediction, sparse=True, verbosity=0): 
    if sparse: 
      activeElementsInInput = set(input) 
      activeElementsInPrediction = set(prediction) 
   else: 
      activeElementsInInput = set(input.nonzero()[0]) 
      activeElementsInPrediction = set(prediction.nonzero()[0]) 
   totalActiveInPrediction = len(activeElementsInPrediction) 
   totalActiveInInput = len(activeElementsInInput) 
   foundInInput = len(activeElementsInPrediction.intersection(activeElementsInInput)) 
   missingFromInput = len(activeElementsInPrediction.difference(activeElementsInInput)) 
   missingFromPrediction = len(activeElementsInInput.difference(activeElementsInPrediction)) 
   if (verbosity >= 1): 
      print 'preds.   found   in   input:', foundInInput, 'out   of', totalActiveInPrediction, 
      print ';   preds.   missing   from   input:', missingFromInput, 'out   of', totalActiveInPrediction, 
      print ';   unexpected   active   in   input:', missingFromPrediction, 'out   of', totalActiveInInput 
   return (foundInInput, totalActiveInInput, missingFromInput, totalActiveInPrediction)"," 'Check the match between prediction and input. 
 The input is a set of active elements, and the prediction is a set of active 
 elements. 
 :param input: The input. 
 :type input: set 
 :param prediction: The prediction. 
 :type prediction: set 
 :param sparse: If True, the input and prediction are sets of active elements. 
 :type sparse: bool 
 :param verbosity: The verbosity level. 
 :type verbosity: int 
 :return: The match. 
 :rtype: tuple'","'Compares the actual input with the predicted input and returns results 
 Parameters: 
 input:          The actual input 
 prediction:     the predicted input 
 verbosity:        If > 0, print debugging messages 
 sparse:         If true, they are in sparse form (list of 
 active indices) 
 retval         (foundInInput, totalActiveInInput, missingFromInput, 
 totalActiveInPrediction) 
 foundInInput:       The number of predicted active elements that were 
 found in the actual input 
 totalActiveInInput: The total number of active elements in the input. 
 missingFromInput:   The number of predicted active elements that were not 
 found in the actual input 
 totalActiveInPrediction:  The total number of active elements in the prediction'"
"def _user_has_module_perms(user, app_label): 
    for backend in auth.get_backends(): 
      if (not hasattr(backend, 'has_module_perms')): 
         continue 
      try: 
         if backend.has_module_perms(user, app_label): 
            return True 
      except PermissionDenied: 
         return False 
   return False"," 'Return True if the user has the required permissions for the app_label 
 provided. 
 If the backend does not have a has_module_perms method, the function will 
 not attempt to check permissions.'",'A backend can raise `PermissionDenied` to short-circuit permission checking.'
"def tostring(raw, **kwargs): 
    xml_declaration = kwargs.pop(u'xml_declaration', False) 
   encoding = kwargs.pop(u'encoding', u'UTF-8') 
   kwargs[u'encoding'] = unicode 
   kwargs[u'xml_declaration'] = False 
   ans = etree.tostring(raw, **kwargs) 
   if xml_declaration: 
      ans = ((u'<?xml   version=""1.0""   encoding=""%s""?>\n' % encoding) + ans) 
   return re.sub(u'&#x([0-9A-Fa-f]+);', (lambda m: mychr(int(m.group(1), 16))), ans).encode(encoding)"," 'Convert an etree.Element to a string. 
 :param raw: an etree.Element 
 :param encoding: the encoding to use (defaults to UTF-8) 
 :param xml_declaration: whether to include the XML declaration 
 :return: a string'","'lxml *sometimes* represents non-ascii characters as hex entities in 
 attribute values. I can\'t figure out exactly what circumstances cause it. 
 It seems to happen when serializing a part of a larger tree. Since we need 
 serialization to be the same when serializing full and partial trees, we 
 manually replace all hex entities with their unicode codepoints.'"
"def decode_entities(html): 
    def decode(m): 
      html = m.group(0) 
      if (html[:2] == u'&#'): 
         try: 
            if (html[:3] == u'&#x'): 
               return chr(int(html[3:(-1)], 16)) 
            else: 
               return chr(int(html[2:(-1)])) 
         except ValueError: 
            pass 
      else: 
         try: 
            html = chr(name2codepoint[html[1:(-1)]]) 
         except KeyError: 
            pass 
      return html 
   return re.sub(u'&#?\\w+;', decode, html.replace(u'&amp;', u'&'))", 'Decode HTML entities.',"'Remove HTML entities from a string. 
 Adapted from http://effbot.org/zone/re-sub.htm#unescape-html'"
"@with_setup(setup, teardown) 
 def test_show_negative_chains(): 
    negative_chains.show_negative_chains('dbm.pkl')", 'Test negative chains','Test the show_negative_chains script main function'
"def escape_ajax(url): 
    (defrag, frag) = urldefrag(url) 
   if (not frag.startswith('!')): 
      return url 
   return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])", 'Escape an ajax url.',"'Return the crawleable url according to: 
 http://code.google.com/web/ajaxcrawling/docs/getting-started.html 
 >>> escape_ajax(""www.example.com/ajax.html#!key=value"") 
 \'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\' 
 >>> escape_ajax(""www.example.com/ajax.html?k1=v1&k2=v2#!key=value"") 
 \'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue\' 
 >>> escape_ajax(""www.example.com/ajax.html?#!key=value"") 
 \'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\' 
 >>> escape_ajax(""www.example.com/ajax.html#!"") 
 \'www.example.com/ajax.html?_escaped_fragment_=\' 
 URLs that are not ""AJAX crawlable"" (according to Google) returned as-is: 
 >>> escape_ajax(""www.example.com/ajax.html#key=value"") 
 \'www.example.com/ajax.html#key=value\' 
 >>> escape_ajax(""www.example.com/ajax.html#"") 
 \'www.example.com/ajax.html#\' 
 >>> escape_ajax(""www.example.com/ajax.html"") 
 \'www.example.com/ajax.html\''"
"def verbose_lookup_expr(lookup_expr): 
    from .conf import settings as app_settings 
   VERBOSE_LOOKUPS = (app_settings.VERBOSE_LOOKUPS or {}) 
   lookups = [force_text(VERBOSE_LOOKUPS.get(lookup, _(lookup))) for lookup in lookup_expr.split(LOOKUP_SEP)] 
   return '   '.join(lookups)", 'Returns a verbose representation of the lookup_expr.',"'Get a verbose, more humanized expression for a given ``lookup_expr``. 
 Each part in the expression is looked up in the ``FILTERS_VERBOSE_LOOKUPS`` 
 dictionary. Missing keys will simply default to itself. 
 ex:: 
 >>> verbose_lookup_expr(\'year__lt\') 
 \'year is less than\' 
 # with `FILTERS_VERBOSE_LOOKUPS = {}` 
 >>> verbose_lookup_expr(\'year__lt\') 
 \'year lt\''"
"def insured(pool, fun, args, kwargs, errback=None, on_revive=None, **opts): 
    errback = (errback or _ensure_errback) 
   with pool.acquire(block=True) as conn: 
      conn.ensure_connection(errback=errback) 
      channel = conn.default_channel 
      revive = partial(revive_connection, conn, on_revive=on_revive) 
      insured = conn.autoretry(fun, channel, errback=errback, on_revive=revive, **opts) 
      (retval, _) = insured(*args, **dict(kwargs, connection=conn)) 
      return retval"," 'Returns a future for a connection. 
 :param pool: A connection pool. 
 :param fun: A function that takes a connection as an argument and 
 returns a future. 
 :param args: Arguments to pass to the function. 
 :param kwargs: Keyword arguments to pass to the function. 
 :param errback: A callback to invoke if the connection is not available. 
 :param on_revive: A callback to invoke when the connection is available. 
 :param opts: Additional keyword arguments to pass to the function. 
 :returns: A future representing the connection. 
 :rtype: :class:`concurrent.futures.Future` 
 :raises: :class:`~.ConnectionError` if the connection is not available.'","'Ensures function performing broker commands completes 
 despite intermittent connection failures.'"
"def read_png_depth(filename): 
    result = None 
   f = open(filename, 'rb') 
   try: 
      f.seek((- (LEN_IEND + LEN_DEPTH)), 2) 
      depthchunk = f.read(LEN_DEPTH) 
      if (not depthchunk.startswith((DEPTH_CHUNK_LEN + DEPTH_CHUNK_START))): 
         return None 
      result = struct.unpack('!i', depthchunk[14:18])[0] 
   finally: 
      f.close() 
   return result"," 'Read the depth from a PNG file. 
 :param filename: The filename to read. 
 :returns: The depth of the image, or None if the image is not a PNG file.'",'Read the special tEXt chunk indicating the depth from a PNG file.'
"def _infer_decorator_callchain(node): 
    if (not isinstance(node, Function)): 
      return 
   if (not node.parent): 
      return 
   try: 
      result = next(node.infer_call_result(node.parent)) 
   except (StopIteration, InferenceError): 
      return 
   if isinstance(result, Instance): 
      result = result._proxied 
   if isinstance(result, Class): 
      if result.is_subtype_of(('%s.classmethod' % BUILTINS)): 
         return 'classmethod' 
      if result.is_subtype_of(('%s.staticmethod' % BUILTINS)): 
         return 'staticmethod'"," 'Returns the decorator that was applied to the function. 
 If the function was decorated with a classmethod or staticmethod 
 decorator, the decorator is returned. 
 If the function was decorated with a function decorator, the name of 
 the decorator is returned.'","'Detect decorator call chaining and see if the end result is a 
 static or a classmethod.'"
"def txt_records_for_name(name): 
    if (not DNS_AVAILABLE): 
      raise errors.DependencyError('{0}   is   required   to   use   this   function'.format(DNS_REQUIREMENT)) 
   try: 
      dns_response = dns.resolver.query(name, 'TXT') 
   except dns.resolver.NXDOMAIN as error: 
      return [] 
   except dns.exception.DNSException as error: 
      logger.error('Error   resolving   %s:   %s', name, str(error)) 
      return [] 
   return [txt_rec.decode('utf-8') for rdata in dns_response for txt_rec in rdata.strings]"," 'Return a list of text records for the specified name. 
 :param name: the name to resolve 
 :type name: str 
 :return: a list of text records for the specified name'","'Resolve the name and return the TXT records. 
 :param unicode name: Domain name being verified. 
 :returns: A list of txt records, if empty the name could not be resolved 
 :rtype: list of unicode'"
"def spherical_yn(n, z, derivative=False): 
    if derivative: 
      return _spherical_yn_d(n, z) 
   else: 
      return _spherical_yn(n, z)"," 'Compute the spherical YN. 
 Parameters 
 n : int 
 The order of the spherical harmonics. 
 z : array 
 The spherical coordinates. 
 derivative : bool, optional 
 If True, the derivative of the spherical YN will be returned. 
 Returns 
 yn : float 
 The spherical YN. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Spherical_harmonics 
 Examples 
 >>> from sympy.physics.vector import spherical_yn 
 >>> from sympy.physics.vector import spherical_z 
 >>> from sympy.physics.vector import spherical_theta 
 >>> x = 3 
 >>> y = 4 
 >>> z = 5 
 >>> theta = 6 
 >>> spherical_yn(2, x, y, z, theta) 
 0.000000000000000000000000000000000000000000000","'Spherical Bessel function of the second kind or its derivative. 
 Defined as [1]_, 
 .. math:: y_n(z) = \sqrt{\frac{\pi}{2z}} Y_{n + 1/2}(z), 
 where :math:`Y_n` is the Bessel function of the second kind. 
 Parameters 
 n : int, array_like 
 Order of the Bessel function (n >= 0). 
 z : complex or float, array_like 
 Argument of the Bessel function. 
 derivative : bool, optional 
 If True, the value of the derivative (rather than the function 
 itself) is returned. 
 Returns 
 yn : ndarray 
 Notes 
 For real arguments, the function is computed using the ascending 
 recurrence [2]_.  For complex arguments, the definitional relation to 
 the cylindrical Bessel function of the second kind is used. 
 The derivative is computed using the relations [3]_, 
 .. math:: 
 y_n\' = y_{n-1} - \frac{n + 1}{2} y_n. 
 y_0\' = -y_1 
 .. versionadded:: 0.18.0 
 References 
 .. [1] http://dlmf.nist.gov/10.47.E4 
 .. [2] http://dlmf.nist.gov/10.51.E1 
 .. [3] http://dlmf.nist.gov/10.51.E2'"
"def write_csv_file(path, app_messages, lang_dict): 
    app_messages.sort((lambda x, y: cmp(x[1], y[1]))) 
   from csv import writer 
   with open(path, u'wb') as msgfile: 
      w = writer(msgfile, lineterminator=u'\n') 
      for (p, m) in app_messages: 
         t = lang_dict.get(m, u'') 
         t = re.sub(u'{\\s?([0-9]+)\\s?}', u'{\\g<1>}', t) 
         w.writerow([(p.encode(u'utf-8') if p else u''), m.encode(u'utf-8'), t.encode(u'utf-8')])"," 'Write a CSV file with the messages for a given app. 
 :param path: path to the CSV file 
 :param app_messages: a list of app, message pairs'","'Write translation CSV file. 
 :param path: File path, usually `[app]/translations`. 
 :param app_messages: Translatable strings for this app. 
 :param lang_dict: Full translated dict.'"
"def current_year_and_week(): 
    _update_week_number() 
   return (_cur_year, _cur_week)", 'Returns the current year and week number.',"'Returns a tuple (year, week) where year is the current year of the season 
 and week is the current week number of games being played. 
 i.e., (2012, 3). 
 N.B. This always downloads the schedule XML data.'"
"def both(): 
    return 23", 'Test both.','@rtype: str'
"def popen_wrapper(args, os_err_exc_type=CommandError): 
    try: 
      p = Popen(args, shell=False, stdout=PIPE, stderr=PIPE, close_fds=(os.name != 'nt'), universal_newlines=True) 
   except OSError as e: 
      six.reraise(os_err_exc_type, os_err_exc_type(('Error   executing   %s:   %s' % (args[0], e.strerror))), sys.exc_info()[2]) 
   (output, errors) = p.communicate() 
   return (output, force_text(errors, DEFAULT_LOCALE_ENCODING, strings_only=True), p.returncode)"," 'Wrapper around subprocess.Popen() that catches OSError exceptions 
 and re-raises them as CommandError. 
 :param args: the command to run 
 :type args: list 
 :param os_err_exc_type: the exception to raise if an error occurs 
 :type os_err_exc_type: type 
 :return: the output, error, and return code of the command 
 :rtype: (str, str, int)'","'Friendly wrapper around Popen. 
 Returns stdout output, stderr output and OS status code.'"
"def staticfiles_urlpatterns(prefix=None): 
    if (prefix is None): 
      prefix = settings.STATIC_URL 
   return static(prefix, view='django.contrib.staticfiles.views.serve')"," 'Serve static files from the STATIC_ROOT directory. 
 This is the default handler for serving static files. 
 The URL prefix is determined by the STATIC_URL setting, which defaults 
 to \'/static\'. 
 This view is not a WSGI view, and so does not need to be imported 
 from django.contrib.staticfiles.views. 
 .. versionadded:: 1.6 
 :param prefix: The prefix of the static URL. 
 .. versionchanged:: 1.6 
 The prefix is now a required parameter. 
 :type prefix: str 
 :rtype: list 
 :return: A list of url patterns for static files. 
 :raises ImproperlyConfigured: If STATIC_ROOT is not set.'",'Helper function to return a URL pattern for serving static files.'
"def match_patterns(pathname, patterns): 
    for pattern in patterns: 
      if fnmatch(pathname, pattern): 
         return True 
   return False"," 'Return True if the path matches any of the given patterns. 
 :param pathname: 
 :param patterns: 
 :return: 
 :rtype: 
 :raises: 
 :param patterns: 
 :param pathname: 
 :return: 
 :rtype: 
 :raises: 
 :param pathname: 
 :param patterns: 
 :return: 
 :rtype: 
 :raises: 
 :param pathname: 
 :param patterns: 
 :return: 
 :rtype: 
 :raises: 
 :param pathname: 
 :param patterns: 
 :return: 
 :rtype: 
 :raises: 
 :param pathname: 
 :param patterns: 
 :return: 
 :rtype: 
 :raises: 
 :param pathname: 
 :param patterns: 
 :return: 
 :rtype: 
 :raises: 
 :param pathname: 
 :param patterns: 
 :return: 
 :rtype: 
 :raises: 
 :",'Returns ``True`` if the pathname matches any of the given patterns.'
"def itemlist(tparams): 
    return [vv for (kk, vv) in tparams.iteritems()]", 'Returns a list of the item values for a dict of type parameters.',"'Get the list of parameters. 
 Note that tparams must be OrderedDict'"
"def tty(*args, **kwargs): 
    return 'ERROR:   This   function   has   been   moved   to   cmd.tty'", 'This function has been moved to cmd.tty',"'Deprecated! Moved to cmdmod. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' test.tty tty0 \'This is a test\' 
 salt \'*\' test.tty pts3 \'This is a test\''"
"def main(argv): 
    parser = optparse.OptionParser() 
   parser.add_option('-e', '--expire-date', dest='expire_date', type='int', default=30, help='number   of   days   before   builds   expire') 
   parser.add_option('-i', '--include', dest='include', type='str', action='append', help='Globs   of   files   to   include') 
   parser.add_option('-n', '--no-exec', dest='fake', action='store_true', help='Do   not   remove   files') 
   parser.add_option('-X', '--exclude', dest='exclude', type='str', action='append', help='Globs   of   files   to   exclude') 
   (args, dirs) = parser.parse_args() 
   if (not dirs): 
      parser.error('you   must   specify   one   or   more   directories') 
   for buildroot in dirs: 
      rm_old_files(buildroot, expire_time=(args.expire_date * DAY), excludes=args.exclude, fake=args.fake, includes=args.include)", 'Remove old files from a build directory','main'
"def GetTokenInformation(token, information_class): 
    data_size = ctypes.wintypes.DWORD() 
   ctypes.windll.advapi32.GetTokenInformation(token, information_class.num, 0, 0, ctypes.byref(data_size)) 
   data = ctypes.create_string_buffer(data_size.value) 
   handle_nonzero_success(ctypes.windll.advapi32.GetTokenInformation(token, information_class.num, ctypes.byref(data), ctypes.sizeof(data), ctypes.byref(data_size))) 
   return ctypes.cast(data, ctypes.POINTER(TOKEN_USER)).contents"," 'GetTokenInformation(token, information_class) -> data 
 Returns the value of the specified token information class for the given 
 token. 
 The token must have been opened with TOKEN_READ. 
 :param token: A handle to a token. 
 :type token: ctypes.HANDLE 
 :param information_class: The token information class to query. 
 :type information_class: TOKEN_INFORMATION_CLASS 
 :return: The value of the specified token information class. 
 :rtype: ctypes.POINTER(ctypes.c_char)'","'Given a token, get the token information for it.'"
"def fill_diagonal(a, val, wrap=False): 
    if (a.ndim < 2): 
      raise ValueError('array   must   be   at   least   2-d') 
   end = None 
   if (a.ndim == 2): 
      step = (a.shape[1] + 1) 
      if (not wrap): 
         end = (a.shape[1] * a.shape[1]) 
   else: 
      if (not numpy.alltrue((numpy.diff(a.shape) == 0))): 
         raise ValueError('All   dimensions   of   input   must   be   of   equal   length') 
      step = (1 + numpy.cumprod(a.shape[:(-1)]).sum()) 
   a.ravel()[:end:step] = val"," 'Fill a diagonal of an array. 
 Parameters 
 a : ndarray 
 Array to fill. 
 val : scalar 
 Value to fill the array with. 
 wrap : bool 
 If True, the array will be filled to the end of the array, otherwise 
 only the first `step` elements will be filled. 
 Examples 
 >>> a = np.arange(9).reshape(3,3) 
 >>> fill_diagonal(a, 3) 
 array([[ 0,  3,  6], 
 [ 1,  4,  7], 
 [ 2,  5,  8]]) 
 >>> fill_diagonal(a, 3, wrap=True) 
 array([[ 0,  3,  6], 
 [ 1,  4,  7], 
 [ 2,  5,  8], 
 [ 3,  6,  9], 
 [ 4,  7, 10], 
 [ 5,  8, 11], 
 [ 6,  9, 12","'Fill the main diagonal of the given array of any dimensionality. 
 For an array `a` with ``a.ndim > 2``, the diagonal is the list of 
 locations with indices ``a[i, i, ..., i]`` all identical. This function 
 modifies the input array in-place, it does not return a value. 
 Args: 
 a (cupy.ndarray): The array, at least 2-D. 
 val (scalar): The value to be written on the diagonal. 
 Its type must be compatible with that of the array a. 
 wrap (bool): If specified, the diagonal is ""wrapped"" after N columns. 
 This affects only tall matrices. 
 Examples 
 >>> a = cupy.zeros((3, 3), int) 
 >>> cupy.fill_diagonal(a, 5) 
 >>> a 
 array([[5, 0, 0], 
 [0, 5, 0], 
 [0, 0, 5]]) 
 .. seealso:: :func:`numpy.fill_diagonal`'"
"def test_multiclass_error(): 
    y = np.linspace(0, 1, 20) 
   bc = BalanceCascade(random_state=RND_SEED) 
   assert_warns(UserWarning, bc.fit, X, y) 
   y = np.array(((([0] * 3) + ([1] * 2)) + ([2] * 15))) 
   bc = BalanceCascade(random_state=RND_SEED) 
   assert_warns(UserWarning, bc.fit, X, y)", 'Test that multiclass balancing is not supported',"'Test either if an error is raised when the target are not binary 
 type.'"
"def full_like(array, fill_value, stream=None): 
    warnings.warn('chainer.cuda.full_like   is   deprecated.   Use   cupy.full_like   instead.', DeprecationWarning) 
   check_cuda_available() 
   assert (stream is None) 
   if isinstance(array, cupy.ndarray): 
      return cupy.full_like(array, fill_value) 
   return cupy.full(array.shape, fill_value, dtype=array.dtype)"," 'Return an array filled with the specified value. 
 Parameters 
 array : array_like 
 Input array. 
 fill_value : scalar 
 The value to be filled. 
 stream : Stream, optional 
 Stream to be used for the copy. 
 Returns 
 out : ndarray 
 The filled array. 
 Notes 
 This function is available on CPU and CUDA. 
 Examples 
 >>> import numpy as np 
 >>> import cupy as cp 
 >>> a = np.arange(10).reshape(2, 5) 
 >>> cp.full_like(a, 0) 
 array([[ 0,  0,  0,  0,  0], 
 [ 0,  0,  0,  0,  0]], dtype=float32) 
 >>> cp.full_like(a, 1) 
 array([[ 1,  1,  1,  1,  1], 
 [ 1,  1,  1,  1,  1]], dtype=float32)'","'Creates a constant-filled :class:`cupy.ndarray` object like the given array. 
 Args: 
 array (cupy.ndarray or numpy.ndarray): Base array. 
 fill_value: Constant value to fill the array by. 
 stream (cupy.cuda.Stream): CUDA stream. 
 Returns: 
 cupy.ndarray: Constant-filled array.'"
"def query(uuid): 
    timings = [] 
   errors = [] 
   for _ in range(0, constants.NUM_SAMPLES): 
      start = time.time() 
      try: 
         query = TestModel.all() 
         query.filter('test_string   =', uuid) 
         query.fetch(constants.NUM_SAMPLES) 
         total_time = (time.time() - start) 
      except Exception as exception: 
         logging.exception(exception) 
         errors.append(str(exception)) 
         total_time = 0 
      timings.append((total_time * constants.SECONDS_TO_MILLI)) 
   return (timings, errors)"," 'Query the database for a specific UUID. 
 Returns a tuple of timings and errors. 
 timings is a list of dictionaries with keys: 
 \'total_time\' 
 \'total_time_ms\' 
 \'total_time_us\' 
 \'avg_time\' 
 \'avg_time_ms\' 
 \'avg_time_us\' 
 \'min_time\' 
 \'min_time_ms\' 
 \'min_time_us\' 
 \'max_time\' 
 \'max_time_ms\' 
 \'max_time_us\' 
 \'errors\' 
 errors is a list of errors encountered while querying the database.'","'Query stored entities and time them. 
 Returns: 
 A tuple of two lists. A list of float times to query 
 all entities, and a list of errors. A zero value signifies 
 a failure.'"
"def instance_add_security_group(context, instance_uuid, security_group_id): 
    sec_group_ref = models.SecurityGroupInstanceAssociation() 
   sec_group_ref.update({'instance_uuid': instance_uuid, 'security_group_id': security_group_id}) 
   sec_group_ref.save()", 'Associate a security group with an instance.','Associate the given security group with the given instance.'
"def parseOptions(args=None): 
    global WORKDIR, DEPSRC, SDKPATH, SRCDIR, DEPTARGET 
   global UNIVERSALOPTS, UNIVERSALARCHS, ARCHLIST, CC, CXX 
   global FW_VERSION_PREFIX 
   global FW_SSL_DIRECTORY 
   if (args is None): 
      args = sys.argv[1:] 
   try: 
      (options, args) = getopt.getopt(args, '?hb', ['build-dir=', 'third-party=', 'sdk-path=', 'src-dir=', 'dep-target=', 'universal-archs=', 'help']) 
   except getopt.GetoptError: 
      print sys.exc_info()[1] 
      sys.exit(1) 
   if args: 
      print 'Additional   arguments' 
      sys.exit(1) 
   deptarget = None 
   for (k, v) in options: 
      if (k in ('-h', '-?', '--help')): 
         print USAGE 
         sys.exit(0) 
      elif (k in ('-d', '--build-dir')): 
         WORKDIR = v 
      elif (k in ('--third-party',)): 
         DEPSRC = v 
      elif (k in ('--sdk-path',)): 
         SDKPATH = v 
      elif (k in ('--src-dir',)): 
         SRCDIR = v 
      elif (k in ('--dep-target',)): 
         DEPTARGET = v 
         deptarget = v 
      elif (k in ('--universal-archs',)): 
         if (v in UNIVERSALOPTS): 
            UNIVERSALARCHS = v 
            ARCHLIST = universal_opts_map[UNIVERSALARCHS] 
            if (deptarget is None): 
               DEPTARGET = default_target_map.get(v, '10.3') 
         else: 
            raise NotImplementedError(v) 
      else: 
         raise NotImplementedError(k) 
   SRCDIR = os.path.abspath(SRCDIR) 
   WORKDIR = os.path.abspath(WORKDIR) 
   SDKPATH = os.path.abspath(SDKPATH) 
   DEPSRC = os.path.abspath(DEPSRC) 
   (CC, CXX) = getTargetCompilers() 
   FW_VERSION_PREFIX = (FW_PREFIX[:] + ['Versions', getVersion()]) 
   FW_SSL_DIRECTORY = (FW_VERSION_PREFIX[:] + ['etc', 'openssl']) 
   print '--   Settings:' 
   print ('         *   Source   directory:            %s' % SRCDIR) 
   print ('         *   Build   directory:               %s' % WORKDIR) 
   print ('         *   SDK   location:                        %s' % SDKPATH) 
   print ('         *   Third-party   source:      %s' % DEPSRC) 
   print ('         *   Deployment   target:         %s' % DEPTARGET) 
   print ('         *   Universal   archs:               %s' % str(ARCHLIST)) 
   print ('         *   C   compiler:                              %s' % CC) 
   print ('         *   C++   compiler:                        %s' % CXX) 
   print '' 
   print ('   --   Building   a   Python   %s   framework   at   patch   level   %s' % (getVersion(), getFullVersion())) 
   print ''"," 'Parse command line options. 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :",'Parse arguments and update global settings.'
"@login_required 
 @expect_json 
 def delete_item(request, usage_key): 
    _delete_item(usage_key, request.user)",100: 'OK','Exposes internal helper method without breaking existing bindings/dependencies'
"def request(http, uri, method='GET', body=None, headers=None, redirections=httplib2.DEFAULT_MAX_REDIRECTS, connection_type=None): 
    http_callable = getattr(http, 'request', http) 
   return http_callable(uri, method=method, body=body, headers=headers, redirections=redirections, connection_type=connection_type)"," 'Requests a resource from a remote server. 
 :param http: A :class:`~requests.Session` instance. 
 :param uri: The URI to request. 
 :param method: The HTTP method to use. 
 :param body: The request body. 
 :param headers: A dictionary of HTTP headers to send. 
 :param redirections: The number of redirections to follow. 
 :param connection_type: The type of connection to use. 
 :rtype: :class:`~requests.Response` 
 :raises requests.ConnectionError: If the server cannot be reached.'","'Make an HTTP request with an HTTP object and arguments. 
 Args: 
 http: httplib2.Http, an http object to be used to make requests. 
 uri: string, The URI to be requested. 
 method: string, The HTTP method to use for the request. Defaults 
 to \'GET\'. 
 body: string, The payload / body in HTTP request. By default 
 there is no payload. 
 headers: dict, Key-value pairs of request headers. By default 
 there are no headers. 
 redirections: int, The number of allowed 203 redirects for 
 the request. Defaults to 5. 
 connection_type: httplib.HTTPConnection, a subclass to be used for 
 establishing connection. If not set, the type 
 will be determined from the ``uri``. 
 Returns: 
 tuple, a pair of a httplib2.Response with the status code and other 
 headers and the bytes of the content returned.'"
"def _chk_asarray(a, axis): 
    if (axis is None): 
      a = ravel(a) 
      outaxis = 0 
   else: 
      a = asarray(a) 
      outaxis = axis 
   return (a, outaxis)"," 'Check that the input is an array and return the axis. 
 Parameters 
 a : array-like 
 Input array. 
 axis : int, optional 
 Axis to check. 
 Returns 
 (a, axis) 
 A tuple of the input array and the axis.'",'Converts a list into an numpy array'
"def simsam_range_to_files(table, tree, simulated_sample_sizes, dissimilarities, output_dir, mapping_f=None, output_table_basename='table', output_map_basename='map'): 
    create_dir(output_dir) 
   for e in simsam_range(table, tree, simulated_sample_sizes, dissimilarities, mapping_f): 
      output_table = e[0] 
      output_mapping_lines = e[1] 
      simulated_sample_size = e[2] 
      dissimilarity = e[3] 
      output_table_fp = join(output_dir, ('%s_n%d_d%r.biom' % (output_table_basename, simulated_sample_size, dissimilarity))) 
      write_biom_table(output_table, output_table_fp) 
      if (output_mapping_lines is not None): 
         output_map_fp = join(output_dir, ('%s_n%d_d%r.txt' % (output_map_basename, simulated_sample_size, dissimilarity))) 
         output_map_f = open(output_map_fp, 'w') 
         output_map_f.write(''.join(output_mapping_lines)) 
         output_map_f.close()"," 'Simulate the range of a table and write the output to files. 
 :param table: table to simulate 
 :param tree: tree to simulate from 
 :param simulated_sample_sizes: sample sizes to simulate 
 :param dissimilarities: dissimilarities to simulate 
 :param output_dir: output directory 
 :param mapping_f: mapping file 
 :param output_table_basename: output table basename 
 :param output_map_basename: output mapping basename'","'Applies sim_otu_table over a range of parameters, writing output to file 
 table: the input table to simulate samples from 
 tree: tree related OTUs in input table 
 simulated_sample_sizes: a list of ints defining how many 
 output samples should be create per input sample 
 dissimilarities: a list of floats containing the 
 dissimilarities to use in simulating tables 
 output_dir: the directory where all output tables and 
 mapping files should be written 
 mapping_f: file handle for metadata mapping file, if 
 a mapping file should be created with the samples from 
 each simulated table 
 output_table_basename: basename for output table files 
 (default: table) 
 output_map_basename: basename for output mapping files 
 (default: map)'"
"def parse_function_plugin(logger, line, state): 
    try: 
      acc = (state['test_acc'] + 1) 
   except KeyError: 
      acc = 1 
   state['test_acc'] = acc 
   res = line.split() 
   res[2] = acc 
   res[3] = {'metric_type': 'counter'} 
   return tuple(res)", 'Parse a function plugin line.','Simple stateful parser'
"def set_subnet_name(name): 
    cmd = 'systemsetup   -setlocalsubnetname   ""{0}""'.format(name) 
   salt.utils.mac_utils.execute_return_success(cmd) 
   return salt.utils.mac_utils.confirm_updated(name, get_subnet_name)"," 'Set the subnet name. 
 :param name: 
 :return: 
 :rtype: 
 :raises: 
 :raises: SaltCloudSystemExit'","'Set the local subnet name 
 :param str name: The new local subnet name 
 .. note:: 
 Spaces are changed to dashes. Other special characters are removed. 
 :return: True if successful, False if not 
 :rtype: bool 
 CLI Example: 
 .. code-block:: bash 
 The following will be set as \'Mikes-Mac\' 
 salt \'*\' system.set_subnet_name ""Mike\'s Mac""'"
"@ignore_warnings 
 def test_sensitivity_specificity_ignored_labels(): 
    y_true = [1, 1, 2, 3] 
   y_pred = [1, 3, 3, 3] 
   specificity_13 = partial(specificity_score, y_true, y_pred, labels=[1, 3]) 
   specificity_all = partial(specificity_score, y_true, y_pred, labels=None) 
   assert_allclose([1.0, 0.33], specificity_13(average=None), rtol=R_TOL) 
   assert_allclose(np.mean([1.0, 0.33]), specificity_13(average='macro'), rtol=R_TOL) 
   assert_allclose(np.average([1.0, 0.33], weights=[2.0, 1.0]), specificity_13(average='weighted'), rtol=R_TOL) 
   assert_allclose((3.0 / (3.0 + 2.0)), specificity_13(average='micro'), rtol=R_TOL) 
   for average in ['macro', 'weighted', 'micro']: 
      assert_not_equal(specificity_13(average=average), specificity_all(average=average))", 'Test sensitivity/specificity ignoring labels','Test a subset of labels may be requested for SS'
"def description(): 
    for desc in _description.splitlines(): 
      print desc", 'Print the description of the module.','Get description of brainstorm (bst_phantom_ctf) dataset.'
"def estimate_beta_ridge(x, y, alpha): 
    beta_initial = [random.random() for x_i in x[0]] 
   return minimize_stochastic(partial(squared_error_ridge, alpha=alpha), partial(squared_error_ridge_gradient, alpha=alpha), x, y, beta_initial, 0.001)"," 'Estimate the parameters of a linear regression model with ridge regression. 
 Parameters 
 x : array 
 The input features. 
 y : array 
 The input labels. 
 alpha : float 
 The ridge parameter. 
 Returns 
 beta : array 
 The estimated coefficients. 
 Notes 
 This function uses the stochastic gradient descent algorithm to estimate the 
 parameters. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Ridge_regression'","'use gradient descent to fit a ridge regression 
 with penalty alpha'"
"def for_name(fq_name, recursive=False): 
    fq_name = str(fq_name) 
   module_name = __name__ 
   short_name = fq_name 
   if (fq_name.rfind('.') >= 0): 
      (module_name, short_name) = (fq_name[:fq_name.rfind('.')], fq_name[(fq_name.rfind('.') + 1):]) 
   try: 
      result = __import__(module_name, None, None, [short_name]) 
      return result.__dict__[short_name] 
   except KeyError: 
      if recursive: 
         raise 
      else: 
         raise ImportError((""Could   not   find   '%s'   on   path   '%s'"" % (short_name, module_name))) 
   except ImportError: 
      try: 
         module = for_name(module_name, recursive=True) 
         if hasattr(module, short_name): 
            return getattr(module, short_name) 
         else: 
            raise KeyError() 
      except KeyError: 
         raise ImportError((""Could   not   find   '%s'   on   path   '%s'"" % (short_name, module_name))) 
      except ImportError: 
         pass 
      raise"," 'Given a fully qualified name, return the module or function. 
 If recursive is True, try to import the module and then try to 
 import the function. 
 If recursive is False, and the function cannot be found, raise an 
 ImportError.'","'Find class/function/method specified by its fully qualified name. 
 Fully qualified can be specified as: 
 * <module_name>.<class_name> 
 * <module_name>.<function_name> 
 * <module_name>.<class_name>.<method_name> (an unbound method will be 
 returned in this case). 
 for_name works by doing __import__ for <module_name>, and looks for 
 <class_name>/<function_name> in module\'s __dict__/attrs. If fully qualified 
 name doesn\'t contain \'.\', the current module will be used. 
 Args: 
 fq_name: fully qualified name of something to find 
 Returns: 
 class object. 
 Raises: 
 ImportError: when specified module could not be loaded or the class 
 was not found in the module.'"
"def vgcreate(vgname, devices, **kwargs): 
    if ((not vgname) or (not devices)): 
      return 'Error:   vgname   and   device(s)   are   both   required' 
   if isinstance(devices, six.string_types): 
      devices = devices.split(',') 
   cmd = ['vgcreate', vgname] 
   for device in devices: 
      cmd.append(device) 
   valid = ('clustered', 'maxlogicalvolumes', 'maxphysicalvolumes', 'vgmetadatacopies', 'metadatacopies', 'physicalextentsize') 
   for var in kwargs: 
      if (kwargs[var] and (var in valid)): 
         cmd.append('--{0}'.format(var)) 
         cmd.append(kwargs[var]) 
   out = __salt__['cmd.run'](cmd, python_shell=False).splitlines() 
   vgdata = vgdisplay(vgname) 
   vgdata['Output   from   vgcreate'] = out[0].strip() 
   return vgdata"," 'Create a volume group. 
 The vgcreate command creates a volume group, which is a logical 
 grouping of physical volumes. 
 This command is used to create a new volume group, or to extend an 
 existing volume group. 
 The vgcreate command requires two arguments: 
 - vgname 
 - devices 
 vgname 
 This is the name of the volume group. 
 devices 
 This is a list of physical volumes to be used for the volume group. 
 The devices can be specified in a number of ways. 
 - as a comma-separated list of physical volumes 
 - as a string of physical volumes 
 - as a list of physical volumes 
 - as a single physical volume 
 - as a single physical volume with the name of the physical volume 
 - as a single physical volume with the name of the device 
 - as a single physical volume with the device name and the physical 
 volume number 
 - as a single physical volume with the device name and the physical 
 volume name 
 - as a single physical volume with the device name and the physical 
 volume name and the physical volume number 
 - as","'Create an LVM volume group 
 CLI Examples: 
 .. code-block:: bash 
 salt mymachine lvm.vgcreate my_vg /dev/sdb1,/dev/sdb2 
 salt mymachine lvm.vgcreate my_vg /dev/sdb1 clustered=y'"
"@scope.define 
 def call(fn, args=(), kwargs={}): 
    return fn(*args, **kwargs)"," 'Decorator for wrapping functions. 
 Parameters 
 fn : function to wrap 
 args : positional arguments to pass to the function 
 kwargs : keyword arguments to pass to the function 
 Returns 
 A function that calls the wrapped function 
 Examples 
 >>> @call 
 ... def f(x): 
 ...     return x 
 >>> f(1) 
 1 
 >>> f(1, 2) 
 1 
 >>> f(1, 2, 3) 
 1 
 >>> f(1, 2, 3, 4) 
 1 
 >>> f(1, 2, 3, 4, 5) 
 1 
 >>> f(1, 2, 3, 4, 5, 6) 
 1'","'call fn with given args and kwargs. 
 This is used to represent Apply.__call__'"
"def test_raise_exception_spatial(): 
    sbn1 = SpatialBatchNormalization((5,)) 
   (yield (assert_raises, (ValueError, sbn1.allocate))) 
   sbn2 = SpatialBatchNormalization(3) 
   (yield (assert_raises, (ValueError, sbn2.allocate))) 
   def do_not_fail(*input_dim): 
      try: 
         sbn = SpatialBatchNormalization(input_dim) 
         sbn.allocate() 
      except ValueError: 
         assert False 
   (yield (do_not_fail, 5, 4, 3)) 
   (yield (do_not_fail, 7, 6)) 
   (yield (do_not_fail, 3, 9, 2, 3))", 'Test for ValueError on SpatialBatchNormalization.allocate()','Test that SpatialBatchNormalization raises an expected exception.'
"def new_table(table, family='ipv4'): 
    if (not table): 
      return 'Error:   table   needs   to   be   specified' 
   if check_table(table, family=family): 
      return 'Error:   table   {0}   in   family   {1}   already   exists'.format(table, family) 
   nft_family = _NFTABLES_FAMILIES[family] 
   cmd = '{0}   add   table   {1}   {2}'.format(_nftables_cmd(), nft_family, table) 
   out = __salt__['cmd.run'](cmd, python_shell=False) 
   if (not out): 
      out = True 
   return out"," 'Create a new table in the specified family. 
 Returns True if the table was created successfully. 
 :param table: The table to create 
 :param family: The family to create the table in (default: ipv4) 
 :return: True if the table was created, False if not.'","'.. versionadded:: 2014.7.0 
 Create new custom table. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' nftables.new_table filter 
 IPv6: 
 salt \'*\' nftables.new_table filter family=ipv6'"
"def function_exponentiation(tokens, local_dict, global_dict): 
    result = [] 
   exponent = [] 
   consuming_exponent = False 
   level = 0 
   for (tok, nextTok) in zip(tokens, tokens[1:]): 
      if ((tok[0] == NAME) and (nextTok[0] == OP) and (nextTok[1] == '**')): 
         if _token_callable(tok, local_dict, global_dict): 
            consuming_exponent = True 
      elif consuming_exponent: 
         exponent.append(tok) 
         if ((tok[0] == nextTok[0] == OP) and (tok[1] == ')') and (nextTok[1] == '(')): 
            consuming_exponent = False 
         if ((tok[0] == nextTok[0] == OP) and (tok[1] == '*') and (nextTok[1] == '(')): 
            consuming_exponent = False 
            del exponent[(-1)] 
         continue 
      elif (exponent and (not consuming_exponent)): 
         if (tok[0] == OP): 
            if (tok[1] == '('): 
               level += 1 
            elif (tok[1] == ')'): 
               level -= 1 
         if (level == 0): 
            result.append(tok) 
            result.extend(exponent) 
            exponent = [] 
            continue 
      result.append(tok) 
   if tokens: 
      result.append(tokens[(-1)]) 
   if exponent: 
      result.extend(exponent) 
   return result"," 'Functions are treated as callables, and can be applied to other 
 functions, as in: 
 f(g(x)) 
 f(g(x)**2) 
 f(g(x)**2)**2 
 f(g(x)**2)**2(g(x)**3) 
 f(g(x)**2)**2(g(x)**3)**2 
 f(g(x)**2)**2(g(x)**3)**2(g(x)**4) 
 f(g(x)**2)**2(g(x)**3)**2(g(x)**4)**2 
 f(g(x)**2)**2(g(x)**3)**2(g(x)**4)**2(g(x)**5) 
 f(g(x)**2)**2(g(x)**3)**2(g(x)**4)**2(g(x)**5)**2 
 f(g(x)**2","'Allows functions to be exponentiated, e.g. ``cos**2(x)``. 
 Examples 
 >>> from sympy.parsing.sympy_parser import (parse_expr, 
 ... standard_transformations, function_exponentiation) 
 >>> transformations = standard_transformations + (function_exponentiation,) 
 >>> parse_expr(\'sin**4(x)\', transformations=transformations) 
 sin(x)**4'"
"@testing.requires_testing_data 
 def test_fine_calibration(): 
    raw = read_crop(raw_fname, (0.0, 1.0)) 
   sss_fine_cal = read_crop(sss_fine_cal_fname) 
   raw_sss = maxwell_filter(raw, calibration=fine_cal_fname, origin=mf_head_origin, regularize=None, bad_condition='ignore') 
   assert_meg_snr(raw_sss, sss_fine_cal, 82, 611) 
   py_cal = raw_sss.info['proc_history'][0]['max_info']['sss_cal'] 
   assert_true((py_cal is not None)) 
   assert_true((len(py_cal) > 0)) 
   mf_cal = sss_fine_cal.info['proc_history'][0]['max_info']['sss_cal'] 
   mf_cal['cal_chans'][((mf_cal['cal_chans'][:, 1] == 3022), 1)] = 3024 
   assert_allclose(py_cal['cal_chans'], mf_cal['cal_chans']) 
   assert_allclose(py_cal['cal_corrs'], mf_cal['cal_corrs'], rtol=0.001, atol=0.001) 
   raw_missing = raw.copy().load_data() 
   raw_missing.info['bads'] = ['MEG0111', 'MEG0943'] 
   raw_missing.info._check_consistency() 
   raw_sss_bad = maxwell_filter(raw_missing, calibration=fine_cal_fname, origin=mf_head_origin, regularize=None, bad_condition='ignore') 
   raw_missing.pick_types() 
   raw_sss_bad.pick_channels(raw_missing.ch_names) 
   with warnings.catch_warnings(record=True): 
      raw_sss_missing = maxwell_filter(raw_missing, calibration=fine_cal_fname, origin=mf_head_origin, regularize=None, bad_condition='ignore') 
   assert_meg_snr(raw_sss_missing, raw_sss_bad, 1000.0, 10000.0) 
   raw_sss_3D = maxwell_filter(raw, calibration=fine_cal_fname_3d, origin=mf_head_origin, regularize=None, bad_condition='ignore') 
   assert_meg_snr(raw_sss_3D, sss_fine_cal, 1.0, 6.0) 
   raw_ctf = read_crop(fname_ctf_raw).apply_gradient_compensation(0) 
   assert_raises(RuntimeError, maxwell_filter, raw_ctf, origin=(0.0, 0.0, 0.04), calibration=fine_cal_fname)", 'Test fine calibration.','Test Maxwell filter fine calibration.'
"def _parse_core_site(): 
    global _CORE_SITE_DICT 
   global _CORE_SITE_PATH 
   for indentifier in conf.HDFS_CLUSTERS.get(): 
      try: 
         _CORE_SITE_PATH = os.path.join(conf.HDFS_CLUSTERS[indentifier].HADOOP_CONF_DIR.get(), 'core-site.xml') 
         data = file(_CORE_SITE_PATH, 'r').read() 
         break 
      except KeyError: 
         data = '' 
      except IOError as err: 
         if (err.errno != errno.ENOENT): 
            LOG.error(('Cannot   read   from   ""%s"":   %s' % (_CORE_SITE_PATH, err))) 
            return 
         data = '' 
   _CORE_SITE_DICT = confparse.ConfParse(data)"," 'Parse the core-site.xml file. 
 :returns: A dictionary with the values from the core-site.xml file.'",'Parse core-site.xml and store in _CORE_SITE_DICT'
"def compute_norms(array, norm_axes=None): 
    if ((not isinstance(array, theano.Variable)) and (not isinstance(array, np.ndarray))): 
      raise RuntimeError('Unsupported   type   {}.   Only   theano   variables   and   numpy   arrays   are   supported'.format(type(array))) 
   ndim = array.ndim 
   if (norm_axes is not None): 
      sum_over = tuple(norm_axes) 
   elif (ndim == 1): 
      sum_over = () 
   elif (ndim == 2): 
      sum_over = (0,) 
   elif (ndim in [3, 4, 5]): 
      sum_over = tuple(range(1, ndim)) 
   else: 
      raise ValueError('Unsupported   tensor   dimensionality   {}.   Must   specify   `norm_axes`'.format(array.ndim)) 
   if isinstance(array, theano.Variable): 
      if (len(sum_over) == 0): 
         norms = T.abs_(array) 
      else: 
         norms = T.sqrt(T.sum((array ** 2), axis=sum_over)) 
   elif isinstance(array, np.ndarray): 
      if (len(sum_over) == 0): 
         norms = abs(array) 
      else: 
         norms = np.sqrt(np.sum((array ** 2), axis=sum_over)) 
   return norms"," 'Compute norms of a tensor. 
 Parameters 
 array : tensor 
 The tensor to compute norms for. 
 norm_axes : tuple of int 
 The axes to compute norms over. If None, the axes are computed 
 automatically based on the dimensionality of the tensor. 
 Returns 
 norms : tensor 
 The norms of the tensor. 
 Examples 
 >>> from theano.tensor.shared import tensor 
 >>> x = tensor(np.ones((3, 2))) 
 >>> norms = compute_norms(x) 
 >>> norms.eval() 
 array([[1., 1.], [1., 1.], [1., 1.]])'","'Compute incoming weight vector norms. 
 Parameters 
 array : numpy array or Theano expression 
 Weight or bias. 
 norm_axes : sequence (list or tuple) 
 The axes over which to compute the norm.  This overrides the 
 default norm axes defined for the number of dimensions 
 in `array`. When this is not specified and `array` is a 2D array, 
 this is set to `(0,)`. If `array` is a 3D, 4D or 5D array, it is 
 set to a tuple listing all axes but axis 0. The former default is 
 useful for working with dense layers, the latter is useful for 1D, 
 2D and 3D convolutional layers. 
 Finally, in case `array` is a vector, `norm_axes` is set to an empty 
 tuple, and this function will simply return the absolute value for 
 each element. This is useful when the function is applied to all 
 parameters of the network, including the bias, without distinction. 
 (Optional) 
 Returns 
 norms : 1D array or Theano vector (1D) 
 1D array or Theano vector of incoming weight/bias vector norms. 
 Examples 
 >>> array = np.random.randn(100, 200) 
 >>> norms = compute_norms(array) 
 >>> norms.shape 
 (200,) 
 >>> norms = compute_norms(array, norm_axes=(1,)) 
 >>> norms.shape 
 (100,)'"
"def hrm_person_controller(**attr): 
    T = current.T 
   db = current.db 
   s3db = current.s3db 
   auth = current.auth 
   response = current.response 
   session = current.session 
   settings = current.deployment_settings 
   s3 = response.s3 
   configure = s3db.configure 
   set_method = s3db.set_method 
   contacts_tabs = settings.get_pr_contacts_tabs() 
   if ('all' in contacts_tabs): 
      set_method('pr', 'person', method='contacts', action=s3db.pr_Contacts) 
   if ('public' in contacts_tabs): 
      set_method('pr', 'person', method='public_contacts', action=s3db.pr_Contacts) 
   if ('private' in contacts_tabs): 
      set_method('pr', 'person', method='private_contacts', action=s3db.pr_Contacts) 
   set_method('pr', 'person', method='cv', action=hrm_CV) 
   set_method('pr', 'person', method='record', action=hrm_Record) 
   if settings.has_module('asset'): 
      s3db.add_components('pr_person', asset_asset='assigned_to_id') 
      configure('asset_asset', deletable=False, editable=False, insertable=False) 
   get_vars = current.request.get_vars 
   group = get_vars.get('group', 'staff') 
   hr_id = get_vars.get('human_resource.id', None) 
   if (not str(hr_id).isdigit()): 
      hr_id = None 
   table = s3db.hrm_human_resource 
   table.type.default = 1 
   get_vars['xsltmode'] = 'staff' 
   if hr_id: 
      hr = db((table.id == hr_id)).select(table.type, limitby=(0, 1)).first() 
      if hr: 
         group = (((hr.type == 2) and 'volunteer') or 'staff') 
         get_vars['group'] = group 
   table = db.pr_person 
   tablename = 'pr_person' 
   configure(tablename, deletable=False) 
   mode = session.s3.hrm.mode 
   if (mode is not None): 
      s3.crud_strings[tablename].update(title_display=T('Personal   Profile'), title_update=T('Personal   Profile')) 
      configure('hrm_human_resource', deletable=False, editable=False, insertable=False) 
      configure('hrm_certification', deletable=True, editable=True, insertable=True) 
      configure('hrm_credential', deletable=False, editable=False, insertable=False) 
      configure('hrm_competency', deletable=False, editable=False, insertable=True) 
      configure('hrm_training', deletable=False, editable=False, insertable=True) 
      configure('hrm_experience', deletable=False, editable=False, insertable=False) 
      configure('pr_group_membership', deletable=False, editable=False, insertable=False) 
   elif (settings.get_hrm_staff_label() == T('Contacts')): 
      s3.crud_strings[tablename].update(title_upload=T('Import   Contacts'), title_display=T('Contact   Details'), title_update=T('Contact   Details')) 
   else: 
      s3.crud_strings[tablename].update(title_upload=T('Import   Staff'), title_display=T('Staff   Member   Details'), title_update=T('Staff   Member   Details')) 
   s3.importerPrep = (lambda : dict(ReplaceOption=T('Remove   existing   data   before   import'))) 
   def import_prep(data, group=group): 
      '\n                                    Deletes   all   HR   records   (of   the   given   group)   of   the\n                                    organisation/branch   before   processing   a   new   data   import\n                        ' 
      (resource, tree) = data 
      xml = current.xml 
      tag = xml.TAG 
      att = xml.ATTRIBUTE 
      if s3.import_replace: 
         if (tree is not None): 
            if (group == 'staff'): 
               group = 1 
            elif (group == 'volunteer'): 
               group = 2 
            else: 
               return 
            root = tree.getroot() 
            expr = (""/%s/%s[@%s='org_organisation']/%s[@%s='name']"" % (tag.root, tag.resource, att.name, tag.data, att.field)) 
            orgs = root.xpath(expr) 
            for org in orgs: 
               org_name = (org.get('value', None) or org.text) 
               if org_name: 
                  try: 
                     org_name = json.loads(xml.xml_decode(org_name)) 
                  except: 
                     pass 
               if org_name: 
                  htable = s3db.hrm_human_resource 
                  otable = s3db.org_organisation 
                  query = (((otable.name == org_name) & (htable.organisation_id == otable.id)) & (htable.type == group)) 
                  resource = s3db.resource('hrm_human_resource', filter=query) 
                  resource.delete(format='xml', cascade=True) 
   s3.import_prep = import_prep 
   def prep(r): 
      S3PersonRoleManager.set_method(r, entity='pr_person') 
      if s3.rtl: 
         f = s3db.pr_phone_contact.value 
         f.represent = s3_phone_represent 
         f.widget = S3PhoneWidget() 
      method = r.method 
      if (r.representation == 's3json'): 
         current.xml.show_ids = True 
      elif (r.interactive and (method != 'import')): 
         if (not r.component): 
            table = r.table 
            table.pe_label.readable = table.pe_label.writable = False 
            table.missing.readable = table.missing.writable = False 
            table.age_group.readable = table.age_group.writable = False 
            dob = table.date_of_birth 
            dob.widget = S3CalendarWidget(past_months=1440, future_months=(-60)) 
            person_details_table = s3db.pr_person_details 
            person_details_table.occupation.readable = person_details_table.occupation.writable = False 
            set_org_dependent_field = settings.set_org_dependent_field 
            set_org_dependent_field('pr_person', 'middle_name') 
            set_org_dependent_field('pr_person_details', 'father_name') 
            set_org_dependent_field('pr_person_details', 'mother_name') 
            set_org_dependent_field('pr_person_details', 'grandfather_name') 
            set_org_dependent_field('pr_person_details', 'affiliations') 
            set_org_dependent_field('pr_person_details', 'company') 
         else: 
            component_name = r.component_name 
            if (component_name == 'physical_description'): 
               table = r.component.table 
               for field in table.fields: 
                  table[field].writable = table[field].readable = False 
               table.ethnicity.writable = table.ethnicity.readable = True 
               table.blood_type.writable = table.blood_type.readable = True 
               table.medical_conditions.writable = table.medical_conditions.readable = True 
               table.other_details.writable = table.other_details.readable = True 
            elif (component_name == 'appraisal'): 
               mission_id = r.get_vars.get('mission_id', None) 
               if mission_id: 
                  hatable = r.component.table 
                  mtable = s3db.deploy_mission 
                  mission = db((mtable.id == mission_id)).select(mtable.code, limitby=(0, 1)).first() 
                  if mission: 
                     hatable.code.default = mission.code 
                  atable = db.deploy_assignment 
                  htable = db.hrm_human_resource 
                  query = (((atable.mission_id == mission_id) & (atable.human_resource_id == htable.id)) & (htable.person_id == r.id)) 
                  assignment = db(query).select(atable.job_title_id, limitby=(0, 1)).first() 
                  if assignment: 
                     hatable.job_title_id.default = assignment.job_title_id 
            elif (component_name == 'asset'): 
               configure('asset_asset', insertable=False, editable=False, deletable=False) 
            elif (component_name == 'group_membership'): 
               hrm_configure_pr_group_membership() 
            elif (component_name == 'salary'): 
               hrm_configure_salary(r) 
         if ((method == 'record') or (r.component_name == 'human_resource')): 
            table = s3db.hrm_human_resource 
            table.person_id.writable = table.person_id.readable = False 
            table.site_id.readable = table.site_id.writable = True 
            org = session.s3.hrm.org 
            f = table.organisation_id 
            if (org is None): 
               f.widget = None 
            else: 
               f.default = org 
               f.readable = f.writable = False 
               table.site_id.requires = IS_EMPTY_OR(IS_ONE_OF(db, ('org_site.%s' % s3db.super_key(db.org_site)), s3db.org_site_represent, filterby='organisation_id', filter_opts=(session.s3.hrm.org,))) 
         elif ((method == 'cv') or (r.component_name == 'training')): 
            list_fields = ['course_id', 'grade'] 
            if settings.get_hrm_course_pass_marks: 
               list_fields.append('grade_details') 
            list_fields.append('date') 
            s3db.configure('hrm_training', list_fields=list_fields) 
         resource = r.resource 
         if (mode is not None): 
            resource.build_query(id=auth.s3_logged_in_person()) 
         elif (method not in ('deduplicate', 'search_ac')): 
            if ((not r.id) and (not hr_id)): 
               if response.error: 
                  session.error = response.error 
               redirect(URL(r=r, f='staff')) 
            if (resource.count() == 1): 
               resource.load() 
               r.record = resource.records().first() 
               if r.record: 
                  r.id = r.record.id 
            if (not r.record): 
               session.error = T('Record   not   found') 
               redirect(URL(f='staff')) 
            if (hr_id and (r.component_name == 'human_resource')): 
               r.component_id = hr_id 
            configure('hrm_human_resource', insertable=False) 
      elif (r.representation == 'aadata'): 
         if (r.component_name == 'group_membership'): 
            hrm_configure_pr_group_membership() 
         elif ((method == 'cv') or (r.component_name == 'training')): 
            list_fields = ['course_id', 'grade'] 
            if settings.get_hrm_course_pass_marks: 
               list_fields.append('grade_details') 
            list_fields.append('date') 
            s3db.configure('hrm_training', list_fields=list_fields) 
      return True 
   s3.prep = prep 
   def postp(r, output): 
      if (r.interactive and r.component): 
         if (r.component_name == 'asset'): 
            output['add_btn'] = A(T('Assign   Asset'), _href=URL(c='asset', f='asset'), _id='add-btn', _class='action-btn') 
      return output 
   s3.postp = postp 
   if (session.s3.hrm.orgname and (mode is None)): 
      orgname = session.s3.hrm.orgname 
   else: 
      orgname = None 
   _attr = dict(csv_stylesheet=('hrm', 'person.xsl'), csv_template='staff', csv_extra_fields=[dict(label='Type', field=s3db.hrm_human_resource.type)], orgname=orgname, replace_option=T('Remove   existing   data   before   import'), rheader=hrm_rheader) 
   _attr.update(attr) 
   output = current.rest_controller('pr', 'person', **_attr) 
   return output"," ' Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that appropriately completes the request. 
 Below is an instruction that describes a task. Write a response that","'Persons Controller, defined in the model for use from 
 multiple controllers for unified menus 
 - used for access to component Tabs, Personal Profile & Imports 
 - includes components relevant to HRM'"
"def _get_basic_stream(stream_name, conn): 
    return _execute_with_retries(conn, 'describe_stream', StreamName=stream_name)", 'Returns the basic information about the given stream.',"'Stream info from AWS, via describe_stream 
 Only returns the first ""page"" of shards (up to 100); use _get_full_stream() for all shards. 
 CLI example:: 
 salt myminion boto_kinesis._get_basic_stream my_stream existing_conn'"
"@protocol.commands.add(u'addid', songpos=protocol.UINT) 
 def addid(context, uri, songpos=None): 
    if (not uri): 
      raise exceptions.MpdNoExistError(u'No   such   song') 
   length = context.core.tracklist.get_length() 
   if ((songpos is not None) and (songpos > length.get())): 
      raise exceptions.MpdArgError(u'Bad   song   index') 
   tl_tracks = context.core.tracklist.add(uris=[uri], at_position=songpos).get() 
   if (not tl_tracks): 
      raise exceptions.MpdNoExistError(u'No   such   song') 
   return (u'Id', tl_tracks[0].tlid)"," 'Adds a track to the playlist. 
 :param uri: The URI of the track. 
 :param songpos: The position to add the track to. If not given, the track is 
 added at the end of the playlist. 
 :returns: The ID of the added track. 
 :raises: :class:`MpdNoExistError` if the track does not exist.'","'*musicpd.org, current playlist section:* 
 ``addid {URI} [POSITION]`` 
 Adds a song to the playlist (non-recursive) and returns the song id. 
 ``URI`` is always a single file or URL. For example:: 
 addid ""foo.mp3"" 
 Id: 999 
 OK 
 *Clarifications:* 
 - ``addid """"`` should return an error.'"
"def SampleRows(df, nrows, replace=False): 
    indices = np.random.choice(df.index, nrows, replace=replace) 
   sample = df.loc[indices] 
   return sample"," 'Returns a sample of the rows of a DataFrame. 
 Parameters 
 df : DataFrame 
 The DataFrame to sample from. 
 nrows : int 
 The number of rows to sample. 
 replace : bool, default False 
 If True, replace the original rows with the sampled rows. 
 Returns 
 DataFrame 
 A sample of the rows of the DataFrame.'","'Choose a sample of rows from a DataFrame. 
 df: DataFrame 
 nrows: number of rows 
 replace: whether to sample with replacement 
 returns: DataDf'"
"def flushall(host=None, port=None, db=None, password=None): 
    server = _connect(host, port, db, password) 
   return server.flushall()"," 'Flush all the data in the database. 
 :param host: the hostname of the server 
 :param port: the port of the server 
 :param db: the database name 
 :param password: the password of the server 
 :return: True if the flush succeeded, False otherwise'","'Remove all keys from all databases 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' redis.flushall'"
"def get_common_path(pathlist): 
    common = osp.normpath(osp.commonprefix(pathlist)) 
   if (len(common) > 1): 
      if (not osp.isdir(common)): 
         return abspardir(common) 
      else: 
         for path in pathlist: 
            if (not osp.isdir(osp.join(common, path[(len(common) + 1):]))): 
               return abspardir(common) 
         else: 
            return osp.abspath(common)"," 'Return the common path of the given pathlist. 
 If the given pathlist is a single path, return that path. 
 If the given pathlist is a list of paths, return the common path. 
 If the common path is not a directory, return it. 
 If the common path is a directory, return the common path of 
 the subpaths of the given pathlist. 
 Examples: 
 >>> from os.path import abspardir, join, normpath 
 >>> get_common_path([join(abspardir(normpath(os.getcwd())), ""foo""), join(abspardir(normpath(os.getcwd())), ""bar"")]) 
 \'/foo/bar\' 
 >>> get_common_path([join(abspardir(normpath(os.getcwd())), ""foo""), join(abspardir(normpath(os.getcwd())), ""bar""), join(abspardir(normpath(os.getcwd())), ""baz"")]) 
 \'/foo/bar\' 
 >>> get_common_path([join(",'Return common path for all paths in pathlist'
"def _getwindowview(folder_alias): 
    attrs = {} 
   args = {} 
   finder = _getfinder() 
   args = {} 
   attrs = {} 
   aeobj_00 = aetypes.ObjectSpecifier(want=aetypes.Type('cfol'), form='alis', seld=folder_alias, fr=None) 
   aeobj_01 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('cwnd'), fr=aeobj_00) 
   aeobj_02 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('pvew'), fr=aeobj_01) 
   args['----'] = aeobj_02 
   (_reply, args, attrs) = finder.send('core', 'getd', args, attrs) 
   if ('errn' in args): 
      raise Error, aetools.decodeerror(args) 
   views = {'iimg': 0, 'pnam': 1, 'lgbu': 2} 
   if ('----' in args): 
      return views[args['----'].enum]"," 'Get the view of the window. 
 :param folder_alias: The folder alias. 
 :type folder_alias: str 
 :return: The view of the window. 
 :rtype: int'",'get the windowview'
"def group_backend_by_type(items): 
    result = defaultdict(list) 
   backends_defined = get_backends() 
   for item in items: 
      name = getattr(item, 'provider', item) 
      backend = backends_defined[name] 
      if issubclass(backend, OpenIdAuth): 
         result['openid'].append(item) 
      elif issubclass(backend, BaseOAuth2): 
         result['oauth2'].append(item) 
      elif issubclass(backend, BaseOAuth1): 
         result['oauth'].append(item) 
   return dict(result)"," 'Return a dict of items grouped by backend type. 
 :param items: 
 :type items: 
 :return: 
 :rtype: 
 :returns: 
 :rtype: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: 
 :returns: ",'Group items by backend type.'
"def test_large_angle_representation(): 
    a = (Angle(350, u.deg) + Angle(350, u.deg)) 
   a.to_string() 
   a.to_string(u.hourangle) 
   repr(a) 
   repr(a.to(u.hourangle)) 
   str(a) 
   str(a.to(u.hourangle))", 'Test that Angle objects can be represented in a large angle format.',"'Test that angles above 360 degrees can be output as strings, 
 in repr, str, and to_string.  (regression test for #1413)'"
"def check_fasta_seqs_lens(input_fasta_fp): 
    seq_lens = defaultdict(int) 
   input_fasta_f = open(input_fasta_fp, 'U') 
   for (label, seq) in parse_fasta(input_fasta_f): 
      seq_lens[len(seq)] += 1 
   input_fasta_f.close() 
   formatted_seq_lens = [] 
   for curr_key in seq_lens: 
      formatted_seq_lens.append((seq_lens[curr_key], curr_key)) 
   formatted_seq_lens.sort(reverse=True) 
   return formatted_seq_lens"," 'Checks the length of each sequence in a fasta file and returns a list of (sequence length, sequence label) tuples.'","'Creates bins of sequence lens 
 Useful for checking for valid aligned sequences. 
 input_fasta_fp:  input fasta filepath'"
"def py_scanstring(s, end, encoding=None, strict=True, _b=BACKSLASH, _m=STRINGCHUNK.match): 
    if (encoding is None): 
      encoding = DEFAULT_ENCODING 
   chunks = [] 
   _append = chunks.append 
   begin = (end - 1) 
   while 1: 
      chunk = _m(s, end) 
      if (chunk is None): 
         raise JSONDecodeError('Unterminated   string   starting   at', s, begin) 
      end = chunk.end() 
      (content, terminator) = chunk.groups() 
      if content: 
         if (not isinstance(content, unicode)): 
            content = unicode(content, encoding) 
         _append(content) 
      if (terminator == '""'): 
         break 
      elif (terminator != '\\'): 
         if strict: 
            msg = ('Invalid   control   character   %r   at' % (terminator,)) 
            raise JSONDecodeError(msg, s, end) 
         else: 
            _append(terminator) 
            continue 
      try: 
         esc = s[end] 
      except IndexError: 
         raise JSONDecodeError('Unterminated   string   starting   at', s, begin) 
      if (esc != 'u'): 
         try: 
            char = _b[esc] 
         except KeyError: 
            msg = ('Invalid   \\escape:   ' + repr(esc)) 
            raise JSONDecodeError(msg, s, end) 
         end += 1 
      else: 
         esc = s[(end + 1):(end + 5)] 
         next_end = (end + 5) 
         if (len(esc) != 4): 
            msg = 'Invalid   \\uXXXX   escape' 
            raise JSONDecodeError(msg, s, end) 
         uni = int(esc, 16) 
         if ((55296 <= uni <= 56319) and (sys.maxunicode > 65535)): 
            msg = 'Invalid   \\uXXXX\\uXXXX   surrogate   pair' 
            if (not (s[(end + 5):(end + 7)] == '\\u')): 
               raise JSONDecodeError(msg, s, end) 
            esc2 = s[(end + 7):(end + 11)] 
            if (len(esc2) != 4): 
               raise JSONDecodeError(msg, s, end) 
            uni2 = int(esc2, 16) 
            uni = (65536 + (((uni - 55296) << 10) | (uni2 - 56320))) 
            next_end += 6 
         char = unichr(uni) 
         end = next_end 
      _append(char) 
   return (u''.join(chunks), end)"," 'Scan a string and return a unicode string and the end of the scan. 
 The string is scanned using the following rules: 
 * If the string begins with a backslash, it is assumed to be an 
 escaped character, and the backslash is removed from the string. 
 * If the string begins with a backslash followed by a u, it is assumed 
 to be a unicode escape, and the next 5 characters are scanned as a 
 unicode character. 
 * If the string begins with a backslash followed by a \u, it is assumed 
 to be a unicode escape, and the next 6 characters are scanned as a 
 unicode character. 
 * If the string begins with a backslash followed by a \U, it is assumed 
 to be a unicode escape, and the next 8 characters are scanned as a 
 unicode character. 
 * If the string begins with a backslash followed by a \uXXXX, it is 
 assumed to be a unicode escape, and the next 4 characters are scanned 
 as a unicode character. 
 * If the string begins with a backslash followed by a \UXXXXXXXX,","'Scan the string s for a JSON string. End is the index of the 
 character in s after the quote that started the JSON string. 
 Unescapes all valid JSON string escape sequences and raises ValueError 
 on attempt to decode an invalid string. If strict is False then literal 
 control characters are allowed in the string. 
 Returns a tuple of the decoded string and the index of the character in s 
 after the end quote.'"
"def harmonicModel(x, fs, w, N, t, nH, minf0, maxf0, f0et): 
    hN = (N / 2) 
   hM1 = int(math.floor(((w.size + 1) / 2))) 
   hM2 = int(math.floor((w.size / 2))) 
   x = np.append(np.zeros(hM2), x) 
   x = np.append(x, np.zeros(hM1)) 
   Ns = 512 
   H = (Ns / 4) 
   hNs = (Ns / 2) 
   pin = max(hNs, hM1) 
   pend = (x.size - max(hNs, hM1)) 
   fftbuffer = np.zeros(N) 
   yh = np.zeros(Ns) 
   y = np.zeros(x.size) 
   w = (w / sum(w)) 
   sw = np.zeros(Ns) 
   ow = triang((2 * H)) 
   sw[(hNs - H):(hNs + H)] = ow 
   bh = blackmanharris(Ns) 
   bh = (bh / sum(bh)) 
   sw[(hNs - H):(hNs + H)] = (sw[(hNs - H):(hNs + H)] / bh[(hNs - H):(hNs + H)]) 
   hfreqp = [] 
   f0t = 0 
   f0stable = 0 
   while (pin < pend): 
      x1 = x[(pin - hM1):(pin + hM2)] 
      (mX, pX) = DFT.dftAnal(x1, w, N) 
      ploc = UF.peakDetection(mX, t) 
      (iploc, ipmag, ipphase) = UF.peakInterp(mX, pX, ploc) 
      ipfreq = ((fs * iploc) / N) 
      f0t = UF.f0Twm(ipfreq, ipmag, f0et, minf0, maxf0, f0stable) 
      if (((f0stable == 0) & (f0t > 0)) or ((f0stable > 0) & (np.abs((f0stable - f0t)) < (f0stable / 5.0)))): 
         f0stable = f0t 
      else: 
         f0stable = 0 
      (hfreq, hmag, hphase) = harmonicDetection(ipfreq, ipmag, ipphase, f0t, nH, hfreqp, fs) 
      hfreqp = hfreq 
      Yh = UF.genSpecSines(hfreq, hmag, hphase, Ns, fs) 
      fftbuffer = np.real(ifft(Yh)) 
      yh[:(hNs - 1)] = fftbuffer[(hNs + 1):] 
      yh[(hNs - 1):] = fftbuffer[:(hNs + 1)] 
      y[(pin - hNs):(pin + hNs)] += (sw * yh) 
      pin += H 
   y = np.delete(y, range(hM2)) 
   y = np.delete(y, range((y.size - hM1), y.size)) 
   return y"," 'Compute a harmonic model for a signal. 
 Parameters 
 x : array 
 The signal to model. 
 fs : float 
 The sampling frequency. 
 w : array 
 The window function. 
 N : int 
 The length of the signal. 
 t : array 
 The time domain signal. 
 nH : int 
 The number of harmonics to model. 
 minf0 : float 
 The minimum frequency to model. 
 maxf0 : float 
 The maximum frequency to model. 
 f0et : float 
 The energy threshold to use. 
 Returns 
 y : array 
 The harmonic model. 
 Notes 
 The harmonic model is computed using the method described in 
 [1]_. 
 References 
 .. [1] M. A. Nielsen, ""The harmonic model of speech,"" in Proc. 
 IEEE Workshop on Applications of Signal Processing to Audio and 
 Acoustics, pp. 217-220, 1983. 
 Examples 
 >>> from scipy.signal import harm","'Analysis/synthesis of a sound using the sinusoidal harmonic model 
 x: input sound, fs: sampling rate, w: analysis window, 
 N: FFT size (minimum 512), t: threshold in negative dB, 
 nH: maximum number of harmonics, minf0: minimum f0 frequency in Hz, 
 maxf0: maximim f0 frequency in Hz, 
 f0et: error threshold in the f0 detection (ex: 5), 
 returns y: output array sound'"
"def _make_allocated_size_testcases(): 
    for unit in (Byte, MB, MiB, GB, GiB): 
      for size in (1, 2, 4, 8): 
         test_case = make_allocated_size_tests(unit(size)) 
         globals()[test_case.__name__] = test_case"," 'Create a testcase for each possible combination of unit and size. 
 The testcase is the name of the testcase, and the testcase itself is the 
 function with the same name.'",'Build test cases for some common allocation_units.'
"def Gamma(name, k, theta): 
    return rv(name, GammaDistribution, (k, theta))"," 'Create a Gamma random variable. 
 Parameters 
 name : str 
 The name of the random variable. 
 k : int 
 The shape of the random variable. 
 theta : float 
 The shape parameter. 
 Returns 
 A :class:`RandomVariable` object. 
 See Also 
 :func:`GammaDistribution`'","'Create a continuous random variable with a Gamma distribution. 
 The density of the Gamma distribution is given by 
 .. math:: 
 f(x) := \frac{1}{\Gamma(k) \theta^k} x^{k - 1} e^{-\frac{x}{\theta}} 
 with :math:`x \in [0,1]`. 
 Parameters 
 k : Real number, `k > 0`, a shape 
 theta : Real number, `\theta > 0`, a scale 
 Returns 
 A RandomSymbol. 
 Examples 
 >>> from sympy.stats import Gamma, density, cdf, E, variance 
 >>> from sympy import Symbol, pprint, simplify 
 >>> k = Symbol(""k"", positive=True) 
 >>> theta = Symbol(""theta"", positive=True) 
 >>> z = Symbol(""z"") 
 >>> X = Gamma(""x"", k, theta) 
 >>> D = density(X)(z) 
 >>> pprint(D, use_unicode=False) 
 -z 
 -k  k - 1  theta 
 theta  *z     *e 
 gamma(k) 
 >>> C = cdf(X, meijerg=True)(z) 
 >>> pprint(C, use_unicode=False) 
 /                                   /     z  \ 
 |                       k*lowergamma|k, -----| 
 |  k*lowergamma(k, 0)               \   theta/ 
 <- ------------------ + ----------------------  for z >= 0 
 |     gamma(k + 1)           gamma(k + 1) 
 \                      0                        otherwise 
 >>> E(X) 
 theta*gamma(k + 1)/gamma(k) 
 >>> V = simplify(variance(X)) 
 >>> pprint(V, use_unicode=False) 
 2 
 k*theta 
 References 
 .. [1] http://en.wikipedia.org/wiki/Gamma_distribution 
 .. [2] http://mathworld.wolfram.com/GammaDistribution.html'"
"def remove_wsgi_intercept(host, port): 
    key = (host, port) 
   if _wsgi_intercept.has_key(key): 
      del _wsgi_intercept[key]", 'Remove the wsgi intercept from the cache.',"'Remove the WSGI intercept call for (host, port).'"
"def build_full_traversal(): 
    TraversalSpec = vmodl.query.PropertyCollector.TraversalSpec 
   SelectionSpec = vmodl.query.PropertyCollector.SelectionSpec 
   rpToRp = TraversalSpec(name='rpToRp', type=vim.ResourcePool, path='resourcePool', skip=False) 
   rpToRp.selectSet.extend((SelectionSpec(name='rpToRp'), SelectionSpec(name='rpToVm'))) 
   rpToVm = TraversalSpec(name='rpToVm', type=vim.ResourcePool, path='vm', skip=False) 
   crToRp = TraversalSpec(name='crToRp', type=vim.ComputeResource, path='resourcePool', skip=False) 
   crToRp.selectSet.extend((SelectionSpec(name='rpToRp'), SelectionSpec(name='rpToVm'))) 
   crToH = TraversalSpec(name='crToH', type=vim.ComputeResource, path='host', skip=False) 
   dcToHf = TraversalSpec(name='dcToHf', type=vim.Datacenter, path='hostFolder', skip=False) 
   dcToHf.selectSet.extend((SelectionSpec(name='visitFolders'),)) 
   dcToVmf = TraversalSpec(name='dcToVmf', type=vim.Datacenter, path='vmFolder', skip=False) 
   dcToVmf.selectSet.extend((SelectionSpec(name='visitFolders'),)) 
   dcToNet = TraversalSpec(name='dcToNet', type=vim.Datacenter, path='networkFolder', skip=False) 
   dcToNet.selectSet.extend((SelectionSpec(name='visitFolders'),)) 
   dcToDs = TraversalSpec(name='dcToDs', type=vim.Datacenter, path='datastore', skip=False) 
   dcToDs.selectSet.extend((SelectionSpec(name='visitFolders'),)) 
   hToVm = TraversalSpec(name='hToVm', type=vim.HostSystem, path='vm', skip=False) 
   hToVm.selectSet.extend((SelectionSpec(name='visitFolders'),)) 
   visitFolders = TraversalSpec(name='visitFolders', type=vim.Folder, path='childEntity', skip=False) 
   visitFolders.selectSet.extend((SelectionSpec(name='visitFolders'), SelectionSpec(name='dcToHf'), SelectionSpec(name='dcToVmf'), SelectionSpec(name='dcToNet'), SelectionSpec(name='crToH'), SelectionSpec(name='crToRp'), SelectionSpec(name='dcToDs'), SelectionSpec(name='hToVm'), SelectionSpec(name='rpToVm'))) 
   fullTraversal = SelectionSpec.Array((visitFolders, dcToHf, dcToVmf, dcToNet, crToH, crToRp, dcToDs, rpToRp, hToVm, rpToVm)) 
   return fullTraversal"," 'Builds a full traversal specification. 
 :return: A traversal specification. 
 :rtype: vmodl.query.PropertyCollector.TraversalSpec'","'Builds a traversal spec that will recurse through all objects .. or at 
 least I think it does. additions welcome. 
 See com.vmware.apputils.vim25.ServiceUtil.buildFullTraversal in the java 
 API. Extended by Sebastian Tello\'s examples from pysphere to reach networks 
 and datastores.'"
"def make_subprocess(cmdline, stdout=False, stderr=False, stdin=False, universal_newlines=False): 
    cmdline = cmdline.encode('ascii') 
   logging.info((""Running   cmd   '%s'"" % cmdline)) 
   kwargs = {} 
   kwargs['stdout'] = ((stdout and subprocess.PIPE) or None) 
   kwargs['stderr'] = ((stderr and subprocess.PIPE) or None) 
   kwargs['stdin'] = ((stdin and subprocess.PIPE) or None) 
   kwargs['universal_newlines'] = universal_newlines 
   args = shlex.split(cmdline) 
   logging.info((""Running   args   '%s'"" % args)) 
   proc = subprocess.Popen(args, **kwargs) 
   return proc"," 'Make a subprocess with the given command line, and return the 
 subprocess object.'",'Make a subprocess according to the given command-line string'
"def __virtual__(): 
    if salt.utils.is_windows(): 
      return __virtualname__ 
   return (False, 'Module   cyg:   module   only   works   on   Windows   systems.')", 'Cygwin module','Only works on Windows systems.'
"def addStreamHandler(): 
    addHandler(logging.StreamHandler(stream=sys.stdout))", 'Add a StreamHandler to the root logger.','Add a stream handler to the default logger.'
"@click.command(name='snapshot') 
 @click.option('--repository', type=str, required=True, help='Snapshot   repository') 
 @click.option('--name', type=str, help='Snapshot   name', show_default=True, default='curator-%Y%m%d%H%M%S') 
 @click.option('--ignore_unavailable', is_flag=True, show_default=True, help='Ignore   unavailable   shards/indices.') 
 @click.option('--include_global_state', type=bool, show_default=True, default=True, expose_value=True, help='Store   cluster   global   state   with   snapshot.') 
 @click.option('--partial', is_flag=True, show_default=True, help='Do   not   fail   if   primary   shard   is   unavailable.') 
 @click.option('--wait_for_completion', type=bool, show_default=True, default=True, help='Wait   for   operation   to   complete') 
 @click.option('--skip_repo_fs_check', is_flag=True, expose_value=True, help='Skip   repository   filesystem   access   validation.') 
 @click.option('--ignore_empty_list', is_flag=True, help='Do   not   raise   exception   if   there   are   no   actionable   indices') 
 @click.option('--filter_list', callback=validate_filter_json, default='{""filtertype"":""none""}', help='JSON   string   representing   an   array   of   filters.') 
 @click.pass_context 
 def snapshot_singleton(ctx, repository, name, ignore_unavailable, include_global_state, partial, skip_repo_fs_check, wait_for_completion, ignore_empty_list, filter_list): 
    action = 'snapshot' 
   action_class = CLASS_MAP[action] 
   c_args = ctx.obj['config']['client'] 
   client = get_client(**c_args) 
   logger = logging.getLogger(__name__) 
   raw_options = {'repository': repository, 'name': name, 'ignore_unavailable': ignore_unavailable, 'include_global_state': include_global_state, 'partial': partial, 'skip_repo_fs_check': skip_repo_fs_check, 'wait_for_completion': wait_for_completion} 
   logger.debug('Validating   provided   options:   {0}'.format(raw_options)) 
   mykwargs = option_schema_check(action, raw_options) 
   logger.debug('Validating   provided   filters:   {0}'.format(filter_list)) 
   clean_filters = {'filters': filter_schema_check(action, filter_list)} 
   ilo = IndexList(client) 
   _do_filters(ilo, clean_filters, ignore_empty_list) 
   action_obj = action_class(ilo, **mykwargs) 
   _actionator(action, action_obj, dry_run=ctx.parent.params['dry_run'])", 'Create a snapshot of the specified indices.','Snapshot indices'
"def info(request, message, extra_tags='', fail_silently=False): 
    add_message(request, constants.INFO, message, extra_tags=extra_tags, fail_silently=fail_silently)", 'Adds an INFO message to the request.','Adds a message with the ``INFO`` level.'
"def string_param(registry, xml_parent, data): 
    base_param(registry, xml_parent, data, True, 'hudson.model.StringParameterDefinition')", 'String parameter.',"'yaml: string 
 A string parameter. 
 :arg str name: the name of the parameter 
 :arg str default: the default value of the parameter (optional) 
 :arg str description: a description of the parameter (optional) 
 Example:: 
 parameters: 
 - string: 
 name: FOO 
 default: bar 
 description: ""A parameter named FOO, defaults to \'bar\'.""'"
"def parse_media_range(range): 
    (type, subtype, params) = parse_mime_type(range) 
   if ((not params.has_key('q')) or (not params['q']) or (not float(params['q'])) or (float(params['q']) > 1) or (float(params['q']) < 0)): 
      params['q'] = '1' 
   return (type, subtype, params)"," 'Parse a media range as defined by RFC 2616, section 14.16.'","'Parse a media-range into its component parts. 
 Carves up a media range and returns a tuple of the (type, subtype, 
 params) where \'params\' is a dictionary of all the parameters for the media 
 range.  For example, the media range \'application/*;q=0.5\' would get parsed 
 into: 
 (\'application\', \'*\', {\'q\', \'0.5\'}) 
 In addition this function also guarantees that there is a value for \'q\' 
 in the params dictionary, filling it in with a proper default if 
 necessary.'"
"def findImageFile(filename): 
    isfile = os.path.isfile 
   if isfile(filename): 
      return filename 
   orig = copy.copy(filename) 
   extensions = ('.jpg', '.png', '.tif', '.bmp', '.gif', '.jpeg', '.tiff') 
   def logCorrected(orig, actual): 
      logging.warn('Requested   image   {!r}   not   found   but   similar   filename   {!r}   exists.   This   will   be   used   instead   but   changing   the   filename   is   advised.'.format(orig, actual)) 
   if filename.endswith(extensions): 
      filename = os.path.splitext(orig)[0] 
   if isfile(filename): 
      logCorrected(orig, filename) 
      return filename 
   for ext in extensions: 
      if isfile((filename + ext)): 
         filename += ext 
         logCorrected(orig, filename) 
         return filename"," 'Return the filename of the image or log a warning if the filename does 
 not exist. 
 Parameters 
 filename : str 
 The filename of the image to find. 
 Returns 
 str 
 The filename of the image.'","'Tests whether the filename is an image file. If not will try some common 
 alternatives (e.g. extensions .jpg .tif...)'"
"def rfft(x, n=None, axis=(-1), overwrite_x=False): 
    tmp = _asfarray(x) 
   if (not numpy.isrealobj(tmp)): 
      raise TypeError('1st   argument   must   be   real   sequence') 
   try: 
      work_function = _DTYPE_TO_RFFT[tmp.dtype] 
   except KeyError: 
      raise ValueError(('type   %s   is   not   supported' % tmp.dtype)) 
   overwrite_x = (overwrite_x or _datacopied(tmp, x)) 
   return _raw_fft(tmp, n, axis, 1, overwrite_x, work_function)"," 'Return the real part of the FFT of ``x``. 
 Parameters 
 x : array_like 
 Input array. 
 n : int, optional 
 The number of points to be taken in the FFT. 
 axis : int, optional 
 The axis along which the FFT is computed. 
 overwrite_x : bool, optional 
 If True, the input array is overwritten. 
 Returns 
 out : ndarray 
 The real part of the FFT of ``x``. 
 Notes 
 The input array must be 1-D, 2-D, or 3-D. 
 If the input array is 1-D, the FFT is computed along the last axis. 
 If the input array is 2-D, the FFT is computed along the last two axes. 
 If the input array is 3-D, the FFT is computed along the last three axes. 
 Examples 
 >>> x = np.arange(24).reshape(3, 4) 
 >>> np.fft.rfft(x) 
 array([[-0.3","'Discrete Fourier transform of a real sequence. 
 Parameters 
 x : array_like, real-valued 
 The data to transform. 
 n : int, optional 
 Defines the length of the Fourier transform.  If `n` is not specified 
 (the default) then ``n = x.shape[axis]``.  If ``n < x.shape[axis]``, 
 `x` is truncated, if ``n > x.shape[axis]``, `x` is zero-padded. 
 axis : int, optional 
 The axis along which the transform is applied.  The default is the 
 last axis. 
 overwrite_x : bool, optional 
 If set to true, the contents of `x` can be overwritten. Default is 
 False. 
 Returns 
 z : real ndarray 
 The returned real array contains:: 
 [y(0),Re(y(1)),Im(y(1)),...,Re(y(n/2))]              if n is even 
 [y(0),Re(y(1)),Im(y(1)),...,Re(y(n/2)),Im(y(n/2))]   if n is odd 
 where:: 
 y(j) = sum[k=0..n-1] x[k] * exp(-sqrt(-1)*j*k*2*pi/n) 
 j = 0..n-1 
 Note that ``y(-j) == y(n-j).conjugate()``. 
 See Also 
 fft, irfft, scipy.fftpack.basic 
 Notes 
 Within numerical accuracy, ``y == rfft(irfft(y))``. 
 Both single and double precision routines are implemented.  Half precision 
 inputs will be converted to single precision.  Non floating-point inputs 
 will be converted to double precision.  Long-double precision inputs are 
 not supported. 
 Examples 
 >>> from scipy.fftpack import fft, rfft 
 >>> a = [9, -9, 1, 3] 
 >>> fft(a) 
 array([  4. +0.j,   8.+12.j,  16. +0.j,   8.-12.j]) 
 >>> rfft(a) 
 array([  4.,   8.,  12.,  16.])'"
"def login_required(handler_method): 
    def check_login(self, *args, **kwargs): 
      if (self.request.method != 'GET'): 
         self.abort(400, detail='The   login_required   decorator   can   only   be   used   for   GET   requests.') 
      user = users.get_current_user() 
      if (not user): 
         return self.redirect(users.create_login_url(self.request.url)) 
      else: 
         handler_method(self, *args, **kwargs) 
   return check_login"," 'Decorator for methods that should only be called by logged-in users. 
 This decorator is used by the :class:`LoginRequiredMixin` class. 
 This decorator should only be used for GET requests. 
 :param handler_method: The handler method to decorate. 
 :type handler_method: function 
 :param args: The arguments to pass to the handler method. 
 :type args: tuple 
 :param kwargs: The keyword arguments to pass to the handler method. 
 :type kwargs: dict 
 :return: The handler method decorated with the login_required decorator. 
 :rtype: function'","'A decorator to require that a user be logged in to access a handler. 
 To use it, decorate your get() method like this:: 
 @login_required 
 def get(self): 
 user = users.get_current_user(self) 
 self.response.out.write(\'Hello, \' + user.nickname()) 
 We will redirect to a login page if the user is not logged in. We always 
 redirect to the request URI, and Google Accounts only redirects back as 
 a GET request, so this should not be used for POSTs.'"
"def get_writer(extension): 
    global FORMAT_WRITERS 
   if (FORMAT_WRITERS is None): 
      _import_writers() 
   return FORMAT_WRITERS.get(extension, None)"," 'Returns a writer for a given file extension. 
 If the extension is not found, returns None. 
 :param extension: the extension of the file to write to 
 :type extension: str 
 :returns: the writer or None'",'Returns None if no writer is found for extension.'
"def _compute_variable_length_solns(model, t0, k0, g, tol): 
    results = {} 
   for integrator in ['dopri5', 'dop853', 'vode', 'lsoda']: 
      discrete_soln = model.solve(t0, k0, h=1.0, g=g, tol=tol, integrator=integrator, atol=1e-14, rtol=1e-11) 
      results[integrator] = discrete_soln 
   return results"," 'Compute the variable length solution of the ODE using the given 
 integrator. 
 Parameters 
 model : Model 
 The model to be solved. 
 t0 : float 
 Initial time. 
 k0 : float 
 Initial value of the state variable. 
 g : Function 
 The right hand side of the ODE. 
 tol : float 
 Tolerance for the error. 
 integrator : str 
 Integrator to be used. 
 Returns 
 results : dict 
 Dictionary containing the solution and the error for each integrator.'",'Returns a dictionary of variable length solution trajectories.'
"def SortedConcatenate(sep=u','): 
    def step(ctxt, ndx, value): 
      if (value is not None): 
         ctxt[ndx] = value 
   def finalize(ctxt): 
      if (len(ctxt) == 0): 
         return None 
      return sep.join(map(ctxt.get, sorted(ctxt.iterkeys()))) 
   return ({}, step, finalize)"," 'A generator that concatenates a sequence of values, sorted by key. 
 This is a generator, so it can be used in a for loop. 
 >>> from pprint import pprint 
 >>> def test(key): 
 ...     pprint(sorted(key)) 
 >>> for x in sortedconcatenate(test): 
 ...     print(x) 
 0 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 9 
 10 
 11 
 12 
 13 
 14 
 15 
 16 
 17 
 18 
 19 
 20 
 21 
 22 
 23 
 24 
 25 
 26 
 27 
 28 
 29 
 30 
 31 
 32 
 33 
 34 
 35 
 3","'String concatenation aggregator for sqlite, sorted by supplied index'"
"def make_headers(keep_alive=None, accept_encoding=None, user_agent=None, basic_auth=None, proxy_basic_auth=None): 
    headers = {} 
   if accept_encoding: 
      if isinstance(accept_encoding, str): 
         pass 
      elif isinstance(accept_encoding, list): 
         accept_encoding = ','.join(accept_encoding) 
      else: 
         accept_encoding = 'gzip,deflate' 
      headers['accept-encoding'] = accept_encoding 
   if user_agent: 
      headers['user-agent'] = user_agent 
   if keep_alive: 
      headers['connection'] = 'keep-alive' 
   if basic_auth: 
      headers['authorization'] = ('Basic   ' + b64encode(six.b(basic_auth)).decode('utf-8')) 
   if proxy_basic_auth: 
      headers['proxy-authorization'] = ('Basic   ' + b64encode(six.b(proxy_basic_auth)).decode('utf-8')) 
   return headers", 'Make a dictionary of HTTP headers.',"'Shortcuts for generating request headers. 
 :param keep_alive: 
 If ``True``, adds \'connection: keep-alive\' header. 
 :param accept_encoding: 
 Can be a boolean, list, or string. 
 ``True`` translates to \'gzip,deflate\'. 
 List will get joined by comma. 
 String will be used as provided. 
 :param user_agent: 
 String representing the user-agent you want, such as 
 ""python-urllib3/0.6"" 
 :param basic_auth: 
 Colon-separated username:password string for \'authorization: basic ...\' 
 auth header. 
 :param proxy_basic_auth: 
 Colon-separated username:password string for \'proxy-authorization: basic ...\' 
 auth header. 
 Example: :: 
 >>> make_headers(keep_alive=True, user_agent=""Batman/1.0"") 
 {\'connection\': \'keep-alive\', \'user-agent\': \'Batman/1.0\'} 
 >>> make_headers(accept_encoding=True) 
 {\'accept-encoding\': \'gzip,deflate\'}'"
"def group(seq, size): 
    def take(seq, n): 
      for i in xrange(n): 
         (yield seq.next()) 
   if (not hasattr(seq, 'next')): 
      seq = iter(seq) 
   while True: 
      x = list(take(seq, size)) 
      if x: 
         (yield x) 
      else: 
         break"," 'Return a generator that yields sequences of size *size* from *seq*. 
 >>> list(group(range(10), 3)) 
 [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]'","'Returns an iterator over a series of lists of length size from iterable. 
 >>> list(group([1,2,3,4], 2)) 
 [[1, 2], [3, 4]] 
 >>> list(group([1,2,3,4,5], 2)) 
 [[1, 2], [3, 4], [5]]'"
"def safe_value(name, value): 
    if (name.lower() in LOGGER_SENSITIVE_HEADERS): 
      prefix_length = logger_settings.get('reveal_sensitive_prefix', 16) 
      prefix_length = int(min(prefix_length, ((len(value) ** 2) / 32), (len(value) / 2))) 
      redacted_value = value[0:prefix_length] 
      return (redacted_value + '...') 
   return value"," 'Returns a sanitized version of a value, if the value is a 
 string, that has sensitive data in it. 
 :param name: the name of the header to sanitize 
 :param value: the value to sanitize'","'Only show up to logger_settings[\'reveal_sensitive_prefix\'] characters 
 from a sensitive header. 
 :param name: Header name 
 :param value: Header value 
 :return: Safe header value'"
"def quote(string, safe=u'/'): 
    if (sys.version_info.major < 3): 
      if isinstance(string, unicode): 
         string = string.encode(u'utf8') 
      string = urllib.quote(string, safe.encode(u'utf8')) 
   else: 
      string = urllib.parse.quote(str(string), safe) 
   return string"," 'Quote a string for use in a URL. 
 :param string: The string to quote. 
 :param safe: A string of safe characters to use in place of unsafe characters. 
 :return: The quoted string.'",'Like urllib2.quote but handles unicode properly.'
"def delete_subnet_group(name, region=None, key=None, keyid=None, profile=None): 
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
   if (not conn): 
      return False 
   try: 
      conn.delete_cache_subnet_group(name) 
      msg = 'Deleted   ElastiCache   subnet   group   {0}.'.format(name) 
      log.info(msg) 
      return True 
   except boto.exception.BotoServerError as e: 
      log.debug(e) 
      msg = 'Failed   to   delete   ElastiCache   subnet   group   {0}'.format(name) 
      log.error(msg) 
      return False"," 'Deletes the specified subnet group. 
 :param name: The name of the subnet group. 
 :type name: str 
 :param region: The region name. 
 :type region: str 
 :param key: The access key. 
 :type key: str 
 :param keyid: The access key ID. 
 :type keyid: str 
 :param profile: The profile name. 
 :type profile: str 
 :return: True if the subnet group was deleted, False otherwise.'","'Delete an ElastiCache subnet group. 
 CLI example:: 
 salt myminion boto_elasticache.delete_subnet_group my-subnet-group                 region=us-east-1'"
"def _in_gce_environment(): 
    if (SETTINGS.env_name is not None): 
      return (SETTINGS.env_name == 'GCE_PRODUCTION') 
   if ((NO_GCE_CHECK != 'True') and _detect_gce_environment()): 
      SETTINGS.env_name = 'GCE_PRODUCTION' 
      return True 
   return False"," 'Detects if the current environment is GCE. 
 If it is, sets the environment to GCE_PRODUCTION. 
 :return: True if in GCE, False otherwise'","'Detect if the code is running in the Compute Engine environment. 
 Returns: 
 True if running in the GCE environment, False otherwise.'"
"def useTest(vm, prompt=Prompt): 
    old = vm.logfile 
   if (old == stdout): 
      log('*   Temporarily   disabling   logging   to   stdout') 
      vm.logfile = None 
   log('*   Switching   to   interactive   use   -   press   control-]   to   exit') 
   vm.interact() 
   if (old == stdout): 
      log('*   Restoring   logging   to   stdout') 
      vm.logfile = stdout"," 'Run the VM in interactive mode, so that the user can enter 
 commands. 
 :param vm: The VM to use. 
 :param prompt: The prompt to display.'",'Use VM interactively - exit by pressing control-]'
"def dimension_mul(a, b): 
    if ((a == datashape.var) or (b == datashape.var)): 
      return datashape.var 
   if isinstance(a, Fixed): 
      a = int(a) 
   if isinstance(b, Fixed): 
      b = int(b) 
   return int((a * b))"," 'Multiply two dimensions. 
 Parameters 
 a : datashape 
 The first dimension. 
 b : datashape 
 The second dimension. 
 Returns 
 datashape 
 The result of multiplying the two dimensions. 
 Examples 
 >>> from sympy.utilities.datashape import dimension_mul 
 >>> from sympy.utilities.datashape import datashape 
 >>> a = datashape([1, 2, 3]) 
 >>> b = datashape([4, 5, 6]) 
 >>> dimension_mul(a, b) 
 [1, 2, 3, 4, 5, 6]'","'Given b number of a\'s how big is our dimension? 
 >>> dimension_mul(2, 5) 
 10 
 We round up 
 >>> dimension_mul(9, 3) 
 27 
 In the case of datashape.var, we resort to var 
 >>> from datashape import var 
 >>> dimension_mul(datashape.var, 5) 
 Var() 
 >>> dimension_mul(10, datashape.var) 
 Var()'"
"def mask_between_time(dts, start, end, include_start=True, include_end=True): 
    time_micros = dts._get_time_micros() 
   start_micros = _time_to_micros(start) 
   end_micros = _time_to_micros(end) 
   (left_op, right_op, join_op) = _opmap[(bool(include_start), bool(include_end), (start_micros <= end_micros))] 
   return join_op(left_op(start_micros, time_micros), right_op(time_micros, end_micros))"," 'Mask between two times. 
 Parameters 
 dts : DataTimeSeries 
 TimeSeries to mask. 
 start : Time 
 Start time to mask. 
 end : Time 
 End time to mask. 
 include_start : bool 
 Whether to include start time. 
 include_end : bool 
 Whether to include end time. 
 Returns 
 TimeSeries 
 TimeSeries with masked data. 
 Examples 
 >>> dts = DataTimeSeries(1, 2, 3, 4) 
 >>> dts.mask_between_time(Time(1, 0), Time(1, 30)) 
 TimeSeries(1, 2, 3, 4) 
 >>> dts.mask_between_time(Time(1, 0), Time(1, 30), include_start=False) 
 TimeSeries(2, 3, 4) 
 >>> dts.mask_between_time(Time(1, 0), Time(1, 30), include_end=False) 
 TimeSeries(1, 2, 3)'","'Return a mask of all of the datetimes in ``dts`` that are between 
 ``start`` and ``end``. 
 Parameters 
 dts : pd.DatetimeIndex 
 The index to mask. 
 start : time 
 Mask away times less than the start. 
 end : time 
 Mask away times greater than the end. 
 include_start : bool, optional 
 Inclusive on ``start``. 
 include_end : bool, optional 
 Inclusive on ``end``. 
 Returns 
 mask : np.ndarray[bool] 
 A bool array masking ``dts``. 
 See Also 
 :meth:`pandas.DatetimeIndex.indexer_between_time`'"
"def line_search_armijo(f, xk, pk, gfk, old_fval, args=(), c1=0.0001, alpha0=1): 
    xk = np.atleast_1d(xk) 
   fc = [0] 
   def phi(alpha1): 
      fc[0] += 1 
      return f((xk + (alpha1 * pk)), *args) 
   if (old_fval is None): 
      phi0 = phi(0.0) 
   else: 
      phi0 = old_fval 
   derphi0 = np.dot(gfk, pk) 
   (alpha, phi1) = scalar_search_armijo(phi, phi0, derphi0, c1=c1, alpha0=alpha0) 
   return (alpha, fc[0], phi1)"," 'Armijo line search. 
 This is a line search that uses the Armijo condition for stopping the 
 line search. 
 Parameters 
 f : function 
 The objective function. 
 xk : array 
 The current iterate. 
 pk : array 
 The step size. 
 gfk : array 
 The gradient of the objective function at xk. 
 old_fval : float 
 The old function value. 
 args : tuple 
 The arguments to the objective function. 
 c1 : float 
 The constant used in the Armijo condition. 
 alpha0 : float 
 The initial value of the step size. 
 Returns 
 alpha : float 
 The step size. 
 fc : float 
 The function value. 
 phi1 : float 
 The function value. 
 Notes 
 This is a line search that uses the Armijo condition for stopping the 
 line search. 
 The Armijo condition is that the function value at the current iterate 
 is no greater than the function value at the previous iterate plus a 
 constant multiple of the step size. 
 The step size is chosen","'Minimize over alpha, the function ``f(xk+alpha pk)``. 
 Parameters 
 f : callable 
 Function to be minimized. 
 xk : array_like 
 Current point. 
 pk : array_like 
 Search direction. 
 gfk : array_like 
 Gradient of `f` at point `xk`. 
 old_fval : float 
 Value of `f` at point `xk`. 
 args : tuple, optional 
 Optional arguments. 
 c1 : float, optional 
 Value to control stopping criterion. 
 alpha0 : scalar, optional 
 Value of `alpha` at start of the optimization. 
 Returns 
 alpha 
 f_count 
 f_val_at_alpha 
 Notes 
 Uses the interpolation algorithm (Armijo backtracking) as suggested by 
 Wright and Nocedal in \'Numerical Optimization\', 1999, pg. 56-57'"
"def compile_and_install_client(project_client, extra_args='', install_client=True): 
    java_args = {} 
   java_args['compile_dir'] = _TMP_COMPILE_DIR 
   java_args['app_dir'] = _DEFAULT_APP_DIR 
   java_args['gwt_dir'] = find_gwt_dir() 
   java_args['extra_args'] = extra_args 
   java_args['project_client'] = project_client 
   cmd = (_COMPILE_LINE % java_args) 
   logging.info('Compiling   client   %s', project_client) 
   try: 
      utils.run(cmd, verbose=True) 
      if install_client: 
         return install_completed_client(java_args['compile_dir'], project_client) 
      return True 
   except error.CmdError: 
      logging.info('Error   compiling   %s,   leaving   old   client', project_client) 
   return False"," 'Compiles and installs the client. 
 :param project_client: The project client to compile. 
 :param extra_args: Extra arguments to pass to the compiler. 
 :param install_client: If True, the client will be installed. 
 :return: True if the client was successfully installed, False if it was not.'","'Compile the client into a temporary directory, if successful 
 call install_completed_client to install the new client. 
 :param project_client: project.client pair e.g. autotest.AfeClient 
 :param install_client: Boolean, if True install the clients 
 :return: True if install and compile was successful False if it failed'"
"def _replication_request(command, host=None, core_name=None, params=None): 
    params = ([] if (params is None) else params) 
   extra = (['command={0}'.format(command)] + params) 
   url = _format_url('replication', host=host, core_name=core_name, extra=extra) 
   return _http_request(url)"," 'Replicate a core. 
 :param command: The command to replicate. 
 :param host: The host to use. 
 :param core_name: The core to replicate. 
 :param params: A list of parameters to append to the URL. 
 :returns: A dictionary of the results.'","'PRIVATE METHOD 
 Performs the requested replication command and returns a dictionary with 
 success, errors and data as keys. The data object will contain the JSON 
 response. 
 command : str 
 The replication command to execute. 
 host : str (None) 
 The solr host to query. __opts__[\'host\'] is default 
 core_name: str (None) 
 The name of the solr core if using cores. Leave this blank if you are 
 not using cores or if you want to check all cores. 
 params : list<str> ([]) 
 Any additional parameters you want to send. Should be a lsit of 
 strings in name=value format. e.g. [\'name=value\'] 
 Return: dict<str, obj>:: 
 {\'success\':boolean, \'data\':dict, \'errors\':list, \'warnings\':list}'"
"def config_value(option): 
    return option_list[option]"," 'Returns the value of a configuration option. 
 :param option: The option to retrieve. 
 :type option: str'",'Return the current configuration value for the given option'
"def synchronize(*klasses): 
    if (threadingmodule is not None): 
      for klass in klasses: 
         for methodName in klass.synchronized: 
            sync = _sync(klass, klass.__dict__[methodName]) 
            setattr(klass, methodName, sync)"," 'Decorator that adds synchronization to a method. 
 This decorator is used to add synchronization to methods of 
 :class:`~.threading.Thread` and :class:`~.threading.Event`. 
 :param klass: The class to decorate. 
 :type klass: type 
 :param methodName: The name of the method to decorate. 
 :type methodName: str 
 :returns: The decorated method. 
 :rtype: function 
 :raises TypeError: If the method is not a method of the class.'","'Make all methods listed in each class\' synchronized attribute synchronized. 
 The synchronized attribute should be a list of strings, consisting of the 
 names of methods that must be synchronized. If we are running in threaded 
 mode these methods will be wrapped with a lock.'"
"def __virtual__(): 
    if (not salt.utils.is_windows()): 
      return (False, u'Module   PSGet:   Module   only   works   on   Windows   systems') 
   powershell_info = __salt__[u'cmd.shell_info'](u'powershell') 
   if ((not powershell_info[u'installed']) or (distutils.version.StrictVersion(powershell_info[u'version']) >= distutils.version.StrictVersion(u'5.0'))): 
      return (False, u'Module   DSC:   Module   only   works   with   PowerShell   5   or   newer.') 
   return __virtualname__", 'Returns a virtual module if the PowerShell module DSC is installed.','Set the system module of the kernel is Windows'
"def request_fingerprint(request, include_headers=None): 
    if include_headers: 
      include_headers = tuple((to_bytes(h.lower()) for h in sorted(include_headers))) 
   cache = _fingerprint_cache.setdefault(request, {}) 
   if (include_headers not in cache): 
      fp = hashlib.sha1() 
      fp.update(to_bytes(request.method)) 
      fp.update(to_bytes(canonicalize_url(request.url))) 
      fp.update((request.body or '')) 
      if include_headers: 
         for hdr in include_headers: 
            if (hdr in request.headers): 
               fp.update(hdr) 
               for v in request.headers.getlist(hdr): 
                  fp.update(v) 
      cache[include_headers] = fp.hexdigest() 
   return cache[include_headers]"," 'Return the fingerprint of the given request. 
 If the request is a GET request, the fingerprint is the hash of the 
 request method, the request URL, and the request body. 
 If the request is a POST request, the fingerprint is the hash of the 
 request method, the request URL, and the request body, and the 
 request headers. 
 If the request is a HEAD request, the fingerprint is the hash of the 
 request method, the request URL, and the request body, and the 
 request headers. 
 If the request is a PUT request, the fingerprint is the hash of the 
 request method, the request URL, and the request body, and the 
 request headers. 
 If the request is a DELETE request, the fingerprint is the hash of the 
 request method, the request URL, and the request body, and the 
 request headers. 
 If the request is a TRACE request, the fingerprint is the hash of the 
 request method, the request URL, and the request body, and the 
 request headers. 
 If the request is a CONNECT request, the fingerprint","'Return the request fingerprint. 
 The request fingerprint is a hash that uniquely identifies the resource the 
 request points to. For example, take the following two urls: 
 http://www.example.com/query?id=111&cat=222 
 http://www.example.com/query?cat=222&id=111 
 Even though those are two different URLs both point to the same resource 
 and are equivalent (ie. they should return the same response). 
 Another example are cookies used to store session ids. Suppose the 
 following page is only accesible to authenticated users: 
 http://www.example.com/members/offers.html 
 Lot of sites use a cookie to store the session id, which adds a random 
 component to the HTTP Request and thus should be ignored when calculating 
 the fingerprint. 
 For this reason, request headers are ignored by default when calculating 
 the fingeprint. If you want to include specific headers use the 
 include_headers argument, which is a list of Request headers to include.'"
"def parse_argv(tokens, options, options_first=False): 
    parsed = [] 
   while (tokens.current() is not None): 
      if (tokens.current() == '--'): 
         return (parsed + [Argument(None, v) for v in tokens]) 
      elif tokens.current().startswith('--'): 
         parsed += parse_long(tokens, options) 
      elif (tokens.current().startswith('-') and (tokens.current() != '-')): 
         parsed += parse_shorts(tokens, options) 
      elif options_first: 
         return (parsed + [Argument(None, v) for v in tokens]) 
      else: 
         parsed.append(Argument(None, tokens.move())) 
   return parsed"," 'Parse command line arguments. 
 :param tokens: 
 :param options: 
 :param options_first: 
 :return: 
 :rtype: 
 :raises: 
 :param tokens: 
 :param options: 
 :param options_first: 
 :return: 
 :rtype: 
 :raises: 
 :param tokens: 
 :param options: 
 :param options_first: 
 :return: 
 :rtype: 
 :raises: 
 :param tokens: 
 :param options: 
 :param options_first: 
 :return: 
 :rtype: 
 :raises: 
 :param tokens: 
 :param options: 
 :param options_first: 
 :return: 
 :rtype: 
 :raises: 
 :param tokens: 
 :param options: 
 :param options_first: 
 :return: 
 :rtype: 
 :raises: 
 :param tokens: 
 :param options: 
 :param options_first: 
 :return: 
 :","'Parse command-line argument vector. 
 If options_first: 
 argv ::= [ long | shorts ]* [ argument ]* [ \'--\' [ argument ]* ] ; 
 else: 
 argv ::= [ long | shorts | argument ]* [ \'--\' [ argument ]* ] ;'"
"def _make_namespaced_xattr_key(key, namespace='user'): 
    namespaced_key = '.'.join([namespace, key]) 
   return namespaced_key", 'Create a namespace-prefixed xattr key from a key.',"'Create a fully-qualified xattr-key by including the intended namespace. 
 Namespacing differs among OSes[1]: 
 FreeBSD: user, system 
 Linux: user, system, trusted, security 
 MacOS X: not needed 
 Mac OS X won\'t break if we include a namespace qualifier, so, for 
 simplicity, we always include it. 
 [1] http://en.wikipedia.org/wiki/Extended_file_attributes'"
"def main(): 
    args = _parse_args() 
   if (args.pkl is not None): 
      dataset = serial.load(args.pkl) 
   else: 
      dataset = NORB(args.which_norb, args.which_set) 
   grid_indices = ([0] * 5) 
   grid_to_short_label = _make_grid_to_short_label(dataset) 
   label_to_row_indices = _make_label_to_row_indices(dataset.y) 
   object_image_index = [0] 
   blank_image_index = [0] 
   blank_label = _get_blank_label(dataset) 
   grid_dimension = [0] 
   dataset_is_stereo = ('s' in dataset.view_converter.axes) 
   (figure, all_axes) = pyplot.subplots(1, (3 if dataset_is_stereo else 2), squeeze=True, figsize=(10, 3.5)) 
   set_name = (os.path.split(args.pkl)[1] if (args.which_set is None) else ('%sing   set' % args.which_set)) 
   figure.canvas.set_window_title(('NORB   dataset   (%s)' % set_name)) 
   label_text = figure.suptitle('Up/down   arrows   choose   label,   left/right   arrows   change   it', x=0.1, horizontalalignment='left') 
   for axes in all_axes: 
      axes.get_xaxis().set_visible(False) 
      axes.get_yaxis().set_visible(False) 
   (text_axes, image_axes) = (all_axes[0], all_axes[1:]) 
   image_captions = (('left', 'right') if dataset_is_stereo else ('mono   image',)) 
   if args.stereo_viewer: 
      image_captions = tuple(reversed(image_captions)) 
   for (image_ax, caption) in safe_zip(image_axes, image_captions): 
      image_ax.set_title(caption) 
   text_axes.set_frame_on(False) 
   def is_blank(grid_indices): 
      assert (len(grid_indices) == 5) 
      assert all(((x >= 0) for x in grid_indices)) 
      ci = dataset.label_name_to_index['category'] 
      category = grid_to_short_label[ci][grid_indices[ci]] 
      category_name = dataset.label_to_value_funcs[ci](category) 
      return (category_name == 'blank') 
   def get_short_label(grid_indices): 
      ""\n                        Returns   the   first   5   elements   of   the   label   vector   pointed   to   by\n                        grid_indices.   We   use   the   first   5,   since   they're   the   labels   used   by\n                        both   the   'big'   and   Small   NORB   datasets.\n                        "" 
      if is_blank(grid_indices): 
         return tuple(blank_label[:5]) 
      else: 
         return tuple((grid_to_short_label[i][g] for (i, g) in enumerate(grid_indices))) 
   def get_row_indices(grid_indices): 
      short_label = get_short_label(grid_indices) 
      return label_to_row_indices.get(short_label, None) 
   axes_to_pixels = {} 
   def redraw(redraw_text, redraw_images): 
      row_indices = get_row_indices(grid_indices) 
      if (row_indices is None): 
         row_index = None 
         image_index = 0 
         num_images = 0 
      else: 
         image_index = (blank_image_index if is_blank(grid_indices) else object_image_index)[0] 
         row_index = row_indices[image_index] 
         num_images = len(row_indices) 
      def draw_text(): 
         if (row_indices is None): 
            padding_length = (dataset.y.shape[1] - len(grid_indices)) 
            current_label = (tuple(get_short_label(grid_indices)) + ((0,) * padding_length)) 
         else: 
            current_label = dataset.y[row_index, :] 
         label_names = dataset.label_index_to_name 
         label_values = [label_to_value(label) for (label_to_value, label) in safe_zip(dataset.label_to_value_funcs, current_label)] 
         lines = [('%s:   %s' % (t, v)) for (t, v) in safe_zip(label_names, label_values)] 
         if (dataset.y.shape[1] > 5): 
            lines = ((lines[:5] + [('No   such   image' if (num_images == 0) else ('image:   %d   of   %d' % ((image_index + 1), num_images))), '\n']) + lines[5:]) 
         lines[grid_dimension[0]] = ('==>   ' + lines[grid_dimension[0]]) 
         text_axes.clear() 
         text_axes.text(0, 0.5, '\n'.join(lines), verticalalignment='center', transform=text_axes.transAxes) 
      def draw_images(): 
         if (row_indices is None): 
            for axis in image_axes: 
               axis.clear() 
         else: 
            data_row = dataset.X[row_index:(row_index + 1), :] 
            axes_names = dataset.view_converter.axes 
            assert (len(axes_names) in (4, 5)) 
            assert (axes_names[0] == 'b') 
            assert (axes_names[(-3)] == 0) 
            assert (axes_names[(-2)] == 1) 
            assert (axes_names[(-1)] == 'c') 
            def draw_image(image, axes): 
               assert (len(image.shape) == 2) 
               norm = (matplotlib.colors.NoNorm() if args.no_norm else None) 
               axes_to_pixels[axes] = image 
               axes.imshow(image, norm=norm, cmap='gray') 
            if ('s' in axes_names): 
               image_pair = dataset.get_topological_view(mat=data_row, single_tensor=True) 
               image_pair = tuple(image_pair[0, :, :, :, 0]) 
               if args.stereo_viewer: 
                  image_pair = tuple(reversed(image_pair)) 
               for (axis, image) in safe_zip(image_axes, image_pair): 
                  draw_image(image, axis) 
            else: 
               image = dataset.get_topological_view(mat=data_row) 
               image = image[0, :, :, 0] 
               draw_image(image, image_axes[0]) 
      if redraw_text: 
         draw_text() 
      if redraw_images: 
         draw_images() 
      figure.canvas.draw() 
   default_status_text = ('mouseover   image%s   for   pixel   values' % ('' if (len(image_axes) == 1) else 's')) 
   status_text = figure.text(0.5, 0.1, default_status_text) 
   def on_mouse_motion(event): 
      original_text = status_text.get_text() 
      if (event.inaxes not in image_axes): 
         status_text.set_text(default_status_text) 
      else: 
         pixels = axes_to_pixels[event.inaxes] 
         row = int((event.ydata + 0.5)) 
         col = int((event.xdata + 0.5)) 
         status_text.set_text(('Pixel   value:   %g' % pixels[(row, col)])) 
      if (status_text.get_text != original_text): 
         figure.canvas.draw() 
   def on_key_press(event): 
      def add_mod(arg, step, size): 
         return (((arg + size) + step) % size) 
      def incr_index_type(step): 
         num_dimensions = len(grid_indices) 
         if (dataset.y.shape[1] > 5): 
            num_dimensions += 1 
         grid_dimension[0] = add_mod(grid_dimension[0], step, num_dimensions) 
      def incr_index(step): 
         assert (step in (0, (-1), 1)), ('Step   was   %d' % step) 
         image_index = (blank_image_index if is_blank(grid_indices) else object_image_index) 
         if (grid_dimension[0] == 5): 
            row_indices = get_row_indices(grid_indices) 
            if (row_indices is None): 
               image_index[0] = 0 
            else: 
               image_index[0] = add_mod(image_index[0], step, len(row_indices)) 
         else: 
            gd = grid_dimension[0] 
            grid_indices[gd] = add_mod(grid_indices[gd], step, len(grid_to_short_label[gd])) 
            row_indices = get_row_indices(grid_indices) 
            if (row_indices is None): 
               image_index[0] = 0 
            else: 
               image_index[0] = min(image_index[0], len(row_indices)) 
      disable_left_right = (is_blank(grid_indices) and (not (grid_dimension[0] in (0, 5)))) 
      if (event.key == 'up'): 
         incr_index_type((-1)) 
         redraw(True, False) 
      elif (event.key == 'down'): 
         incr_index_type(1) 
         redraw(True, False) 
      elif (event.key == 'q'): 
         sys.exit(0) 
      elif (not disable_left_right): 
         if (event.key == 'left'): 
            incr_index((-1)) 
            redraw(True, True) 
         elif (event.key == 'right'): 
            incr_index(1) 
            redraw(True, True) 
   figure.canvas.mpl_connect('key_press_event', on_key_press) 
   figure.canvas.mpl_connect('motion_notify_event', on_mouse_motion) 
   redraw(True, True) 
   pyplot.show()"," 'main() 
 This is the main function of the script. 
 It parses the command-line arguments and then calls the 
 ``main_loop()`` function. 
 Parameters 
 args : argparse.Namespace 
 The parsed arguments. 
 Returns 
 None 
 Notes 
 This function is called by the ``main()`` function of the main 
 script. 
 See Also 
 main 
 main_loop' below below 
 Examples 
 >>> import matplotlib.pyplot as plt 
 >>> import matplotlib 
 >>> import numpy 
 >>> from matplotlib.pyplot import show 
 >>> from matplotlib.pyplot import figure 
 >>> from matplotlib.pyplot import plt 
 >>> from matplotlib.pyplot import imshow 
 >>> from matplotlib.pyplot import grid 
 >>> from matplotlib.pyplot import get_cmap 
 >>> from matplotlib.colors import ListedColormap 
 >>> from matplotlib.colors import Normalize 
 >>> from matplotlib.colors import BoundaryNorm 
 >>> from matplotlib.colors import LinearSegmentedColormap 
 >>> from matplotlib.colors import LinearSegmentedColormap ",'Top-level function.'
"@slow_test 
 @requires_sklearn_0_15 
 def test_generalization_across_time(): 
    from sklearn.svm import SVC 
   from sklearn.base import is_classifier 
   from sklearn.kernel_ridge import KernelRidge 
   from sklearn.preprocessing import LabelEncoder 
   from sklearn.metrics import roc_auc_score, mean_squared_error 
   epochs = make_epochs() 
   y_4classes = np.hstack((epochs.events[:7, 2], (epochs.events[7:, 2] + 1))) 
   if check_version('sklearn', '0.18'): 
      from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, LeaveOneGroupOut 
      cv = LeaveOneGroupOut() 
      cv_shuffle = ShuffleSplit() 
      cv_lolo = [(train, test) for (train, test) in cv.split(y_4classes, y_4classes, y_4classes)] 
      scorer_regress = None 
   else: 
      from sklearn.cross_validation import KFold, StratifiedKFold, ShuffleSplit, LeaveOneLabelOut 
      cv_shuffle = ShuffleSplit(len(epochs)) 
      cv_lolo = LeaveOneLabelOut(y_4classes) 
      scorer_regress = mean_squared_error 
   gat = GeneralizationAcrossTime(picks='foo') 
   assert_equal('<GAT   |   no   fit,   no   prediction,   no   score>', ('%s' % gat)) 
   assert_raises(ValueError, gat.fit, epochs) 
   with warnings.catch_warnings(record=True): 
      gat.picks = [0] 
      gat.fit(epochs) 
      gat.picks = None 
      gat.fit(epochs, y=epochs.events[:, 2]) 
      gat.fit(epochs, y=epochs.events[:, 2].tolist()) 
   assert_equal(len(gat.picks_), len(gat.ch_names), 1) 
   assert_equal('<GAT   |   fitted,   start   :   -0.200   (s),   stop   :   0.499   (s),   no   prediction,   no   score>', ('%s' % gat)) 
   assert_equal(gat.ch_names, epochs.ch_names) 
   gat = GeneralizationAcrossTime(predict_method='decision_function') 
   gat.fit(epochs) 
   assert_true((gat.cv_.__class__ == StratifiedKFold)) 
   gat.predict(epochs) 
   assert_array_equal(np.shape(gat.y_pred_), (15, 15, 14, 1)) 
   gat.predict_method = 'predict_proba' 
   gat.predict(epochs) 
   assert_array_equal(np.shape(gat.y_pred_), (15, 15, 14, 2)) 
   gat.predict_method = 'foo' 
   assert_raises(NotImplementedError, gat.predict, epochs) 
   gat.predict_method = 'predict' 
   gat.predict(epochs) 
   assert_array_equal(np.shape(gat.y_pred_), (15, 15, 14, 1)) 
   assert_equal('<GAT   |   fitted,   start   :   -0.200   (s),   stop   :   0.499   (s),   predicted   14   epochs,   no   score>', ('%s' % gat)) 
   gat.score(epochs) 
   assert_true((gat.scorer_.__name__ == 'accuracy_score')) 
   gat.scorer = None 
   gat.predict_method = 'decision_function' 
   assert_raises(ValueError, gat.score, epochs) 
   gat.predict_method = 'predict' 
   gat.score(epochs, y=epochs.events[:, 2]) 
   gat.score(epochs, y=epochs.events[:, 2].tolist()) 
   assert_equal('<GAT   |   fitted,   start   :   -0.200   (s),   stop   :   0.499   (s),   predicted   14   epochs,\n   scored   (accuracy_score)>', ('%s' % gat)) 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs, y=epochs.events[:, 2]) 
   old_mode = gat.predict_mode 
   gat.predict_mode = 'super-foo-mode' 
   assert_raises(ValueError, gat.predict, epochs) 
   gat.predict_mode = old_mode 
   gat.score(epochs, y=epochs.events[:, 2]) 
   assert_true(('accuracy_score' in ('%s' % gat.scorer_))) 
   epochs2 = epochs.copy() 
   assert_equal('<DecodingTime   |   start:   -0.200   (s),   stop:   0.499   (s),   step:   0.050   (s),   length:   0.050   (s),   n_time_windows:   15>', ('%s' % gat.train_times_)) 
   assert_equal('<DecodingTime   |   start:   -0.200   (s),   stop:   0.499   (s),   step:   0.050   (s),   length:   0.050   (s),   n_time_windows:   15   x   15>', ('%s' % gat.test_times_)) 
   gat.predict_mode = 'mean-prediction' 
   epochs2.events[:, 2] += 10 
   gat_ = copy.deepcopy(gat) 
   with use_log_level('error'): 
      assert_raises(ValueError, gat_.score, epochs2) 
   gat.predict_mode = 'cross-validation' 
   assert_true((gat.y_train_.shape[0] == gat.y_true_.shape[0] == len(gat.y_pred_[0][0]) == 14)) 
   assert_true((np.shape(gat.estimators_)[1] == gat.cv)) 
   assert_true((len(gat.train_times_['slices']) == 15 == np.shape(gat.estimators_)[0])) 
   assert_true((len(gat.test_times_['slices']) == 15 == np.shape(gat.scores_)[0])) 
   assert_true((len(gat.test_times_['slices'][0]) == 15 == np.shape(gat.scores_)[1])) 
   gat.score_mode = 'foo' 
   assert_raises(ValueError, gat.score, epochs) 
   gat.score_mode = 'fold-wise' 
   scores = gat.score(epochs) 
   assert_array_equal(np.shape(scores), [15, 15, 5]) 
   gat.score_mode = 'mean-sample-wise' 
   scores = gat.score(epochs) 
   assert_array_equal(np.shape(scores), [15, 15]) 
   gat.score_mode = 'mean-fold-wise' 
   scores = gat.score(epochs) 
   assert_array_equal(np.shape(scores), [15, 15]) 
   gat.predict_mode = 'mean-prediction' 
   with warnings.catch_warnings(record=True) as w: 
      gat.score(epochs) 
      assert_true(any((('score_mode   changed   from   ' in str(ww.message)) for ww in w))) 
   gat = GeneralizationAcrossTime(train_times={'length': 0.1}) 
   with warnings.catch_warnings(record=True): 
      gat2 = gat.fit(epochs) 
   assert_true((gat is gat2)) 
   assert_true(hasattr(gat2, 'cv_')) 
   assert_true((gat2.cv_ != gat.cv)) 
   with warnings.catch_warnings(record=True): 
      scores = gat.score(epochs) 
   assert_true(isinstance(scores, np.ndarray)) 
   assert_equal(len(scores[0]), len(scores)) 
   assert_equal(len(gat.test_times_['slices'][0][0]), 2) 
   gat = GeneralizationAcrossTime(train_times={'step': 0.1}) 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs) 
   gat.score(epochs) 
   assert_true((len(gat.scores_) == len(gat.estimators_) == 8)) 
   assert_equal(len(gat.scores_[0]), 15) 
   y_4classes = np.hstack((epochs.events[:7, 2], (epochs.events[7:, 2] + 1))) 
   train_times = dict(start=0.09, stop=0.25) 
   gat = GeneralizationAcrossTime(cv=cv_lolo, train_times=train_times) 
   assert_raises(RuntimeError, gat.predict, epochs) 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs, y=y_4classes) 
   gat.score(epochs) 
   assert_equal(len(gat.scores_), 4) 
   assert_equal(gat.train_times_['times'][0], epochs.times[6]) 
   assert_equal(gat.train_times_['times'][(-1)], epochs.times[9]) 
   gat = GeneralizationAcrossTime(test_times='diagonal') 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs) 
   assert_raises(RuntimeError, gat.score) 
   with warnings.catch_warnings(record=True): 
      gat.predict(epochs) 
   scores = gat.score() 
   assert_true((scores is gat.scores_)) 
   assert_equal(np.shape(gat.scores_), (15, 1)) 
   assert_array_equal([tim for ttime in gat.test_times_['times'] for tim in ttime], gat.train_times_['times']) 
   gat = GeneralizationAcrossTime(predict_mode='mean-prediction', cv=2) 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs[0:6]) 
   with warnings.catch_warnings(record=True): 
      gat.predict(epochs[7:]) 
      gat.score(epochs[7:]) 
   gat_ = copy.deepcopy(gat) 
   gat_.train_times = dict(start=(-999.0)) 
   with use_log_level('error'): 
      assert_raises(ValueError, gat_.fit, epochs) 
   gat_.train_times = dict(start=999.0) 
   assert_raises(ValueError, gat_.fit, epochs) 
   gat_.train_times = dict(step=1e-06) 
   assert_raises(ValueError, gat_.fit, epochs) 
   gat_.train_times = dict(length=1e-06) 
   assert_raises(ValueError, gat_.fit, epochs) 
   gat_.train_times = dict(length=999.0) 
   assert_raises(ValueError, gat_.fit, epochs) 
   gat.test_times = dict(start=(-999.0)) 
   with warnings.catch_warnings(record=True): 
      assert_raises(ValueError, gat.predict, epochs) 
   gat.test_times = dict(start=999.0) 
   with warnings.catch_warnings(record=True): 
      assert_raises(ValueError, gat.predict, epochs) 
   gat.test_times = dict(step=1e-06) 
   with warnings.catch_warnings(record=True): 
      assert_raises(ValueError, gat.predict, epochs) 
   gat_ = copy.deepcopy(gat) 
   gat_.train_times_['length'] = 1e-06 
   gat_.test_times = dict(length=1e-06) 
   with warnings.catch_warnings(record=True): 
      assert_raises(ValueError, gat_.predict, epochs) 
   gat.test_times = dict(step=0.15) 
   with warnings.catch_warnings(record=True): 
      gat.predict(epochs) 
   assert_array_equal(np.shape(gat.y_pred_), (15, 5, 14, 1)) 
   gat.test_times = 'foo' 
   with warnings.catch_warnings(record=True): 
      assert_raises(ValueError, gat.predict, epochs) 
   assert_raises(RuntimeError, gat.score) 
   gat.test_times = dict(length=0.15) 
   assert_raises(ValueError, gat.predict, epochs) 
   train_times = dict(slices=[[0, 1], [1]]) 
   test_times = dict(slices=[[[0, 1]], [[0], [1]]]) 
   gat = GeneralizationAcrossTime(train_times=train_times, test_times=test_times) 
   gat.fit(epochs) 
   with warnings.catch_warnings(record=True): 
      gat.score(epochs) 
   assert_array_equal(np.shape(gat.y_pred_[0]), [1, len(epochs), 1]) 
   assert_array_equal(np.shape(gat.y_pred_[1]), [2, len(epochs), 1]) 
   gat.test_times = None 
   assert_raises(ValueError, gat.predict, epochs) 
   svc = SVC(C=1, kernel='linear', probability=True) 
   gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction') 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs) 
   with use_log_level('error'): 
      assert_raises(ValueError, gat.score, epochs2) 
      gat.score(epochs) 
   assert_true((0.0 <= np.min(scores) <= 1.0)) 
   assert_true((0.0 <= np.max(scores) <= 1.0)) 
   gat = GeneralizationAcrossTime(cv=cv_shuffle, predict_mode='cross-validation') 
   gat.fit(epochs) 
   assert_raises(ValueError, gat.predict, epochs) 
   gat = GeneralizationAcrossTime(cv=cv_shuffle, predict_mode='mean-prediction') 
   gat.fit(epochs) 
   gat.predict(epochs) 
   gat = GeneralizationAcrossTime() 
   gat.fit(epochs) 
   with warnings.catch_warnings(record=True): 
      gat.fit(epochs) 
   gat.predict(epochs) 
   assert_raises(ValueError, gat.predict, epochs[:10]) 
   gat._cv_splits[0] = [gat._cv_splits[0][0], np.empty(0)] 
   with warnings.catch_warnings(record=True) as w: 
      gat.predict(epochs) 
      assert_true((len(w) > 0)) 
      assert_true(any((('do   not   have   any   test   epochs' in str(ww.message)) for ww in w))) 
   gat = GeneralizationAcrossTime(cv=[([0], [1]), ([], [0])]) 
   assert_raises(ValueError, gat.fit, epochs[:2]) 
   if check_version('sklearn', '0.17'): 
      gat = GeneralizationAcrossTime(clf=KernelRidge(), cv=2) 
      epochs.crop(None, epochs.times[2]) 
      gat.fit(epochs) 
      assert_true((gat.cv_.__class__ == KFold)) 
      gat.score(epochs) 
      assert_true((gat.scorer_.__name__ == 'mean_squared_error')) 
   n_classes = [2, 4] 
   le = LabelEncoder() 
   y = le.fit_transform(epochs.events[:, 2]) 
   y[(len(y) // 2):] += 2 
   ys = (y, (y + 1000)) 
   svc = SVC(C=1, kernel='linear', probability=True) 
   reg = KernelRidge() 
   def scorer_proba(y_true, y_pred): 
      return roc_auc_score(y_true, y_pred[:, 0]) 
   scorers = [None, scorer_proba, scorer_regress] 
   predict_methods = [None, 'predict_proba', None] 
   clfs = [svc, svc, reg] 
   for (clf, predict_method, scorer) in zip(clfs, predict_methods, scorers): 
      for y in ys: 
         for n_class in n_classes: 
            for predict_mode in ['cross-validation', 'mean-prediction']: 
               if ((predict_method == 'predict_proba') and (n_class != 2)): 
                  continue 
               y_ = (y % n_class) 
               with warnings.catch_warnings(record=True): 
                  gat = GeneralizationAcrossTime(cv=2, clf=clf, scorer=scorer, predict_mode=predict_mode) 
                  gat.fit(epochs, y=y_) 
                  gat.score(epochs, y=y_) 
               scorer_name = gat.scorer_.__name__ 
               if (scorer is None): 
                  if is_classifier(clf): 
                     assert_equal(scorer_name, 'accuracy_score') 
                  else: 
                     assert_equal(scorer_name, 'mean_squared_error') 
               else: 
                  assert_equal(scorer_name, scorer.__name__)","


  


  
  



   below the below the below the below the below the below the below the below the below the below the below the below the below the below the below the below the below the below Below Below the below the below below Below below below below below below Below Below Below below below below below below below the below below below below the below the below below thebelow below thepy








py
py below




py

py      
py below    ellow below.pyera.py below.
py below
below  
below below   below below below below below below below below below below below the below the below the below the
test   below below below the below the below   below below   below below below below below    astellow below below below below below below below below below below below below below below below below below below below      py  py     below   below below   below below below below the below below below below below   below below below below the   below below the below the below the   below the below below below below below the below the below below the below below below below below below the below the",'Test time generalization decoding'
"def _zpklp2hp(z, p, k, wo=1.0): 
    z = atleast_1d(z) 
   p = atleast_1d(p) 
   wo = float(wo) 
   degree = _relative_degree(z, p) 
   z_hp = (wo / z) 
   p_hp = (wo / p) 
   z_hp = append(z_hp, zeros(degree)) 
   k_hp = (k * real((prod((- z)) / prod((- p))))) 
   return (z_hp, p_hp, k_hp)"," 'Compute the HP transform of the given ZPK. 
 Parameters 
 z : ndarray 
 The Z component of the ZPK. 
 p : ndarray 
 The P component of the ZPK. 
 k : ndarray 
 The K component of the ZPK. 
 wo : float 
 The weighting factor for the Z component. 
 Returns 
 z_hp : ndarray 
 The HP transform of the Z component. 
 p_hp : ndarray 
 The HP transform of the P component. 
 k_hp : ndarray 
 The HP transform of the K component. 
 Examples 
 >>> from sympy.physics.mechanics import ZPK 
 >>> from sympy.physics.mechanics.mechanical_dynamics import _zpklp2hp 
 >>> z = [1, 2, 3] 
 >>> p = [1, 2, 3] 
 >>> k = [1, 2, 3] 
 >>> _zpklp2hp(z, p, k) 
 (","'Transform a lowpass filter prototype to a highpass filter. 
 Return an analog high-pass filter with cutoff frequency `wo` 
 from an analog low-pass filter prototype with unity cutoff frequency, 
 using zeros, poles, and gain (\'zpk\') representation. 
 Parameters 
 z : array_like 
 Zeros of the analog IIR filter transfer function. 
 p : array_like 
 Poles of the analog IIR filter transfer function. 
 k : float 
 System gain of the analog IIR filter transfer function. 
 wo : float 
 Desired cutoff, as angular frequency (e.g. rad/s). 
 Defaults to no change. 
 Returns 
 z : ndarray 
 Zeros of the transformed high-pass filter transfer function. 
 p : ndarray 
 Poles of the transformed high-pass filter transfer function. 
 k : float 
 System gain of the transformed high-pass filter. 
 Notes 
 This is derived from the s-plane substitution 
 .. math:: s \rightarrow \frac{\omega_0}{s} 
 This maintains symmetry of the lowpass and highpass responses on a 
 logarithmic scale.'"
"def _getlabel(object_alias): 
    finder = _getfinder() 
   args = {} 
   attrs = {} 
   aeobj_00 = aetypes.ObjectSpecifier(want=aetypes.Type('cobj'), form='alis', seld=object_alias, fr=None) 
   aeobj_01 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('labi'), fr=aeobj_00) 
   args['----'] = aeobj_01 
   (_reply, args, attrs) = finder.send('core', 'getd', args, attrs) 
   if args.has_key('errn'): 
      raise Error, aetools.decodeerror(args) 
   if args.has_key('----'): 
      return args['----']"," 'Returns the label of the object specified by the object alias. 
 :param object_alias: the object alias 
 :type object_alias: str 
 :returns: the label of the object specified by the object alias 
 :rtype: str'",'label: Get the label for the object.'
"def snipmate_files_for(ft): 
    if (ft == 'all'): 
      ft = '_' 
   patterns = [('%s.snippets' % ft), os.path.join(ft, '*.snippets'), os.path.join(ft, '*.snippet'), os.path.join(ft, '*/*.snippet')] 
   ret = set() 
   for rtp in _vim.eval('&runtimepath').split(','): 
      path = os.path.realpath(os.path.expanduser(os.path.join(rtp, 'snippets'))) 
      for pattern in patterns: 
         for fn in glob.glob(os.path.join(path, pattern)): 
            ret.add(fn) 
   return ret", 'Returns a set of all the snippets files in the runtimepath.','Returns all snipMate files we need to look at for \'ft\'.'
"def dbsize(host=None, port=None, db=None, password=None): 
    server = _connect(host, port, db, password) 
   return server.dbsize()", 'Return the size of the database in bytes.',"'Return the number of keys in the selected database 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' redis.dbsize'"
"def sign_entity_descriptor(edesc, ident, secc): 
    if (not ident): 
      ident = sid() 
   edesc.signature = pre_signature_part(ident, secc.my_cert, 1) 
   edesc.id = ident 
   xmldoc = secc.sign_statement(('%s' % edesc), class_name(edesc)) 
   edesc = md.entity_descriptor_from_string(xmldoc) 
   return (edesc, xmldoc)"," 'Sign an entity descriptor. 
 :param edesc: entity descriptor to be signed 
 :type edesc: L{EntityDescriptor} 
 :param ident: entity identifier 
 :type ident: str 
 :param secc: security context 
 :type secc: L{SecurityContext} 
 :return: entity descriptor and signed statement 
 :rtype: tuple(L{EntityDescriptor}, str)'","':param edesc: EntityDescriptor instance 
 :param ident: EntityDescriptor identifier 
 :param secc: Security context 
 :return: Tuple with EntityDescriptor instance and Signed XML document'"
"def multi_future(children, quiet_exceptions=()): 
    if isinstance(children, dict): 
      keys = list(children.keys()) 
      children = children.values() 
   else: 
      keys = None 
   children = list(map(convert_yielded, children)) 
   assert all((is_future(i) for i in children)) 
   unfinished_children = set(children) 
   future = Future() 
   if (not children): 
      future.set_result(({} if (keys is not None) else [])) 
   def callback(f): 
      unfinished_children.remove(f) 
      if (not unfinished_children): 
         result_list = [] 
         for f in children: 
            try: 
               result_list.append(f.result()) 
            except Exception as e: 
               if future.done(): 
                  if (not isinstance(e, quiet_exceptions)): 
                     app_log.error('Multiple   exceptions   in   yield   list', exc_info=True) 
               else: 
                  future.set_exc_info(sys.exc_info()) 
         if (not future.done()): 
            if (keys is not None): 
               future.set_result(dict(zip(keys, result_list))) 
            else: 
               future.set_result(result_list) 
   listening = set() 
   for f in children: 
      if (f not in listening): 
         listening.add(f) 
         f.add_done_callback(callback) 
   return future"," 'Returns a future that will be completed when all of the children have 
 been completed. 
 :param children: A list of futures or a dict of futures to complete. 
 :param quiet_exceptions: A tuple of exceptions that will be ignored. 
 :returns: A future that will be completed when all of the children have 
 been completed.'","'Wait for multiple asynchronous futures in parallel. 
 This function is similar to `multi`, but does not support 
 `YieldPoints <YieldPoint>`. 
 .. versionadded:: 4.0 
 .. versionchanged:: 4.2 
 If multiple ``Futures`` fail, any exceptions after the first (which is 
 raised) will be logged. Added the ``quiet_exceptions`` 
 argument to suppress this logging for selected exception types. 
 .. deprecated:: 4.3 
 Use `multi` instead.'"
"def serve(request, path, document_root=None, insecure=False, **kwargs): 
    if ((not settings.DEBUG) and (not insecure)): 
      raise ImproperlyConfigured(""The   staticfiles   view   can   only   be   used   in   debug   mode   or   if   the   the   --insecure   option   of   'runserver'   is   used"") 
   normalized_path = posixpath.normpath(urllib.unquote(path)).lstrip('/') 
   absolute_path = finders.find(normalized_path) 
   if (not absolute_path): 
      if (path.endswith('/') or (path == '')): 
         raise Http404('Directory   indexes   are   not   allowed   here.') 
      raise Http404((""'%s'   could   not   be   found"" % path)) 
   (document_root, path) = os.path.split(absolute_path) 
   return static.serve(request, path, document_root=document_root, **kwargs)"," 'Serve static files from the given path. 
 This view is only available in debug mode or when the 
 --insecure option of the runserver command is used. 
 :param request: The current request. 
 :param path: The path to serve. 
 :param document_root: The root of the document tree. 
 :param insecure: Whether to serve the files insecurely. 
 :param kwargs: Additional keyword arguments are passed to 
 :func:`django.contrib.staticfiles.views.serve`.'","'Serve static files below a given point in the directory structure or 
 from locations inferred from the staticfiles finders. 
 To use, put a URL pattern such as:: 
 (r\'^(?P<path>.*)$\', \'django.contrib.staticfiles.views.serve\') 
 in your URLconf. 
 It uses the django.views.static view to serve the found files.'"
"def ant(registry, xml_parent, data): 
    ant = XML.SubElement(xml_parent, 'hudson.tasks.Ant') 
   if (type(data) is str): 
      data = {'targets': data} 
   for (setting, value) in sorted(data.items()): 
      if (setting == 'targets'): 
         targets = XML.SubElement(ant, 'targets') 
         targets.text = value 
      if (setting == 'buildfile'): 
         buildfile = XML.SubElement(ant, 'buildFile') 
         buildfile.text = value 
      if (setting == 'properties'): 
         properties = data['properties'] 
         prop_string = '' 
         for (prop, val) in properties.items(): 
            prop_string += ('%s=%s\n' % (prop, val)) 
         prop_element = XML.SubElement(ant, 'properties') 
         prop_element.text = prop_string 
      if (setting == 'java-opts'): 
         javaopts = data['java-opts'] 
         jopt_string = '   '.join(javaopts) 
         jopt_element = XML.SubElement(ant, 'antOpts') 
         jopt_element.text = jopt_string 
   XML.SubElement(ant, 'antName').text = data.get('ant-name', 'default')", 'Create an Ant task',"'yaml: ant 
 Execute an ant target. Requires the Jenkins :jenkins-wiki:`Ant Plugin 
 <Ant+Plugin>`. 
 To setup this builder you can either reference the list of targets 
 or use named parameters. Below is a description of both forms: 
 *1) Listing targets:* 
 After the ant directive, simply pass as argument a space separated list 
 of targets to build. 
 :Parameter: space separated list of Ant targets 
 Example to call two Ant targets: 
 .. literalinclude:: ../../tests/builders/fixtures/ant001.yaml 
 :language: yaml 
 The build file would be whatever the Jenkins Ant Plugin is set to use 
 per default (i.e build.xml in the workspace root). 
 *2) Using named parameters:* 
 :arg str targets: the space separated list of ANT targets. 
 :arg str buildfile: the path to the ANT build file. 
 :arg list properties: Passed to ant script using -Dkey=value (optional) 
 :arg str ant-name: the name of the ant installation, 
 (default \'default\') (optional) 
 :arg str java-opts: java options for ant, can have multiples, 
 must be in quotes (optional) 
 Example specifying the build file too and several targets: 
 .. literalinclude:: ../../tests/builders/fixtures/ant002.yaml 
 :language: yaml'"
"def _arcball(x, y, w, h): 
    r = ((w + h) / 2.0) 
   (x, y) = (((- ((2.0 * x) - w)) / r), ((- ((2.0 * y) - h)) / r)) 
   h = np.sqrt(((x * x) + (y * y))) 
   return ((0.0, (x / h), (y / h), 0.0) if (h > 1.0) else (0.0, x, y, np.sqrt((1.0 - (h * h)))))"," 'Returns a tuple of (x, y, z, w) where x, y, z are the arcball coordinates 
 and w is the radius of the arcball.'","'Convert x,y coordinates to w,x,y,z Quaternion parameters 
 Adapted from: 
 linalg library 
 Copyright (c) 2010-2015, Renaud Blanch <rndblnch at gmail dot com> 
 Licence at your convenience: 
 GPLv3 or higher <http://www.gnu.org/licenses/gpl.html> 
 BSD new <http://opensource.org/licenses/BSD-3-Clause>'"
"def test_coordinate_vars(): 
    A = CoordSysCartesian('A') 
   assert (BaseScalar('A.x', 0, A, 'A_x', '\\mathbf{{x}_{A}}') == A.x) 
   assert (BaseScalar('A.y', 1, A, 'A_y', '\\mathbf{{y}_{A}}') == A.y) 
   assert (BaseScalar('A.z', 2, A, 'A_z', '\\mathbf{{z}_{A}}') == A.z) 
   assert (BaseScalar('A.x', 0, A, 'A_x', '\\mathbf{{x}_{A}}').__hash__() == A.x.__hash__()) 
   assert (isinstance(A.x, BaseScalar) and isinstance(A.y, BaseScalar) and isinstance(A.z, BaseScalar)) 
   assert ((A.x * A.y) == (A.y * A.x)) 
   assert (A.scalar_map(A) == {A.x: A.x, A.y: A.y, A.z: A.z}) 
   assert (A.x.system == A) 
   assert (A.x.diff(A.x) == 1) 
   B = A.orient_new_axis('B', q, A.k) 
   assert (B.scalar_map(A) == {B.z: A.z, B.y: (((- A.x) * sin(q)) + (A.y * cos(q))), B.x: ((A.x * cos(q)) + (A.y * sin(q)))}) 
   assert (A.scalar_map(B) == {A.x: ((B.x * cos(q)) - (B.y * sin(q))), A.y: ((B.x * sin(q)) + (B.y * cos(q))), A.z: B.z}) 
   assert (express(B.x, A, variables=True) == ((A.x * cos(q)) + (A.y * sin(q)))) 
   assert (express(B.y, A, variables=True) == (((- A.x) * sin(q)) + (A.y * cos(q)))) 
   assert (express(B.z, A, variables=True) == A.z) 
   assert (expand(express(((B.x * B.y) * B.z), A, variables=True)) == expand(((A.z * (((- A.x) * sin(q)) + (A.y * cos(q)))) * ((A.x * cos(q)) + (A.y * sin(q)))))) 
   assert (express((((B.x * B.i) + (B.y * B.j)) + (B.z * B.k)), A) == (((((B.x * cos(q)) - (B.y * sin(q))) * A.i) + (((B.x * sin(q)) + (B.y * cos(q))) * A.j)) + (B.z * A.k))) 
   assert (simplify(express((((B.x * B.i) + (B.y * B.j)) + (B.z * B.k)), A, variables=True)) == (((A.x * A.i) + (A.y * A.j)) + (A.z * A.k))) 
   assert (express((((A.x * A.i) + (A.y * A.j)) + (A.z * A.k)), B) == (((((A.x * cos(q)) + (A.y * sin(q))) * B.i) + ((((- A.x) * sin(q)) + (A.y * cos(q))) * B.j)) + (A.z * B.k))) 
   assert (simplify(express((((A.x * A.i) + (A.y * A.j)) + (A.z * A.k)), B, variables=True)) == (((B.x * B.i) + (B.y * B.j)) + (B.z * B.k))) 
   N = B.orient_new_axis('N', (- q), B.k) 
   assert (N.scalar_map(A) == {N.x: A.x, N.z: A.z, N.y: A.y}) 
   C = A.orient_new_axis('C', q, ((A.i + A.j) + A.k)) 
   mapping = A.scalar_map(C) 
   assert (mapping[A.x] == ((((C.x * ((2 * cos(q)) + 1)) / 3) + ((C.y * (((-2) * sin((q + (pi / 6)))) + 1)) / 3)) + ((C.z * (((-2) * cos((q + (pi / 3)))) + 1)) / 3))) 
   assert (mapping[A.y] == ((((C.x * (((-2) * cos((q + (pi / 3)))) + 1)) / 3) + ((C.y * ((2 * cos(q)) + 1)) / 3)) + ((C.z * (((-2) * sin((q + (pi / 6)))) + 1)) / 3))) 
   assert (mapping[A.z] == ((((C.x * (((-2) * sin((q + (pi / 6)))) + 1)) / 3) + ((C.y * (((-2) * cos((q + (pi / 3)))) + 1)) / 3)) + ((C.z * ((2 * cos(q)) + 1)) / 3))) 
   D = A.locate_new('D', (((a * A.i) + (b * A.j)) + (c * A.k))) 
   assert (D.scalar_map(A) == {D.z: (A.z - c), D.x: (A.x - a), D.y: (A.y - b)}) 
   E = A.orient_new_axis('E', a, A.k, (((a * A.i) + (b * A.j)) + (c * A.k))) 
   assert (A.scalar_map(E) == {A.z: (E.z + c), A.x: (((E.x * cos(a)) - (E.y * sin(a))) + a), A.y: (((E.x * sin(a)) + (E.y * cos(a))) + b)}) 
   assert (E.scalar_map(A) == {E.x: (((A.x - a) * cos(a)) + ((A.y - b) * sin(a))), E.y: ((((- A.x) + a) * sin(a)) + ((A.y - b) * cos(a))), E.z: (A.z - c)}) 
   F = A.locate_new('F', Vector.zero) 
   assert (A.scalar_map(F) == {A.z: F.z, A.x: F.x, A.y: F.y})", 'Test the coordinate variables.',"'Tests the coordinate variables functionality with respect to 
 reorientation of coordinate systems.'"
"def CheckBraces(filename, clean_lines, linenum, error): 
    line = clean_lines.elided[linenum] 
   if Match('\\s*{\\s*$', line): 
      prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0] 
      if ((not Search('[,;:}{(]\\s*$', prevline)) and (not Match('\\s*#', prevline)) and (not ((GetLineWidth(prevline) > (_line_length - 2)) and ('[]' in prevline)))): 
         error(filename, linenum, 'whitespace/braces', 4, '{   should   almost   always   be   at   the   end   of   the   previous   line') 
   if Match('\\s*else\\b\\s*(?:if\\b|\\{|$)', line): 
      prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0] 
      if Match('\\s*}\\s*$', prevline): 
         error(filename, linenum, 'whitespace/newline', 4, 'An   else   should   appear   on   the   same   line   as   the   preceding   }') 
   if Search('else   if\\s*\\(', line): 
      brace_on_left = bool(Search('}\\s*else   if\\s*\\(', line)) 
      pos = line.find('else   if') 
      pos = line.find('(', pos) 
      if (pos > 0): 
         (endline, _, endpos) = CloseExpression(clean_lines, linenum, pos) 
         brace_on_right = (endline[endpos:].find('{') != (-1)) 
         if (brace_on_left != brace_on_right): 
            error(filename, linenum, 'readability/braces', 5, 'If   an   else   has   a   brace   on   one   side,   it   should   have   it   on   both') 
   elif (Search('}\\s*else[^{]*$', line) or Match('[^}]*else\\s*{', line)): 
      error(filename, linenum, 'readability/braces', 5, 'If   an   else   has   a   brace   on   one   side,   it   should   have   it   on   both') 
   if (Search('\\belse   [^\\s{]', line) and (not Search('\\belse   if\\b', line))): 
      error(filename, linenum, 'whitespace/newline', 4, 'Else   clause   should   never   be   on   same   line   as   else   (use   2   lines)') 
   if Match('\\s*do   [^\\s{]', line): 
      error(filename, linenum, 'whitespace/newline', 4, 'do/while   clauses   should   not   be   on   a   single   line') 
   if_else_match = Search('\\b(if\\s*\\(|else\\b)', line) 
   if (if_else_match and (not Match('\\s*#', line))): 
      if_indent = GetIndentLevel(line) 
      (endline, endlinenum, endpos) = (line, linenum, if_else_match.end()) 
      if_match = Search('\\bif\\s*\\(', line) 
      if if_match: 
         pos = (if_match.end() - 1) 
         (endline, endlinenum, endpos) = CloseExpression(clean_lines, linenum, pos) 
      if ((not Match('\\s*{', endline[endpos:])) and (not (Match('\\s*$', endline[endpos:]) and (endlinenum < (len(clean_lines.elided) - 1)) and Match('\\s*{', clean_lines.elided[(endlinenum + 1)])))): 
         while ((endlinenum < len(clean_lines.elided)) and (';' not in clean_lines.elided[endlinenum][endpos:])): 
            endlinenum += 1 
            endpos = 0 
         if (endlinenum < len(clean_lines.elided)): 
            endline = clean_lines.elided[endlinenum] 
            endpos = endline.find(';') 
            if (not Match(';[\\s}]*(\\\\?)$', endline[endpos:])): 
               if (not Match('^[^{};]*\\[[^\\[\\]]*\\][^{}]*\\{[^{}]*\\}\\s*\\)*[;,]\\s*$', endline)): 
                  error(filename, linenum, 'readability/braces', 4, 'If/else   bodies   with   multiple   statements   require   braces') 
            elif (endlinenum < (len(clean_lines.elided) - 1)): 
               next_line = clean_lines.elided[(endlinenum + 1)] 
               next_indent = GetIndentLevel(next_line) 
               if (if_match and Match('\\s*else\\b', next_line) and (next_indent != if_indent)): 
                  error(filename, linenum, 'readability/braces', 4, 'Else   clause   should   be   indented   at   the   same   level   as   if.   Ambiguous   nested   if/else   chains   require   braces.') 
               elif (next_indent > if_indent): 
                  error(filename, linenum, 'readability/braces', 4, 'If/else   bodies   with   multiple   statements   require   braces')", 'Check for braces in a file.',"'Looks for misplaced braces (e.g. at the end of line). 
 Args: 
 filename: The name of the current file. 
 clean_lines: A CleansedLines instance containing the file. 
 linenum: The number of the line to check. 
 error: The function to call with any errors found.'"
"def full_restart(name): 
    restart(name)", 'Restart the VM in full',"'Calls s6.restart() function 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' s6.full_restart <service name>'"
"def validate_auth_option(option, value): 
    (lower, value) = validate(option, value) 
   if (lower not in _AUTH_OPTIONS): 
      raise ConfigurationError(('Unknown   authentication   option:   %s' % (option,))) 
   return (lower, value)"," 'Validates an authentication option and returns the lowercase version of the 
 option and the value. 
 :param option: The authentication option to validate. 
 :type option: str 
 :param value: The value of the option. 
 :type value: str 
 :return: The lowercase version of the option and the value. 
 :rtype: (str, str)'",'Validate optional authentication parameters.'
"def regen(it): 
    if isinstance(it, (list, tuple)): 
      return it 
   return _regen(it)"," 'Return a regenerated version of the given iterable. 
 :param it: 
 :type it: 
 :return: 
 :rtype: 
 :raise ValueError: 
 If the given iterable is not a list, tuple, or generator.'","'Convert iterator to an object that can be consumed multiple times. 
 ``Regen`` takes any iterable, and if the object is an 
 generator it will cache the evaluated list on first access, 
 so that the generator can be ""consumed"" multiple times.'"
"@constructor 
 def round(a, mode='half_away_from_zero'): 
    if (mode == 'half_away_from_zero'): 
      return round_half_away_from_zero(a) 
   elif (mode == 'half_to_even'): 
      return round_half_to_even(a) 
   else: 
      raise Exception(('round   mode   %s   is   not   implemented.' % mode))"," 'Round a floating point number to a given mode. 
 The mode can be either ""half_away_from_zero"", ""half_to_even"" or 
 ""half_to_nearest"". 
 Parameters 
 a : float 
 The number to round. 
 mode : str 
 The mode to use. 
 Returns 
 The rounded number. 
 Examples 
 >>> round(1.234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123","'round_mode(a) with mode in [half_away_from_zero, half_to_even]'"
"def is_arity(n, func, sigspec=None): 
    (sigspec, rv) = _check_sigspec(sigspec, func, _sigs._is_arity, n, func) 
   if (sigspec is None): 
      return rv 
   num = num_required_args(func, sigspec) 
   if (num is not None): 
      num = (num == n) 
      if (not num): 
         return False 
   varargs = has_varargs(func, sigspec) 
   if varargs: 
      return False 
   keywords = has_keywords(func, sigspec) 
   if keywords: 
      return False 
   if ((num is None) or (varargs is None) or (keywords is None)): 
      return None 
   return True"," 'Check if a function has the correct arity. 
 Args: 
 n: the number of required arguments. 
 func: the function to check. 
 sigspec: the signature specification to check against. 
 Returns: 
 True if the function has the correct arity, False otherwise. 
 Raises: 
 TypeError: if the function does not have the correct arity. 
 See Also: 
 is_callable, is_method, is_builtin, is_subclass'","'Does a function have only n positional arguments? 
 This function relies on introspection and does not call the function. 
 Returns None if validity can\'t be determined. 
 >>> def f(x): 
 ...     return x 
 >>> is_arity(1, f) 
 True 
 >>> def g(x, y=1): 
 ...     return x + y 
 >>> is_arity(1, g) 
 False'"
"def UINT(value): 
    if (value is None): 
      raise ValueError(u'None   is   not   a   valid   integer') 
   if (not value.isdigit()): 
      raise ValueError(u'Only   positive   numbers   are   allowed') 
   return int(value)"," 'Convert a value to a 32-bit unsigned integer. 
 :param value: 
 :type value: 
 :return: 
 :rtype:'",'Converts a value that matches \d+ into an integer.'
"def _json_plays(drive, data): 
    plays = [] 
   seen_ids = set() 
   seen_desc = set() 
   for playid in map(str, sorted(map(int, data))): 
      p = data[playid] 
      desc = (p['desc'], p['time'], p['yrdln'], p['qtr']) 
      if ((playid in seen_ids) or (desc in seen_desc)): 
         continue 
      seen_ids.add(playid) 
      seen_desc.add(desc) 
      plays.append(Play(drive, playid, data[playid])) 
   return plays", 'Return a list of plays from the json data',"'Takes a single JSON drive entry (data) and converts it to a list 
 of Play objects. This includes trying to resolve duplicate play 
 conflicts by only taking the first instance of a play.'"
"def add_trailing_slash(path): 
    if ((len(path) > 0) and (path[(-1)] == os.sep)): 
      return path 
   else: 
      return (path + os.sep)"," 'Add a trailing slash to a path. 
 :param path: The path to add a trailing slash to. 
 :return: The path with a trailing slash.'","'If path does not end with /, add it and return.'"
"@api_versions.wraps('2.17') 
 @utils.arg('server', metavar='<server>', help=_('Name   or   ID   of   server.')) 
 def do_trigger_crash_dump(cs, args): 
    server = _find_server(cs, args.server) 
   server.trigger_crash_dump()", 'Trigger a crash dump on a server.','Trigger crash dump in an instance.'
"def libvlc_video_set_spu(p_mi, i_spu): 
    f = (_Cfunctions.get('libvlc_video_set_spu', None) or _Cfunction('libvlc_video_set_spu', ((1,), (1,)), None, ctypes.c_int, MediaPlayer, ctypes.c_int)) 
   return f(p_mi, i_spu)"," 'Set the SPU for the media player. 
 @param p_mi: the media player. 
 @param i_spu: the SPU to use. 
 @version: LibVLC 2.1.0 or later.'","'Set new video subtitle. 
 @param p_mi: the media player. 
 @param i_spu: video subtitle track to select (i_id from track description). 
 @return: 0 on success, -1 if out of range.'"
"def gf_eval(f, a, p, K): 
    result = K.zero 
   for c in f: 
      result *= a 
      result += c 
      result %= p 
   return result"," 'Evaluate ``f`` at ``a`` in ``GF(p)[x]``. 
 Examples 
 >>> from sympy.polys.galoistools import gf_eval 
 >>> gf_eval(x**2 + x, 3, 5) 
 15/5 
 See Also 
 gf_mul, gf_sub, gf_add, gf_pow'","'Evaluate ``f(a)`` in ``GF(p)`` using Horner scheme. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import gf_eval 
 >>> gf_eval([3, 2, 4], 2, 5, ZZ) 
 0'"
"def available(name): 
    return (name in _get_svc_list(name))"," 'Return True if service is available. 
 :param name: Service name 
 :type name: str 
 :return: True if service is available, False otherwise 
 :rtype: bool'","'Returns ``True`` if the specified service is available, otherwise returns 
 ``False``. 
 name 
 the service\'s name 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' runit.available <service name>'"
"def TestH2a(sequenceLength, nTests, cellsPerColumn, numCols=100, nSequences=[2], pctShared=0.02, seqGenMode='shared   sequence', shouldFail=False): 
    print 'Test   H2a   -   second   repetition   of   the   same   sequence   should   not   add   synapses' 
   nFailed = 0 
   subsequenceStartPos = 10 
   assert (subsequenceStartPos < sequenceLength) 
   for numSequences in nSequences: 
      print 'Higher   order   test   with   sequenceLength=', sequenceLength, 
      print 'cellsPerColumn=', cellsPerColumn, 'nTests=', nTests, 'numCols=', numCols 
      print 'numSequences=', numSequences, 'pctShared=', pctShared, 
      print 'sharing   mode=', seqGenMode 
      for _ in range(nTests): 
         trainingSet = buildTrainingSet(numSequences=numSequences, sequenceLength=sequenceLength, pctShared=pctShared, seqGenMode=seqGenMode, subsequenceStartPos=subsequenceStartPos, numCols=numCols, minOnes=21, maxOnes=25) 
         print '==============   10   ======================' 
         (numFailures3, numStrictErrors3, numPerfect3, tp3) = testSequence(trainingSet, nTrainingReps=10, numberOfCols=numCols, cellsPerColumn=cellsPerColumn, initialPerm=0.4, connectedPerm=0.7, minThreshold=12, permanenceInc=0.1, permanenceDec=0.1, permanenceMax=1, globalDecay=0.0, newSynapseCount=15, activationThreshold=12, doPooling=False, shouldFail=shouldFail) 
         print '==============   2   ======================' 
         (numFailures, numStrictErrors, numPerfect, tp2) = testSequence(trainingSet, nTrainingReps=2, numberOfCols=numCols, cellsPerColumn=cellsPerColumn, initialPerm=0.8, connectedPerm=0.7, minThreshold=12, permanenceInc=0.1, permanenceDec=0, permanenceMax=1, globalDecay=0.0, newSynapseCount=15, activationThreshold=12, doPooling=False, shouldFail=shouldFail) 
         print '==============   1   ======================' 
         (numFailures1, numStrictErrors1, numPerfect1, tp1) = testSequence(trainingSet, nTrainingReps=1, numberOfCols=numCols, cellsPerColumn=cellsPerColumn, initialPerm=0.8, connectedPerm=0.7, minThreshold=12, permanenceInc=0.1, permanenceDec=0, permanenceMax=1, globalDecay=0.0, newSynapseCount=15, activationThreshold=12, doPooling=False, shouldFail=shouldFail) 
         segmentInfo1 = tp1.getSegmentInfo() 
         segmentInfo2 = tp2.getSegmentInfo() 
         if ((abs((segmentInfo1[0] - segmentInfo2[0])) > 3) or (abs((segmentInfo1[1] - segmentInfo2[1])) > (3 * 15))): 
            print 'Training   twice   incorrectly   resulted   in   too   many   segments   or   synapses' 
            print segmentInfo1 
            print segmentInfo2 
            print tp3.getSegmentInfo() 
            tp3.trimSegments() 
            print tp3.getSegmentInfo() 
            print 'Failures   for   1,   2,   and   N   reps' 
            print numFailures1, numStrictErrors1, numPerfect1 
            print numFailures, numStrictErrors, numPerfect 
            print numFailures3, numStrictErrors3, numPerfect3 
            numFailures += 1 
         if (((numFailures == 0) and (not shouldFail)) or ((numFailures > 0) and shouldFail)): 
            print 'Test   PASS', 
            if shouldFail: 
               print '(should   fail,   and   failed)' 
            else: 
               print 
         else: 
            print 'Test   FAILED' 
            nFailed = (nFailed + 1) 
            print 'numFailures=', numFailures 
            print 'numStrictErrors=', numStrictErrors 
            print 'numPerfect=', numPerfect 
   return nFailed", 'Tests the second repetition of the same sequence should not add synapses',"'Still need to test: 
 Two overlapping sequences. OK to get new segments but check that we can 
 get correct high order prediction after multiple reps.'"
"@block_user_agents 
 @require_GET 
 def top_level(request): 
    docs = Document.objects.filter_for_list(locale=request.LANGUAGE_CODE, toplevel=True) 
   paginated_docs = paginate(request, docs, per_page=DOCUMENTS_PER_PAGE) 
   context = {'documents': paginated_docs, 'count': docs.count(), 'toplevel': True} 
   return render(request, 'wiki/list/documents.html', context)"," 'Lists the top level documents. 
 :param request: The current request. 
 :type request: django.http.HttpRequest 
 :return: A response. 
 :rtype: django.http.HttpResponse'",'Lists documents directly under /docs/'
"def find_native_user_instrumentation_hook(cls): 
    return getattr(cls, INSTRUMENTATION_MANAGER, None)"," 'Find the native user instrumentation hook. 
 :param cls: The class to check. 
 :return: The native user instrumentation hook, or None if there is none.'",'Find user-specified instrumentation management for a class.'
"def addFacesByConcaveLoop(faces, indexedLoop): 
    if (len(indexedLoop) < 3): 
      return 
   remainingLoop = indexedLoop[:] 
   while (len(remainingLoop) > 2): 
      remainingLoop = getRemainingLoopAddFace(faces, remainingLoop)"," 'Adds faces to the list of faces to be added by the concave hull algorithm 
 by removing the concave loop from the list of loops to be added. 
 Parameters 
 faces : list of faces 
 indexedLoop : list of indices of the loop to be removed 
 Returns 
 list of faces'",'Add faces from a polygon which is concave.'
"def init(mpstate): 
    return SerialModule(mpstate)", 'Create a SerialModule object.','initialise module'
"def cache_project(cls): 
    projects = dict() 
   resources = dict() 
   def get_ctx(*args, **kwargs): 
      path = env.curbuf.name 
      if resources.get(path): 
         return resources.get(path) 
      project_path = env.var('g:pymode_rope_project_root') 
      if (not project_path): 
         project_path = env.curdir 
         env.debug('Look   ctx', project_path) 
         if env.var('g:pymode_rope_lookup_project', True): 
            project_path = look_ropeproject(project_path) 
      if (not os.path.exists(project_path)): 
         env.error(('Rope   project   root   not   exist:   %s' % project_path)) 
         ctx = None 
      else: 
         ctx = projects.get(project_path) 
      if (not ctx): 
         projects[project_path] = ctx = cls(path, project_path) 
      resources[path] = ctx 
      return ctx 
   return get_ctx", 'Caches project for each project root.',"'Cache projects. 
 :return func:'"
"@require_GET 
 def aggregated_metrics(request): 
    today = date.today() 
   locales = get_locales_by_visit((today - timedelta(days=30)), today) 
   product = _get_product(request) 
   return render(request, 'dashboards/aggregated_metrics.html', {'locales_json': json.dumps(settings.SUMO_LANGUAGES), 'locales': locales, 'product': product, 'products': Product.objects.filter(visible=True)})", 'Aggregated metrics for the last 30 days','The aggregated (all locales) kb metrics dashboard.'
"def RewriteResponse(response_file, response_rewriters=None, request_headers=None, env_dict=None): 
    if (response_rewriters is None): 
      response_rewriters = CreateResponseRewritersChain() 
   response = AppServerResponse(response_file) 
   for response_rewriter in response_rewriters: 
      if (response_rewriter.func_code.co_argcount == 1): 
         response_rewriter(response) 
      elif (response_rewriter.func_code.co_argcount == 2): 
         response_rewriter(response, request_headers) 
      else: 
         response_rewriter(response, request_headers, env_dict) 
   return response"," 'Rewrites a response file. 
 :param response_file: the response file to rewrite. 
 :param response_rewriters: a list of response rewriters. 
 :param request_headers: the request headers. 
 :param env_dict: the environment dictionary. 
 :return: the rewritten response.'","'Allows final rewrite of dev_appserver response. 
 This function receives the unparsed HTTP response from the application 
 or internal handler, parses out the basic structure and feeds that structure 
 in to a chain of response rewriters. 
 It also makes sure the final HTTP headers are properly terminated. 
 For more about response rewriters, please see documentation for 
 CreateResponeRewritersChain. 
 Args: 
 response_file: File-like object containing the full HTTP response including 
 the response code, all headers, and the request body. 
 response_rewriters: A list of response rewriters.  If none is provided it 
 will create a new chain using CreateResponseRewritersChain. 
 request_headers: Original request headers. 
 env_dict: Environment dictionary. 
 Returns: 
 An AppServerResponse instance configured with the rewritten response.'"
"def addBevelGear(derivation, extrudeDerivation, pitchRadius, positives, teeth, vector3GearProfile): 
    totalPitchRadius = (derivation.pitchRadiusComplement + derivation.pitchRadius) 
   totalTeeth = (derivation.teethPinion + derivation.teethComplement) 
   portionDirections = extrude.getSpacedPortionDirections(extrudeDerivation.interpolationDictionary) 
   loopLists = extrude.getLoopListsByPath(extrudeDerivation, None, vector3GearProfile[0], portionDirections) 
   firstLoopList = loopLists[0] 
   gearOverPinion = (float((totalTeeth - teeth)) / float(teeth)) 
   thirdLayerHeight = (0.33333333333 * setting.getLayerHeight(derivation.elementNode)) 
   pitchRadian = math.atan((math.sin(derivation.operatingRadian) / (gearOverPinion + math.cos(derivation.operatingRadian)))) 
   coneDistance = (pitchRadius / math.sin(pitchRadian)) 
   apex = Vector3(0.0, 0.0, math.sqrt(((coneDistance * coneDistance) - (pitchRadius * pitchRadius)))) 
   cosPitch = (apex.z / coneDistance) 
   sinPitch = math.sin(pitchRadian) 
   for loop in firstLoopList: 
      for point in loop: 
         alongWay = (point.z / coneDistance) 
         oneMinusAlongWay = (1.0 - alongWay) 
         pointComplex = point.dropAxis() 
         pointComplexLength = abs(pointComplex) 
         deltaRadius = (pointComplexLength - pitchRadius) 
         cosDeltaRadius = (cosPitch * deltaRadius) 
         sinDeltaRadius = (sinPitch * deltaRadius) 
         pointComplex *= ((cosDeltaRadius + pitchRadius) / pointComplexLength) 
         point.x = pointComplex.real 
         point.y = pointComplex.imag 
         point.z += sinDeltaRadius 
         point.x *= oneMinusAlongWay 
         point.y *= oneMinusAlongWay 
   addBottomLoop((- thirdLayerHeight), firstLoopList) 
   topLoop = firstLoopList[(-1)] 
   topAddition = [] 
   topZ = (euclidean.getTopPath(topLoop) + thirdLayerHeight) 
   oldIndex = topLoop[(-1)].index 
   for point in topLoop: 
      oldIndex += 1 
      topAddition.append(Vector3Index(oldIndex, (0.8 * point.x), (0.8 * point.y), topZ)) 
   firstLoopList.append(topAddition) 
   translation = Vector3(0.0, 0.0, (- euclidean.getBottomByPaths(firstLoopList))) 
   euclidean.translateVector3Paths(firstLoopList, translation) 
   geometryOutput = triangle_mesh.getPillarsOutput(loopLists) 
   positives.append(geometryOutput)"," 'Adds a bevel gear to the extrude. 
 Args: 
 derivation: The extrude derivation. 
 extrudeDerivation: The extrude derivation. 
 pitchRadius: The pitch radius of the bevel gear. 
 positives: The positive extrudes. 
 teeth: The number of teeth. 
 vector3GearProfile: The vector3 gear profile. 
 Returns: 
 The number of positives. 
 Raises: 
 None 
 Notes: 
 This method uses the bevel gear profile to create the gear teeth. 
 The gear profile is defined as a vector3 with the following format: 
 (0.0, 0.0, 0.0) 
 (0.0, 0.0, pitchRadius) 
 (0.0, pitchRadius, 0.0) 
 (pitchRadius, 0.0, 0.0) 
 (pitchRadius, pitchRadius, 0.0) 
 The gear profile is used to create the gear teeth. 
 The teeth are created by moving the g",'Get extrude output for a cylinder gear.'
"def t_LPAR(t): 
    t.lexer.paren_count += 1 
   return t"," 'Match a left parenthesis. 
 .. versionadded:: 0.10'",''
"def cachefile(src, dst, api=None, logger=None): 
    lcache = os.path.join(os.path.dirname(os.path.dirname(dst)), '.link_cache') 
   if (not os.path.isdir(lcache)): 
      os.mkdir(lcache) 
   key = hashfile(src, lcache=lcache, logger=logger) 
   cachefile = os.path.join(lcache, key) 
   if (not os.path.exists(cachefile)): 
      logger.info(('trying   to   create   cache   file   %s' % cachefile)) 
      copyfile(src, cachefile, api=api, logger=logger) 
   logger.debug(('trying   cachelink   %s   ->   %s   ->   %s' % (src, cachefile, dst))) 
   os.link(cachefile, dst)"," 'Copies a file to a cache file, and then links it to the destination. 
 This is useful for large files that are going to be downloaded from the 
 network, and then used many times. 
 :param src: the source file 
 :param dst: the destination file 
 :param api: the API to use for the copy and link 
 :param logger: the logger to use for the copy and link 
 :return: None'","'Copy a file into a cache and link it into place. 
 Use this with caution, otherwise you could end up 
 copying data twice if the cache is not on the same device 
 as the destination'"
"def get_course_updates(location, provided_id, user_id): 
    try: 
      course_updates = modulestore().get_item(location) 
   except ItemNotFoundError: 
      course_updates = modulestore().create_item(user_id, location.course_key, location.block_type, location.block_id) 
   course_update_items = get_course_update_items(course_updates, _get_index(provided_id)) 
   return _get_visible_update(course_update_items)"," 'Given a location, a provided_id, and a user_id, return the course 
 updates for the given course. 
 Args: 
 location: The course key of the course. 
 provided_id: The provided id of the course. 
 user_id: The user id of the user. 
 Returns: 
 The course updates for the course.'","'Retrieve the relevant course_info updates and unpack into the model which the client expects: 
 [{id : index, date : string, content : html string}]'"
"def served_by_perl(url): 
    r = requests.get(url, allow_redirects=False) 
   status = (r.status_code == 302) 
   perl = (('x-perl-redirect' in r.headers) and (r.headers['x-perl-redirect'] == 'True')) 
   return all([status, perl])", 'Return True if the given url is served by perl.','Return True if url returns 200 and is served by Perl.'
"def func1(x): 
    return (((np.sin((x * 5)) / x) + (2.0 * x)) - (1.0 * (x ** 2)))", 'A simple example of a function that is not differentiable at 0.',"'made up example with sin, square'"
"def library_check_auth(content, renew=False): 
    user_library_info = get_user_library_info(openid) 
   if user_library_info: 
      cipher = AESCipher(app.config['PASSWORD_SECRET_KEY']) 
      librarypwd = cipher.decrypt(user_library_info['librarypwd']) 
      library.borrowing_record.delay(openid, user_library_info['libraryid'], librarypwd, renew=renew) 
      return wechat.response_text(content) 
   else: 
      url = ((app.config['HOST_URL'] + '/auth-library/') + openid) 
      content = (app.config['AUTH_LIBRARY_TEXT'] % url) 
      return wechat.response_text(content)"," 'Check if user has borrowing permission in library. 
 :param content: The content to be returned to the user. 
 :param renew: If True, renew the borrowing record. 
 :return: The content to be returned to the user.'",''
"def b64c(b): 
    return string.translate(b, B64C_TRANSLATE, B64C_STRIP)"," 'Convert a string to BASE64-C encoded string. 
 The output is a string, not a byte string.'","'Rewrite a base64 string: 
 - Remove LF and = characters 
 - Replace slashes by underscores 
 >>> b64c(""abc123456def"") 
 \'abc123456def\' 
 >>> b64c(""\na/=b=c/"") 
 \'a_bc_\' 
 >>> b64c(""a+b+c+123+"") 
 \'a+b+c+123+\''"
"@not_implemented_for('directed') 
 def common_neighbors(G, u, v): 
    if (u not in G): 
      raise nx.NetworkXError('u   is   not   in   the   graph.') 
   if (v not in G): 
      raise nx.NetworkXError('v   is   not   in   the   graph.') 
   return (w for w in G[u] if ((w in G[v]) and (w not in (u, v))))"," 'Return a list of common neighbors of two vertices in a graph. 
 Parameters 
 G : NetworkX graph 
 The graph. 
 u, v : vertices 
 The two vertices to check for common neighbors. 
 Returns 
 A list of common neighbors of u and v in G. 
 Notes 
 This function is a wrapper for the Python implementation of 
 common_neighbors from the NetworkX package. 
 See Also 
 NetworkX.common_neighbors'","'Return the common neighbors of two nodes in a graph. 
 Parameters 
 G : graph 
 A NetworkX undirected graph. 
 u, v : nodes 
 Nodes in the graph. 
 Returns 
 cnbors : iterator 
 Iterator of common neighbors of u and v in the graph. 
 Raises 
 NetworkXError 
 If u or v is not a node in the graph. 
 Examples 
 >>> G = nx.complete_graph(5) 
 >>> sorted(nx.common_neighbors(G, 0, 1)) 
 [2, 3, 4]'"
"def filename_match(filename, patterns, default=True): 
    if (not patterns): 
      return default 
   return any((fnmatch(filename, pattern) for pattern in patterns))"," 'Return True if filename matches any of the given patterns. 
 If no patterns are given, return True if filename is not empty. 
 :param filename: The filename to match. 
 :param patterns: The patterns to match against. 
 :param default: If no patterns match, return True. 
 :return: True if the filename matches any of the patterns, False otherwise.'","'Check if patterns contains a pattern that matches filename. 
 If patterns is unspecified, this always returns True.'"
"def connect_configservice(aws_access_key_id=None, aws_secret_access_key=None, **kwargs): 
    from boto.configservice.layer1 import ConfigServiceConnection 
   return ConfigServiceConnection(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, **kwargs)"," 'Create a new ConfigServiceConnection. 
 :param aws_access_key_id: The access key id for the AWS account. 
 :param aws_secret_access_key: The secret access key for the AWS account. 
 :param region_name: The name of the AWS region. 
 :param api_version: The API version to use. 
 :param endpoint: The endpoint to use. 
 :param use_ssl: Whether to use SSL. 
 :param ssl_ca_cert: The CA certificate to use. 
 :param ssl_verify: Whether to verify the SSL certificate. 
 :param ssl_cert: The SSL certificate to use. 
 :param ssl_key: The SSL key to use. 
 :param client_side_validation: Whether to validate the configuration against 
 the AWS Config API. 
 :param client_side_validation_type: The type of validation to use. 
 :param client_side_validation_parameters: The parameters to use for client-side 
 validation. 
 :param use_path_style_endpoint: Whether to use path-style endpoints. 
 :param use_path","'Connect to AWS Config 
 :type aws_access_key_id: string 
 :param aws_access_key_id: Your AWS Access Key ID 
 :type aws_secret_access_key: string 
 :param aws_secret_access_key: Your AWS Secret Access Key 
 rtype: :class:`boto.kms.layer1.ConfigServiceConnection` 
 :return: A connection to the AWS Config service'"
"@utils.arg('ip_range', metavar='<range>', help=_('Address   range   to   create.')) 
 @utils.arg('--pool', dest='pool', metavar='<pool>', default=None, help=_('Pool   for   new   Floating   IPs.')) 
 @utils.arg('--interface', metavar='<interface>', default=None, help=_('Interface   for   new   Floating   IPs.')) 
 @deprecated_network 
 def do_floating_ip_bulk_create(cs, args): 
    cs.floating_ips_bulk.create(args.ip_range, args.pool, args.interface)", 'Create a bulk of floating IPs.','Bulk create floating IPs by range (nova-network only).'
"def _decode_preferred_encoding(s): 
    enc = locale.getpreferredencoding() 
   try: 
      try: 
         return s.decode(enc) 
      except LookupError: 
         enc = _DEFAULT_ENCODING 
      return s.decode(enc) 
   except UnicodeDecodeError: 
      return s.decode(u'latin-1')"," 'Decode the string using the preferred encoding. 
 If the string cannot be decoded, try to decode it with latin-1. 
 If that fails, raise UnicodeDecodeError. 
 This function is used by the `encode` method to decode the string 
 when it is returned. 
 :param s: the string to decode 
 :type s: str 
 :return: the decoded string'","'Decode the supplied byte string using the preferred encoding 
 for the locale (`locale.getpreferredencoding`) or, if the default encoding 
 is invalid, fall back first on utf-8, then on latin-1 if the message cannot 
 be decoded with utf-8.'"
"def run_migrations_offline(): 
    set_mysql_engine() 
   kwargs = dict() 
   if neutron_config.database.connection: 
      kwargs['url'] = neutron_config.database.connection 
   else: 
      kwargs['dialect_name'] = neutron_config.database.engine 
   kwargs['include_object'] = include_object 
   context.configure(**kwargs) 
   with context.begin_transaction(): 
      context.run_migrations()"," 'Run migrations in \'offline\' mode. 
 This configuration is useful for running migrations in a testing environment 
 without actually creating a new database.'","'Run migrations in \'offline\' mode. 
 This configures the context with either a URL 
 or an Engine. 
 Calls to context.execute() here emit the given string to the 
 script output.'"
"def create_resource(): 
    task_schema = get_task_schema() 
   partial_task_schema = _get_partial_task_schema() 
   deserializer = RequestDeserializer(task_schema) 
   serializer = ResponseSerializer(task_schema, partial_task_schema) 
   controller = TasksController() 
   return wsgi.Resource(controller, deserializer, serializer)", 'Create a resource object for the tasks API.','Task resource factory method'
"def conjuncts(expr): 
    return And.make_args(expr)", 'Return a conjunction of the given expression\'s arguments.',"'Return a list of the conjuncts in the expr s. 
 Examples 
 >>> from sympy.logic.boolalg import conjuncts 
 >>> from sympy.abc import A, B 
 >>> conjuncts(A & B) 
 frozenset({A, B}) 
 >>> conjuncts(A | B) 
 frozenset({Or(A, B)})'"
"def no_real_gs_credentials(): 
    if (parse_boolean_envvar(os.getenv('WALE_GS_INTEGRATION_TESTS')) is not True): 
      return True 
   if (os.getenv('GOOGLE_APPLICATION_CREDENTIALS') is None): 
      return True 
   return False"," 'Check if the environment variables for GCE credentials are set. 
 If not, return True, otherwise return False. 
 :param no_real_gs_credentials: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :","'Helps skip integration tests without live credentials. 
 Phrased in the negative to make it read better with \'skipif\'.'"
"def invert_docs_link_map(docs_links): 
    files_to_docs = defaultdict(list) 
   for (doc, files) in docs_links.iteritems(): 
      for file in files: 
         files_to_docs[file].append(doc) 
         files_to_docs[file] = list(set(files_to_docs[file])) 
   return files_to_docs", 'Invert a dictionary of links to documents.',"'The docs links map is in this format: 
 ""doc_path"": [ 
 ""file_path"", 
 This transforms it to: 
 ""file_path"": [ 
 ""doc_path"",'"
"@require_global_staff 
 @require_POST 
 def generate_example_certificates(request, course_id=None): 
    course_key = CourseKey.from_string(course_id) 
   certs_api.generate_example_certificates(course_key) 
   return redirect(_instructor_dash_url(course_key, section='certificates'))"," 'Generate example certificates for the given course. 
 Redirects to the certificates page.'","'Start generating a set of example certificates. 
 Example certificates are used to verify that certificates have 
 been configured correctly for the course. 
 Redirects back to the intructor dashboard once certificate 
 generation has begun.'"
"def start(): 
    try: 
      from . import saltnado 
   except ImportError as err: 
      logger.error('ImportError!   {0}'.format(str(err))) 
      return None 
   mod_opts = __opts__.get(__virtualname__, {}) 
   if ('num_processes' not in mod_opts): 
      mod_opts['num_processes'] = 1 
   if ((mod_opts['num_processes'] > 1) and (mod_opts.get('debug', False) is True)): 
      raise Exception(""Tornado's   debug   implementation   is   not   compatible   with   multiprocess.   Either   disable   debug,   or   set   num_processes   to   1."") 
   paths = [('/', saltnado.SaltAPIHandler), ('/login', saltnado.SaltAuthHandler), ('/minions/(.*)', saltnado.MinionSaltAPIHandler), ('/minions', saltnado.MinionSaltAPIHandler), ('/jobs/(.*)', saltnado.JobsSaltAPIHandler), ('/jobs', saltnado.JobsSaltAPIHandler), ('/run', saltnado.RunSaltAPIHandler), ('/events', saltnado.EventsSaltAPIHandler), ('/hook(/.*)?', saltnado.WebhookSaltAPIHandler)] 
   if mod_opts.get('websockets', False): 
      from . import saltnado_websockets 
      token_pattern = '([0-9A-Fa-f]{{{0}}})'.format(len(getattr(hashlib, __opts__.get('hash_type', 'md5'))().hexdigest())) 
      all_events_pattern = '/all_events/{0}'.format(token_pattern) 
      formatted_events_pattern = '/formatted_events/{0}'.format(token_pattern) 
      logger.debug('All   events   URL   pattern   is   {0}'.format(all_events_pattern)) 
      paths += [(all_events_pattern, saltnado_websockets.AllEventsHandler), (formatted_events_pattern, saltnado_websockets.FormattedEventsHandler)] 
   application = tornado.web.Application(paths, debug=mod_opts.get('debug', False)) 
   application.opts = __opts__ 
   application.mod_opts = mod_opts 
   application.auth = salt.auth.LoadAuth(__opts__) 
   kwargs = {} 
   if (not mod_opts.get('disable_ssl', False)): 
      if ('ssl_crt' not in mod_opts): 
         logger.error(""Not   starting   '%s'.   Options   'ssl_crt'   and   'ssl_key'   are   required   if   SSL   is   not   disabled."", __name__) 
         return None 
      ssl_opts = {'certfile': mod_opts['ssl_crt']} 
      if mod_opts.get('ssl_key', False): 
         ssl_opts.update({'keyfile': mod_opts['ssl_key']}) 
      kwargs['ssl_options'] = ssl_opts 
   http_server = tornado.httpserver.HTTPServer(application, **kwargs) 
   try: 
      http_server.bind(mod_opts['port'], address=mod_opts.get('address'), backlog=mod_opts.get('backlog', 128)) 
      http_server.start(mod_opts['num_processes']) 
   except: 
      logger.error('Rest_tornado   unable   to   bind   to   port   {0}'.format(mod_opts['port']), exc_info=True) 
      raise SystemExit(1) 
   try: 
      tornado.ioloop.IOLoop.instance().start() 
   except KeyboardInterrupt: 
      raise SystemExit(0)", 'Starts the Tornado server','Start the saltnado!'
"def create_submission(conf, transform_valid, transform_test=None, features=None): 
    if (transform_test is None): 
      transform_test = transform_valid 
   kwargs = subdict(conf, ['dataset', 'normalize', 'normalize_on_the_fly', 'sparse']) 
   kwargs.update(randomize_valid=False, randomize_test=False) 
   (valid_set, test_set) = load_data(kwargs)[1:3] 
   if (not conf.get('sparse', False)): 
      valid_set = valid_set.get_value(borrow=True) 
      test_set = test_set.get_value(borrow=True) 
   if (features is not None): 
      valid_set = valid_set[:, features] 
      test_set = test_set[:, features] 
   valid_repr = transform_valid(valid_set) 
   test_repr = transform_test(test_set) 
   save_submission(conf, valid_repr, test_repr)", 'Creates a submission for the specified transforms and features.',"'Create a submission file given a configuration dictionary and a 
 computation function. 
 Note that it always reload the datasets to ensure valid & test 
 are not permuted. 
 Parameters 
 conf : WRITEME 
 transform_valid : WRITEME 
 transform_test : WRITEME 
 features : WRITEME'"
"def translate(s, a, b=None, c=None): 
    from sympy.core.compatibility import maketrans 
   try: 
      ''.translate(None, '') 
      py3 = False 
   except TypeError: 
      py3 = True 
   mr = {} 
   if (a is None): 
      assert (c is None) 
      if (not b): 
         return s 
      c = b 
      a = b = '' 
   elif (type(a) is dict): 
      short = {} 
      for k in list(a.keys()): 
         if ((len(k) == 1) and (len(a[k]) == 1)): 
            short[k] = a.pop(k) 
      mr = a 
      c = b 
      if short: 
         (a, b) = [''.join(i) for i in list(zip(*short.items()))] 
      else: 
         a = b = '' 
   else: 
      assert (len(a) == len(b)) 
   if py3: 
      if c: 
         s = s.translate(maketrans('', '', c)) 
      s = replace(s, mr) 
      return s.translate(maketrans(a, b)) 
   else: 
      if c: 
         c = list(c) 
         rem = {} 
         for i in range((-1), ((-1) - len(c)), (-1)): 
            if (ord(c[i]) > 255): 
               rem[c[i]] = '' 
               c.pop(i) 
         s = s.translate(None, ''.join(c)) 
         s = replace(s, rem) 
         if a: 
            a = list(a) 
            b = list(b) 
            for i in range((-1), ((-1) - len(a)), (-1)): 
               if ((ord(a[i]) > 255) or (ord(b[i]) > 255)): 
                  mr[a.pop(i)] = b.pop(i) 
            a = ''.join(a) 
            b = ''.join(b) 
      s = replace(s, mr) 
      table = maketrans(a, b) 
      if ((type(table) is str) and (type(s) is str)): 
         s = s.translate(table) 
      else: 
         s = s.translate(dict([(i, ord(c)) for (i, c) in enumerate(table)])) 
      return s"," 'Translate a string from one alphabet to another. 
 This is a very simple translation function that is intended to be 
 used as a drop-in replacement for the Python string.translate() 
 method. 
 Examples 
 >>> from sympy.utilities.translation import translate 
 >>> translate(\'abc\', \'ABC\') 
 \'ABCc\''","'Return ``s`` where characters have been replaced or deleted. 
 SYNTAX 
 translate(s, None, deletechars): 
 all characters in ``deletechars`` are deleted 
 translate(s, map [,deletechars]): 
 all characters in ``deletechars`` (if provided) are deleted 
 then the replacements defined by map are made; if the keys 
 of map are strings then the longer ones are handled first. 
 Multicharacter deletions should have a value of \'\'. 
 translate(s, oldchars, newchars, deletechars) 
 all characters in ``deletechars`` are deleted 
 then each character in ``oldchars`` is replaced with the 
 corresponding character in ``newchars`` 
 Examples 
 >>> from sympy.utilities.misc import translate 
 >>> from sympy.core.compatibility import unichr 
 >>> abc = \'abc\' 
 >>> translate(abc, None, \'a\') 
 \'bc\' 
 >>> translate(abc, {\'a\': \'x\'}, \'c\') 
 \'xb\' 
 >>> translate(abc, {\'abc\': \'x\', \'a\': \'y\'}) 
 \'x\' 
 >>> translate(\'abcd\', \'ac\', \'AC\', \'d\') 
 \'AbC\' 
 There is no guarantee that a unique answer will be 
 obtained if keys in a mapping overlap are the same 
 length and have some identical sequences at the 
 beginning/end: 
 >>> translate(abc, {\'ab\': \'x\', \'bc\': \'y\'}) in (\'xc\', \'ay\') 
 True'"
"@handle_response_format 
 @treeio_login_required 
 def sla_view(request, sla_id, response_format='html'): 
    sla = get_object_or_404(ServiceLevelAgreement, pk=sla_id) 
   if (not request.user.profile.has_permission(sla)): 
      return user_denied(request, message=""You   don't   have   access   to   this   Service   Level   Agreement"") 
   context = _get_default_context(request) 
   context.update({'sla': sla}) 
   return render_to_response('services/sla_view', context, context_instance=RequestContext(request), response_format=response_format)", 'View the sla','ServiceLevelAgreement view'
"def multi_replace(text, word_dic): 
    rc = re.compile('|'.join(map(re.escape, word_dic))) 
   def translate(match): 
      return word_dic[match.group(0)] 
   return rc.sub(translate, text)"," 'Replace multiple words in a string. 
 >>> multi_replace(\'This is a sentence\', \'is\')) 
 \'This is a sentence\''","'Takes a string and replace words that match a key in a dictionary with the associated value, 
 then returns the changed text 
 :rtype str'"
"def get_azimuth_value(label): 
    _check_is_integral('azimuth', label) 
   if (label == (-1)): 
      return None 
   else: 
      if (((label % 2) != 0) or (label < 0) or (label > 34)): 
         raise ValueError(('Expected   azimuth   to   be   an   even   number   between   0   and   34   inclusive,   or   -1,   but   got   %s   instead.' % str(label))) 
      return (label * 10)"," 'Convert a label to an azimuth value. 
 :param label: A label for the azimuth. 
 :return: The azimuth value. 
 :raises ValueError: If the azimuth value is not in the correct range.'","'Returns the angle in degrees represented by a azimuth label int. 
 Parameters 
 label: int 
 Azimuth label.'"
"def is_automated(): 
    is_automated = False 
   try: 
      dist_dir_i = sys.argv.index('--dist-dir') 
   except ValueError: 
      dist_dir_i = None 
   if (dist_dir_i is not None): 
      dist_dir = sys.argv[(dist_dir_i + 1)] 
      if ('egg-dist-tmp' in dist_dir): 
         is_automated = True 
   if ((sys.argv in [['-c', 'develop', '--no-deps'], ['--no-deps', '-c', 'develop'], ['-c', 'egg_info']]) or ('pip-egg-info' in sys.argv) or (sys.argv[:3] == ['-c', 'install', '--record']) or (sys.argv[:4] == ['-c', 'install', '--single-version-externally-managed', '--record'])): 
      is_automated = True 
   return is_automated", 'Determine if the installation was done by a buildbot or manually','Check for installation with easy_install or pip.'
"@pytest.mark.installed 
 def test_activate_does_not_leak_echo_setting(shell): 
    if ((not on_win) or (shell != u'cmd.exe')): 
      pytest.skip(u""echo   leaking   is   only   relevant   on   Window's   CMD.EXE"") 
   shell_vars = _format_vars(shell) 
   with TemporaryDirectory(prefix=u'envs', dir=os.path.dirname(__file__)) as envs: 
      (env_dirs, env_vars) = gen_test_env_paths(envs, shell) 
      scripts = [] 
      src_activate = shell_vars[u'source'].format(u'{syspath}{binpath}activate{suffix_executable}') 
      scripts += [dedent(u'                                    @ECHO   ON\n                                    {}   ""{{env_dirs[0]}}""\n                                    @ECHO\n                                    ')] 
      for script in scripts: 
         script = script.format(src_activate) 
         script = script.format(env_vars=env_vars, env_dirs=env_dirs, **shell_vars) 
         commands = (shell_vars[u'command_setup'] + script) 
         (stdout, stderr) = run_in(commands, shell) 
         print(u'commands:', commands) 
         print(u'stdout:', stdout) 
         print(u'stderr:', stderr) 
         assert_equals(stdout, u'ECHO   is   on.', stderr) 
         assert_equals(stderr, u'')", 'Test that activate does not leak the echo setting.','Test that activate\'s setting of echo to off does not disrupt later echo calls'
"def gf_berlekamp(f, p, K): 
    Q = gf_Qmatrix(f, p, K) 
   V = gf_Qbasis(Q, p, K) 
   for (i, v) in enumerate(V): 
      V[i] = gf_strip(list(reversed(v))) 
   factors = [f] 
   for k in range(1, len(V)): 
      for f in list(factors): 
         s = K.zero 
         while (s < p): 
            g = gf_sub_ground(V[k], s, p, K) 
            h = gf_gcd(f, g, p, K) 
            if ((h != [K.one]) and (h != f)): 
               factors.remove(f) 
               f = gf_quo(f, h, p, K) 
               factors.extend([f, h]) 
            if (len(factors) == len(V)): 
               return _sort_factors(factors, multiple=False) 
            s += K.one 
   return _sort_factors(factors, multiple=False)"," 'Compute the Berlekamp factorization of ``f`` in ``K[x]``. 
 Examples 
 >>> from sympy.polys.galoistools import gf_berlekamp 
 >>> gf_berlekamp(x**2 + x, 3, QQ) 
 [x + 1, x + 1, x + 1, x + 1]'","'Factor a square-free ``f`` in ``GF(p)[x]`` for small ``p``. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import gf_berlekamp 
 >>> gf_berlekamp([1, 0, 0, 0, 1], 5, ZZ) 
 [[1, 0, 2], [1, 0, 3]]'"
"def notify_status(doc, method=None): 
    party_type = None 
   for (key, doctypes) in status_depends_on.iteritems(): 
      if (doc.doctype in doctypes): 
         party_type = key 
         break 
   if (not party_type): 
      return 
   name = doc.get(party_type.lower()) 
   if (not name): 
      return 
   party = frappe.get_doc(party_type, name) 
   filters = get_filters_for(doc.doctype) 
   party.flags.ignore_mandatory = True 
   status = None 
   if filters: 
      if evaluate_filters(doc, filters): 
         status = u'Open' 
   if (status == u'Open'): 
      if (party.status != u'Open'): 
         party.status = u'Open' 
         party.save(ignore_permissions=True) 
   elif (party.status == u'Open'): 
      update_status(party) 
   party.update_modified() 
   party.notify_update()"," 'Notify status change for a party 
 :param doc: doc dict 
 :param method: method name 
 :returns: None'","'Notify status to customer, supplier'"
"def ajax_editable_boolean(attr, short_description): 
    def _fn(self, item): 
      return ajax_editable_boolean_cell(item, attr) 
   _fn.short_description = short_description 
   _fn.editable_boolean_field = attr 
   return _fn", 'Ajax-editable boolean cell.',"'Convenience function: Assign the return value of this method to a variable 
 of your ModelAdmin class and put the variable name into list_display. 
 Example:: 
 class MyTreeEditor(TreeEditor): 
 list_display = (\'__str__\', \'active_toggle\') 
 active_toggle = ajax_editable_boolean(\'active\', _(\'is active\'))'"
"def from_text(textring): 
    keyring = {} 
   for keytext in textring: 
      keyname = dns.name.from_text(keytext) 
      secret = base64.decodestring(textring[keytext]) 
      keyring[keyname] = secret 
   return keyring", 'Returns a dictionary of DNS keyring values from a text string.',"'Convert a dictionary containing (textual DNS name, base64 secret) pairs 
 into a binary keyring which has (dns.name.Name, binary secret) pairs. 
 @rtype: dict'"
"def _create_base_cipher(dict_parameters): 
    use_aesni = dict_parameters.pop('use_aesni', True) 
   try: 
      key = dict_parameters.pop('key') 
   except KeyError: 
      raise TypeError(""Missing   'key'   parameter"") 
   expect_byte_string(key) 
   if (len(key) not in key_size): 
      raise ValueError(('Incorrect   AES   key   length   (%d   bytes)' % len(key))) 
   if (use_aesni and _raw_aesni_lib): 
      start_operation = _raw_aesni_lib.AESNI_start_operation 
      stop_operation = _raw_aesni_lib.AESNI_stop_operation 
   else: 
      start_operation = _raw_aes_lib.AES_start_operation 
      stop_operation = _raw_aes_lib.AES_stop_operation 
   cipher = VoidPointer() 
   result = start_operation(key, c_size_t(len(key)), cipher.address_of()) 
   if result: 
      raise ValueError(('Error   %X   while   instantiating   the   AES   cipher' % result)) 
   return SmartPointer(cipher.get(), stop_operation)"," 'Create a new AES cipher instance. 
 :param dict_parameters: 
 Dictionary with parameters to be passed to the cipher creation. 
 :param use_aesni: 
 Whether to use AES-NI or not. 
 :returns: 
 A smart pointer to the created cipher instance.'","'This method instantiates and returns a handle to a low-level 
 base cipher. It will absorb named parameters in the process.'"
"def _lscmp(a, b): 
    return ((not sum(((0 if (x == y) else 1) for (x, y) in zip(a, b)))) and (len(a) == len(b)))"," 'Return True if a and b are equal, False otherwise.'","'Compares two strings in a cryptographically safe way: 
 Runtime is not affected by length of common prefix.'"
"def get_minions(): 
    serv = _get_serv(ret=None) 
   sql = 'select   distinct(id)   from   returns' 
   data = serv.query(sql) 
   ret = [] 
   if data: 
      for jid in data[0]['points']: 
         ret.append(jid[1]) 
   return ret", 'get the list of minions','Return a list of minions'
"def get_writer_names(): 
    return set([k for (k, v) in six.iteritems(REPORT_WRITERS_MAP) if v])", 'Return a set of writer names.','Get the registered writer names'
"def install(pkgs=None, requirements=None, bin_env=None, use_wheel=False, no_use_wheel=False, log=None, proxy=None, timeout=None, editable=None, find_links=None, index_url=None, extra_index_url=None, no_index=False, mirrors=None, build=None, target=None, download=None, download_cache=None, source=None, upgrade=False, force_reinstall=False, ignore_installed=False, exists_action=None, no_deps=False, no_install=False, no_download=False, global_options=None, install_options=None, user=None, no_chown=False, cwd=None, pre_releases=False, cert=None, allow_all_external=False, allow_external=None, allow_unverified=None, process_dependency_links=False, saltenv='base', env_vars=None, use_vt=False, trusted_host=None, no_cache_dir=False): 
    pip_bin = _get_pip_bin(bin_env) 
   cmd = [pip_bin, 'install'] 
   (cleanup_requirements, error) = _process_requirements(requirements=requirements, cmd=cmd, cwd=cwd, saltenv=saltenv, user=user) 
   if error: 
      return error 
   if use_wheel: 
      min_version = '1.4' 
      cur_version = __salt__['pip.version'](bin_env) 
      if (not salt.utils.compare_versions(ver1=cur_version, oper='>=', ver2=min_version)): 
         logger.error('The   --use-wheel   option   is   only   supported   in   pip   {0}   and   newer.   The   version   of   pip   detected   is   {1}.   This   option   will   be   ignored.'.format(min_version, cur_version)) 
      else: 
         cmd.append('--use-wheel') 
   if no_use_wheel: 
      min_version = '1.4' 
      cur_version = __salt__['pip.version'](bin_env) 
      if (not salt.utils.compare_versions(ver1=cur_version, oper='>=', ver2=min_version)): 
         logger.error('The   --no-use-wheel   option   is   only   supported   in   pip   {0}   and   newer.   The   version   of   pip   detected   is   {1}.   This   option   will   be   ignored.'.format(min_version, cur_version)) 
      else: 
         cmd.append('--no-use-wheel') 
   if log: 
      if os.path.isdir(log): 
         raise IOError(""'{0}'   is   a   directory.   Use   --log   path_to_file"".format(log)) 
      elif (not os.access(log, os.W_OK)): 
         raise IOError(""'{0}'   is   not   writeable"".format(log)) 
      cmd.extend(['--log', log]) 
   if proxy: 
      cmd.extend(['--proxy', proxy]) 
   if timeout: 
      try: 
         if isinstance(timeout, float): 
            raise ValueError('Timeout   cannot   be   a   float') 
         int(timeout) 
      except ValueError: 
         raise ValueError(""'{0}'   is   not   a   valid   timeout,   must   be   an   integer"".format(timeout)) 
      cmd.extend(['--timeout', timeout]) 
   if find_links: 
      if isinstance(find_links, string_types): 
         find_links = [l.strip() for l in find_links.split(',')] 
      for link in find_links: 
         if (not (salt.utils.url.validate(link, VALID_PROTOS) or os.path.exists(link))): 
            raise CommandExecutionError(""'{0}'   is   not   a   valid   URL   or   path"".format(link)) 
         cmd.extend(['--find-links', link]) 
   if (no_index and (index_url or extra_index_url)): 
      raise CommandExecutionError(""'no_index'   and   ('index_url'   or   'extra_index_url')   are   mutually   exclusive."") 
   if index_url: 
      if (not salt.utils.url.validate(index_url, VALID_PROTOS)): 
         raise CommandExecutionError(""'{0}'   is   not   a   valid   URL"".format(index_url)) 
      cmd.extend(['--index-url', index_url]) 
   if extra_index_url: 
      if (not salt.utils.url.validate(extra_index_url, VALID_PROTOS)): 
         raise CommandExecutionError(""'{0}'   is   not   a   valid   URL"".format(extra_index_url)) 
      cmd.extend(['--extra-index-url', extra_index_url]) 
   if no_index: 
      cmd.append('--no-index') 
   if mirrors: 
      pip_version = version(pip_bin) 
      if salt.utils.compare_versions(ver1=pip_version, oper='>=', ver2='7.0.0'): 
         raise CommandExecutionError('pip   >=   7.0.0   does   not   support   mirror   argument:   use   index_url   and/or   extra_index_url   instead') 
      if isinstance(mirrors, string_types): 
         mirrors = [m.strip() for m in mirrors.split(',')] 
      cmd.append('--use-mirrors') 
      for mirror in mirrors: 
         if (not mirror.startswith('http://')): 
            raise CommandExecutionError(""'{0}'   is   not   a   valid   URL"".format(mirror)) 
         cmd.extend(['--mirrors', mirror]) 
   if build: 
      cmd.extend(['--build', build]) 
   if target: 
      cmd.extend(['--target', target]) 
   if download: 
      cmd.extend(['--download', download]) 
   if download_cache: 
      cmd.extend(['--download-cache', download_cache]) 
   if source: 
      cmd.extend(['--source', source]) 
   if upgrade: 
      cmd.append('--upgrade') 
   if force_reinstall: 
      cmd.append('--force-reinstall') 
   if ignore_installed: 
      cmd.append('--ignore-installed') 
   if exists_action: 
      if (exists_action.lower() not in ('s', 'i', 'w', 'b')): 
         raise CommandExecutionError(""The   exists_action   pip   option   only   supports   the   values   s,   i,   w,   and   b.   '{0}'   is   not   valid."".format(exists_action)) 
      cmd.extend(['--exists-action', exists_action]) 
   if no_deps: 
      cmd.append('--no-deps') 
   if no_install: 
      cmd.append('--no-install') 
   if no_download: 
      cmd.append('--no-download') 
   if no_cache_dir: 
      cmd.append('--no-cache-dir') 
   if pre_releases: 
      pip_version = version(pip_bin) 
      if salt.utils.compare_versions(ver1=pip_version, oper='>=', ver2='1.4'): 
         cmd.append('--pre') 
   if cert: 
      cmd.extend(['--cert', cert]) 
   if global_options: 
      if isinstance(global_options, string_types): 
         global_options = [go.strip() for go in global_options.split(',')] 
      for opt in global_options: 
         cmd.extend(['--global-option', opt]) 
   if install_options: 
      if isinstance(install_options, string_types): 
         install_options = [io.strip() for io in install_options.split(',')] 
      for opt in install_options: 
         cmd.extend(['--install-option', opt]) 
   if pkgs: 
      if isinstance(pkgs, string_types): 
         pkgs = [p.strip() for p in pkgs.split(',')] 
      cmd.extend(['{0}'.format(p.replace(';', ',')) for p in pkgs]) 
   if editable: 
      egg_match = re.compile('(?:#|#.*?&)egg=([^&]*)') 
      if isinstance(editable, string_types): 
         editable = [e.strip() for e in editable.split(',')] 
      for entry in editable: 
         if (not ((entry == '.') or entry.startswith(('file://', '/')))): 
            match = egg_match.search(entry) 
            if ((not match) or (not match.group(1))): 
               raise CommandExecutionError('You   must   specify   an   egg   for   this   editable') 
         cmd.extend(['--editable', entry]) 
   if allow_all_external: 
      cmd.append('--allow-all-external') 
   if allow_external: 
      if isinstance(allow_external, string_types): 
         allow_external = [p.strip() for p in allow_external.split(',')] 
      for pkg in allow_external: 
         cmd.extend(['--allow-external', pkg]) 
   if allow_unverified: 
      if isinstance(allow_unverified, string_types): 
         allow_unverified = [p.strip() for p in allow_unverified.split(',')] 
      for pkg in allow_unverified: 
         cmd.extend(['--allow-unverified', pkg]) 
   if process_dependency_links: 
      cmd.append('--process-dependency-links') 
   if env_vars: 
      if isinstance(env_vars, dict): 
         for (k, v) in iteritems(env_vars): 
            if (not isinstance(v, string_types)): 
               env_vars[k] = str(v) 
         os.environ.update(env_vars) 
      else: 
         raise CommandExecutionError('env_vars   {0}   is   not   a   dictionary'.format(env_vars)) 
   if trusted_host: 
      cmd.extend(['--trusted-host', trusted_host]) 
   try: 
      cmd_kwargs = dict(saltenv=saltenv, use_vt=use_vt, runas=user) 
      if cwd: 
         cmd_kwargs['cwd'] = cwd 
      if (bin_env and os.path.isdir(bin_env)): 
         cmd_kwargs['env'] = {'VIRTUAL_ENV': bin_env} 
      logger.debug('TRY   BLOCK:   end   of   pip.install   --   cmd:   %s,   cmd_kwargs:   %s', str(cmd), str(cmd_kwargs)) 
      return __salt__['cmd.run_all'](cmd, python_shell=False, **cmd_kwargs) 
   finally: 
      for tempdir in [cr for cr in cleanup_requirements if (cr is not None)]: 
         if os.path.isdir(tempdir): 
            shutil.rmtree(tempdir)"," 'Install a package with pip 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package with pip. 
 This function will install a package","'Install packages with pip 
 Install packages individually or from a pip requirements file. Install 
 packages globally or to a virtualenv. 
 pkgs 
 Comma separated list of packages to install 
 requirements 
 Path to requirements 
 bin_env 
 Path to pip bin or path to virtualenv. If doing a system install, 
 and want to use a specific pip bin (pip-2.7, pip-2.6, etc..) just 
 specify the pip bin you want. 
 .. note:: 
 If installing into a virtualenv, just use the path to the 
 virtualenv (e.g. ``/home/code/path/to/virtualenv/``) 
 use_wheel 
 Prefer wheel archives (requires pip>=1.4) 
 no_use_wheel 
 Force to not use wheel archives (requires pip>=1.4) 
 log 
 Log file where a complete (maximum verbosity) record will be kept 
 proxy 
 Specify a proxy in the form ``user:passwd@proxy.server:port``. Note 
 that the ``user:password@`` is optional and required only if you are 
 behind an authenticated proxy. If you provide 
 ``user@proxy.server:port`` then you will be prompted for a password. 
 timeout 
 Set the socket timeout (default 15 seconds) 
 editable 
 install something editable (e.g. 
 ``git+https://github.com/worldcompany/djangoembed.git#egg=djangoembed``) 
 find_links 
 URL to search for packages 
 index_url 
 Base URL of Python Package Index 
 extra_index_url 
 Extra URLs of package indexes to use in addition to ``index_url`` 
 no_index 
 Ignore package index 
 mirrors 
 Specific mirror URL(s) to query (automatically adds --use-mirrors) 
 .. warning:: 
 This option has been deprecated and removed in pip version 7.0.0. 
 Please use ``index_url`` and/or ``extra_index_url`` instead. 
 build 
 Unpack packages into ``build`` dir 
 target 
 Install packages into ``target`` dir 
 download 
 Download packages into ``download`` instead of installing them 
 download_cache 
 Cache downloaded packages in ``download_cache`` dir 
 source 
 Check out ``editable`` packages into ``source`` dir 
 upgrade 
 Upgrade all packages to the newest available version 
 force_reinstall 
 When upgrading, reinstall all packages even if they are already 
 up-to-date. 
 ignore_installed 
 Ignore the installed packages (reinstalling instead) 
 exists_action 
 Default action when a path already exists: (s)witch, (i)gnore, (w)ipe, 
 (b)ackup 
 no_deps 
 Ignore package dependencies 
 no_install 
 Download and unpack all packages, but don\'t actually install them 
 no_download 
 Don\'t download any packages, just install the ones already downloaded 
 (completes an install run with ``--no-install``) 
 install_options 
 Extra arguments to be supplied to the setup.py install command (e.g. 
 like ``--install-option=\'--install-scripts=/usr/local/bin\'``).  Use 
 multiple --install-option options to pass multiple options to setup.py 
 install. If you are using an option with a directory path, be sure to 
 use absolute path. 
 global_options 
 Extra global options to be supplied to the setup.py call before the 
 install command. 
 user 
 The user under which to run pip 
 no_chown 
 When user is given, do not attempt to copy and chown a requirements 
 file 
 cwd 
 Current working directory to run pip from 
 pre_releases 
 Include pre-releases in the available versions 
 cert 
 Provide a path to an alternate CA bundle 
 allow_all_external 
 Allow the installation of all externally hosted files 
 allow_external 
 Allow the installation of externally hosted files (comma separated 
 list) 
 allow_unverified 
 Allow the installation of insecure and unverifiable files (comma 
 separated list) 
 process_dependency_links 
 Enable the processing of dependency links 
 env_vars 
 Set environment variables that some builds will depend on. For example, 
 a Python C-module may have a Makefile that needs INCLUDE_PATH set to 
 pick up a header file while compiling.  This must be in the form of a 
 dictionary or a mapping. 
 Example: 
 .. code-block:: bash 
 salt \'*\' pip.install django_app env_vars=""{\'CUSTOM_PATH\': \'/opt/django_app\'}"" 
 trusted_host 
 Mark this host as trusted, even though it does not have valid or any 
 HTTPS. 
 use_vt 
 Use VT terminal emulation (see output while installing) 
 no_cache_dir 
 Disable the cache. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pip.install <package name>,<package2 name> 
 salt \'*\' pip.install requirements=/path/to/requirements.txt 
 salt \'*\' pip.install <package name> bin_env=/path/to/virtualenv 
 salt \'*\' pip.install <package name> bin_env=/path/to/pip_bin 
 Complicated CLI example:: 
 salt \'*\' pip.install markdown,django                 editable=git+https://github.com/worldcompany/djangoembed.git#egg=djangoembed upgrade=True no_deps=True'"
"def _onPygletText(text, emulated=False): 
    global useText 
   if (not useText): 
      return 
   keyTime = psychopy.core.getTime() 
   if emulated: 
      keySource = 'EmulatedKey' 
   else: 
      keySource = 'KeyPress' 
   _keyBuffer.append((text, keyTime)) 
   logging.data(('%s:   %s' % (keySource, text)))"," 'Log a key press in the _keyBuffer. 
 Args: 
 text: The text that was pressed 
 emulated: If True, the keypress was emulated by the pyglet app 
 Returns: 
 None'","'handler for on_text pyglet events, or call directly to emulate a text 
 event. 
 S Mathot 2012: This function only acts when the key that is pressed 
 corresponds to a non-ASCII text character (Greek, Arabic, Hebrew, etc.). 
 In that case the symbol that is passed to _onPygletKey() is translated 
 into a useless \'user_key()\' string. If this happens, _onPygletText takes 
 over the role of capturing the key. Unfortunately, _onPygletText() 
 cannot solely handle all input, because it does not respond to spacebar 
 presses, etc.'"
"@gen.coroutine 
 def _UploadWelcomePhotos(http_client, client, user, upload_request): 
    obj_store = ObjectStore.GetInstance(ObjectStore.PHOTO) 
   welcome_path = os.path.join(ResourcesManager.Instance().resources_path, 'welcome') 
   (yield _SetWelcomeIds(user, upload_request)) 
   upload_request = deepcopy(upload_request) 
   upload_request_copy = deepcopy(upload_request) 
   [ph_dict.pop('name') for ph_dict in upload_request_copy['photos']] 
   upload_response = (yield UploadEpisode(client, obj_store, user.user_id, user.webapp_dev_id, upload_request_copy)) 
   for (request_ph_dict, response_ph_dict) in zip(upload_request['photos'], upload_response['photos']): 
      for format in ('full', 'med', 'tn'): 
         f = open(os.path.join(welcome_path, ('%s_%s.jpg' % (request_ph_dict['name'], format))), 'r') 
         image_data = f.read() 
         f.close() 
         photo_url = response_ph_dict[(format + '_put_url')] 
         content_md5 = base64.b64encode(request_ph_dict[(format + '_md5')].decode('hex')) 
         headers = {'Content-Type': 'image/jpeg', 'Content-MD5': content_md5} 
         validate_cert = (not options.options.fileobjstore) 
         response = (yield gen.Task(http_client.fetch, photo_url, method='PUT', body=image_data, follow_redirects=False, validate_cert=validate_cert, headers=headers)) 
         if (response.code != 200): 
            raise Exception(('Cannot   upload   photo   ""%s"".   HTTP   error   code   %d.   Is   server   running   and   accessible?' % (request_ph_dict['photo_id'], response.code)))"," 'Upload welcome photos. 
 :param http_client: HTTP client. 
 :param client: Client. 
 :param user: User. 
 :param upload_request: Request to upload photos. 
 :return: Upload response. 
 :rtype: dict(str, str)'","'Uploads a set of photos that will be used in the new user welcome conversation. These 
 photos are uploaded to the given user account. ""upload_request"" is in the UPLOAD_EPISODE_REQUEST 
 format in json_schema.py, except: 
 1. Activity, episode, and photo ids are added by this method. 
 2. Each photo dict must contain an additional ""name"" field which gives the start of the 
 filename of a jpg file in the backend/resources/welcome directory. Three files must 
 exist there, in this format: <name>_full.jpg, <name>_med.jpg, <name>_tn.jpg.'"
"def _get_next_prev(generic_view, date, is_previous, period): 
    date_field = generic_view.get_date_field() 
   allow_empty = generic_view.get_allow_empty() 
   allow_future = generic_view.get_allow_future() 
   get_current = getattr(generic_view, (u'_get_current_%s' % period)) 
   get_next = getattr(generic_view, (u'_get_next_%s' % period)) 
   (start, end) = (get_current(date), get_next(date)) 
   if allow_empty: 
      if is_previous: 
         result = get_current((start - datetime.timedelta(days=1))) 
      else: 
         result = end 
      if (allow_future or (result <= timezone_today())): 
         return result 
      else: 
         return None 
   else: 
      if is_previous: 
         lookup = {(u'%s__lt' % date_field): generic_view._make_date_lookup_arg(start)} 
         ordering = (u'-%s' % date_field) 
      else: 
         lookup = {(u'%s__gte' % date_field): generic_view._make_date_lookup_arg(end)} 
         ordering = date_field 
      if (not allow_future): 
         if generic_view.uses_datetime_field: 
            now = timezone.now() 
         else: 
            now = timezone_today() 
         lookup[(u'%s__lte' % date_field)] = now 
      qs = generic_view.get_queryset().filter(**lookup).order_by(ordering) 
      try: 
         result = getattr(qs[0], date_field) 
      except IndexError: 
         return None 
      if generic_view.uses_datetime_field: 
         if settings.USE_TZ: 
            result = timezone.localtime(result) 
         result = result.date() 
      return get_current(result)"," 'Return the next or previous date, depending on the period. 
 The next or previous date is the first date in the next or previous 
 period that is not empty or in the future. 
 :param generic_view: The view to get the date from 
 :param date: The date to get the next or previous date for 
 :param is_previous: True if the next or previous date is being requested 
 :param period: The period to get the next or previous date for 
 :return: The next or previous date, or None if there is no next or previous date 
 :rtype: datetime.date or None'","'Helper: Get the next or the previous valid date. The idea is to allow 
 links on month/day views to never be 404s by never providing a date 
 that\'ll be invalid for the given view. 
 This is a bit complicated since it handles different intervals of time, 
 hence the coupling to generic_view. 
 However in essence the logic comes down to: 
 * If allow_empty and allow_future are both true, this is easy: just 
 return the naive result (just the next/previous day/week/month, 
 reguardless of object existence.) 
 * If allow_empty is true, allow_future is false, and the naive result 
 isn\'t in the future, then return it; otherwise return None. 
 * If allow_empty is false and allow_future is true, return the next 
 date *that contains a valid object*, even if it\'s in the future. If 
 there are no next objects, return None. 
 * If allow_empty is false and allow_future is false, return the next 
 date that contains a valid object. If that date is in the future, or 
 if there are no next objects, return None.'"
"@not_implemented_for('undirected') 
 def antichains(G): 
    TC = nx.transitive_closure(G) 
   antichains_stacks = [([], list(reversed(list(nx.topological_sort(G)))))] 
   while antichains_stacks: 
      (antichain, stack) = antichains_stacks.pop() 
      (yield antichain) 
      while stack: 
         x = stack.pop() 
         new_antichain = (antichain + [x]) 
         new_stack = [t for t in stack if (not ((t in TC[x]) or (x in TC[t])))] 
         antichains_stacks.append((new_antichain, new_stack))", 'Return an iterator over antichains of G.',"'Generates antichains from a DAG. 
 An antichain is a subset of a partially ordered set such that any 
 two elements in the subset are incomparable. 
 Parameters 
 G : NetworkX DiGraph 
 Graph 
 Returns 
 antichain : generator object 
 Raises 
 NetworkXNotImplemented 
 If G is not directed 
 NetworkXUnfeasible 
 If G contains a cycle 
 Notes 
 This function was originally developed by Peter Jipsen and Franco Saliola 
 for the SAGE project. It\'s included in NetworkX with permission from the 
 authors. Original SAGE code at: 
 https://sage.informatik.uni-goettingen.de/src/combinat/posets/hasse_diagram.py 
 References 
 .. [1] Free Lattices, by R. Freese, J. Jezek and J. B. Nation, 
 AMS, Vol 42, 1995, p. 226.'"
"def get_build_version(): 
    prefix = 'MSC   v.' 
   i = string.find(sys.version, prefix) 
   if (i == (-1)): 
      return 6 
   i = (i + len(prefix)) 
   (s, rest) = sys.version[i:].split('   ', 1) 
   majorVersion = (int(s[:(-2)]) - 6) 
   minorVersion = (int(s[2:3]) / 10.0) 
   if (majorVersion == 6): 
      minorVersion = 0 
   if (majorVersion >= 6): 
      return (majorVersion + minorVersion) 
   return None"," 'Returns the version of the Python interpreter. 
 This is a wrapper around sys.version_info.major, 
 sys.version_info.minor, and sys.version_info.micro. 
 This is used to determine which version of Python to use 
 when deciding whether to use the Python 2.3 or 2.4 
 implementation of the decimal module. 
 This function is also used to determine the version of 
 Python that is being used to build the Python C-API. 
 This is used to determine which version of the Python C-API 
 is being used when compiling extensions. 
 This is used to determine which version of the Python C-API 
 is being used when building the Python C-API. 
 This is used to determine which version of the Python C-API 
 is being used when building the Python C-API. 
 This is used to determine which version of the Python C-API 
 is being used when building the Python C-API. 
 This is used to determine which version of the Python C-API 
 is being used when building the Python C-API. 
 This is used to determine","'Return the version of MSVC that was used to build Python. 
 For Python 2.3 and up, the version number is included in 
 sys.version.  For earlier versions, assume the compiler is MSVC 6.'"
"def update_content_in_local_cache(url, content, method='GET'): 
    if (local_cache_enable and (method == 'GET') and cache.is_cached(url)): 
      info_dict = cache.get_info(url) 
      resp = cache.get_obj(url) 
      resp.set_data(content) 
      info_dict['without_content'] = False 
      if (verbose_level >= 4): 
         dbgprint('LocalCache_UpdateCache', url, content[:30], len(content)) 
      cache.put_obj(url, resp, obj_size=len(content), expires=get_expire_from_mime(parse.mime), last_modified=info_dict.get('last_modified'), info_dict=info_dict)"," 'Update content in local cache 
 :param url: URL to update 
 :param content: Content to update 
 :param method: HTTP method to update'","'æ´æ° local_cache ä¸­ç¼å­çèµæº, è¿½å content 
 å¨streamæ¨¡å¼ä¸­ä½¿ç¨'"
"def daemonize(pidfile=None, uid=None, umask=63): 
    if os.fork(): 
      return 1 
   os.setsid() 
   if os.fork(): 
      return 2 
   os.chdir('/') 
   null = os.open('/dev/null', os.O_RDWR) 
   for i in range(3): 
      try: 
         os.dup2(null, i) 
      except OSError as e: 
         if (e.errno != errno.EBADF): 
            raise 
   os.close(null) 
   warnings.filterwarnings('ignore') 
   if pidfile: 
      piddir = os.path.dirname(pidfile) 
      if (not os.path.exists(piddir)): 
         os.makedirs(piddir) 
      f = file(pidfile, 'w') 
      f.write(str(os.getpid())) 
      f.close() 
   if (umask is not None): 
      os.umask(umask) 
   if uid: 
      setugid(uid) 
   return None"," 'Daemonize the process. 
 :param pidfile: Path to a file where the process id will be stored. 
 :param uid: The uid of the process. 
 :param umask: The umask to use when opening files. 
 :returns: None on success, otherwise the exit code of the process.'","'daemonize a Unix process. Set paranoid umask by default. 
 Return 1 in the original process, 2 in the first fork, and None for the 
 second fork (eg daemon process).'"
"def _blockdevice_id_to_dataset_id(blockdevice_id): 
    return UUID(blockdevice_id[len(_PREFIX):])", 'Convert a blockdevice_id to a dataset_id',"'Computes a dataset_id from a blockdevice_id. 
 :param unicode blockdevice_id: The blockdevice_id to get the dataset_id 
 for. 
 :returns UUID: The corresponding dataset_id.'"
"def legitimize(text, os=platform.system()): 
    text = text.translate({0: None, ord('/'): '-', ord('|'): '-'}) 
   if (os == 'Windows'): 
      text = text.translate({ord(':'): '-', ord('*'): '-', ord('?'): '-', ord('\\'): '-', ord('""'): ""'"", ord('+'): '-', ord('<'): '-', ord('>'): '-', ord('['): '(', ord(']'): ')'}) 
   else: 
      if (os == 'Darwin'): 
         text = text.translate({ord(':'): '-'}) 
      if text.startswith('.'): 
         text = text[1:] 
   text = text[:82] 
   return text"," 'Returns the text with all non-ASCII characters replaced by 
 their hexadecimal representation. 
 :param text: 
 :type text: 
 :param os: 
 :type os: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return:",'Converts a string to a valid filename.'
"def decrypt(stream, parameters): 
    encodedStream = '' 
   if ((parameters == None) or (parameters == {})): 
      return (0, stream) 
   elif ((not parameters.has_key('/Name')) or (parameters['/Name'] == None)): 
      return (0, stream) 
   else: 
      cryptFilterName = parameters['/Name'].getValue() 
      if (cryptFilterName == 'Identity'): 
         return (0, stream) 
      else: 
         return ((-1), 'Decrypt   not   supported   yet')"," 'Decrypts a stream. 
 :param stream: The stream to decrypt. 
 :param parameters: The parameters to use to decrypt the stream. 
 :returns: A tuple of the length of the decrypted stream and the decrypted stream.'","'Method to decrypt streams using a PDF security handler (NOT IMPLEMENTED YET) 
 @param stream: A PDF stream 
 @return: A tuple (status,statusContent), where statusContent is the decrypted PDF stream in case status = 0 or an error in case status = -1'"
"def UploadSeparatePatches(issue, rpc_server, patchset, data, options): 
    patches = SplitPatch(data) 
   rv = [] 
   for patch in patches: 
      set_status(('uploading   patch   for   ' + patch[0])) 
      if (len(patch[1]) > MAX_UPLOAD_SIZE): 
         print (('Not   uploading   the   patch   for   ' + patch[0]) + '   because   the   file   is   too   large.') 
         continue 
      form_fields = [('filename', patch[0])] 
      if (not options.download_base): 
         form_fields.append(('content_upload', '1')) 
      files = [('data', 'data.diff', patch[1])] 
      (ctype, body) = EncodeMultipartFormData(form_fields, files) 
      url = ('/%d/upload_patch/%d' % (int(issue), int(patchset))) 
      print ('Uploading   patch   for   ' + patch[0]) 
      response_body = rpc_server.Send(url, body, content_type=ctype) 
      lines = response_body.splitlines() 
      if ((not lines) or (lines[0] != 'OK')): 
         StatusUpdate(('      -->   %s' % response_body)) 
         sys.exit(1) 
      rv.append([lines[1], patch[0]]) 
   return rv"," 'Upload patches to the server. 
 :param issue: the issue number 
 :param rpc_server: the RPC server 
 :param patchset: the patchset number 
 :param data: the data to upload 
 :param options: the options 
 :returns: a list of (status, patchset) tuples 
 :rtype: list(tuple(str, int))'","'Uploads a separate patch for each file in the diff output. 
 Returns a list of [patch_key, filename] for each file.'"
"@world.absorb 
 def css_html(css_selector, index=0): 
    assert is_css_present(css_selector) 
   return retry_on_exception((lambda : css_find(css_selector)[index].html))"," 'Return the text of the element with the given CSS selector. 
 :param css_selector: CSS selector of the element to get the text of 
 :type css_selector: str 
 :param index: Index of the element to get the text of 
 :type index: int 
 :returns: Text of the element 
 :rtype: str'",'Returns the HTML of a css_selector'
"def delete_multi(blob_keys, **options): 
    fut = delete_multi_async(blob_keys, **options) 
   fut.get_result()"," 'Delete multiple blobs. 
 :param blob_keys: List of blob keys. 
 :param options: Dictionary of options. 
 :param options.delete_snapshots: Delete snapshots. 
 :param options.delete_stored_access_policy: Delete stored access policy. 
 :param options.delete_legal_hold: Delete legal hold. 
 :param options.delete_immutability_policy: Delete immutability policy. 
 :param options.delete_legal_hold: Delete legal hold. 
 :param options.delete_immutability_policy: Delete immutability policy. 
 :param options.delete_tags: Delete tags. 
 :param options.delete_legal_hold: Delete legal hold. 
 :param options.delete_immutability_policy: Delete immutability policy. 
 :param options.delete_tags: Delete tags. 
 :param options.delete_immutability_policy: Delete immutability policy. 
 :param options.delete_tags: Delete tags. 
 :param options.delete_immutability_policy: Delete immutability policy. 
","'Delete blobs from Blobstore. 
 Args: 
 blob_keys: A list of blob keys. 
 **options: Options for create_rpc().'"
"def show_check(request, name): 
    try: 
      check = CHECKS[name] 
   except KeyError: 
      raise Http404('No   check   matches   the   given   query.') 
   ignore = ('ignored' in request.GET) 
   url_params = {} 
   if ignore: 
      url_params['ignored'] = 'true' 
   checks = acl_checks(request.user).filter(check=name, ignore=ignore) 
   if ('language' in request.GET): 
      checks = checks.filter(language__code=request.GET['language']) 
      url_params['language'] = request.GET['language'] 
   if ('project' in request.GET): 
      return redirect_param('show_check_project', encode_optional(url_params), project=request.GET['project'], name=name) 
   checks = checks.values('project__slug').annotate(count=Count('id')) 
   return render(request, 'check.html', {'checks': checks, 'title': check.name, 'check': check, 'url_params': encode_optional(url_params)})"," 'Show a check. 
 :param request: The request to show the check. 
 :param name: The name of the check to show. 
 :return: A rendered check view.'",'Details about failing check.'
"def install(config, plugins): 
    try: 
      (installer, _) = plug_sel.choose_configurator_plugins(config, plugins, 'install') 
   except errors.PluginSelectionError as e: 
      return e.message 
   (domains, _) = _find_domains_or_certname(config, installer) 
   le_client = _init_le_client(config, authenticator=None, installer=installer) 
   assert (config.cert_path is not None) 
   le_client.deploy_certificate(domains, config.key_path, config.cert_path, config.chain_path, config.fullchain_path) 
   le_client.enhance_config(domains, config.chain_path)"," 'Install a certificate for a domain or a certname. 
 :param config: A configuration object. 
 :param plugins: A list of plugin objects. 
 :return: None.'",'Install a previously obtained cert in a server.'
"def get_base_path(base): 
    base_parsed = urlparse(base) 
   if (not base_parsed.path): 
      sub_path = u'' 
   else: 
      sub_path = base_parsed.path 
   if sub_path.endswith(u'/'): 
      return sub_path 
   else: 
      return (sub_path + u'/')"," 'Returns a path that is the base path of the given path. 
 If the given path is not a full path, then it will be appended with 
 a trailing slash. 
 :param base: The base path to get the base path of. 
 :type base: str 
 :return: The base path of the given path. 
 :rtype: str'","'Return the path of a base URL if it contains one. 
 >>> get_base_path(\'http://some.site\') == \'/\' 
 True 
 >>> get_base_path(\'http://some.site/\') == \'/\' 
 True 
 >>> get_base_path(\'http://some.site/some/sub-path\') == \'/some/sub-path/\' 
 True 
 >>> get_base_path(\'http://some.site/some/sub-path/\') == \'/some/sub-path/\' 
 True'"
"def constant_time_compare(val1, val2): 
    if (len(val1) != len(val2)): 
      return False 
   result = 0 
   if (six.PY3 and isinstance(val1, bytes) and isinstance(val2, bytes)): 
      for (x, y) in zip(val1, val2): 
         result |= (x ^ y) 
   else: 
      for (x, y) in zip(val1, val2): 
         result |= (ord(x) ^ ord(y)) 
   return (result == 0)"," 'Return True if the two values are equal, False otherwise.'","'Returns True if the two strings are equal, False otherwise. 
 The time taken is independent of the number of characters that match.'"
"def location_to_string(locationID): 
    loc = ['{}-'.format((locationID >> 24))] 
   while (locationID & 15728640): 
      if (len(loc) > 1): 
         loc.append('.') 
      loc.append('{}'.format(((locationID >> 20) & 15))) 
      locationID <<= 4 
   return ''.join(loc)"," 'Convert a location ID to a string. 
 :param locationID: A location ID, in the form of a 32-bit integer. 
 :return: A string representation of the location ID. 
 :rtype: str'",'helper to calculate port and bus number from locationID'
"@login_check 
 def components_delete_layout(request): 
    try: 
      id = int(request.GET['id']) 
   except: 
      id = 0 
   if (not id): 
      return HttpResponseRedirect('/wc/components/show_layout') 
   ServerLayout.objects.filter(pk=id).delete() 
   return HttpResponseRedirect('/wc/components/show_layout')", 'Delete a layout component.',''
"def _sphinx_version(): 
    (major, minor, micro, level, serial) = sys.version_info 
   release = ('%s%s' % (major, minor)) 
   if micro: 
      release += ('%s' % (micro,)) 
   if (level == 'candidate'): 
      release += ('rc%s' % (serial,)) 
   elif (level != 'final'): 
      release += ('%s%s' % (level[0], serial)) 
   return release", 'Return the version of Sphinx used to build the documentation.','Format sys.version_info to produce the Sphinx version string used to install the chm docs'
"def get_subordinate_users(user, site): 
    from cms.utils.page_permissions import get_change_permissions_id_list 
   try: 
      user_level = get_user_permission_level(user, site) 
   except NoPermissionsException: 
      qs = get_user_model().objects.distinct().filter(((Q(is_staff=True) & Q(pageuser__created_by=user)) & Q(pagepermission__page=None))) 
      qs = qs.exclude(pk=user.pk).exclude(groups__user__pk=user.pk) 
      return qs 
   if (user_level == ROOT_USER_LEVEL): 
      return get_user_model().objects.all() 
   page_id_allow_list = get_change_permissions_id_list(user, site, check_global=False) 
   qs = get_user_model().objects.distinct().filter(((Q(is_staff=True) & (Q(pagepermission__page__id__in=page_id_allow_list) & Q(pagepermission__page__depth__gte=user_level))) | (Q(pageuser__created_by=user) & Q(pagepermission__page=None)))) 
   qs = qs.exclude(pk=user.pk).exclude(groups__user__pk=user.pk) 
   return qs"," 'Returns a list of all users who are subordinate to the given user. 
 :param user: the user to get the subordinate users for 
 :param site: the site to get the users for 
 :returns: a list of users'","'Returns users queryset, containing all subordinate users to given user 
 including users created by given user and not assigned to any page. 
 Not assigned users must be returned, because they shouldn\'t get lost, and 
 user should still have possibility to see them. 
 Only users created_by given user which are on the same, or lover level are 
 returned. 
 If user haves global permissions or is a superuser, then he can see all the 
 users. 
 This function is currently used in PagePermissionInlineAdminForm for limit 
 users in permission combobox. 
 Example: 
 A,W                    level 0 
 /                              user    B,GroupE           level 1 
 Z       /                           C,X     D,Y,W                  level 2 
 Rules: W was created by user, Z was created by user, but is not assigned 
 to any page. 
 Will return [user, C, X, D, Y, Z]. W was created by user, but is also 
 assigned to higher level.'"
"def get_numpy_status(): 
    numpy_status = {} 
   try: 
      import numpy 
      numpy_version = numpy.__version__ 
      numpy_status['up_to_date'] = (parse_version(numpy_version) >= parse_version(NUMPY_MIN_VERSION)) 
      numpy_status['version'] = numpy_version 
   except ImportError: 
      traceback.print_exc() 
      numpy_status['up_to_date'] = False 
      numpy_status['version'] = '' 
   return numpy_status", 'Returns a dictionary with status of numpy and its version.',"'Returns a dictionary containing a boolean specifying whether NumPy 
 is up-to-date, along with the version string (empty string if 
 not installed).'"
"def triangulate_point(x1, x2, P1, P2): 
    M = zeros((6, 6)) 
   M[:3, :4] = P1 
   M[3:, :4] = P2 
   M[:3, 4] = (- x1) 
   M[3:, 5] = (- x2) 
   (U, S, V) = linalg.svd(M) 
   X = V[(-1), :4] 
   return (X / X[3])"," 'Triangulate point (x1, x2) on the line segment (P1, P2). 
 Parameters 
 x1 : float 
 x2 : float 
 P1 : (x1, y1, z1) 
 P2 : (x2, y2, z2) 
 Returns 
 X : (x, y, z) 
 The point (x, y, z) on the line segment (P1, P2).'","'Point pair triangulation from 
 least squares solution.'"
"def is_installed(pkg_name): 
    manager = MANAGER 
   with settings(hide('running', 'stdout', 'stderr', 'warnings'), warn_only=True): 
      res = run(('%(manager)s   -p   %(pkg_name)s' % locals())) 
   if (not res.succeeded): 
      return False 
   if pkg_name.startswith('='): 
      pkg_name = pkg_name[1:] 
   match = re.search(('\\n\\[ebuild   +(?P<code>\\w+)   *\\]   .*%(pkg_name)s.*' % locals()), res.stdout) 
   if (match and (match.groupdict()['code'] in ('U', 'R'))): 
      return True 
   else: 
      return False"," 'Check if a package is installed 
 :param pkg_name: name of the package to check 
 :return: True if the package is installed, False otherwise'",'Check if a Portage package is installed.'
"def knownfailureif(fail_condition, msg=None): 
    if (msg is None): 
      msg = 'Test   skipped   due   to   known   failure' 
   if callable(fail_condition): 
      fail_val = (lambda : fail_condition()) 
   else: 
      fail_val = (lambda : fail_condition) 
   def knownfail_decorator(f): 
      import nose 
      def knownfailer(*args, **kwargs): 
         if fail_val(): 
            raise KnownFailureTest(msg) 
         else: 
            return f(*args, **kwargs) 
      return nose.tools.make_decorator(f)(knownfailer) 
   return knownfail_decorator"," 'Known failure decorator. 
 :param fail_condition: 
 :param msg: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :","'Make function raise KnownFailureTest exception if given condition is true. 
 If the condition is a callable, it is used at runtime to dynamically 
 make the decision. This is useful for tests that may require costly 
 imports, to delay the cost until the test suite is actually executed. 
 Parameters 
 fail_condition : bool or callable 
 Flag to determine whether to mark the decorated test as a known 
 failure (if True) or not (if False). 
 msg : str, optional 
 Message to give on raising a KnownFailureTest exception. 
 Default is None. 
 Returns 
 decorator : function 
 Decorator, which, when applied to a function, causes SkipTest 
 to be raised when `skip_condition` is True, and the function 
 to be called normally otherwise. 
 Notes 
 The decorator itself is decorated with the ``nose.tools.make_decorator`` 
 function in order to transmit function name, and various other metadata.'"
"@api_versions.wraps('2.10') 
 @utils.arg('name', metavar='<name>', help=_('Keypair   name   to   delete.')) 
 @utils.arg('--user', metavar='<user-id>', default=None, help=_('ID   of   key-pair   owner   (Admin   only).')) 
 def do_keypair_delete(cs, args): 
    cs.keypairs.delete(args.name, args.user)", 'Delete a keypair.','Delete keypair given by its name.'
"def _parse_date_rfc822(date): 
    daynames = set([u'mon', u'tue', u'wed', u'thu', u'fri', u'sat', u'sun']) 
   months = {u'jan': 1, u'feb': 2, u'mar': 3, u'apr': 4, u'may': 5, u'jun': 6, u'jul': 7, u'aug': 8, u'sep': 9, u'oct': 10, u'nov': 11, u'dec': 12} 
   parts = date.lower().split() 
   if (len(parts) < 5): 
      parts.extend((u'00:00:00', u'0000')) 
   if (parts[0][:3] in daynames): 
      parts = parts[1:] 
   if (len(parts) < 5): 
      return None 
   try: 
      day = int(parts[0]) 
   except ValueError: 
      if months.get(parts[0][:3]): 
         try: 
            day = int(parts[1]) 
         except ValueError: 
            return None 
         else: 
            parts[1] = parts[0] 
      else: 
         return None 
   month = months.get(parts[1][:3]) 
   if (not month): 
      return None 
   try: 
      year = int(parts[2]) 
   except ValueError: 
      return None 
   if (len(parts[2]) <= 2): 
      year += (1900, 2000)[(year < 90)] 
   timeparts = parts[3].split(u':') 
   timeparts = (timeparts + ([0] * (3 - len(timeparts)))) 
   try: 
      (hour, minute, second) = map(int, timeparts) 
   except ValueError: 
      return None 
   tzhour = 0 
   tzmin = 0 
   if parts[4].startswith(u'etc/'): 
      parts[4] = parts[4][4:] 
   if parts[4].startswith(u'gmt'): 
      parts[4] = (u''.join(parts[4][3:].split(u':')) or u'gmt') 
   if (parts[4] and (parts[4][0] in (u'-', u'+'))): 
      try: 
         tzhour = int(parts[4][1:3]) 
         tzmin = int(parts[4][3:]) 
      except ValueError: 
         return None 
      if parts[4].startswith(u'-'): 
         tzhour = (tzhour * (-1)) 
         tzmin = (tzmin * (-1)) 
   else: 
      tzhour = timezonenames.get(parts[4], 0) 
   try: 
      stamp = datetime.datetime(year, month, day, hour, minute, second) 
   except ValueError: 
      return None 
   delta = datetime.timedelta(0, 0, 0, 0, tzmin, tzhour) 
   try: 
      return (stamp - delta).utctimetuple() 
   except (OverflowError, ValueError): 
      return None"," 'Parses a date in RFC822 format. 
 Returns None if the date cannot be parsed. 
 RFC822 format: 
 ""Mon, 28 Jul 2008 10:15:30 +0000"" 
 RFC822 format: 
 ""Sun, 27 Jul 2008 10:15:30 -0700"" 
 RFC822 format: 
 ""Sun, 27 Jul 2008 10:15:30 GMT"" 
 RFC822 format: 
 ""Sun, 27 Jul 2008 10:15:30 -0700 GMT"" 
 RFC822 format: 
 ""Sun, 27 Jul 2008 10:15:30 GMT -0700"" 
 RFC822 format: 
 ""Sun, 27 Jul 2008 10:15:30 GMT -","'Parse RFC 822 dates and times 
 http://tools.ietf.org/html/rfc822#section-5 
 There are some formatting differences that are accounted for: 
 1. Years may be two or four digits. 
 2. The month and day can be swapped. 
 3. Additional timezone names are supported. 
 4. A default time and timezone are assumed if only a date is present.'"
"def Zero(dtype=None): 
    return Constant(0.0, dtype=dtype)", 'Returns the zero constant.',"'Returns initializer that initializes array with the all-zero array. 
 Args: 
 dtype: Data type specifier. 
 Returns: 
 numpy.ndarray or cupy.ndarray: An initialized array.'"
"def griddata(points, values, xi, method='linear', fill_value=np.nan, rescale=False): 
    points = _ndim_coords_from_arrays(points) 
   if (points.ndim < 2): 
      ndim = points.ndim 
   else: 
      ndim = points.shape[(-1)] 
   if ((ndim == 1) and (method in ('nearest', 'linear', 'cubic'))): 
      from .interpolate import interp1d 
      points = points.ravel() 
      if isinstance(xi, tuple): 
         if (len(xi) != 1): 
            raise ValueError('invalid   number   of   dimensions   in   xi') 
         (xi,) = xi 
      idx = np.argsort(points) 
      points = points[idx] 
      values = values[idx] 
      if (method == 'nearest'): 
         fill_value = 'extrapolate' 
      ip = interp1d(points, values, kind=method, axis=0, bounds_error=False, fill_value=fill_value) 
      return ip(xi) 
   elif (method == 'nearest'): 
      ip = NearestNDInterpolator(points, values, rescale=rescale) 
      return ip(xi) 
   elif (method == 'linear'): 
      ip = LinearNDInterpolator(points, values, fill_value=fill_value, rescale=rescale) 
      return ip(xi) 
   elif ((method == 'cubic') and (ndim == 2)): 
      ip = CloughTocher2DInterpolator(points, values, fill_value=fill_value, rescale=rescale) 
      return ip(xi) 
   else: 
      raise ValueError(('Unknown   interpolation   method   %r   for   %d   dimensional   data' % (method, ndim)))"," 'Interpolate a 2D or 3D array of points using linear, nearest-neighbor, 
 cubic, or Clough-Tocher interpolation. 
 Parameters 
 points : array 
 The points to interpolate. 
 values : array 
 The values at the interpolated points. 
 xi : array 
 The x-coordinates of the points. 
 method : string, optional 
 The interpolation method. 
 If ``'nearest'``, the nearest-neighbor method is used. 
 If ``'linear'``, linear interpolation is used. 
 If ``'cubic'``, cubic interpolation is used. 
 If ``'clough_tocher'``, Clough-Tocher interpolation is used. 
 fill_value : scalar, optional 
 The value to use when the interpolation fails. 
 rescale : boolean, optional 
 Whether to rescale the interpolation. 
 Returns 
 The interpolated values. 
 Notes 
 If the number of dimensions of ``points`` is less than the number of 
 dimensions of ``xi``, then the interpolation will be done in the 
 last","'Interpolate unstructured D-dimensional data. 
 Parameters 
 points : ndarray of floats, shape (n, D) 
 Data point coordinates. Can either be an array of 
 shape (n, D), or a tuple of `ndim` arrays. 
 values : ndarray of float or complex, shape (n,) 
 Data values. 
 xi : ndarray of float, shape (M, D) 
 Points at which to interpolate data. 
 method : {\'linear\', \'nearest\', \'cubic\'}, optional 
 Method of interpolation. One of 
 ``nearest`` 
 return the value at the data point closest to 
 the point of interpolation.  See `NearestNDInterpolator` for 
 more details. 
 ``linear`` 
 tesselate the input point set to n-dimensional 
 simplices, and interpolate linearly on each simplex.  See 
 `LinearNDInterpolator` for more details. 
 ``cubic`` (1-D) 
 return the value determined from a cubic 
 spline. 
 ``cubic`` (2-D) 
 return the value determined from a 
 piecewise cubic, continuously differentiable (C1), and 
 approximately curvature-minimizing polynomial surface. See 
 `CloughTocher2DInterpolator` for more details. 
 fill_value : float, optional 
 Value used to fill in for requested points outside of the 
 convex hull of the input points.  If not provided, then the 
 default is ``nan``. This option has no effect for the 
 \'nearest\' method. 
 rescale : bool, optional 
 Rescale points to unit cube before performing interpolation. 
 This is useful if some of the input dimensions have 
 incommensurable units and differ by many orders of magnitude. 
 .. versionadded:: 0.14.0 
 Notes 
 .. versionadded:: 0.9 
 Examples 
 Suppose we want to interpolate the 2-D function 
 >>> def func(x, y): 
 ...     return x*(1-x)*np.cos(4*np.pi*x) * np.sin(4*np.pi*y**2)**2 
 on a grid in [0, 1]x[0, 1] 
 >>> grid_x, grid_y = np.mgrid[0:1:100j, 0:1:200j] 
 but we only know its values at 1000 data points: 
 >>> points = np.random.rand(1000, 2) 
 >>> values = func(points[:,0], points[:,1]) 
 This can be done with `griddata` -- below we try out all of the 
 interpolation methods: 
 >>> from scipy.interpolate import griddata 
 >>> grid_z0 = griddata(points, values, (grid_x, grid_y), method=\'nearest\') 
 >>> grid_z1 = griddata(points, values, (grid_x, grid_y), method=\'linear\') 
 >>> grid_z2 = griddata(points, values, (grid_x, grid_y), method=\'cubic\') 
 One can see that the exact result is reproduced by all of the 
 methods to some degree, but for this smooth function the piecewise 
 cubic interpolant gives the best results: 
 >>> import matplotlib.pyplot as plt 
 >>> plt.subplot(221) 
 >>> plt.imshow(func(grid_x, grid_y).T, extent=(0,1,0,1), origin=\'lower\') 
 >>> plt.plot(points[:,0], points[:,1], \'k.\', ms=1) 
 >>> plt.title(\'Original\') 
 >>> plt.subplot(222) 
 >>> plt.imshow(grid_z0.T, extent=(0,1,0,1), origin=\'lower\') 
 >>> plt.title(\'Nearest\') 
 >>> plt.subplot(223) 
 >>> plt.imshow(grid_z1.T, extent=(0,1,0,1), origin=\'lower\') 
 >>> plt.title(\'Linear\') 
 >>> plt.subplot(224) 
 >>> plt.imshow(grid_z2.T, extent=(0,1,0,1), origin=\'lower\') 
 >>> plt.title(\'Cubic\') 
 >>> plt.gcf().set_size_inches(6, 6) 
 >>> plt.show()'"
"def match(string, trie): 
    longest = None 
   for i in range(len(string)): 
      substr = string[:(i + 1)] 
      if (not trie.has_prefix(substr)): 
         break 
      if (substr in trie): 
         longest = substr 
   return longest", 'Returns the longest substring of `string` that is also in `trie`.',"'match(string, trie) -> longest key or None 
 Find the longest key in the trie that matches the beginning of the 
 string.'"
"def _removeIfPresent(filename): 
    try: 
      os.unlink(filename) 
   except OSError as why: 
      if (why.errno == ENOENT): 
         return 0 
      else: 
         raise 
   else: 
      return 1", 'Remove the file if it exists.',"'Attempt to remove a file, returning whether the file existed at 
 the time of the call. 
 str -> bool'"
"def coerce_kw_type(kw, key, type_, flexi_bool=True): 
    if ((key in kw) and (type(kw[key]) is not type_) and (kw[key] is not None)): 
      if ((type_ is bool) and flexi_bool): 
         kw[key] = asbool(kw[key]) 
      else: 
         kw[key] = type_(kw[key])"," 'Coerce kwargs to a given type. 
 :param kw: 
 :param key: 
 :param type_: 
 :param flexi_bool: 
 :return: 
 :rtype: 
 :raises: 
 :param type_: 
 :param flexi_bool: 
 :param kw: 
 :param key: 
 :param kwargs: 
 :param type_: 
 :param flexi_bool: 
 :return: 
 :rtype: 
 :raises: 
 :param type_: 
 :param flexi_bool: 
 :param kw: 
 :param key: 
 :param kwargs: 
 :param type_: 
 :param flexi_bool: 
 :return: 
 :rtype: 
 :raises: 
 :param type_: 
 :param flexi_bool: 
 :param kw: 
 :param key: 
 :param kwargs: 
 :param type_: 
 :param flexi_bool: 
 :return: 
 :rtype: 
","'If \'key\' is present in dict \'kw\', coerce its value to type \'type\_\' if 
 necessary.  If \'flexi_bool\' is True, the string \'0\' is considered false 
 when coercing to boolean.'"
"def test_validate_estimator_default(): 
    smt = SMOTETomek(random_state=RND_SEED) 
   (X_resampled, y_resampled) = smt.fit_sample(X, Y) 
   X_gt = np.array([[0.20622591, 0.0582794], [0.68481731, 0.51935141], [1.34192108, (-0.13367336)], [0.62366841, (-0.21312976)], [1.61091956, (-0.40283504)], [(-0.37162401), (-2.19400981)], [0.74680821, 1.63827342], [0.61472253, (-0.82309052)], [0.19893132, (-0.47761769)], [0.97407872, 0.44454207], [1.40301027, (-0.83648734)], [(-1.20515198), (-1.02689695)], [(-0.23374509), 0.18370049], [(-0.32635887), (-0.29299653)], [(-0.00288378), 0.84259929], [1.79580611, (-0.02219234)], [0.38307743, (-0.05670439)], [0.93976473, (-0.06570176)], [0.70319159, (-0.02571668)], [0.75052536, (-0.19246517)]]) 
   y_gt = np.array([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0]) 
   assert_allclose(X_resampled, X_gt, rtol=R_TOL) 
   assert_array_equal(y_resampled, y_gt)", 'Test that the default estimator works correctly.','Test right processing while passing no object as initialization'
"def split_at_whitespace(string): 
    return re.split(__WHITESPACE_SPLIT, string)"," 'Split string at whitespace characters. 
 This is a Python 2-specific function that does not exist in Python 3. 
 :param string: String to split. 
 :return: List of strings.'","'Like string.split(), but keeps empty words as empty words.'"
"def test_batch_normalized_mlp_mean_only_propagated_at_alloc(): 
    mlp = BatchNormalizedMLP([Tanh(), Tanh()], [5, 7, 9], mean_only=True) 
   assert mlp.mean_only 
   assert (not any((act.children[0].mean_only for act in mlp.activations))) 
   mlp.allocate() 
   assert all((act.children[0].mean_only for act in mlp.activations))", 'Test batch normalized MLP mean_only propagated at allocate','Test that setting mean_only on a BatchNormalizedMLP works.'
"def wrap_aws_conn(raw_conn): 
    def retry_if(ex): 
      'Retry   if   we   get   a   server   error   indicating   throttling.   Also\n                        handle   spurious   505s   that   are   thought   to   be   part   of   a   load\n                        balancer   issue   inside   AWS.' 
      return ((isinstance(ex, boto.exception.BotoServerError) and (('Throttling' in ex.body) or ('RequestExpired' in ex.body) or (ex.status == 505))) or (isinstance(ex, socket.error) and (ex.args in ((104, 'Connection   reset   by   peer'), (110, 'Connection   timed   out'))))) 
   return RetryWrapper(raw_conn, retry_if=retry_if, backoff=_EMR_BACKOFF, multiplier=_EMR_BACKOFF_MULTIPLIER, max_tries=_EMR_MAX_TRIES)"," 'Wrap a raw boto connection with a RetryWrapper that will retry if 
 it receives a 505 or 503 from the server. 
 :param raw_conn: raw boto connection 
 :type raw_conn: boto.connection.Connection 
 :return: wrapped boto connection'","'Wrap a given boto Connection object so that it can retry when 
 throttled.'"
"def test_conversion_qtable_table(): 
    qt = QTable(MIXIN_COLS) 
   names = qt.colnames 
   for name in names: 
      qt[name].info.description = name 
   t = Table(qt) 
   for name in names: 
      assert (t[name].info.description == name) 
      if (name == 'quantity'): 
         assert np.all((t['quantity'] == qt['quantity'].value)) 
         assert np.all((t['quantity'].unit is qt['quantity'].unit)) 
         assert isinstance(t['quantity'], t.ColumnClass) 
      else: 
         assert_table_name_col_equal(t, name, qt[name]) 
   qt2 = QTable(qt) 
   for name in names: 
      assert (qt2[name].info.description == name) 
      assert_table_name_col_equal(qt2, name, qt[name])", 'Test that a QTable can be converted to a Table','Test that a table round trips from QTable => Table => QTable'
"@frame_transform_graph.transform(coord.StaticMatrixTransform, coord.Galactic, Sagittarius) 
 def galactic_to_sgr(): 
    return SGR_MATRIX"," 'Convert a Galactic frame transform to a Sagittarius frame transform. 
 Parameters 
 - transform : StaticMatrixTransform 
 The Galactic transform. 
 Returns 
 - StaticMatrixTransform 
 The Sagittarius transform.'","'Compute the transformation matrix from Galactic spherical to 
 heliocentric Sgr coordinates.'"
"def _StructPackDecoder(wire_type, format): 
    value_size = struct.calcsize(format) 
   local_unpack = struct.unpack 
   def InnerDecode(buffer, pos): 
      new_pos = (pos + value_size) 
      result = local_unpack(format, buffer[pos:new_pos])[0] 
      return (result, new_pos) 
   return _SimpleDecoder(wire_type, InnerDecode)"," 'Returns a decoder that unpacks the specified format from a buffer. 
 :param wire_type: The wire type to use. 
 :param format: The format to use. 
 :return: A decoder that can unpack the format from a buffer. 
 :rtype: _SimpleDecoder'","'Return a constructor for a decoder for a fixed-width field. 
 Args: 
 wire_type:  The field\'s wire type. 
 format:  The format string to pass to struct.unpack().'"
"def dt_row_cnt(reporter, check=(), quiet=True, utObj=None): 
    config = current.test_config 
   browser = config.browser 
   elem = browser.find_element_by_id('datatable_info') 
   details = elem.text 
   if (not quiet): 
      reporter(details) 
   words = details.split() 
   start = int(words[1]) 
   end = int(words[3]) 
   length = int(words[5]) 
   filtered = None 
   if (len(words) > 10): 
      filtered = int(words[9]) 
   if (check != ()): 
      if (len(check) == 3): 
         expected = ('Showing   %d   to   %d   of   %d   entries' % check) 
         actual = ('Showing   %d   to   %d   of   %d   entries' % (start, end, length)) 
         msg = (""Expected   result   of   '%s'   doesn't   equal   '%s'"" % (expected, actual)) 
         if (utObj != None): 
            utObj.assertEqual(((start, end, length) == check), msg) 
         else: 
            assert ((start, end, length) == check), msg 
      elif (len(check) == 4): 
         expected = ('Showing   %d   to   %d   of   %d   entries   (filtered   from   %d   total   entries)' % check) 
         if filtered: 
            actual = ('Showing   %d   to   %d   of   %d   entries   (filtered   from   %d   total   entries)' % (start, end, length, filtered)) 
         else: 
            actual = ('Showing   %d   to   %d   of   %d   entries' % (start, end, length)) 
         msg = (""Expected   result   of   '%s'   doesn't   equal   '%s'"" % (expected, actual)) 
         if (utObj != None): 
            utObj.assertEqual(((start, end, length) == check), msg) 
         else: 
            assert ((start, end, length, filtered) == check), msg 
   if (len(words) > 10): 
      return (start, end, length, filtered) 
   else: 
      return (start, end, length)"," 'Returns the number of rows currently shown in the datatable 
 :param reporter: function to call with the details 
 :param check: (optional) expected number of rows to be shown 
 :param quiet: (optional) don\'t print the details to the screen'",'return the rows that are being displayed and the total rows in the dataTable'
"def pagerank(matrix, d_factor=0.85): 
    size = len(matrix) 
   epsilon = 0.0001 
   matrix = matrix.copy() 
   for i in xrange(0, size): 
      col_sum = matrix[:, i].sum() 
      if col_sum: 
         matrix[:, i] /= col_sum 
   e = (((1.0 - d_factor) / size) * numpy.ones((size, size))) 
   matrix = ((d_factor * matrix) + e) 
   result = (numpy.ones(size) / size) 
   prev = (numpy.ones(size) / size) 
   iteration = 0 
   while True: 
      result = numpy.dot(matrix, result) 
      result /= result.sum() 
      diff = numpy.abs((result - prev)).sum() 
      print ('Iteration   %d,   change   %f' % (iteration, diff)) 
      if (diff < epsilon): 
         break 
      prev = result 
      iteration += 1 
   return result"," 'Returns the PageRank of a matrix. 
 Parameters 
 matrix : array_like 
 The input matrix. 
 d_factor : float, optional 
 The d_factor for the matrix. 
 Returns 
 result : ndarray 
 The PageRank of the matrix. 
 Examples 
 >>> from sympy.matrices.dense import Matrix 
 >>> from sympy.matrices.dense import pagerank 
 >>> M = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 >>> pagerank(M, d_factor=0.85) 
 array([ 0.16666666,  0.55555556,  0.27777778])'","'Calculate the pagerank vector of a given adjacency matrix (using 
 the power method). 
 :param matrix: an adjacency matrix 
 :param d_factor: the damping factor'"
"def login(studentid, studentpwd, url, session, proxy): 
    if (not proxy): 
      pre_login = session.get(url, allow_redirects=False, timeout=5) 
   else: 
      pre_login = session.get(url, allow_redirects=False, timeout=5, proxies=app.config['SCHOOL_LAN_PROXIES']) 
   pre_login.raise_for_status() 
   pre_login_soup = BeautifulSoup(pre_login.text, 'html.parser', parse_only=SoupStrainer('input')) 
   login_view_state = pre_login_soup.find(attrs={'name': '__VIEWSTATE'})['value'] 
   payload = {'__VIEWSTATE': login_view_state, 'TextBox1': studentid, 'TextBox2': studentpwd, 'RadioButtonList1': u'\u5b66\u751f', 'Button1': u'   \u767b   \u5f55   '} 
   if (not proxy): 
      res = session.post(url, data=payload, allow_redirects=False, timeout=5) 
   else: 
      res = session.post(url, data=payload, allow_redirects=False, timeout=5, proxies=app.config['SCHOOL_LAN_PROXIES']) 
   return res", 'login to the site','ç»å½è·å cookie'
"def main(global_config, **settings): 
    config = Configurator(root_factory=root_factory, settings=settings) 
   config.include('pyramid_chameleon') 
   config.add_static_view('static', 'static', cache_max_age=3600) 
   config.scan() 
   return config.make_wsgi_app()", 'Create a Pyramid application','This function returns a Pyramid WSGI application.'
"def isPointInsideLoop(loop, point): 
    return ((getNumberOfIntersectionsToLeft(loop, point) % 2) == 1)"," 'Return True if point is inside the loop. 
 :param loop: A loop 
 :param point: A point 
 :return: True if point is inside the loop, False otherwise'",'Determine if a point is inside another loop.'
"@frappe.whitelist() 
 def enroll_student(source_name): 
    student = get_mapped_doc(u'Student   Applicant', source_name, {u'Student   Applicant': {u'doctype': u'Student', u'field_map': {u'name': u'student_applicant'}}}, ignore_permissions=True) 
   student.save() 
   program_enrollment = frappe.new_doc(u'Program   Enrollment') 
   program_enrollment.student = student.name 
   program_enrollment.student_name = student.title 
   program_enrollment.program = frappe.db.get_value(u'Student   Applicant', source_name, u'program') 
   return program_enrollment"," 'Enroll a student into a program. 
 :param source_name: name of the student applicant'","'Creates a Student Record and returns a Program Enrollment. 
 :param source_name: Student Applicant.'"
"def fill_gaps(*args, **kwargs): 
    generator = fill_gaps_generator(*args, **kwargs) 
   return list(generator)"," 'Returns a generator that fills in the gaps in the given list. 
 :param args: The list to fill in the gaps of. 
 :param kwargs: Additional arguments to pass to fill_gaps_generator. 
 :return: A generator that returns the filled list.'",'Listify the generator returned by fill_gaps_generator for `memoize`.'
"@verbose 
 def tweets_by_user_demo(user='NLTK_org', count=200): 
    oauth = credsfromfile() 
   client = Query(**oauth) 
   client.register(TweetWriter()) 
   client.user_tweets(user, count)"," 'Get the top 200 tweets from a user. 
 :param user: the twitter username 
 :param count: the number of tweets to retrieve 
 :type user: str 
 :type count: int'",'Use the REST API to search for past tweets by a given user.'
"def grad_clip(x, lower_bound, upper_bound): 
    return GradClip(lower_bound, upper_bound)(x)"," 'Clip the gradient of a tensor. 
 Parameters 
 x : tensor 
 The gradient of the tensor. 
 lower_bound : scalar 
 The lower bound of the gradient. 
 upper_bound : scalar 
 The upper bound of the gradient. 
 Returns 
 out : tensor 
 The clipped gradient. 
 Notes 
 This function is equivalent to the following: 
 .. code-block:: python 
 $out = x - (x * (upper_bound - lower_bound)) / (upper_bound - lower_bound)$ 
 Examples 
 >>> x = Tensor(np.array([[1, 2, 3], [4, 5, 6]])) 
 >>> grad_clip(x, 0, 10) 
 Tensor([-2, -1])'","'This op do a view in the forward, but clip the gradient. 
 This is an elemwise operation. 
 :param x: the variable we want its gradient inputs clipped 
 :param lower_bound: The lower bound of the gradient value 
 :param upper_bound: The upper bound of the gradient value. 
 :examples: 
 x = theano.tensor.scalar() 
 z = theano.tensor.grad(grad_clip(x, -1, 1)**2, x) 
 z2 = theano.tensor.grad(x**2, x) 
 f = theano.function([x], outputs = [z, z2]) 
 print(f(2.0))  # output (1.0, 4.0) 
 :note: We register an opt in tensor/opt.py that remove the GradClip. 
 So it have 0 cost in the forward and only do work in the grad.'"
"def merge(file, names, config, coord): 
    inputs = get_tiles(names, config, coord) 
   output = {'type': 'Topology', 'transform': inputs[0]['transform'], 'objects': dict(), 'arcs': list()} 
   for (name, input) in zip(names, inputs): 
      for (index, object) in enumerate(input['objects'].values()): 
         if (len(input['objects']) > 1): 
            output['objects'][('%(name)s-%(index)d' % locals())] = object 
         else: 
            output['objects'][name] = object 
         for geometry in object['geometries']: 
            update_arc_indexes(geometry, output['arcs'], input['arcs']) 
   file.write(json.dumps(output, separators=(',', ':')).encode('utf8'))", 'Merge the tiles into a single Topology file.',"'Retrieve a list of TopoJSON tile responses and merge them into one. 
 get_tiles() retrieves data and performs basic integrity checks.'"
"def script(vm_): 
    return salt.utils.cloud.os_script(config.get_cloud_config_value('script', vm_, __opts__), vm_, __opts__, salt.utils.cloud.salt_config_to_yaml(salt.utils.cloud.minion_config(__opts__, vm_)))", 'Runs a script on a VM.','Return the script deployment object'
"def _do_surface_dots_subset(intrad, rsurf, rmags, rref, refl, lsurf, rlens, this_nn, cosmags, ws, volume, lut, n_fact, ch_type, idx): 
    products = _fast_sphere_dot_r0(intrad, rsurf, rmags, lsurf, rlens, this_nn, cosmags, None, ws, volume, lut, n_fact, ch_type).T 
   if (rref is not None): 
      raise NotImplementedError 
   return products"," 'Compute the dot products of the intrinsic and observed 
 surface brightnesses with the surface brightness of the 
 reference object. 
 Parameters 
 intrad : ndarray 
 The intrinsic surface brightness of the reference object. 
 rsurf : ndarray 
 The observed surface brightness of the reference object. 
 rmags : ndarray 
 The reference object magnitudes. 
 rref : ndarray 
 The reference object redshifts. 
 refl : ndarray 
 The reference object redshifts. 
 lsurf : ndarray 
 The observed surface brightness of the source object. 
 rlens : ndarray 
 The redshift of the source object. 
 this_nn : ndarray 
 The angular separation of the source and reference object. 
 cosmags : ndarray 
 The cosmological distance of the source object. 
 ws : ndarray 
 The weighting function. 
 volume : ndarray 
 The volume of the survey. 
 lut : ndarray 
 The luminosity function. 
 n_fact :","'Helper for parallelization. 
 Parameters 
 refl : array | None 
 If ch_type is \'eeg\', the magnitude of position vector of the 
 virtual reference (never used). 
 lsurf : array 
 Magnitude of position vector of the surface points. 
 rlens : list of arrays of length n_coils 
 Magnitude of position vector. 
 this_nn : array, shape (n_vertices, 3) 
 Surface normals. 
 cosmags : list of array. 
 Direction of the integration points in the coils. 
 ws : list of array 
 Integration weights of the coils. 
 volume : bool 
 If True, compute volume integral. 
 lut : callable 
 Look-up table for evaluating Legendre polynomials. 
 n_fact : array 
 Coefficients in the integration sum. 
 ch_type : str 
 \'meg\' or \'eeg\' 
 idx : array, shape (n_coils x 1) 
 Index of coil. 
 Returns 
 products : array, shape (n_coils, n_coils) 
 The integration products.'"
"def gf_pow_mod(f, n, g, p, K): 
    if (not n): 
      return [K.one] 
   elif (n == 1): 
      return gf_rem(f, g, p, K) 
   elif (n == 2): 
      return gf_rem(gf_sqr(f, p, K), g, p, K) 
   h = [K.one] 
   while True: 
      if (n & 1): 
         h = gf_mul(h, f, p, K) 
         h = gf_rem(h, g, p, K) 
         n -= 1 
      n >>= 1 
      if (not n): 
         break 
      f = gf_sqr(f, p, K) 
      f = gf_rem(f, g, p, K) 
   return h"," 'Return the result of ``f**n`` modulo ``g**p`` in ``K[X]``. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import gf_pow_mod 
 >>> gf_pow_mod([1, 2, 3], 2, [1, 2, 3], 3, ZZ) 
 [1, 2, 3]'","'Compute ``f**n`` in ``GF(p)[x]/(g)`` using repeated squaring. 
 Given polynomials ``f`` and ``g`` in ``GF(p)[x]`` and a non-negative 
 integer ``n``, efficiently computes ``f**n (mod g)`` i.e. the remainder 
 of ``f**n`` from division by ``g``, using the repeated squaring algorithm. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.galoistools import gf_pow_mod 
 >>> gf_pow_mod(ZZ.map([3, 2, 4]), 3, ZZ.map([1, 1]), 5, ZZ) 
 References 
 1. [Gathen99]_'"
"def computeOverlap(x, y): 
    return (x & y).sum()"," 'Computes the intersection of two sets of bits. 
 Parameters 
 x : set of bits 
 y : set of bits 
 Returns 
 The intersection of the two sets of bits.'","'Given two binary arrays, compute their overlap. The overlap is the number 
 of bits where x[i] and y[i] are both 1'"
"def list_job(jid, ext_source=None, display_progress=False): 
    ret = {'jid': jid} 
   mminion = salt.minion.MasterMinion(__opts__) 
   returner = _get_returner((__opts__['ext_job_cache'], ext_source, __opts__['master_job_cache'])) 
   if display_progress: 
      __jid_event__.fire_event({'message': 'Querying   returner:   {0}'.format(returner)}, 'progress') 
   job = mminion.returners['{0}.get_load'.format(returner)](jid) 
   ret.update(_format_jid_instance(jid, job)) 
   ret['Result'] = mminion.returners['{0}.get_jid'.format(returner)](jid) 
   fstr = '{0}.get_endtime'.format(__opts__['master_job_cache']) 
   if (__opts__.get('job_cache_store_endtime') and (fstr in mminion.returners)): 
      endtime = mminion.returners[fstr](jid) 
      if endtime: 
         ret['EndTime'] = endtime 
   return ret", 'Returns a dictionary of the job\'s state and result.',"'List a specific job given by its jid 
 ext_source 
 If provided, specifies which external job cache to use. 
 display_progress : False 
 If ``True``, fire progress events. 
 .. versionadded:: 2015.8.8 
 CLI Example: 
 .. code-block:: bash 
 salt-run jobs.list_job 20130916125524463507 
 salt-run jobs.list_job 20130916125524463507 --out=pprint'"
"def DEFINE_choice(name, default, choices, help): 
    CONFIG.AddOption(type_info.Choice(name=name, default=default, choices=choices, description=help))"," 'Registers a choice option with the given name, default, and choices. 
 The help string is used to describe the option.'",'A helper for defining choice string options.'
"def _fulfills_version_spec(versions, oper, desired_version, ignore_epoch=False): 
    cmp_func = __salt__.get('pkg.version_cmp') 
   if salt.utils.is_freebsd(): 
      if (isinstance(versions, dict) and ('version' in versions)): 
         versions = versions['version'] 
   for ver in versions: 
      if salt.utils.compare_versions(ver1=ver, oper=oper, ver2=desired_version, cmp_func=cmp_func, ignore_epoch=ignore_epoch): 
         return True 
   return False"," 'Checks if the versions fulfill the version spec. 
 This function is used by the ``pkg`` module to check if the versions 
 fulfill the version spec. 
 :param versions: list of versions 
 :param oper: operator 
 :param desired_version: desired version 
 :param ignore_epoch: if True, ignore the epoch in the version spec 
 :return: True if the versions fulfill the version spec, False otherwise'","'Returns True if any of the installed versions match the specified version, 
 otherwise returns False'"
"def list_exports(exports='/etc/exports'): 
    ret = {} 
   with salt.utils.fopen(exports, 'r') as efl: 
      for line in efl.read().splitlines(): 
         if (not line): 
            continue 
         if line.startswith('#'): 
            continue 
         comps = line.split() 
         ret[comps[0]] = [] 
         newshares = [] 
         for perm in comps[1:]: 
            if perm.startswith('/'): 
               newshares.append(perm) 
               continue 
            permcomps = perm.split('(') 
            permcomps[1] = permcomps[1].replace(')', '') 
            hosts = permcomps[0].split(',') 
            options = permcomps[1].split(',') 
            ret[comps[0]].append({'hosts': hosts, 'options': options}) 
         for share in newshares: 
            ret[share] = ret[comps[0]] 
   return ret", 'Return a dictionary of all the exports from the specified file.',"'List configured exports 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' nfs.list_exports'"
"def mkdir(path, owner=None, grant_perms=None, deny_perms=None, inheritance=True): 
    drive = os.path.splitdrive(path)[0] 
   if (not os.path.isdir(drive)): 
      raise CommandExecutionError('Drive   {0}   is   not   mapped'.format(drive)) 
   path = os.path.expanduser(path) 
   path = os.path.expandvars(path) 
   if (not os.path.isdir(path)): 
      os.mkdir(path) 
      if owner: 
         salt.utils.win_dacl.set_owner(path, owner) 
      set_perms(path, grant_perms, deny_perms, inheritance) 
   return True"," 'Create a directory. 
 :param path: The path to create 
 :param owner: The owner of the directory 
 :param grant_perms: The permissions to grant 
 :param deny_perms: The permissions to deny 
 :param inheritance: Whether inheritance should be enabled 
 :return: True on success, False on failure'","'Ensure that the directory is available and permissions are set. 
 Args: 
 path (str): The full path to the directory. 
 owner (str): The owner of the directory. If not passed, it will be the 
 account that created the directory, likely SYSTEM 
 grant_perms (dict): A dictionary containing the user/group and the basic 
 permissions to grant, ie: ``{\'user\': {\'perms\': \'basic_permission\'}}``. 
 You can also set the ``applies_to`` setting here. The default is 
 ``this_folder_subfolders_files``. Specify another ``applies_to`` setting 
 like this: 
 .. code-block:: yaml 
 {\'user\': {\'perms\': \'full_control\', \'applies_to\': \'this_folder\'}} 
 To set advanced permissions use a list for the ``perms`` parameter, ie: 
 .. code-block:: yaml 
 {\'user\': {\'perms\': [\'read_attributes\', \'read_ea\'], \'applies_to\': \'this_folder\'}} 
 deny_perms (dict): A dictionary containing the user/group and 
 permissions to deny along with the ``applies_to`` setting. Use the same 
 format used for the ``grant_perms`` parameter. Remember, deny 
 permissions supersede grant permissions. 
 inheritance (bool): If True the object will inherit permissions from the 
 parent, if False, inheritance will be disabled. Inheritance setting will 
 not apply to parent directories if they must be created 
 Returns: 
 bool: True if successful, otherwise raise an error 
 CLI Example: 
 .. code-block:: bash 
 # To grant the \'Users\' group \'read & execute\' permissions. 
 salt \'*\' file.mkdir C:\Temp\ Administrators ""{\'Users\': {\'perms\': \'read_execute\'}}"" 
 # Locally using salt call 
 salt-call file.mkdir C:\Temp\ Administrators ""{\'Users\': {\'perms\': \'read_execute\', \'applies_to\': \'this_folder_only\'}}"" 
 # Specify advanced attributes with a list 
 salt \'*\' file.mkdir C:\Temp\ Administrators ""{\'jsnuffy\': {\'perms\': [\'read_attributes\', \'read_ea\'], \'applies_to\': \'this_folder_only\'}}""'"
"@snippet 
 def client_list_subscriptions(client, to_delete): 
    def do_something_with(sub): 
      pass 
   for subscription in client.list_subscriptions(): 
      do_something_with(subscription)", 'Client-specific list of subscriptions.','List all subscriptions for a project.'
"def article(word, function=INDEFINITE): 
    return (((function == DEFINITE) and definite_article(word)) or indefinite_article(word))", 'Returns the appropriate article for the given word.','Returns the indefinite or definite article for the given word.'
"@csrf_exempt 
 def notify_url_handler(request): 
    logger1.info('>>notify   url   handler   start...') 
   if (request.method == 'POST'): 
      if notify_verify(request.POST): 
         logger1.info('pass   verification...') 
         tn = request.POST.get('out_trade_no') 
         logger1.info(('Change   the   status   of   bill   %s' % tn)) 
         bill = Bill.objects.get(pk=tn) 
         trade_status = request.POST.get('trade_status') 
         logger1.info(('the   status   of   bill   %s   changed   to   %s' % (tn, trade_status))) 
         bill.trade_status = trade_status 
         bill.save() 
         trade_no = request.POST.get('trade_no') 
         if (trade_status == 'WAIT_SELLER_SEND_GOODS'): 
            logger1.info('It   is   WAIT_SELLER_SEND_GOODS,   so   upgrade   bill') 
            upgrade_bill(bill, ((6 * 30) + 7)) 
            url = send_goods_confirm_by_platform(trade_no) 
            logger1.info(('send   goods   confirmation.   %s' % url)) 
            req = urllib.urlopen(url) 
            return HttpResponse('success') 
         else: 
            logger1.info(('##info:   Status   of   %s' % trade_status)) 
            return HttpResponse('success') 
   return HttpResponse('fail')", 'notify url handler',"'Handler for notify_url for asynchronous updating billing information. 
 Logging the information.'"
"def validate_bool_maybe_none(b): 
    if (type(b) is str): 
      b = b.lower() 
   if (b == 'none'): 
      return None 
   if (b in ('t', 'y', 'yes', 'on', 'true', '1', 1, True)): 
      return True 
   elif (b in ('f', 'n', 'no', 'off', 'false', '0', 0, False)): 
      return False 
   else: 
      raise ValueError(('Could   not   convert   ""%s""   to   boolean' % b))"," 'Validates a boolean value, returning None if the value is ""none"".'",'Convert b to a boolean or raise'
"def test_frame_init(): 
    sc = SkyCoord(RA, DEC, frame=u'icrs') 
   assert (sc.frame.name == u'icrs') 
   sc = SkyCoord(RA, DEC, frame=ICRS) 
   assert (sc.frame.name == u'icrs') 
   with catch_warnings(AstropyDeprecationWarning) as w: 
      sc = SkyCoord(RA, DEC, u'icrs') 
   assert (sc.frame.name == u'icrs') 
   assert (len(w) == 1) 
   assert (str(w[0].message) == FRAME_DEPRECATION_WARNING) 
   with catch_warnings(AstropyDeprecationWarning) as w: 
      sc = SkyCoord(RA, DEC, ICRS) 
   assert (sc.frame.name == u'icrs') 
   assert (len(w) == 1) 
   assert (str(w[0].message) == FRAME_DEPRECATION_WARNING) 
   with catch_warnings(AstropyDeprecationWarning) as w: 
      sc = SkyCoord(u'icrs', RA, DEC) 
   assert (sc.frame.name == u'icrs') 
   assert (len(w) == 1) 
   assert (str(w[0].message) == FRAME_DEPRECATION_WARNING) 
   with catch_warnings(AstropyDeprecationWarning) as w: 
      sc = SkyCoord(ICRS, RA, DEC) 
   assert (sc.frame.name == u'icrs') 
   assert (len(w) == 1) 
   assert (str(w[0].message) == FRAME_DEPRECATION_WARNING) 
   sc = SkyCoord(sc) 
   assert (sc.frame.name == u'icrs') 
   sc = SkyCoord(C_ICRS) 
   assert (sc.frame.name == u'icrs') 
   SkyCoord(C_ICRS, frame=u'icrs') 
   assert (sc.frame.name == u'icrs') 
   with pytest.raises(ValueError) as err: 
      SkyCoord(C_ICRS, frame=u'galactic') 
   assert (u'Cannot   override   frame=' in str(err))", 'Test that SkyCoord can be initialized with a frame','Different ways of providing the frame.'
"def random_bytes(n): 
    return os.urandom(n)", 'Returns n bytes of random data.','Returns n bytes of strong random data.'
"def has_player(accessing_obj, accessed_obj, *args, **kwargs): 
    return (hasattr(accessing_obj, 'has_player') and accessing_obj.has_player)"," 'Returns True if accessing_obj has a player, False otherwise.'","'Only returns true if accessing_obj has_player is true, that is, 
 this is a player-controlled object. It fails on actual players! 
 This is a useful lock for traverse-locking Exits to restrain NPC 
 mobiles from moving outside their areas.'"
"def CalculateGeneratorInputInfo(params): 
    toplevel = params['options'].toplevel_dir 
   qualified_out_dir = os.path.normpath(os.path.join(toplevel, ComputeOutputDir(params), 'gypfiles')) 
   global generator_filelist_paths 
   generator_filelist_paths = {'toplevel': toplevel, 'qualified_out_dir': qualified_out_dir}"," 'Calculate the generator input info for a given set of parameters. 
 This is used to generate the generator input info in the output directory.'",'Called by __init__ to initialize generator values based on params.'
"@testing.requires_testing_data 
 def test_add_reference(): 
    raw = read_raw_fif(fif_fname, preload=True) 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   assert_raises(ValueError, add_reference_channels, raw, raw.info['ch_names'][0]) 
   raw_ref = add_reference_channels(raw, 'Ref', copy=True) 
   assert_equal(raw_ref._data.shape[0], (raw._data.shape[0] + 1)) 
   assert_array_equal(raw._data[picks_eeg, :], raw_ref._data[picks_eeg, :]) 
   _check_channel_names(raw_ref, 'Ref') 
   orig_nchan = raw.info['nchan'] 
   raw = add_reference_channels(raw, 'Ref', copy=False) 
   assert_array_equal(raw._data, raw_ref._data) 
   assert_equal(raw.info['nchan'], (orig_nchan + 1)) 
   _check_channel_names(raw, 'Ref') 
   assert_allclose(raw.info['chs'][(-1)]['loc'][:3], raw.info['chs'][picks_eeg[0]]['loc'][3:6], 1e-06) 
   ref_idx = raw.ch_names.index('Ref') 
   (ref_data, _) = raw[ref_idx] 
   assert_array_equal(ref_data, 0) 
   raw = read_raw_fif(fif_fname).crop(0, 1).load_data() 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   del raw.info['dig'] 
   raw_ref = add_reference_channels(raw, 'Ref', copy=True) 
   assert_equal(raw_ref._data.shape[0], (raw._data.shape[0] + 1)) 
   assert_array_equal(raw._data[picks_eeg, :], raw_ref._data[picks_eeg, :]) 
   _check_channel_names(raw_ref, 'Ref') 
   orig_nchan = raw.info['nchan'] 
   raw = add_reference_channels(raw, 'Ref', copy=False) 
   assert_array_equal(raw._data, raw_ref._data) 
   assert_equal(raw.info['nchan'], (orig_nchan + 1)) 
   _check_channel_names(raw, 'Ref') 
   assert_raises(ValueError, add_reference_channels, raw, raw.info['ch_names'][0]) 
   raw_ref = add_reference_channels(raw, ['M1', 'M2'], copy=True) 
   _check_channel_names(raw_ref, ['M1', 'M2']) 
   assert_equal(raw_ref._data.shape[0], (raw._data.shape[0] + 2)) 
   assert_array_equal(raw._data[picks_eeg, :], raw_ref._data[picks_eeg, :]) 
   assert_array_equal(raw_ref._data[(-2):, :], 0) 
   raw = add_reference_channels(raw, ['M1', 'M2'], copy=False) 
   _check_channel_names(raw, ['M1', 'M2']) 
   ref_idx = raw.ch_names.index('M1') 
   ref_idy = raw.ch_names.index('M2') 
   (ref_data, _) = raw[[ref_idx, ref_idy]] 
   assert_array_equal(ref_data, 0) 
   raw = read_raw_fif(fif_fname, preload=True) 
   events = read_events(eve_fname) 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   epochs = Epochs(raw, events=events, event_id=1, tmin=(-0.2), tmax=0.5, picks=picks_eeg, preload=True) 
   assert_raises(RuntimeError, add_reference_channels, epochs, 'Ref') 
   epochs = Epochs(raw, events=events, event_id=1, tmin=(-0.2), tmax=0.5, picks=picks_eeg, preload=True, proj='delayed') 
   epochs_ref = add_reference_channels(epochs, 'Ref', copy=True) 
   assert_equal(epochs_ref._data.shape[1], (epochs._data.shape[1] + 1)) 
   _check_channel_names(epochs_ref, 'Ref') 
   ref_idx = epochs_ref.ch_names.index('Ref') 
   ref_data = epochs_ref.get_data()[:, ref_idx, :] 
   assert_array_equal(ref_data, 0) 
   picks_eeg = pick_types(epochs.info, meg=False, eeg=True) 
   assert_array_equal(epochs.get_data()[:, picks_eeg, :], epochs_ref.get_data()[:, picks_eeg, :]) 
   raw = read_raw_fif(fif_fname, preload=True) 
   events = read_events(eve_fname) 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   epochs = Epochs(raw, events=events, event_id=1, tmin=(-0.2), tmax=0.5, picks=picks_eeg, preload=True, proj='delayed') 
   with warnings.catch_warnings(record=True): 
      epochs_ref = add_reference_channels(epochs, ['M1', 'M2'], copy=True) 
   assert_equal(epochs_ref._data.shape[1], (epochs._data.shape[1] + 2)) 
   _check_channel_names(epochs_ref, ['M1', 'M2']) 
   ref_idx = epochs_ref.ch_names.index('M1') 
   ref_idy = epochs_ref.ch_names.index('M2') 
   assert_equal(epochs_ref.info['chs'][ref_idx]['ch_name'], 'M1') 
   assert_equal(epochs_ref.info['chs'][ref_idy]['ch_name'], 'M2') 
   ref_data = epochs_ref.get_data()[:, [ref_idx, ref_idy], :] 
   assert_array_equal(ref_data, 0) 
   picks_eeg = pick_types(epochs.info, meg=False, eeg=True) 
   assert_array_equal(epochs.get_data()[:, picks_eeg, :], epochs_ref.get_data()[:, picks_eeg, :]) 
   raw = read_raw_fif(fif_fname, preload=True) 
   events = read_events(eve_fname) 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   epochs = Epochs(raw, events=events, event_id=1, tmin=(-0.2), tmax=0.5, picks=picks_eeg, preload=True, proj='delayed') 
   evoked = epochs.average() 
   evoked_ref = add_reference_channels(evoked, 'Ref', copy=True) 
   assert_equal(evoked_ref.data.shape[0], (evoked.data.shape[0] + 1)) 
   _check_channel_names(evoked_ref, 'Ref') 
   ref_idx = evoked_ref.ch_names.index('Ref') 
   ref_data = evoked_ref.data[ref_idx, :] 
   assert_array_equal(ref_data, 0) 
   picks_eeg = pick_types(evoked.info, meg=False, eeg=True) 
   assert_array_equal(evoked.data[picks_eeg, :], evoked_ref.data[picks_eeg, :]) 
   raw = read_raw_fif(fif_fname, preload=True) 
   events = read_events(eve_fname) 
   picks_eeg = pick_types(raw.info, meg=False, eeg=True) 
   epochs = Epochs(raw, events=events, event_id=1, tmin=(-0.2), tmax=0.5, picks=picks_eeg, preload=True, proj='delayed') 
   evoked = epochs.average() 
   with warnings.catch_warnings(record=True): 
      evoked_ref = add_reference_channels(evoked, ['M1', 'M2'], copy=True) 
   assert_equal(evoked_ref.data.shape[0], (evoked.data.shape[0] + 2)) 
   _check_channel_names(evoked_ref, ['M1', 'M2']) 
   ref_idx = evoked_ref.ch_names.index('M1') 
   ref_idy = evoked_ref.ch_names.index('M2') 
   ref_data = evoked_ref.data[[ref_idx, ref_idy], :] 
   assert_array_equal(ref_data, 0) 
   picks_eeg = pick_types(evoked.info, meg=False, eeg=True) 
   assert_array_equal(evoked.data[picks_eeg, :], evoked_ref.data[picks_eeg, :]) 
   raw_np = read_raw_fif(fif_fname, preload=False) 
   assert_raises(RuntimeError, add_reference_channels, raw_np, ['Ref']) 
   assert_raises(ValueError, add_reference_channels, raw, 1)", 'Test add_reference_channels.','Test adding a reference.'
"def module_s1(nfm, first=False): 
    sidepath = (Conv(**conv_params(1, (nfm * 4), 1, False, False)) if first else SkipNode()) 
   mainpath = ([] if first else [BatchNorm(), Activation(Rectlin())]) 
   mainpath.append(Conv(**conv_params(1, nfm))) 
   mainpath.append(Conv(**conv_params(3, nfm))) 
   mainpath.append(Conv(**conv_params(1, (nfm * 4), relu=False, batch_norm=False))) 
   return MergeSum([sidepath, mainpath])", 'A simple module for S1.','non-strided'
"def _mocked_presets(*args, **kwargs): 
    return [MockPreset('1')]", 'Return a list of mocked presets.','Return a list of mocked presets.'
"def runSome(): 
    tests = [] 
   names = ['testParseHostname', 'testExtractMastersSingle', 'testExtractMastersMultiple'] 
   tests.extend(list(list(map(BasicTestCase, names)))) 
   suite = unittest.TestSuite(tests) 
   unittest.TextTestRunner(verbosity=2).run(suite)", 'Run the test suite.','Unittest runner'
"def pportD5(state): 
    global dataReg 
   if (state == 0): 
      dataReg = (dataReg & (~ 32)) 
   else: 
      dataReg = (dataReg | 32) 
   port.DlPortWritePortUchar(baseAddress, dataReg)"," 'Set the D5 pin of the device to the state indicated by the argument. 
 The state of the pin is returned in dataReg.'",'toggle data register D5 bit'
"def is_executable(exe): 
    return os.access(exe, os.X_OK)"," 'Return True if the given executable can be executed, False otherwise.'",'Checks a file is executable'
"def option_present(name, value, reload=False): 
    ret = {'name': 'testing   mode', 'changes': {}, 'result': True, 'comment': 'Option   already   present.'} 
   option = name 
   current_option = __salt__['csf.get_option'](option) 
   if current_option: 
      l = __salt__['csf.split_option'](current_option) 
      option_value = l[1] 
      if ('""{0}""'.format(value) == option_value): 
         return ret 
      else: 
         result = __salt__['csf.set_option'](option, value) 
         ret['comment'] = 'Option   modified.' 
         ret['changes']['Option'] = 'Changed' 
   else: 
      result = __salt__['file.append']('/etc/csf/csf.conf', args='{0}   =   ""{1}""'.format(option, value)) 
      ret['comment'] = 'Option   not   present.   Appended   to   csf.conf' 
      ret['changes']['Option'] = 'Changed.' 
   if reload: 
      if __salt__['csf.reload'](): 
         ret['comment'] += '.   Csf   reloaded.' 
      else: 
         ret['comment'] += '.   Csf   failed   to   reload.' 
         ret['result'] = False 
   return ret"," 'Check if an option is already present. 
 :param name: Name of the option 
 :param value: Value of the option 
 :param reload: Reload the config file after setting the option 
 :return: The option is present or not'","'Ensure the state of a particular option/setting in csf. 
 name 
 The option name in csf.conf 
 value 
 The value it should be set to. 
 reload 
 Boolean. If set to true, csf will be reloaded after.'"
"@validator 
 def app(environ, start_response): 
    if (environ['REQUEST_METHOD'].upper() != 'POST'): 
      data = 'Hello,   World!\n' 
   else: 
      data = environ['wsgi.input'].read() 
   status = '200   OK' 
   response_headers = [('Content-type', 'text/plain'), ('Content-Length', str(len(data))), ('X-Gunicorn-Version', __version__), ('Test', 'test   \xd1\x82\xd0\xb5\xd1\x81\xd1\x82')] 
   start_response(status, response_headers) 
   return iter([data])", 'A simple app for testing wsgi.input.','Simplest possible application object'
"def attach_total_points(queryset, as_field='total_points_attr'): 
    model = queryset.model 
   sql = 'SELECT   SUM(projects_points.value)\n                                                            FROM   userstories_rolepoints\n                                                            INNER   JOIN   userstories_userstory   ON   userstories_userstory.id   =   userstories_rolepoints.user_story_id\n                                                            INNER   JOIN   projects_points   ON   userstories_rolepoints.points_id   =   projects_points.id\n                                                            WHERE   userstories_userstory.milestone_id   =   {tbl}.id' 
   sql = sql.format(tbl=model._meta.db_table) 
   queryset = queryset.extra(select={as_field: sql}) 
   return queryset"," 'Attaches the total points of all the rolepoints in a userstory to the 
 userstory. 
 :param queryset: The queryset of userstories. 
 :type queryset: django.db.models.query.QuerySet 
 :param as_field: The name of the field to attach the total points to. 
 :type as_field: str 
 :return: The queryset with the total points attached. 
 :rtype: django.db.models.query.QuerySet'","'Attach total of point values to each object of the queryset. 
 :param queryset: A Django milestones queryset object. 
 :param as_field: Attach the points as an attribute with this name. 
 :return: Queryset object with the additional `as_field` field.'"
"def create_vdir(name, site, sourcepath, app='/'): 
    ret = {'name': name, 'changes': {}, 'comment': str(), 'result': None} 
   current_vdirs = __salt__['win_iis.list_vdirs'](site, app) 
   if (name in current_vdirs): 
      ret['comment'] = 'Virtual   directory   already   present:   {0}'.format(name) 
      ret['result'] = True 
   elif __opts__['test']: 
      ret['comment'] = 'Virtual   directory   will   be   created:   {0}'.format(name) 
      ret['changes'] = {'old': None, 'new': name} 
   else: 
      ret['comment'] = 'Created   virtual   directory:   {0}'.format(name) 
      ret['changes'] = {'old': None, 'new': name} 
      ret['result'] = __salt__['win_iis.create_vdir'](name, site, sourcepath, app) 
   return ret"," 'Create a virtual directory. 
 :param name: Name of the virtual directory to be created. 
 :param site: The site to create the virtual directory in. 
 :param sourcepath: The path to the source of the virtual directory. 
 :param app: The application to create the virtual directory in. 
 :returns: A dictionary containing the result of the operation, 
 and a comment on what was done.'","'Create an IIS virtual directory. 
 .. note: 
 This function only validates against the virtual directory name, and will return 
 True even if the virtual directory already exists with a different configuration. 
 It will not modify the configuration of an existing virtual directory. 
 :param str name: The virtual directory name. 
 :param str site: The IIS site name. 
 :param str sourcepath: The physical path. 
 :param str app: The IIS application. 
 Example of usage with only the required arguments: 
 .. code-block:: yaml 
 site0-foo-vdir: 
 win_iis.create_vdir: 
 - name: foo 
 - site: site0 
 - sourcepath: C:\inetpub\vdirs\foo 
 Example of usage specifying all available arguments: 
 .. code-block:: yaml 
 site0-foo-vdir: 
 win_iis.create_vdir: 
 - name: foo 
 - site: site0 
 - sourcepath: C:\inetpub\vdirs\foo 
 - app: v1'"
"def _test_args(): 
    import pandas as pd 
   return {'start': pd.Timestamp('2004', tz='utc'), 'end': pd.Timestamp('2008', tz='utc')}", 'Test args','Extra arguments to use when zipline\'s automated tests run this example.'
"def run_tests_in_emulator(package): 
    env_vars = PACKAGE_INFO[package] 
   start_command = get_start_command(package) 
   proc_start = subprocess.Popen(start_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE) 
   try: 
      wait_ready(package, proc_start) 
      env_init_command = get_env_init_command(package) 
      proc_env = subprocess.Popen(env_init_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE) 
      env_status = proc_env.wait() 
      if (env_status != 0): 
         raise RuntimeError(env_status, proc_env.stderr.read()) 
      env_lines = proc_env.stdout.read().strip().split('\n') 
      for env_var in env_vars: 
         line_prefix = (('export   ' + env_var) + '=') 
         (value,) = [line.split(line_prefix, 1)[1] for line in env_lines if line.startswith(line_prefix)] 
         os.environ[env_var] = value 
      run_module_tests(package, ignore_requirements=True) 
   finally: 
      cleanup(proc_start.pid)"," 'Runs the tests for a package in an emulator. 
 :param package: The package to run the tests for. 
 :return: None'","'Spawn an emulator instance and run the system tests. 
 :type package: str 
 :param package: The package to run system tests against.'"
"def test_hsl_to_rgb_part_4(): 
    assert (hsl_to_rgb(0, 100, 50) == (255, 0, 0)) 
   assert (hsl_to_rgb(12, 100, 50) == (255, 51, 0)) 
   assert (hsl_to_rgb(24, 100, 50) == (255, 102, 0)) 
   assert (hsl_to_rgb(36, 100, 50) == (255, 153, 0)) 
   assert (hsl_to_rgb(48, 100, 50) == (255, 204, 0)) 
   assert (hsl_to_rgb(60, 100, 50) == (255, 255, 0)) 
   assert (hsl_to_rgb(72, 100, 50) == (204, 255, 0)) 
   assert (hsl_to_rgb(84, 100, 50) == (153, 255, 0)) 
   assert (hsl_to_rgb(96, 100, 50) == (102, 255, 0)) 
   assert (hsl_to_rgb(108, 100, 50) == (51, 255, 0)) 
   assert (hsl_to_rgb(120, 100, 50) == (0, 255, 0))", 'Test hsl to rgb conversion','Test hsl to rgb color function'
"def from_agraph(A, create_using=None): 
    if (create_using is None): 
      if A.is_directed(): 
         if A.is_strict(): 
            create_using = nx.DiGraph() 
         else: 
            create_using = nx.MultiDiGraph() 
      elif A.is_strict(): 
         create_using = nx.Graph() 
      else: 
         create_using = nx.MultiGraph() 
   N = nx.empty_graph(0, create_using) 
   N.name = '' 
   if (A.name is not None): 
      N.name = A.name 
   for n in A.nodes(): 
      str_attr = dict(((str(k), v) for (k, v) in n.attr.items())) 
      N.add_node(str(n), **str_attr) 
   for e in A.edges(): 
      (u, v) = (str(e[0]), str(e[1])) 
      attr = dict(e.attr) 
      str_attr = dict(((str(k), v) for (k, v) in attr.items())) 
      if (not N.is_multigraph()): 
         if (e.name is not None): 
            str_attr['key'] = e.name 
         N.add_edge(u, v, **str_attr) 
      else: 
         N.add_edge(u, v, key=e.name, **str_attr) 
   N.graph['graph'] = dict(A.graph_attr) 
   N.graph['node'] = dict(A.node_attr) 
   N.graph['edge'] = dict(A.edge_attr) 
   return N", 'Returns a networkx graph from an ArgoGraph.',"'Return a NetworkX Graph or DiGraph from a PyGraphviz graph. 
 Parameters 
 A : PyGraphviz AGraph 
 A graph created with PyGraphviz 
 create_using : NetworkX graph class instance 
 The output is created using the given graph class instance 
 Examples 
 >>> K5 = nx.complete_graph(5) 
 >>> A = nx.nx_agraph.to_agraph(K5) 
 >>> G = nx.nx_agraph.from_agraph(A) 
 >>> G = nx.nx_agraph.from_agraph(A) 
 Notes 
 The Graph G will have a dictionary G.graph_attr containing 
 the default graphviz attributes for graphs, nodes and edges. 
 Default node attributes will be in the dictionary G.node_attr 
 which is keyed by node. 
 Edge attributes will be returned as edge data in G.  With 
 edge_attr=False the edge data will be the Graphviz edge weight 
 attribute or the value 1 if no edge weight attribute is found.'"
"def filter_tool(context, tool): 
    return False", 'Filter out tools that are not relevant to the current context.','Test Filter Tool'
"def toggle(device, partition, flag): 
    _validate_device(device) 
   try: 
      int(partition) 
   except Exception: 
      raise CommandExecutionError('Invalid   partition   number   passed   to   partition.toggle') 
   if (flag not in set(['bios_grub', 'legacy_boot', 'boot', 'lba', 'root', 'swap', 'hidden', 'raid', 'LVM', 'PALO', 'PREP', 'DIAG'])): 
      raise CommandExecutionError('Invalid   flag   passed   to   partition.toggle') 
   cmd = 'parted   -m   -s   {0}   toggle   {1}   {2}'.format(device, partition, flag) 
   out = __salt__['cmd.run'](cmd).splitlines() 
   return out"," 'Toggle a partition flag. 
 Arguments: 
 device (str): The device to operate on. 
 partition (int): The partition number. 
 flag (str): The flag to toggle. 
 Returns: 
 out: The output of the command. 
 Example: 
 {{ salt['cmd.run']('parted -m -s /dev/sda toggle bios_grub 1') }}'","'Toggle the state of <flag> on <partition>. Valid flags are the same as 
 the set command. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' partition.toggle /dev/sda 1 boot'"
"def parse_editable(editable_req, default_vcs=None): 
    url = editable_req 
   if (os.path.isdir(url) and os.path.exists(os.path.join(url, 'setup.py'))): 
      url = path_to_url(url) 
   if url.lower().startswith('file:'): 
      return (None, url) 
   for version_control in vcs: 
      if url.lower().startswith(('%s:' % version_control)): 
         url = ('%s+%s' % (version_control, url)) 
   if ('+' not in url): 
      if default_vcs: 
         url = ((default_vcs + '+') + url) 
      else: 
         raise InstallationError(('--editable=%s   should   be   formatted   with   svn+URL,   git+URL,   hg+URL   or   bzr+URL' % editable_req)) 
   vc_type = url.split('+', 1)[0].lower() 
   if (not vcs.get_backend(vc_type)): 
      raise InstallationError(('For   --editable=%s   only   svn   (svn+URL),   Git   (git+URL),   Mercurial   (hg+URL)   and   Bazaar   (bzr+URL)   is   currently   supported' % editable_req)) 
   match = re.search('(?:#|#.*?&)egg=([^&]*)', editable_req) 
   if (((not match) or (not match.group(1))) and vcs.get_backend(vc_type)): 
      parts = [p for p in editable_req.split('#', 1)[0].split('/') if p] 
      if (parts[(-2)] in ('tags', 'branches', 'tag', 'branch')): 
         req = parts[(-3)] 
      elif (parts[(-1)] == 'trunk'): 
         req = parts[(-2)] 
      else: 
         raise InstallationError(('--editable=%s   is   not   the   right   format;   it   must   have   #egg=Package' % editable_req)) 
   else: 
      req = match.group(1) 
   match = re.search('^(.*?)(?:-dev|-\\d.*)$', req) 
   if match: 
      req = match.group(1) 
   return (req, url)"," 'Parse the --editable option. 
 :param editable_req: the editable request 
 :type editable_req: str 
 :return: the requested package and the url 
 :rtype: tuple(str, str)'","'Parses svn+http://blahblah@rev#egg=Foobar into a requirement 
 (Foobar) and a URL'"
"def processElementNodeByDerivation(derivation, elementNode): 
    if (derivation == None): 
      derivation = SolidDerivation(elementNode) 
   elementAttributesCopy = elementNode.attributes.copy() 
   for target in derivation.targets: 
      targetAttributesCopy = target.attributes.copy() 
      target.attributes = elementAttributesCopy 
      processTarget(target) 
      target.attributes = targetAttributesCopy"," 'Processes a target element node by applying the derivation to it. 
 This function is used to process the target element node when the 
 derivation is applied to the target element node. 
 :param derivation: The derivation to apply to the target element node. 
 :param elementNode: The target element node.'",'Process the xml element by derivation.'
"@contextfilter 
 def do_map(*args, **kwargs): 
    context = args[0] 
   seq = args[1] 
   if ((len(args) == 2) and ('attribute' in kwargs)): 
      attribute = kwargs.pop('attribute') 
      if kwargs: 
         raise FilterArgumentError(('Unexpected   keyword   argument   %r' % next(iter(kwargs)))) 
      func = make_attrgetter(context.environment, attribute) 
   else: 
      try: 
         name = args[2] 
         args = args[3:] 
      except LookupError: 
         raise FilterArgumentError('map   requires   a   filter   argument') 
      func = (lambda item: context.environment.call_filter(name, item, args, kwargs, context=context)) 
   if seq: 
      for item in seq: 
         (yield func(item))", 'Returns a generator that applies a filter to each item in the sequence.',"'Applies a filter on a sequence of objects or looks up an attribute. 
 This is useful when dealing with lists of objects but you are really 
 only interested in a certain value of it. 
 The basic usage is mapping on an attribute.  Imagine you have a list 
 of users but you are only interested in a list of usernames: 
 .. sourcecode:: jinja 
 Users on this page: {{ users|map(attribute=\'username\')|join(\', \') }} 
 Alternatively you can let it invoke a filter by passing the name of the 
 filter and the arguments afterwards.  A good example would be applying a 
 text conversion filter on a sequence: 
 .. sourcecode:: jinja 
 Users on this page: {{ titles|map(\'lower\')|join(\', \') }} 
 .. versionadded:: 2.7'"
"def test_parser_without_subparser_recieves_root_entry(complete_parser): 
    result = convert(complete_parser) 
   assert ('primary' in result['widgets'])", 'Test that a parser with no subparsers can be converted to a widget.','Non-subparser setups should receive a default root key called \'primary\''
"def test_table_with_no_newline(): 
    table = BytesIO() 
   with pytest.raises(ascii.InconsistentTableError): 
      ascii.read(table) 
   table = BytesIO() 
   with pytest.raises(ValueError) as err: 
      ascii.read(table, guess=False, fast_reader=False, format='basic') 
   assert ('No   header   line   found' in str(err.value)) 
   table = BytesIO() 
   with pytest.raises(ValueError) as err: 
      ascii.read(table, guess=False, fast_reader=True, format='fast_basic') 
   assert ('Inconsistent   data   column   lengths' in str(err.value)) 
   for kwargs in [dict(), dict(guess=False, fast_reader=False, format='basic'), dict(guess=False, fast_reader=True, format='fast_basic')]: 
      table = BytesIO() 
      table.write('a   b') 
      t = ascii.read(table, **kwargs) 
      assert (t.colnames == ['a', 'b']) 
      assert (len(t) == 0)"," 'Test that table_reader.read() raises an error if there is no newline in 
 the input.'","'Test that an input file which is completely empty fails in the expected way. 
 Test that an input file with one line but no newline succeeds.'"
"def gauge(): 
    def prep(r): 
      if r.interactive: 
         pass 
      elif (r.representation == 'plain'): 
         r.table.image_url.readable = False 
      return True 
   s3.prep = prep 
   def postp(r, output): 
      if r.interactive: 
         pass 
      elif (r.representation == 'plain'): 
         image_url = r.record.image_url 
         if image_url: 
            output['item'].append(IMG(_src=image_url, _width=400, _height=310)) 
      return output 
   s3.postp = postp 
   output = s3_rest_controller() 
   return output"," 'Gauge the S3 REST API. 
 :return: A dict of the output from the S3 REST API. 
 :rtype: dict'","'Flood Gauges, RESTful controller'"
"def initiate_deletion(req, location_data, id): 
    store_utils.delete_image_location_from_backend(req.context, id, location_data)", 'Initiate a deletion of an image location in the backend.',"'Deletes image data from the location of backend store. 
 :param req: The WSGI/Webob Request object 
 :param location_data: Location to the image data in a data store 
 :param id: Opaque image identifier'"
"def timedelta_to_integral_minutes(delta): 
    return (timedelta_to_integral_seconds(delta) // 60)", 'Convert a timedelta to an integral number of minutes.','Convert a pd.Timedelta to a number of minutes as an int.'
"def purge(name=None, pkgs=None, **kwargs): 
    return _uninstall(action='purge', name=name, pkgs=pkgs, **kwargs)"," 'Purge a package. 
 :param name: Name of the package to purge. 
 :param pkgs: List of packages to purge. 
 :param kwargs: Additional arguments to pass to :func:`_uninstall`.'","'.. versionchanged:: 2015.8.12,2016.3.3,2016.11.0 
 On minions running systemd>=205, `systemd-run(1)`_ is now used to 
 isolate commands which modify installed packages from the 
 ``salt-minion`` daemon\'s control group. This is done to keep systemd 
 from killing any apt-get/dpkg commands spawned by Salt when the 
 ``salt-minion`` service is restarted. (see ``KillMode`` in the 
 `systemd.kill(5)`_ manpage for more information). If desired, usage of 
 `systemd-run(1)`_ can be suppressed by setting a :mod:`config option 
 <salt.modules.config.get>` called ``systemd.scope``, with a value of 
 ``False`` (no quotes). 
 .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html 
 .. _`systemd.kill(5)`: https://www.freedesktop.org/software/systemd/man/systemd.kill.html 
 Remove packages via ``apt-get purge`` along with all configuration files. 
 name 
 The name of the package to be deleted. 
 Multiple Package Options: 
 pkgs 
 A list of packages to delete. Must be passed as a python list. The 
 ``name`` parameter will be ignored if this option is passed. 
 .. versionadded:: 0.16.0 
 Returns a dict containing the changes. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pkg.purge <package name> 
 salt \'*\' pkg.purge <package1>,<package2>,<package3> 
 salt \'*\' pkg.purge pkgs=\'[""foo"", ""bar""]\''"
"def matchOnlyAtCol(n): 
    def verifyCol(strg, locn, toks): 
      if (col(locn, strg) != n): 
         raise ParseException(strg, locn, ('matched   token   not   at   column   %d' % n)) 
   return verifyCol"," 'Verify that the token at location `locn` in `strg` is at column `n` 
 :param n: the column to check against 
 :param strg: the string to check 
 :param locn: the location in `strg` to check 
 :param toks: the tokens to check 
 :raises ParseException: if the token is not at the correct column'","'Helper method for defining parse actions that require matching at a specific 
 column in the input text.'"
"def test_odd(value): 
    return ((value % 2) == 1)", 'Return True if value is odd.','Return true if the variable is odd.'
"def show_key(kwargs=None, call=None): 
    if (call != 'function'): 
      log.error('The   list_keys   function   must   be   called   with   -f   or   --function.') 
      return False 
   if (not kwargs): 
      kwargs = {} 
   if ('keyname' not in kwargs): 
      log.error('A   keyname   is   required.') 
      return False 
   (rcode, data) = query(command='my/keys/{0}'.format(kwargs['keyname']), method='GET') 
   return {'keys': {data['name']: data['key']}}", 'Show a key\'s name and public key.','List the keys available'
"def adjacency(graph, directed=False, reversed=False, stochastic=False, heuristic=None): 
    if ((graph._adjacency is not None) and (graph._adjacency[1:] == (directed, reversed, stochastic, (heuristic and heuristic.func_code)))): 
      return graph._adjacency[0] 
   map = {} 
   for n in graph.nodes: 
      map[n.id] = {} 
   for e in graph.edges: 
      (id1, id2) = (((not reversed) and (e.node1.id, e.node2.id)) or (e.node2.id, e.node1.id)) 
      map[id1][id2] = (1.0 - (0.5 * e.weight)) 
      if heuristic: 
         map[id1][id2] += heuristic(id1, id2) 
      if (not directed): 
         map[id2][id1] = map[id1][id2] 
   if stochastic: 
      for id1 in map: 
         n = sum(map[id1].values()) 
         for id2 in map[id1]: 
            map[id1][id2] /= n 
   graph._adjacency = (map, directed, reversed, stochastic, (heuristic and heuristic.func_code)) 
   return map"," 'Returns the adjacency matrix of a graph. 
 If the graph has a pre-computed adjacency matrix, then this function 
 will return the same matrix. 
 Parameters 
 graph : NetworkX graph 
 The graph whose adjacency matrix is returned. 
 directed : boolean, optional 
 Whether the graph is directed or undirected. 
 reversed : boolean, optional 
 Whether the graph is reversed. 
 stochastic : boolean, optional 
 Whether the graph is stochastic. 
 heuristic : callable, optional 
 A function that takes two node ids and returns a number. 
 Returns 
 adjacency : dictionary 
 A dictionary mapping node ids to dictionaries mapping node ids to numbers. 
 If the graph has a pre-computed adjacency matrix, then this function 
 will return the same matrix. 
 Notes 
 This function returns the adjacency matrix of the graph, which is a 
 dictionary mapping node ids to dictionaries mapping node ids to numbers. 
 The dictionary maps from node ids to numbers in the range 
 [0, 1]. 
 The numbers in the dictionary are","'Returns a dictionary indexed by node id1\'s, 
 in which each value is a dictionary of connected node id2\'s linking to the edge weight. 
 If directed=True, edges go from id1 to id2, but not the other way. 
 If stochastic=True, all the weights for the neighbors of a given node sum to 1. 
 A heuristic function can be given that takes two node id\'s and returns 
 an additional cost for movement between the two nodes.'"
"def get_name_levels(node): 
    visitor = _NodeNameCollector() 
   ast.walk(node, visitor) 
   return visitor.names", 'Returns a list of name levels for a given node.',"'Return a list of ``(name, level)`` tuples for assigned names 
 The `level` is `None` for simple assignments and is a list of 
 numbers for tuple assignments for example in:: 
 a, (b, c) = x 
 The levels for for `a` is ``[0]``, for `b` is ``[1, 0]`` and for 
 `c` is ``[1, 1]``.'"
"def condentropy(px, py, pxpy=None, logbase=2): 
    if ((not _isproperdist(px)) or (not _isproperdist(py))): 
      raise ValueError('px   or   py   is   not   a   proper   probability   distribution') 
   if ((pxpy != None) and (not _isproperdist(pxpy))): 
      raise ValueError('pxpy   is   not   a   proper   joint   distribtion') 
   if (pxpy == None): 
      pxpy = np.outer(py, px) 
   condent = np.sum((pxpy * np.nan_to_num(np.log2((py / pxpy))))) 
   if (logbase == 2): 
      return condent 
   else: 
      return (logbasechange(2, logbase) * condent)"," 'Compute the joint entropy of the distribution given by px and py. 
 Parameters 
 px : ndarray 
 The first distribution. 
 py : ndarray 
 The second distribution. 
 pxpy : ndarray, optional 
 The joint distribution. 
 logbase : int, optional 
 The base of the logarithm. 
 Returns 
 condent : float 
 The entropy of the joint distribution. 
 Notes 
 The entropy is computed as the sum of the entropy of the two 
 distributions multiplied by the log of the ratio of the two 
 distributions. 
 Examples 
 >>> from scipy.stats import beta, binom, uniform 
 >>> px = uniform(0, 1, 100) 
 >>> py = uniform(0, 1, 100) 
 >>> pxpy = np.outer(py, px) 
 >>> condentropy(px, py) 
 0.041182665625008942'","'Return the conditional entropy of X given Y. 
 Parameters 
 px : array-like 
 py : array-like 
 pxpy : array-like, optional 
 If pxpy is None, the distributions are assumed to be independent 
 and conendtropy(px,py) = shannonentropy(px) 
 logbase : int or np.e 
 Returns 
 sum_{kj}log(q_{j}/w_{kj} 
 where q_{j} = Y[j] 
 and w_kj = X[k,j]'"
"def task(*args, **kwargs): 
    kwargs.setdefault('accept_magic_kwargs', False) 
   return app_or_default().task(*args, **kwargs)"," 'Decorator to run a task. 
 :param args: positional arguments 
 :param kwargs: keyword arguments 
 :param accept_magic_kwargs: If True, magic keyword arguments will be accepted 
 :return: The task result'","'Decorator to create a task class out of any callable. 
 **Examples** 
 .. code-block:: python 
 @task 
 def refresh_feed(url): 
 return Feed.objects.get(url=url).refresh() 
 With setting extra options and using retry. 
 .. code-block:: python 
 @task(max_retries=10) 
 def refresh_feed(url): 
 try: 
 return Feed.objects.get(url=url).refresh() 
 except socket.error, exc: 
 refresh_feed.retry(exc=exc) 
 Calling the resulting task: 
 >>> refresh_feed(""http://example.com/rss"") # Regular 
 <Feed: http://example.com/rss> 
 >>> refresh_feed.delay(""http://example.com/rss"") # Async 
 <AsyncResult: 8998d0f4-da0b-4669-ba03-d5ab5ac6ad5d>'"
"def _is_image_available(context, image): 
    if (hasattr(context, 'auth_token') and context.auth_token): 
      return True 
   def _is_image_public(image): 
      if hasattr(image, 'visibility'): 
         return (str(image.visibility).lower() == 'public') 
      else: 
         return image.is_public 
   if (context.is_admin or _is_image_public(image)): 
      return True 
   properties = image.properties 
   if (context.project_id and ('owner_id' in properties)): 
      return (str(properties['owner_id']) == str(context.project_id)) 
   if (context.project_id and ('project_id' in properties)): 
      return (str(properties['project_id']) == str(context.project_id)) 
   try: 
      user_id = properties['user_id'] 
   except KeyError: 
      return False 
   return (str(user_id) == str(context.user_id))", 'Checks if the image is available for the current context.',"'Check image availability. 
 This check is needed in case Nova and Glance are deployed 
 without authentication turned on.'"
"def publish_exploration_and_update_user_profiles(committer_id, exp_id): 
    rights_manager.publish_exploration(committer_id, exp_id) 
   contribution_time_msec = utils.get_current_time_in_millisecs() 
   contributor_ids = get_exploration_summary_by_id(exp_id).contributor_ids 
   for contributor in contributor_ids: 
      user_services.update_first_contribution_msec_if_not_set(contributor, contribution_time_msec)", 'Publishes an exploration and updates user profiles.',"'Publishes the exploration with publish_exploration() function in 
 rights_manager.py, as well as updates first_contribution_msec. 
 It is the responsibility of the caller to check that the exploration is 
 valid prior to publication.'"
"def subsample(inputs, factor, scope=None): 
    if (factor == 1): 
      return inputs 
   else: 
      return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)"," 'Subsample the inputs with factor. 
 Args: 
 inputs: A tensor. 
 factor: The factor. 
 scope: Optional scope. 
 Returns: 
 A tensor. 
 Raises: 
 TypeError: If `inputs` is not a tensor. 
 ValueError: If `factor` is not a positive number. 
 RuntimeError: If `factor` is not a positive number. 
 RuntimeError: If `factor` is not a positive number.'","'Subsamples the input along the spatial dimensions. 
 Args: 
 inputs: A `Tensor` of size [batch, height_in, width_in, channels]. 
 factor: The subsampling factor. 
 scope: Optional variable_scope. 
 Returns: 
 output: A `Tensor` of size [batch, height_out, width_out, channels] with the 
 input, either intact (if factor == 1) or subsampled (if factor > 1).'"
"@membership_required 
 def group_invite(request, slug, template_name='groups/group_invite.html'): 
    group = get_object_or_404(Group, slug=slug, is_active=True) 
   form = GroupInviteForm(initial={'group': group.pk, 'user': request.user.pk}) 
   return render(request, template_name, {'group': group, 'form': form})", 'Display a form to invite a user to a group.',"'Returns an invite form. 
 Templates: ``groups/group_invite.html`` 
 Context: 
 form 
 GroupInviteForm object'"
"def strip_files(files, argv_max=(256 * 1024)): 
    while files: 
      cmd = list(STRIPCMD) 
      pathlen = sum(((len(s) + 1) for s in cmd)) 
      while ((pathlen < argv_max) and files): 
         f = files.pop() 
         cmd.append(f) 
         pathlen += (len(f) + 1) 
      if (len(cmd) > len(STRIPCMD)): 
         all_files = cmd[len(STRIPCMD):] 
         unwritable_files = tuple(filter(None, ((None if os.access(x, os.W_OK) else (x, os.stat(x).st_mode)) for x in all_files))) 
         [os.chmod(x, (stat.S_IWRITE | old_mode)) for (x, old_mode) in unwritable_files] 
         subprocess.check_call(cmd) 
         [os.chmod(x, old_mode) for (x, old_mode) in unwritable_files]"," 'Strip the paths of files in a list to the minimum length. 
 :param files: A list of files to strip. 
 :param argv_max: The maximum number of characters allowed in the command line. 
 :return: A list of the stripped filenames.'",'Strip a list of files'
"def riemann_cyclic(t2): 
    if isinstance(t2, (TensMul, Tensor)): 
      args = [t2] 
   else: 
      args = t2.args 
   a1 = [x.split() for x in args] 
   a2 = [[riemann_cyclic_replace(tx) for tx in y] for y in a1] 
   a3 = [tensor_mul(*v) for v in a2] 
   t3 = TensAdd(*a3) 
   if (not t3): 
      return t3 
   else: 
      return canon_bp(t3)"," 'Returns a cyclic tensor with the given shape and the same 
 structure as the given tensor. 
 The cyclic tensor has the same shape as the given tensor, 
 but the indices are cyclic, i.e. the indices of the tensor 
 are not unique. 
 Examples 
 >>> from sympy.tensor.core import TensMul, Tensor, TensAdd 
 >>> from sympy.tensor.riemann_cyclic import riemann_cyclic 
 >>> t1 = TensMul(Tensor([[1, 2], [3, 4]]), Tensor([[5, 6], [7, 8]])) 
 >>> t2 = riemann_cyclic(t1) 
 >>> t2 
 TensMul([[1, 2], [3, 4]], [5, 6], [7, 8]) 
 >>> t3 = riemann_cyclic(TensMul(Tensor([[1, 2], [3, 4]]), Tensor([[5, 6], [7, 8]]))) 
 >>> t3 ","'replace each Riemann tensor with an equivalent expression 
 satisfying the cyclic identity. 
 This trick is discussed in the reference guide to Cadabra. 
 Examples 
 >>> from sympy.tensor.tensor import TensorIndexType, tensor_indices, tensorhead, riemann_cyclic 
 >>> Lorentz = TensorIndexType(\'Lorentz\', dummy_fmt=\'L\') 
 >>> i, j, k, l = tensor_indices(\'i,j,k,l\', Lorentz) 
 >>> R = tensorhead(\'R\', [Lorentz]*4, [[2, 2]]) 
 >>> t = R(i,j,k,l)*(R(-i,-j,-k,-l) - 2*R(-i,-k,-j,-l)) 
 >>> riemann_cyclic(t) 
 0'"
"def sanitize_html(html_code): 
    attributes = bleach.ALLOWED_ATTRIBUTES.copy() 
   if (u'data' not in bleach.BleachSanitizer.allowed_protocols): 
      bleach.BleachSanitizer.allowed_protocols.append(u'data') 
   attributes.update({'*': ['class', 'style', 'id'], 'audio': ['controls', 'autobuffer', 'autoplay', 'src'], 'img': ['src', 'width', 'height', 'class']}) 
   output = bleach.clean(html_code, tags=(bleach.ALLOWED_TAGS + ['div', 'p', 'audio', 'pre', 'img', 'span']), styles=['white-space'], attributes=attributes) 
   return output"," 'Sanitize html code. 
 :param html_code: 
 :type html_code: str 
 :return: sanitized html code.'","'Sanitize html_code for safe embed on LMS pages. 
 Used to sanitize XQueue responses from Matlab.'"
"def read_local(tex_root, name): 
    cache_path = _local_cache_path(tex_root) 
   _validate_life_span(cache_path) 
   return _read(cache_path, name)", 'Read a local file.',"'Reads the object from the local cache using pickle. 
 The local cache is per tex document and the path will extracted 
 from the tex root 
 Arguments: 
 tex_root -- the root of the tex file (for the folder of the cache) 
 name -- the relative file name to read the object 
 Returns: 
 The object at the location with the name'"
"def cut_threshold(labels, rag, thresh, in_place=True): 
    if (not in_place): 
      rag = rag.copy() 
   to_remove = [(x, y) for (x, y, d) in rag.edges_iter(data=True) if (d['weight'] >= thresh)] 
   rag.remove_edges_from(to_remove) 
   comps = nx.connected_components(rag) 
   map_array = np.arange((labels.max() + 1), dtype=labels.dtype) 
   for (i, nodes) in enumerate(comps): 
      for node in nodes: 
         for label in rag.node[node]['labels']: 
            map_array[label] = i 
   return map_array[labels]"," 'Cut the labels in the graph. 
 Parameters 
 labels : array 
 Array of labels. 
 rag : NetworkX Graph 
 The graph. 
 thresh : float 
 The threshold. 
 in_place : bool 
 If True, modify the graph in-place. 
 Returns 
 labels : array 
 The labels of the graph. 
 Examples 
 >>> from networkx.algorithms.community.label_propagation import ( 
 ...     cut_threshold) 
 >>> G = nx.Graph() 
 >>> G.add_edges_from([(0, 1), (1, 2), (0, 2)]) 
 >>> labels = [0, 1, 2] 
 >>> cut_threshold(labels, G, 0.2) 
 array([0, 2])'","'Combine regions separated by weight less than threshold. 
 Given an image\'s labels and its RAG, output new labels by 
 combining regions whose nodes are separated by a weight less 
 than the given threshold. 
 Parameters 
 labels : ndarray 
 The array of labels. 
 rag : RAG 
 The region adjacency graph. 
 thresh : float 
 The threshold. Regions connected by edges with smaller weights are 
 combined. 
 in_place : bool 
 If set, modifies `rag` in place. The function will remove the edges 
 with weights less that `thresh`. If set to `False` the function 
 makes a copy of `rag` before proceeding. 
 Returns 
 out : ndarray 
 The new labelled array. 
 Examples 
 >>> from skimage import data, segmentation 
 >>> from skimage.future import graph 
 >>> img = data.astronaut() 
 >>> labels = segmentation.slic(img) 
 >>> rag = graph.rag_mean_color(img, labels) 
 >>> new_labels = graph.cut_threshold(labels, rag, 10) 
 References 
 .. [1] Alain Tremeau and Philippe Colantoni 
 ""Regions Adjacency Graph Applied To Color Image Segmentation"" 
 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.5274'"
"@login_required 
 @require_POST 
 def add_leader(request, group_slug): 
    prof = get_object_or_404(GroupProfile, slug=group_slug) 
   if (not _user_can_manage_leaders(request.user, prof)): 
      raise PermissionDenied 
   form = AddUserForm(request.POST) 
   if form.is_valid(): 
      for user in form.cleaned_data['users']: 
         if (prof.group not in user.groups.all()): 
            user.groups.add(prof.group) 
         prof.leaders.add(user) 
      msg = _('{users}   added   to   the   group   leaders   successfully!').format(users=request.POST.get('users')) 
      messages.add_message(request, messages.SUCCESS, msg) 
      return HttpResponseRedirect(prof.get_absolute_url()) 
   msg = _('There   were   errors   adding   leaders   to   the   group,   see   below.') 
   messages.add_message(request, messages.ERROR, msg) 
   return profile(request, group_slug, leader_form=form)"," 'Adds the specified users to the group\'s leader list. 
 :param request: The request object. 
 :param group_slug: The slug of the group to add users to. 
 :param form: The form to validate. 
 :return: The response.'",'Add a leader to the group.'
"@register.inclusion_tag('addons/impala/contribution.html') 
 @jinja2.contextfunction 
 def impala_contribution(context, addon, text=None, src='', show_install=False, show_help=True, large=False, contribution_src=None): 
    if (not contribution_src): 
      contribution_src = src 
   has_suggested = bool(addon.suggested_amount) 
   comment_limit = PAYPAL_MAX_COMMENT_LENGTH 
   return new_context(**locals())", 'Render the contribution form for the given addon.',"'Show a contribution box. 
 Parameters: 
 addon 
 text: The begging text at the top of the box. 
 src: The page where the contribution link is coming from. 
 show_install: Whether or not to show the install button. 
 show_help: Show ""What\'s this?"" link? 
 contribution_src: The source for the contribution src, 
 will use src if not provided.'"
"def kinetic_energy(vel): 
    return (0.5 * (vel ** 2).sum(axis=1))"," 'Kinetic energy of a vector field. 
 Parameters 
 vel : array 
 Vector field. 
 Returns 
 kinetic_energy : float 
 Kinetic energy of the vector field. 
 Notes 
 This is the same as the kinetic energy of a particle, but applied to a 
 vector field. 
 Examples 
 >>> from sympy.physics.vector import kinetic_energy 
 >>> from sympy import sin, pi 
 >>> vel = sin(pi * x) * cos(pi * y) 
 >>> kinetic_energy(vel) 
 1/2 * (sin(pi * x) * cos(pi * y)) ** 2 
 References 
 .. [1] http://en.wikipedia.org/wiki/Kinetic_energy'","'Returns the kinetic energy associated with the given velocity 
 and mass of 1. 
 Parameters 
 vel: theano matrix 
 Symbolic matrix whose rows are velocity vectors. 
 Returns 
 return: theano vector 
 Vector whose i-th entry is the kinetic entry associated with vel[i].'"
"def ldata(): 
    try: 
      _id = request.args[0] 
   except: 
      raise HTTP(400) 
   translate = settings.get_L10n_translate_gis_location() 
   if translate: 
      language = session.s3.language 
      if (language == settings.get_L10n_default_language()): 
         translate = False 
   table = s3db.gis_location 
   query = ((((table.deleted == False) & (table.level != None)) & (table.end_date == None)) & ((table.parent == _id) | (table.id == _id))) 
   fields = [table.id, table.name, table.level, table.parent, table.lon_min, table.lat_min, table.lon_max, table.lat_max] 
   if translate: 
      ntable = s3db.gis_location_name 
      fields.append(ntable.name_l10n) 
      left = ntable.on((((ntable.deleted == False) & (ntable.language == language)) & (ntable.location_id == table.id))) 
   else: 
      left = None 
   locations = db(query).select(left=left, *fields) 
   if translate: 
      try: 
         id_level = int(locations.as_dict(key='gis_location.id')[int(_id)]['gis_location']['level'][1:]) 
      except: 
         return '' 
   else: 
      try: 
         id_level = int(locations.as_dict()[int(_id)]['level'][1:]) 
      except: 
         return '' 
   output_level = (id_level + 1) 
   search_level = ('L%s' % output_level) 
   location_dict = {} 
   if translate: 
      for location in locations: 
         l = location['gis_location'] 
         if (l.level == search_level): 
            this_level = output_level 
            f = int(l.parent) 
         else: 
            this_level = int(l.level[1:]) 
            parent = l.parent 
            if parent: 
               f = int(parent) 
            else: 
               f = None 
         name = (location['gis_location_name.name_l10n'] or l.name) 
         if (l.lon_min is not None): 
            location_dict[int(l.id)] = dict(n=name, l=this_level, f=f, b=[l.lon_min, l.lat_min, l.lon_max, l.lat_max]) 
         else: 
            location_dict[int(l.id)] = dict(n=name, l=this_level, f=f) 
   else: 
      for l in locations: 
         if (l.level == search_level): 
            this_level = output_level 
            f = int(l.parent) 
         else: 
            this_level = int(l.level[1:]) 
            parent = l.parent 
            if parent: 
               f = int(parent) 
            else: 
               f = None 
         if (l.lon_min is not None): 
            location_dict[int(l.id)] = dict(n=l.name, l=this_level, f=f, b=[l.lon_min, l.lat_min, l.lon_max, l.lat_max]) 
         else: 
            location_dict[int(l.id)] = dict(n=l.name, l=this_level, f=f) 
   script = ('n=%s\n' % json.dumps(location_dict, separators=SEPARATORS)) 
   response.headers['Content-Type'] = 'application/json' 
   return script", 'Get the parent levels of a location.',"'Return JSON of location hierarchy suitable for use by S3LocationSelector 
 \'/eden/gis/ldata/\' + id 
 n = {id : {\'n\' : name, 
 \'l\' : level, 
 \'f\' : parent, 
 \'b\' : [lon_min, lat_min, lon_max, lat_max]'"
"def flavor_destroy(context, flavor_id): 
    return IMPL.flavor_destroy(context, flavor_id)", 'Destroy a flavor.','Delete an instance type.'
"def generate_module(src): 
    if (not src): 
      return UNKNOWN_MODULE 
   (filename, ext) = splitext(urlsplit(src).path) 
   if (ext not in ('.js', '.jsx', '.coffee')): 
      return UNKNOWN_MODULE 
   if filename.endswith('.min'): 
      filename = filename[:(-4)] 
   tokens = filename.split('/') 
   for (idx, token) in enumerate(tokens): 
      if VERSION_RE.match(token): 
         return '/'.join(tokens[(idx + 1):]) 
   return (CLEAN_MODULE_RE.sub('', filename) or UNKNOWN_MODULE)"," 'Generate a module name from a url. 
 :param src: A url to a module. 
 :type src: str 
 :return: A module name. 
 :rtype: str'","'Converts a url into a made-up module name by doing the following: 
 * Extract just the path name ignoring querystrings 
 * Trimming off the initial / 
 * Trimming off the file extension 
 * Removes off useless folder prefixes 
 e.g. http://google.com/js/v1.0/foo/bar/baz.js -> foo/bar/baz'"
"@contextmanager 
 def set_trap(app): 
    trap = Trap() 
   prev_tls = _state._tls 
   _state.set_default_app(trap) 
   class NonTLS(object, ): 
      current_app = trap 
   _state._tls = NonTLS() 
   (yield) 
   _state._tls = prev_tls"," 'Set the application to be used for testing. 
 :param app: The application to use. 
 :type app: WSGI application 
 :return: The original application. 
 :rtype: WSGI application'","'Contextmanager that installs the trap app. 
 The trap means that anything trying to use the current or default app 
 will raise an exception.'"
"@functools.lru_cache() 
 def get_languages(): 
    return OrderedDict(settings.LANGUAGES)"," 'Returns an ordered dictionary of language names and their 
 respective language code.'",'Cache of settings.LANGUAGES in an OrderedDict for easy lookups by key.'
"def is_clean_uri(uri): 
    return (not bool(BAD_URI_CHARS_RE.search(uri)))"," 'Checks if the given URI is clean. 
 :param uri: The URI to check. 
 :type uri: str 
 :return: True if the URI is clean, False otherwise. 
 :rtype: bool'","'>>> is_clean_uri(""ABC!"") 
 True 
 >>> is_clean_uri(u""ABC!"") 
 True 
 >>> is_clean_uri(""ABC|"") 
 False 
 >>> is_clean_uri(u""ABC|"") 
 False 
 >>> is_clean_uri(""http://example.com/0"") 
 True 
 >>> is_clean_uri(u""http://example.com/0"") 
 True'"
"def build_model(): 
    net = {} 
   net['input'] = InputLayer((None, 3, 16, 112, 112)) 
   net['conv1a'] = Conv3DDNNLayer(net['input'], 64, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify, flip_filters=False) 
   net['pool1'] = MaxPool3DDNNLayer(net['conv1a'], pool_size=(1, 2, 2), stride=(1, 2, 2)) 
   net['conv2a'] = Conv3DDNNLayer(net['pool1'], 128, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['pool2'] = MaxPool3DDNNLayer(net['conv2a'], pool_size=(2, 2, 2), stride=(2, 2, 2)) 
   net['conv3a'] = Conv3DDNNLayer(net['pool2'], 256, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['conv3b'] = Conv3DDNNLayer(net['conv3a'], 256, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['pool3'] = MaxPool3DDNNLayer(net['conv3b'], pool_size=(2, 2, 2), stride=(2, 2, 2)) 
   net['conv4a'] = Conv3DDNNLayer(net['pool3'], 512, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['conv4b'] = Conv3DDNNLayer(net['conv4a'], 512, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['pool4'] = MaxPool3DDNNLayer(net['conv4b'], pool_size=(2, 2, 2), stride=(2, 2, 2)) 
   net['conv5a'] = Conv3DDNNLayer(net['pool4'], 512, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['conv5b'] = Conv3DDNNLayer(net['conv5a'], 512, (3, 3, 3), pad=1, nonlinearity=lasagne.nonlinearities.rectify) 
   net['pad'] = PadLayer(net['conv5b'], width=[(0, 1), (0, 1)], batch_ndim=3) 
   net['pool5'] = MaxPool3DDNNLayer(net['pad'], pool_size=(2, 2, 2), pad=(0, 0, 0), stride=(2, 2, 2)) 
   net['fc6-1'] = DenseLayer(net['pool5'], num_units=4096, nonlinearity=lasagne.nonlinearities.rectify) 
   net['fc7-1'] = DenseLayer(net['fc6-1'], num_units=4096, nonlinearity=lasagne.nonlinearities.rectify) 
   net['fc8-1'] = DenseLayer(net['fc7-1'], num_units=487, nonlinearity=None) 
   net['prob'] = NonlinearityLayer(net['fc8-1'], softmax) 
   return net"," 'Build a model for the MNIST dataset. 
 Parameters 
 None 
 Returns 
 A dictionary containing the layers of the model. 
 Notes 
 The model is a convolutional neural network with 
 10 convolutional layers, 2 max pooling layers, 2 dense 
 layers and a softmax layer. 
 References 
 .. [1] http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf'","'Builds C3D model 
 Returns 
 dict 
 A dictionary containing the network layers, where the output layer is at key \'prob\''"
"def _get_ssh_interface(vm_): 
    return config.get_cloud_config_value('ssh_interface', vm_, __opts__, default='public_ips', search_global=False)", 'Returns the ssh_interface for a VM.',"'Return the ssh_interface type to connect to. Either \'public_ips\' (default) 
 or \'private_ips\'.'"
"def get_tenancy(vm_): 
    return config.get_cloud_config_value('tenancy', vm_, __opts__, search_global=False)", 'Get the tenancy of the VM.',"'Returns the Tenancy to use. 
 Can be ""dedicated"" or ""default"". Cannot be present for spot instances.'"
"def true_dot(x, y, grad_preserves_dense=True): 
    if hasattr(x, 'getnnz'): 
      x = as_sparse_variable(x) 
      assert (x.format in ['csr', 'csc']) 
   if hasattr(y, 'getnnz'): 
      y = as_sparse_variable(y) 
      assert (y.format in ['csr', 'csc']) 
   x_is_sparse_variable = _is_sparse_variable(x) 
   y_is_sparse_variable = _is_sparse_variable(y) 
   if ((not x_is_sparse_variable) and (not y_is_sparse_variable)): 
      raise TypeError() 
   if x_is_sparse_variable: 
      return TrueDot(grad_preserves_dense)(x, y) 
   else: 
      assert y_is_sparse_variable 
      return transpose(TrueDot(grad_preserves_dense)(y.T, x.T))"," 'Returns true_dot(x, y) or true_dot(y, x) 
 If both x and y are sparse, then true_dot(x, y) is returned. 
 Otherwise, true_dot(y, x) is returned. 
 Parameters 
 x : SparseMatrix 
 y : SparseMatrix 
 grad_preserves_dense : bool, optional 
 If True, the gradients of true_dot are computed using dense matrices. 
 If False, the gradients of true_dot are computed using sparse matrices. 
 Examples 
 >>> from sympy.sparse.sparsefuncs import true_dot 
 >>> from sympy import Matrix 
 >>> x = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 >>> y = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 >>> true_dot(x, y) 
 [[ 1,  4,  7], 
 [ 2,  5,  8","'Operation for efficiently calculating the dot product when 
 one or all operands are sparse. Supported formats are CSC and CSR. 
 The output of the operation is sparse. 
 Parameters 
 x 
 Sparse matrix. 
 y 
 Sparse matrix or 2d tensor variable. 
 grad_preserves_dense : bool 
 If True (default), makes the grad of dense inputs dense. 
 Otherwise the grad is always sparse. 
 Returns 
 The dot product `x`.`y` in a sparse format. 
 Notex 
 The grad implemented is regular, i.e. not structured.'"
"def parse_owner_mappings(type, options, fatal): 
    opt_name = ('--map-' + type) 
   value_rx = '^([^=]+)=([^=]*)$' 
   if (type in ('uid', 'gid')): 
      value_rx = '^(-?[0-9]+)=(-?[0-9]+)$' 
   owner_map = {} 
   for flag in options: 
      (option, parameter) = flag 
      if (option != opt_name): 
         continue 
      match = re.match(value_rx, parameter) 
      if (not match): 
         raise fatal((""couldn't   parse   %s   as   %s   mapping"" % (parameter, type))) 
      (old_id, new_id) = match.groups() 
      if (type in ('uid', 'gid')): 
         old_id = int(old_id) 
         new_id = int(new_id) 
      owner_map[old_id] = new_id 
   return owner_map"," 'Parse the owner mapping options. 
 :param type: \'uid\' or \'gid\'. 
 :param options: The options to parse. 
 :param fatal: Raise a fatal error if the parsing fails. 
 :return: A mapping from old_id to new_id.'","'Traverse the options and parse all --map-TYPEs, or call Option.fatal().'"
"def parse_xmlrpc(xml_string): 
    handler = XmlRpcReadHandler() 
   xml.sax.parseString(xml_string, handler) 
   return handler", 'Parse a XML-RPC request.',"'The user should call these functions: parse_xmlrpc and build_xmlrpc. 
 :param xml_string: The original XML string that we got from the browser. 
 :return: A handler that can then be used to access the result information 
 from: 
 - handler.fuzzable_parameters 
 - handler.all_parameters 
 - handler.get_data_container'"
"def distrib_release(): 
    with settings(hide('running', 'stdout')): 
      kernel = run('uname   -s') 
      if (kernel == 'Linux'): 
         return run('lsb_release   -r   --short') 
      elif (kernel == 'SunOS'): 
         return run('uname   -v')", 'Returns the distribution release.',"'Get the release number of the distribution. 
 Example:: 
 from fabtools.system import distrib_id, distrib_release 
 if distrib_id() == \'CentOS\' and distrib_release() == \'6.1\': 
 print(u""CentOS 6.2 has been released. Please upgrade."")'"
"def get_total_project_memberships(project): 
    return project.memberships.count()", 'Returns the total number of memberships for the project.',"'Return tha total of memberships of a project (members and unaccepted invitations). 
 :param project: A project object. 
 :return: a number.'"
"def fit(function, x, y): 
    p0 = [guess_plateau(x, y), 4.0, guess_lag(x, y), 0.1, min(y)] 
   (params, pcov) = curve_fit(function, x, y, p0=p0) 
   return (params, pcov)"," 'Fit the function to the data. 
 Parameters 
 function : function to fit 
 x : list of x-coordinates 
 y : list of y-coordinates 
 Returns 
 params : list of parameters 
 pcov : list of parameter covariances 
 Examples 
 >>> from sympy import Function, cos, exp, log, sin 
 >>> from sympy.utilities.plotting import plot, show 
 >>> from sympy.abc import x 
 >>> def func(x): 
 ...     return 2*cos(x) - exp(x) 
 >>> func.diff(x, x, 1) 
 2*cos(x) - exp(x) 
 >>> plot(func(x), x, y=func.diff(x, x, 1)) 
 >>> show() 
 >>> func.diff(x, x, 1).subs(x, 1) 
 -2*cos(1) - exp(1) 
 >>> plot(func(x), x, y=func.diff(x, x, 1).subs(x, 1","'Fit the provided functrion to the x and y values. 
 The function parameters and the parameters covariance.'"
"def indexable(*iterables): 
    result = [] 
   for X in iterables: 
      if sp.issparse(X): 
         result.append(X.tocsr()) 
      elif (hasattr(X, '__getitem__') or hasattr(X, 'iloc')): 
         result.append(X) 
      elif (X is None): 
         result.append(X) 
      else: 
         result.append(np.array(X)) 
   check_consistent_length(*result) 
   return result"," 'Return a list of sparse matrices. 
 Parameters 
 iterables : iterables 
 A list of iterables. 
 Returns 
 A list of sparse matrices. 
 Examples 
 >>> from sympy import Matrix 
 >>> from sympy.sparse.sparse import indexable 
 >>> from sympy.abc import x, y 
 >>> A = Matrix([[1, 2], [3, 4]]) 
 >>> B = Matrix([[1, 2], [3, 4]]) 
 >>> C = Matrix([[1, 2], [3, 4]]) 
 >>> D = Matrix([[1, 2], [3, 4]]) 
 >>> indexable([A, B, C, D]) 
 [[1, 2], [1, 2], [1, 2], [1, 2]]'","'Make arrays indexable for cross-validation. 
 Checks consistent length, passes through None, and ensures that everything 
 can be indexed by converting sparse matrices to csr and converting 
 non-interable objects to arrays. 
 Parameters 
 *iterables : lists, dataframes, arrays, sparse matrices 
 List of objects to ensure sliceability.'"
"@decorators.api_view(['GET']) 
 @decorators.permission_classes((permissions.AllowAny,)) 
 @decorators.renderer_classes((JSONRenderer,)) 
 def section_search(request): 
    query = request.GET.get('q', None) 
   if (not query): 
      return Response({'error': 'Search   term   required.   Use   the   ""q""   GET   arg   to   search.   '}, status=status.HTTP_400_BAD_REQUEST) 
   project_slug = request.GET.get('project', None) 
   version_slug = request.GET.get('version', LATEST) 
   path = request.GET.get('path', None) 
   log.debug('(API   Section   Search)   [%s:%s]   %s', project_slug, version_slug, query) 
   results = search_section(request=request, query=query, project_slug=project_slug, version_slug=version_slug, path=path) 
   return Response({'results': results})"," 'Searches for sections. 
 :param request: The request. 
 :param query: The search term. 
 :param project_slug: The project slug. 
 :param version_slug: The version slug. 
 :param path: The path to the search. 
 :returns: A response. 
 :rtype: Response'","'Section search 
 Queries with query ``q`` across all documents and projects. Queries can be 
 limited to a single project or version by using the ``project`` and 
 ``version`` GET arguments in your request. 
 When you search, you will have a ``project`` facet, which includes the 
 number of matching sections per project. When you search inside a project, 
 the ``path`` facet will show the number of matching sections per page. 
 Possible GET args 
 q **(required)** 
 The query string **Required** 
 project 
 A project slug 
 version 
 A version slug 
 path 
 A file path slug 
 Example:: 
 GET /api/v2/search/section/?q=virtualenv&project=django'"
"def remove_non_release_groups(name): 
    if (not name): 
      return name 
   removeWordsList = {u'\\[rartv\\]$': u'searchre', u'\\[rarbg\\]$': u'searchre', u'\\[eztv\\]$': u'searchre', u'\\[ettv\\]$': u'searchre', u'\\[cttv\\]$': u'searchre', u'\\[vtv\\]$': u'searchre', u'\\[EtHD\\]$': u'searchre', u'\\[GloDLS\\]$': u'searchre', u'\\[silv4\\]$': u'searchre', u'\\[Seedbox\\]$': u'searchre', u'\\[PublicHD\\]$': u'searchre', u'\\[AndroidTwoU\\]$': u'searchre', u'\\.\\[BT\\]$': u'searchre', u'   \\[1044\\]$': u'searchre', u'\\.RiPSaLoT$': u'searchre', u'\\.GiuseppeTnT$': u'searchre', u'\\.Renc$': u'searchre', u'-NZBGEEK$': u'searchre', u'-Siklopentan$': u'searchre', u'-\\[SpastikusTV\\]$': u'searchre', u'-RP$': u'searchre', u'-20-40$': u'searchre', u'\\.\\[www\\.usabit\\.com\\]$': u'searchre', u'^\\[www\\.Cpasbien\\.pe\\]   ': u'searchre', u'^\\[www\\.Cpasbien\\.com\\]   ': u'searchre', u'^\\[   www\\.Cpasbien\\.pw   \\]   ': u'searchre', u'^\\.www\\.Cpasbien\\.pw': u'searchre', u'^\\[www\\.newpct1\\.com\\]': u'searchre', u'^\\[   www\\.Cpasbien\\.com   \\]   ': u'searchre', u'-   \\{   www\\.SceneTime\\.com   \\}$': u'searchre', u'^\\{   www\\.SceneTime\\.com   \\}   -   ': u'searchre', u'^\\]\\.\\[www\\.tensiontorrent.com\\]   -   ': u'searchre', u'^\\]\\.\\[   www\\.tensiontorrent.com   \\]   -   ': u'searchre', u'-   \\[   www\\.torrentday\\.com   \\]$': u'searchre', u'^\\[   www\\.TorrentDay\\.com   \\]   -   ': u'searchre', u'\\[NO-RAR\\]   -   \\[   www\\.torrentday\\.com   \\]$': u'searchre'} 
   _name = name 
   for (remove_string, remove_type) in removeWordsList.items(): 
      if (remove_type == u'search'): 
         _name = _name.replace(remove_string, u'') 
      elif (remove_type == u'searchre'): 
         _name = re.sub((u'(?i)' + remove_string), u'', _name) 
   return _name.strip(u'.-   []{}')"," 'Removes non-release groups from a release name. 
 This function will remove the following: 
 - RARBG 
 - EZTV 
 - ETTV 
 - CTTV 
 - VTV 
 - EtHD 
 - GloDLS 
 - Silv4 
 - Seedbox 
 - PublicHD 
 - AndroidTwoU 
 - BT 
 - 1044 
 - RiPSaLoT 
 - GiuseppeTnT 
 - Renc 
 - NZBGEEK 
 - Siklopentan 
 - SpastikusTV 
 - RP 
 - 20-40 
 - www.usabit.com 
 - www.Cpasbien.pe 
 - www.Cpasbien.com 
 - www.Cpasbien.pw 
 - www.newpct1.com 
 - www.SceneTime.com 
 - www.SceneTime.com 
 - www.tensiontorrent.com 
 - www.TorrentDay.com 
 - NO-",'Remove non release groups from name'
"def is_filelike(obj): 
    if (isinstance(obj, str) or isinstance(obj, unicode)): 
      return False 
   res = True 
   for a in ('read', 'tell', 'seek'): 
      res = (res and hasattr(obj, a)) 
   if (not res): 
      raise ValueError('Invalid   object   passed   as   file') 
   return True", 'Check if obj is a file-like object.','Filename or file object?'
"def get_netrc_auth(url): 
    try: 
      locations = (os.path.expanduser('~/{0}'.format(f)) for f in NETRC_FILES) 
      netrc_path = None 
      for loc in locations: 
         if (os.path.exists(loc) and (not netrc_path)): 
            netrc_path = loc 
      if (netrc_path is None): 
         return netrc_path 
      ri = urlparse(url) 
      host = ri.netloc.split(':')[0] 
      try: 
         _netrc = netrc(netrc_path).authenticators(host) 
         if _netrc: 
            login_i = (0 if _netrc[0] else 1) 
            return (_netrc[login_i], _netrc[2]) 
      except (NetrcParseError, IOError): 
         pass 
   except (ImportError, AttributeError): 
      pass"," 'Return the auth info for the given url, or None if it cannot be 
 determined.'",'Returns the Requests tuple auth for a given url from netrc.'
"def _tmp_name(input): 
    if (input is not None): 
      input = os.path.dirname(input) 
   (f, fn) = tempfile.mkstemp(dir=input) 
   os.close(f) 
   return fn"," 'Return a temporary file name. 
 :param input: Path to parent directory of the temporary file. 
 :returns: Name of the temporary file. 
 :rtype: str'","'Create a temporary file name which should not already exist.  Use the 
 directory of the input file as the base name of the mkstemp() output.'"
"@register.filter(is_safe=True, needs_autoescape=True) 
 @stringfilter 
 def linenumbers(value, autoescape=True): 
    lines = value.split('\n') 
   width = str(len(str(len(lines)))) 
   if ((not autoescape) or isinstance(value, SafeData)): 
      for (i, line) in enumerate(lines): 
         lines[i] = ((('%0' + width) + 'd.   %s') % ((i + 1), line)) 
   else: 
      for (i, line) in enumerate(lines): 
         lines[i] = ((('%0' + width) + 'd.   %s') % ((i + 1), escape(line))) 
   return mark_safe('\n'.join(lines))"," 'Adds line numbers to each line of the given text. 
 :param value: The text to add line numbers to. 
 :param autoescape: If True, the text is escaped before line numbers are 
 added. 
 :type value: str 
 :type autoescape: bool 
 :return: The text with line numbers added. 
 :rtype: str'",'Displays text with line numbers.'
"def name(pretty=False): 
    return _distro.name(pretty)"," 'Returns the name of the operating system. 
 If pretty is True, the name will be formatted as a string. 
 Otherwise, it will be a tuple.'","'Return the name of the current Linux distribution, as a human-readable 
 string. 
 If *pretty* is false, the name is returned without version or codename. 
 (e.g. ""CentOS Linux"") 
 If *pretty* is true, the version and codename are appended. 
 (e.g. ""CentOS Linux 7.1.1503 (Core)"") 
 **Lookup hierarchy:** 
 The name is obtained from the following sources, in the specified order. 
 The first available and non-empty value is used: 
 * If *pretty* is false: 
 - the value of the ""NAME"" attribute of the os-release file, 
 - the value of the ""Distributor ID"" attribute returned by the lsb_release 
 command, 
 - the value of the ""<name>"" field of the distro release file. 
 * If *pretty* is true: 
 - the value of the ""PRETTY_NAME"" attribute of the os-release file, 
 - the value of the ""Description"" attribute returned by the lsb_release 
 command, 
 - the value of the ""<name>"" field of the distro release file, appended 
 with the value of the pretty version (""<version_id>"" and ""<codename>"" 
 fields) of the distro release file, if available.'"
"def get_logger(name): 
    old_class = logging.getLoggerClass() 
   logging.setLoggerClass(logging.Logger) 
   logger = logging.getLogger(name) 
   logging.setLoggerClass(old_class) 
   return logger"," 'Return a logger with the given name. 
 :param name: The name of the logger to create. 
 :type name: str 
 :return: A logger with the given name. 
 :rtype: logging.Logger'","'Always use logging.Logger class. 
 The user code may change the loggerClass (e.g. pyinotify), 
 and will cause exception when format log message.'"
"def prepare_database(db, coord, projection): 
    db.execute('CREATE   TEMPORARY   TABLE   box_node_list   (id   bigint   PRIMARY   KEY)   ON   COMMIT   DROP') 
   db.execute('CREATE   TEMPORARY   TABLE   box_way_list   (id   bigint   PRIMARY   KEY)   ON   COMMIT   DROP') 
   db.execute('CREATE   TEMPORARY   TABLE   box_relation_list   (id   bigint   PRIMARY   KEY)   ON   COMMIT   DROP') 
   (n, s, e, w) = coordinate_bbox(coord, projection) 
   bbox = ('ST_SetSRID(ST_MakeBox2D(ST_MakePoint(%.7f,   %.7f),   ST_MakePoint(%.7f,   %.7f)),   4326)' % (w, s, e, n)) 
   db.execute(('INSERT   INTO   box_node_list\n                                                      SELECT   id\n                                                      FROM   nodes\n                                                      WHERE   (geom   &&   %(bbox)s)' % locals())) 
   db.execute('INSERT   INTO   box_way_list\n                                                      SELECT   wn.way_id\n                                                      FROM   way_nodes   wn\n                                                      INNER   JOIN   box_node_list   n\n                                                      ON   wn.node_id   =   n.id\n                                                      GROUP   BY   wn.way_id') 
   db.execute(""INSERT   INTO   box_relation_list\n                                                      (\n                                                            SELECT   rm.relation_id   AS   relation_id\n                                                            FROM   relation_members   rm\n                                                            INNER   JOIN   box_node_list   n\n                                                            ON   rm.member_id   =   n.id\n                                                            WHERE   rm.member_type   =   'N'\n                                                      UNION\n                                                            SELECT   rm.relation_id   AS   relation_id\n                                                            FROM   relation_members   rm\n                                                            INNER   JOIN   box_way_list   w\n                                                            ON   rm.member_id   =   w.id\n                                                            WHERE   rm.member_type   =   'W'\n                                                      )"") 
   db.execute(""INSERT   INTO   box_relation_list\n                                                      SELECT   rm.relation_id   AS   relation_id\n                                                      FROM   relation_members   rm\n                                                      INNER   JOIN   box_relation_list   r\n                                                      ON   rm.member_id   =   r.id\n                                                      WHERE   rm.member_type   =   'R'\n                                                      EXCEPT\n                                                      SELECT   id   AS   relation_id\n                                                      FROM   box_relation_list"") 
   db.execute('ANALYZE   box_node_list') 
   db.execute('ANALYZE   box_way_list') 
   db.execute('ANALYZE   box_relation_list')"," 'Prepares the database for use with the box query. 
 :param db: a database connection 
 :param coord: a coordinate 
 :param projection: a projection'",''
"def getTricomplexTimesColumn(firstTricomplex, otherColumn): 
    dotProductX = ((firstTricomplex[0].real * otherColumn.real) + (firstTricomplex[1].real * otherColumn.imag)) 
   dotProductY = ((firstTricomplex[0].imag * otherColumn.real) + (firstTricomplex[1].imag * otherColumn.imag)) 
   return complex(dotProductX, dotProductY)"," 'Returns the tricomplex times column of a tricomplex matrix. 
 Parameters 
 firstTricomplex : tricomplex matrix 
 otherColumn : tricomplex column 
 Returns 
 tricomplex column'",'Get this matrix multiplied by the otherColumn.'
"def getFabmetheusUtilitiesPath(subName=''): 
    return getJoinedPath(getFabmetheusPath('fabmetheus_utilities'), subName)"," 'Get the path to Fabmetheus utilities. 
 Parameters 
 subName : str 
 The sub directory to look for Fabmetheus utilities in. 
 Returns 
 The path to Fabmetheus utilities. 
 Raises 
 ValueError 
 If the path could not be found.'",'Get the fabmetheus utilities directory path.'
"def file_upload_echo(request): 
    r = {k: f.name for (k, f) in request.FILES.items()} 
   return HttpResponse(json.dumps(r))"," 'Echo back the file name for each file uploaded. 
 :param request: The request object. 
 :return: JSON response. 
 :rtype: HttpResponse'",'Simple view to echo back info about uploaded files for tests.'
"def _setSocketPreConnect(): 
    if conf.disablePrecon: 
      return 
   def _(): 
      while (kb.get('threadContinue') and (not conf.get('disablePrecon'))): 
         try: 
            for key in socket._ready: 
               if (len(socket._ready[key]) < SOCKET_PRE_CONNECT_QUEUE_SIZE): 
                  (family, type, proto, address) = key 
                  s = socket.socket(family, type, proto) 
                  s._connect(address) 
                  with kb.locks.socket: 
                     socket._ready[key].append((s._sock, time.time())) 
         except KeyboardInterrupt: 
            break 
         except: 
            pass 
         finally: 
            time.sleep(0.01) 
   def connect(self, address): 
      found = False 
      key = (self.family, self.type, self.proto, address) 
      with kb.locks.socket: 
         if (key not in socket._ready): 
            socket._ready[key] = [] 
         while (len(socket._ready[key]) > 0): 
            (candidate, created) = socket._ready[key].pop(0) 
            if ((time.time() - created) < PRECONNECT_CANDIDATE_TIMEOUT): 
               self._sock = candidate 
               found = True 
               break 
            else: 
               try: 
                  candidate.close() 
               except socket.error: 
                  pass 
      if (not found): 
         self._connect(address) 
   if (not hasattr(socket.socket, '_connect')): 
      socket._ready = {} 
      socket.socket._connect = socket.socket.connect 
      socket.socket.connect = connect 
      thread = threading.Thread(target=_) 
      setDaemon(thread) 
      thread.start()"," 'Set socket preconnect. 
 :param conf: configuration 
 :type conf: :class:`~.conf.Configuration` 
 :param socket: socket instance 
 :type socket: :class:`~.socket.Socket` 
 :return: 
 :rtype: 
 :raises: 
 :class:`KeyboardInterrupt` 
 :class:`SystemExit'`",'Makes a pre-connect version of socket.connect'
"def urlquote_plus(url, safe=''): 
    return force_unicode(urllib.quote_plus(smart_str(url), safe))", 'Return a URL with all characters escaped.',"'A version of Python\'s urllib.quote_plus() function that can operate on 
 unicode strings. The url is first UTF-8 encoded before quoting. The 
 returned string can safely be used as part of an argument to a subsequent 
 iri_to_uri() call without double-quoting occurring.'"
"def mock_open(mock=None, read_data=''): 
    def _readlines_side_effect(*args, **kwargs): 
      if (handle.readlines.return_value is not None): 
         return handle.readlines.return_value 
      return list(_data) 
   def _read_side_effect(*args, **kwargs): 
      if (handle.read.return_value is not None): 
         return handle.read.return_value 
      return ''.join(_data) 
   def _readline_side_effect(): 
      if (handle.readline.return_value is not None): 
         while True: 
            (yield handle.readline.return_value) 
      for line in _data: 
         (yield line) 
   global file_spec 
   if (file_spec is None): 
      import _io 
      file_spec = list(set(dir(_io.TextIOWrapper)).union(set(dir(_io.BytesIO)))) 
   if (mock is None): 
      mock = MagicMock(name='open', spec=open) 
   handle = MagicMock(spec=file_spec) 
   handle.__enter__.return_value = handle 
   _data = _iterate_read_data(read_data) 
   handle.write.return_value = None 
   handle.read.return_value = None 
   handle.readline.return_value = None 
   handle.readlines.return_value = None 
   handle.read.side_effect = _read_side_effect 
   handle.readline.side_effect = _readline_side_effect() 
   handle.readlines.side_effect = _readlines_side_effect 
   mock.return_value = handle 
   return mock", 'Mock open() with given read_data.',"'A helper function to create a mock to replace the use of `open`. It works 
 for `open` called directly or used as a context manager. 
 The `mock` argument is the mock object to configure. If `None` (the 
 default) then a `MagicMock` will be created for you, with the API limited 
 to methods or attributes available on standard file handles. 
 `read_data` is a string for the `read` methoddline`, and `readlines` of the 
 file handle to return.  This is an empty string by default.'"
"def parse_only_date(raw, assume_utc=True, as_utc=True): 
    f = (utcnow if assume_utc else now) 
   default = f().replace(hour=0, minute=0, second=0, microsecond=0, day=15) 
   return fix_only_date(parse_date(raw, default=default, assume_utc=assume_utc, as_utc=as_utc))"," 'Parse a date, with the assumption that it is in UTC. 
 :param raw: 
 :param assume_utc: 
 :param as_utc: 
 :return: 
 :rtype: datetime.datetime'","'Parse a date string that contains no time information in a manner that 
 guarantees that the month and year are always correct in all timezones, and 
 the day is at most one day wrong.'"
"@jit(nopython=True, cache=True) 
 def get_mixed_actions(tableaux, bases): 
    nums_actions = (tableaux[1].shape[0], tableaux[0].shape[0]) 
   num = (nums_actions[0] + nums_actions[1]) 
   out = np.zeros(num) 
   for (pl, (start, stop)) in enumerate(zip((0, nums_actions[0]), (nums_actions[0], num))): 
      sum_ = 0.0 
      for i in range(nums_actions[(1 - pl)]): 
         k = bases[pl][i] 
         if (start <= k < stop): 
            out[k] = tableaux[pl][(i, (-1))] 
            sum_ += tableaux[pl][(i, (-1))] 
      if (sum_ != 0): 
         out[start:stop] /= sum_ 
   return (out[:nums_actions[0]], out[nums_actions[0]:])", 'Returns a list of tuples of actions for each base.',"'From `tableaux` and `bases`, extract non-slack basic variables and 
 return a tuple of the corresponding, normalized mixed actions. 
 Parameters 
 tableaux : tuple(ndarray(float, ndim=2)) 
 Tuple of two arrays containing the tableaux, of shape (n, m+n+1) 
 and (m, m+n+1), respectively. 
 bases : tuple(ndarray(int, ndim=1)) 
 Tuple of two arrays containing the bases, of shape (n,) and 
 (m,), respectively. 
 Returns 
 tuple(ndarray(float, ndim=1)) 
 Tuple of mixed actions as given by the non-slack basic variables 
 in the tableaux.'"
"def make_pkgng_aware(jname): 
    ret = {'changes': {}} 
   cdir = _config_dir() 
   if (not os.path.isdir(cdir)): 
      os.makedirs(cdir) 
      if os.path.isdir(cdir): 
         ret['changes'] = 'Created   poudriere   make   file   dir   {0}'.format(cdir) 
      else: 
         return 'Could   not   create   or   find   required   directory   {0}'.format(cdir) 
   __salt__['file.write']('{0}-make.conf'.format(os.path.join(cdir, jname)), 'WITH_PKGNG=yes') 
   if os.path.isfile((os.path.join(cdir, jname) + '-make.conf')): 
      ret['changes'] = 'Created   {0}'.format(os.path.join(cdir, '{0}-make.conf'.format(jname))) 
      return ret 
   else: 
      return 'Looks   like   file   {0}   could   not   be   created'.format(os.path.join(cdir, (jname + '-make.conf')))", 'Make poudriere aware of pkgng',"'Make jail ``jname`` pkgng aware 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' poudriere.make_pkgng_aware <jail name>'"
"@csrf_exempt 
 @gzip_page 
 @require_sync_session 
 @api_handle_error_with_json 
 def device_download(data, session): 
    zone = session.client_device.get_zone() 
   devicezones = list(DeviceZone.all_objects.filter(zone=zone, device__in=data['devices'])) 
   devices = [devicezone.device for devicezone in devicezones] 
   session.models_downloaded += (len(devices) + len(devicezones)) 
   return JsonResponse({'devices': serialize((devices + devicezones), dest_version=session.client_version, ensure_ascii=False)})"," 'Download device data to the client. 
 :param data: 
 :type data: 
 :param session: 
 :type session: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :",'This device is having its own devices downloaded'
"def load_auth_tokens(user=None): 
    if (user is None): 
      user = users.get_current_user() 
   if (user is None): 
      return {} 
   pickled_tokens = memcache.get(('gdata_pickled_tokens:%s' % user)) 
   if pickled_tokens: 
      return pickle.loads(pickled_tokens) 
   user_tokens = TokenCollection.all().filter('user   =', user).get() 
   if user_tokens: 
      memcache.set(('gdata_pickled_tokens:%s' % user), user_tokens.pickled_tokens) 
      return pickle.loads(user_tokens.pickled_tokens) 
   return {}"," 'Returns a dictionary of auth tokens for the current user. 
 If the user is not logged in, returns an empty dictionary.'","'Reads a dictionary of the current user\'s tokens from the datastore. 
 If there is no current user (a user is not signed in to the app) or the user 
 does not have any tokens, an empty dictionary is returned.'"
"def modify_monitor(hostname, username, password, monitor_type, name, **kwargs): 
    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''} 
   if __opts__['test']: 
      params = {'hostname': hostname, 'username': username, 'password': password, 'monitor_type': monitor_type, 'name': name} 
      for (key, value) in six.iteritems(kwargs): 
         params[key] = value 
      return _test_output(ret, 'modify', params) 
   existing = __salt__['bigip.list_monitor'](hostname, username, password, monitor_type, name) 
   if (existing['code'] == 200): 
      modified = __salt__['bigip.modify_monitor'](hostname, username, password, monitor_type, name, **kwargs) 
      if (modified['code'] == 200): 
         del existing['content']['selfLink'] 
         del modified['content']['selfLink'] 
         ret = _check_for_changes('Monitor', ret, existing, modified) 
      else: 
         ret = _load_result(modified, ret) 
   elif (existing['code'] == 404): 
      ret['comment'] = 'A   Monitor   with   this   name   was   not   found.' 
   else: 
      ret = _load_result(existing, ret) 
   return ret"," 'Modify a monitor. 
 :param hostname: The hostname of the BIG-IP to connect to. 
 :param username: The username to use to connect to the BIG-IP. 
 :param password: The password to use to connect to the BIG-IP. 
 :param monitor_type: The type of monitor to modify. 
 :param name: The name of the monitor to modify. 
 :param kwargs: Additional arguments to pass to the ``modify`` command. 
 :returns: The modified monitor. 
 :rtype: dict'","'Modify an existing monitor.  If it does exists, only 
 the parameters specified will be enforced. 
 hostname 
 The host/address of the bigip device 
 username 
 The iControl REST username 
 password 
 The iControl REST password 
 monitor_type 
 The type of monitor to create 
 name 
 The name of the monitor to create 
 kwargs 
 [ arg=val ] ... 
 Consult F5 BIGIP user guide for specific options for each monitor type. 
 Typically, tmsh arg names are used.'"
"def sample_from_model(hps, logdir, traindir): 
    hps.batch_size = 100 
   with tf.Graph().as_default(): 
      with tf.device('/cpu:0'): 
         with tf.variable_scope('model') as var_scope: 
            eval_model = RealNVP(hps, sampling=True) 
            summary_writer = tf.summary.FileWriter(logdir) 
            var_scope.reuse_variables() 
            summary_op = tf.summary.merge_all() 
         saver = tf.train.Saver() 
         sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) 
         coord = tf.train.Coordinator() 
         threads = tf.train.start_queue_runners(sess=sess, coord=coord) 
         previous_global_step = 0 
         initialized = False 
         with sess.as_default(): 
            while True: 
               ckpt_state = tf.train.get_checkpoint_state(traindir) 
               if (not (ckpt_state and ckpt_state.model_checkpoint_path)): 
                  if (not initialized): 
                     print ('No   model   to   eval   yet   at   %s' % traindir) 
                     time.sleep(30) 
                     continue 
               else: 
                  print ('Loading   file   %s' % ckpt_state.model_checkpoint_path) 
                  saver.restore(sess, ckpt_state.model_checkpoint_path) 
               current_step = tf.train.global_step(sess, eval_model.step) 
               if (current_step == previous_global_step): 
                  print 'Waiting   for   the   checkpoint   to   be   updated.' 
                  time.sleep(30) 
                  continue 
               previous_global_step = current_step 
               fetches = [summary_op] 
               outputs = sess.run(fetches) 
               summary_writer.add_summary(outputs[0], current_step) 
         coord.request_stop() 
         coord.join(threads)"," 'Sample from a trained model. 
 :param hps: Hyperparameters 
 :param logdir: Path to the log directory 
 :param traindir: Path to the training directory'",'Sampling.'
"@pytest.mark.parametrize('fast_reader', [True, False, 'force']) 
 def test_read_rdb_wrong_type(fast_reader): 
    table = 'col1 DCTB col2\nN DCTB N\n1 DCTB Hello' 
   with pytest.raises(ValueError): 
      ascii.read(table, Reader=ascii.Rdb, fast_reader=fast_reader)", 'Test the wrong type of table','Read RDB data with inconstent data type (except failure)'
"def get_arg_text(ob): 
    arg_text = '' 
   if (ob is not None): 
      arg_offset = 0 
      if (type(ob) in (types.ClassType, types.TypeType)): 
         fob = _find_constructor(ob) 
         if (fob is None): 
            fob = (lambda : None) 
         else: 
            arg_offset = 1 
      elif (type(ob) == types.MethodType): 
         fob = ob.im_func 
         arg_offset = 1 
      else: 
         fob = ob 
      if (type(fob) in [types.FunctionType, types.LambdaType]): 
         argcount = fob.func_code.co_argcount 
         real_args = fob.func_code.co_varnames[arg_offset:argcount] 
         defaults = (fob.func_defaults or []) 
         defaults = list(map((lambda name: ('=%s' % repr(name))), defaults)) 
         defaults = (([''] * (len(real_args) - len(defaults))) + defaults) 
         items = map((lambda arg, dflt: (arg + dflt)), real_args, defaults) 
         if (fob.func_code.co_flags & 4): 
            items.append('...') 
         if (fob.func_code.co_flags & 8): 
            items.append('***') 
         arg_text = ',   '.join(items) 
         arg_text = ('(%s)' % re.sub('\\.\\d+', '<tuple>', arg_text)) 
      doc = getattr(ob, '__doc__', '') 
      if doc: 
         doc = doc.lstrip() 
         pos = doc.find('\n') 
         if ((pos < 0) or (pos > 70)): 
            pos = 70 
         if arg_text: 
            arg_text += '\n' 
         arg_text += doc[:pos] 
   return arg_text", 'Return a string that describes the argument to a function.','Get a string describing the arguments for the given object'
"@sopel.module.commands(u'reload') 
 @sopel.module.priority(u'low') 
 @sopel.module.thread(False) 
 def pm_f_reload(bot, trigger): 
    if trigger.is_privmsg: 
      f_reload(bot, trigger)", 'Reload the module','Wrapper for allowing delivery of .reload command via PM'
"def _safeFormat(fmtString, fmtDict): 
    try: 
      text = (fmtString % fmtDict) 
   except KeyboardInterrupt: 
      raise 
   except: 
      try: 
         text = ('Invalid   format   string   or   unformattable   object   in   log   message:   %r,   %s' % (fmtString, fmtDict)) 
      except: 
         try: 
            text = ('UNFORMATTABLE   OBJECT   WRITTEN   TO   LOG   with   fmt   %r,   MESSAGE   LOST' % (fmtString,)) 
         except: 
            text = 'PATHOLOGICAL   ERROR   IN   BOTH   FORMAT   STRING   AND   MESSAGE   DETAILS,   MESSAGE   LOST' 
   return text"," 'Format a string with a format string and dict. 
 :param fmtString: The format string. 
 :param fmtDict: The dict to format. 
 :return: The formatted string.'","'Try to format the string C{fmtString} using C{fmtDict} arguments, 
 swallowing all errors to always return a string.'"
"def getInsetLoopsFromLoop(loop, radius, thresholdRatio=0.9): 
    if (radius == 0.0): 
      return [loop] 
   isInset = (radius > 0) 
   insetLoops = [] 
   isLoopWiddershins = euclidean.isWiddershins(loop) 
   arounds = getAroundsFromLoop(loop, radius, thresholdRatio) 
   for around in arounds: 
      leftPoint = euclidean.getLeftPoint(around) 
      shouldBeWithin = (isInset == isLoopWiddershins) 
      if (euclidean.isPointInsideLoop(loop, leftPoint) == shouldBeWithin): 
         if (isLoopWiddershins != euclidean.isWiddershins(around)): 
            around.reverse() 
         insetLoops.append(around) 
   return insetLoops"," 'Returns a list of inset loops from the given loop. 
 :param loop: The loop to get the inset loops from 
 :type loop: :class:`~pymatgen.analysis.loops.Loop` 
 :param radius: The radius of the inset 
 :type radius: float 
 :param thresholdRatio: The threshold ratio 
 :type thresholdRatio: float 
 :return: A list of :class:`~pymatgen.analysis.loops.Loop` objects 
 :rtype: list(:class:`~pymatgen.analysis.loops.Loop`)'","'Get the inset loops, which might overlap.'"
"def testInterface(algo): 
    emptyalgo = algo() 
   try: 
      emptyalgo.learn(0) 
      return 'Failed   to   throw   missing   evaluator   error?' 
   except AssertionError: 
      pass 
   emptyalgo.setEvaluator(sf, xa1) 
   emptyalgo.learn(0) 
   algo(sf, xa1) 
   algo(sf, xa100) 
   algo(sf, numParameters=2) 
   try: 
      algo(sf) 
      return 'Failed   to   throw   unknown   dimension   error' 
   except ValueError: 
      pass 
   algo(ife1) 
   algo(ife2, pc2) 
   return True"," 'Test that an algorithm throws the right error when given the wrong 
 interface.'","'Tests whether the algorithm is properly implementing the 
 correct Blackbox-optimization interface.'"
"def build_api_error(message, **kwargs): 
    return {'developer_message': message.format(**kwargs), 'user_message': _(message).format(**kwargs)}", 'Build a JSON-API error object.',"'Build an error dict corresponding to edX API conventions. 
 Args: 
 message (string): The string to use for developer and user messages. 
 The user message will be translated, but for this to work message 
 must have already been scraped. ugettext_noop is useful for this. 
 **kwargs: format parameters for message'"
"def p_definition_token(p): 
    for i in p[3]: 
      if (i[0] not in '\'""'): 
         tokenlist.append(i) 
   if (p[1] == '%left'): 
      preclist.append((('left',) + tuple(p[3]))) 
   elif (p[1] == '%right'): 
      preclist.append((('right',) + tuple(p[3]))) 
   elif (p[1] == '%nonassoc'): 
      preclist.append((('nonassoc',) + tuple(p[3])))", 'Parses a definition token.','definition : toktype opttype idlist optsemi'
"def win32_clipboard_get(): 
    try: 
      import win32clipboard 
   except ImportError: 
      raise TryNext('Getting   text   from   the   clipboard   requires   the   pywin32   extensions:   http://sourceforge.net/projects/pywin32/') 
   win32clipboard.OpenClipboard() 
   try: 
      text = win32clipboard.GetClipboardData(win32clipboard.CF_UNICODETEXT) 
   except (TypeError, win32clipboard.error): 
      try: 
         text = win32clipboard.GetClipboardData(win32clipboard.CF_TEXT) 
         text = py3compat.cast_unicode(text, py3compat.DEFAULT_ENCODING) 
      except (TypeError, win32clipboard.error): 
         raise ClipboardEmpty 
   finally: 
      win32clipboard.CloseClipboard() 
   return text"," 'Get text from the Windows clipboard. 
 This function requires the pywin32 extensions to be installed. 
 See http://sourceforge.net/projects/pywin32/ 
 :rtype: unicode'","'Get the current clipboard\'s text on Windows. 
 Requires Mark Hammond\'s pywin32 extensions.'"
"def _setup_fixtures(doctest_item): 
    def func(): 
      pass 
   doctest_item.funcargs = {} 
   fm = doctest_item.session._fixturemanager 
   doctest_item._fixtureinfo = fm.getfixtureinfo(node=doctest_item, func=func, cls=None, funcargs=False) 
   fixture_request = FixtureRequest(doctest_item) 
   fixture_request._fillfixtures() 
   return fixture_request"," 'Setup fixtures for a doctest item. 
 :param doctest_item: The doctest item. 
 :return: A FixtureRequest instance.'",'Used by DoctestTextfile and DoctestItem to setup fixture information.'
"def uniq_stable(elems): 
    seen = set() 
   return [x for x in elems if ((x not in seen) and (not seen.add(x)))]", 'Returns a list of elements that are unique and stable under insertion.',"'uniq_stable(elems) -> list 
 Return from an iterable, a list of all the unique elements in the input, 
 but maintaining the order in which they first appear. 
 Note: All elements in the input must be hashable for this routine 
 to work, as it internally uses a set for efficiency reasons.'"
"def migrate_cohort_settings(course): 
    (cohort_settings, created) = CourseCohortsSettings.objects.get_or_create(course_id=course.id, defaults={'is_cohorted': course.is_cohorted, 'cohorted_discussions': list(course.cohorted_discussions), 'always_cohort_inline_discussions': course.always_cohort_inline_discussions}) 
   if created: 
      manual_cohorts = CourseUserGroup.objects.filter(course_id=course.id, group_type=CourseUserGroup.COHORT).exclude(name__in=course.auto_cohort_groups) 
      for cohort in manual_cohorts: 
         CourseCohort.create(course_user_group=cohort) 
      for group_name in course.auto_cohort_groups: 
         CourseCohort.create(cohort_name=group_name, course_id=course.id, assignment_type=CourseCohort.RANDOM) 
   return cohort_settings", 'Migrate cohort settings from old to new format.',"'Migrate all the cohort settings associated with this course from modulestore to mysql. 
 After that we will never touch modulestore for any cohort related settings.'"
"@register.tag 
 def app_reverse(parser, token): 
    bits = token.split_contents() 
   if (len(bits) < 3): 
      raise TemplateSyntaxError((u""'%s'   takes   at   least   two   arguments   (path   to   a   view   and   a   urlconf)"" % bits[0])) 
   viewname = parser.compile_filter(bits[1]) 
   urlconf = parser.compile_filter(bits[2]) 
   args = [] 
   kwargs = {} 
   asvar = None 
   bits = bits[3:] 
   if ((len(bits) >= 2) and (bits[(-2)] == u'as')): 
      asvar = bits[(-1)] 
      bits = bits[:(-2)] 
   if len(bits): 
      for bit in bits: 
         match = kwarg_re.match(bit) 
         if (not match): 
            raise TemplateSyntaxError(u'Malformed   arguments   to   app_reverse   tag') 
         (name, value) = match.groups() 
         if name: 
            kwargs[name] = parser.compile_filter(value) 
         else: 
            args.append(parser.compile_filter(value)) 
   return AppReverseNode(viewname, urlconf, args, kwargs, asvar)"," 'Reverse a URL to a view. 
 The first argument is the name of the view to reverse. 
 The second argument is the URLconf to use. 
 The third argument is the list of arguments to pass to the view. 
 The fourth argument is the list of keyword arguments to pass to the view. 
 The fifth argument is the name of the variable to assign to the view 
 output, if any. 
 Example: 
 {% load static %} 
 {% app_reverse ""static"" ""static/"" %} 
 {% app_reverse ""static"" ""static/"" as ""static_url"" %} 
 {% app_reverse ""static"" ""static/"" as ""static_url"" %} 
 {% app_reverse ""static"" ""static/"" as ""static_url"" %} 
 {% app_reverse ""static"" ""static/"" as ""static_url"" %} 
 {% app_reverse ""static"" ""static/"" as ""static_url"" %} 
 {% app_reverse ""static"" ""static/"" as ""static_url"" %} 
 {% app_reverse ""static"" ""static/"" as ""static_url"" %} 
 {% app_reverse ""static""","'Returns an absolute URL for applications integrated with ApplicationContent 
 The tag mostly works the same way as Django\'s own {% url %} tag:: 
 {% load applicationcontent_tags %} 
 {% app_reverse ""mymodel_detail"" ""myapp.urls"" arg1 arg2 %} 
 or 
 {% load applicationcontent_tags %} 
 {% app_reverse ""mymodel_detail"" ""myapp.urls"" name1=value1 %} 
 The first argument is a path to a view. The second argument is the URLconf 
 under which this app is known to the ApplicationContent. The second 
 argument may also be a request object if you want to reverse an URL 
 belonging to the current application content. 
 Other arguments are space-separated values that will be filled in place of 
 positional and keyword arguments in the URL. Don\'t mix positional and 
 keyword arguments. 
 If you want to store the URL in a variable instead of showing it right away 
 you can do so too:: 
 {% app_reverse ""mymodel_detail"" ""myapp.urls"" arg1 arg2 as url %}'"
"@register.filter 
 def break_long_headers(header): 
    if ((len(header) > 160) and (u',' in header)): 
      header = mark_safe((u'<br>   ' + u',   <br>'.join(header.split(u',')))) 
   return header", 'Filter long headers.',"'Breaks headers longer than 160 characters (~page length) 
 when possible (are comma separated)'"
"def _ensure_exists(name, path=None): 
    if (not exists(name, path=path)): 
      raise CommandExecutionError(""Container   '{0}'   does   not   exist"".format(name))", 'Ensure that a container exists.','Raise an exception if the container does not exist'
"def _convert_java_pattern_to_python(pattern): 
    s = list(pattern) 
   i = 0 
   while (i < (len(s) - 1)): 
      c = s[i] 
      if ((c == '$') and (s[(i + 1)] in '0123456789')): 
         s[i] = '\\' 
      elif ((c == '\\') and (s[(i + 1)] == '$')): 
         s[i] = '' 
         i += 1 
      i += 1 
   return pattern[:0].join(s)"," 'Convert a Java pattern to a Python pattern. 
 :param pattern: A Java pattern to convert. 
 :returns: The converted Python pattern.'",'Convert a replacement pattern from the Java-style `$5` to the Python-style `\5`.'
"def rgConv(inpedfilepath, outhtmlname, outfilepath): 
    recode = {'A': '1', 'C': '2', 'G': '3', 'T': '4', 'N': '0', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4'} 
   basename = os.path.split(inpedfilepath)[(-1)] 
   inmap = ('%s.map' % inpedfilepath) 
   inped = ('%s.ped' % inpedfilepath) 
   outf = ('%s.ped' % basename) 
   outfpath = os.path.join(outfilepath, outf) 
   try: 
      mf = open(inmap, 'r') 
   except: 
      sys.stderr.write(('%s   cannot   open   inmap   file   %s   -   do   you   have   permission?\n' % (prog, inmap))) 
      sys.exit(1) 
   try: 
      rsl = [x.split()[1] for x in mf] 
   except: 
      sys.stderr.write(('##   cannot   parse   %s' % inmap)) 
      sys.exit(1) 
   try: 
      os.makedirs(outfilepath) 
   except: 
      pass 
   head = '   '.join(rsl) 
   pedf = open(inped, 'r') 
   o = open(outfpath, 'w', (2 ** 20)) 
   o.write(head) 
   o.write('\n') 
   for (i, row) in enumerate(pedf): 
      if (i == 0): 
         lrow = row.split() 
         try: 
            [int(x) for x in lrow[10:50]] 
         except: 
            dorecode = 1 
      if dorecode: 
         lrow = row.strip().split() 
         p = lrow[:6] 
         g = lrow[6:] 
         gc = [recode.get(z, '0') for z in g] 
         lrow = (p + gc) 
         row = ('%s\n' % '   '.join(lrow)) 
      o.write(row) 
   o.close()"," 'Convert an input ped file to a ped file with 
 the genome and transcripts re-encoded.'",'convert linkage ped/map to fbat'
"def format_correlation_info(corr_coeff, param_p_val, nonparam_p_val, conf_interval, num_permutations, header=''): 
    result = '' 
   if (header != ''): 
      result += (header + '\n') 
   result += ('Correlation   coefficient DCTB Parametric   p-value DCTB ' + 'Nonparametric   p-value DCTB CI   (lower) DCTB CI   (upper)\n') 
   if (num_permutations > 0): 
      nonparam_p_val_str = format_p_value_for_num_iters(nonparam_p_val, num_permutations) 
   else: 
      nonparam_p_val_str = 'N/A' 
   if (conf_interval == (None, None)): 
      conf_interval_str = 'N/A DCTB N/A' 
   else: 
      conf_interval_str = ('%.4f DCTB %.4f' % conf_interval) 
   result += ('%.4f DCTB %.4f DCTB %s DCTB %s\n' % (corr_coeff, param_p_val, nonparam_p_val_str, conf_interval_str)) 
   return result"," 'Format correlation info into a string for display. 
 Parameters 
 corr_coeff : float 
 The correlation coefficient. 
 param_p_val : float 
 The p-value for the parametric test. 
 nonparam_p_val : float 
 The p-value for the nonparametric test. 
 conf_interval : tuple 
 The confidence interval for the correlation coefficient. 
 num_permutations : int 
 The number of permutations for the nonparametric test. 
 header : str 
 The header to prepend to the results. 
 Returns 
 str 
 The formatted results.'","'Formats correlation information to be suitable for writing to a file. 
 Returns a string containing a header and a single line (with a newline at 
 the end) that has the input correlation information in tab-separated 
 format, with nonparametric p-value formatted according to the number of 
 permutations. 
 If the confidence interval is not valid for this dataset (i.e. the 
 input CI is (None, None)), the confidence interval will be formatted as 
 \'N/A\' for both lower and upper endpoints. 
 Arguments: 
 corr_coeff - the correlation coefficient (a float) 
 param_p_val - the parametric p-value (a float) 
 nonparam_p_val - the nonparametric p-value (a float) 
 conf_interval - a tuple containing the lower and upper bounds of the 
 confidence interval 
 num_permutations - the number of permutations that were used to 
 calculate the nonparametric p-value. Will be used to format the 
 correct number of digits for this p-value. If less than 1, the 
 p-value will be \'N/A\' 
 header - if provided, this string will be inserted at the beginning of 
 the returned string. For example, might be useful to add a comment 
 describing what correlation coefficient was used. This string does 
 not need to contain a newline at the end'"
"def test_mark_done(pg_xlog): 
    seg = make_segment(1, explicit=False) 
   pg_xlog.touch(seg.name, '.ready') 
   seg.mark_done()", 'Test mark_done.','Check non-explicit segments can be .mark_done\'d.'
"def dict_union(*dicts, **kwargs): 
    dicts = list(dicts) 
   if (dicts and isinstance(dicts[0], OrderedDict)): 
      result = OrderedDict() 
   else: 
      result = {} 
   for d in (list(dicts) + [kwargs]): 
      duplicate_keys = (set(result.keys()) & set(d.keys())) 
      if duplicate_keys: 
         raise ValueError('The   following   keys   have   duplicate   entries:   {}'.format(',   '.join((str(key) for key in duplicate_keys)))) 
      result.update(d) 
   return result"," 'Merge the contents of multiple dicts. 
 If any of the dicts are OrderedDicts, the result will be an OrderedDict. 
 If the result is an OrderedDict, the keys will be sorted in the same order 
 as the input dicts. 
 If any of the dicts are not OrderedDicts, the keys will be sorted in the 
 order of the input dicts. 
 The keys of the input dicts are not sorted. 
 :param dicts: The dicts to merge. 
 :param kwargs: Any additional keyword arguments to merge. 
 :return: A new dict with the merged contents. 
 :rtype: dict'","'Return union of a sequence of disjoint dictionaries. 
 Parameters 
 dicts : dicts 
 A set of dictionaries with no keys in common. If the first 
 dictionary in the sequence is an instance of `OrderedDict`, the 
 result will be OrderedDict. 
 \*\*kwargs 
 Keywords and values to add to the resulting dictionary. 
 Raises 
 ValueError 
 If a key appears twice in the dictionaries or keyword arguments.'"
"def disabled(name, **kwargs): 
    return (not enabled(name, **kwargs))"," 'Returns a disabled version of the given function. 
 This is useful for testing. 
 :param name: the name of the function to disable. 
 :param kwargs: any keyword arguments to pass to the function. 
 :return: a disabled version of the function.'","'Return True if the named service is enabled, false otherwise 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' service.disabled <service name>'"
"def make_path_searcher(path_generator, target_predicate, target_normalizer, extra_paths, **kwargs): 
    def path_searcher(target, extra_dirs=extra_paths): 
      matches = itertools.ifilter(target_predicate, path_generator(target, extra_dirs, **kwargs)) 
      paths = itertools.imap(target_normalizer, matches) 
      return next(paths, '') 
   return path_searcher"," 'Returns a function that searches the given path generator for paths that 
 match the given target predicate. The function returns the first path that 
 matches or an empty string if no paths match. 
 The function takes the following arguments: 
 :param path_generator: A function that takes a target and a list of extra 
 directories and returns a generator of paths. 
 :param target_predicate: A function that takes a path and returns a boolean 
 indicating if the path matches the target. 
 :param target_normalizer: A function that takes a path and returns the 
 normalized path. 
 :param extra_paths: A list of extra directories to search. 
 :param kwargs: Any additional keyword arguments to pass to the path 
 generator. 
 :returns: A function that takes a target and returns a path.'","'Universal search function generator using lazy evaluation. 
 Generate a function that will iterate over all the paths from path_generator using 
 target_predicate to filter matching paths.  Each matching path is then noramlized by target_predicate. 
 Only the first match is returned. 
 :param path_generator: all paths to test with target_predicate 
 :type path_generator: iterator 
 :param target_predicate: boolean function that tests a given path 
 :type target_predicate: function 
 :param target_normalizer: function that transforms a matching path to some noramlized form 
 :type target_normalizer: function 
 :param extra_paths: extra paths to pass to the path_generator 
 :type extra_paths: iterator 
 :return: the path searching function 
 :rtype:  function'"
"def validate(): 
    cmd = 'monit   validate' 
   return (not __salt__['cmd.retcode'](cmd, python_shell=False))", 'Validate monit configuration',"'.. versionadded:: 2016.3.0 
 Check all services 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' monit.validate'"
"def update_transferability(site_id=None): 
    db = current.db 
   s3db = current.s3db 
   now = current.request.utcnow 
   from dateutil.relativedelta import relativedelta 
   TODAY = now.date() 
   ONE_YEAR_AGO = (now - relativedelta(years=1)).date() 
   ptable = s3db.pr_person 
   ctable = s3db.dvr_case 
   stable = s3db.dvr_case_status 
   ftable = s3db.dvr_case_flag 
   cftable = s3db.dvr_case_flag_case 
   utable = s3db.cr_shelter_unit 
   rtable = s3db.cr_shelter_registration 
   ttable = s3db.dvr_case_appointment_type 
   atable = s3db.dvr_case_appointment 
   COMPLETED = 4 
   NOT_REQUIRED = 7 
   query = (ctable.deleted != True) 
   db(query).update(transferable=False, household_transferable=False) 
   query = (((ttable.name == 'Reported   Transferable') | (ttable.name == 'Transfer')) & (ttable.deleted != True)) 
   rows = db(query).select(ttable.id, limitby=(0, 2)) 
   if rows: 
      transferability_complete = set((row.id for row in rows)) 
   else: 
      transferability_complete = None 
   query = (((stable.is_closed == False) | (stable.is_closed == None)) & (stable.deleted != True)) 
   rows = db(query).select(stable.id) 
   if rows: 
      OPEN = set((row.id for row in rows)) 
   else: 
      OPEN = None 
   query = ((ftable.is_not_transferable == True) & (ftable.deleted != True)) 
   rows = db(query).select(ftable.id) 
   if rows: 
      NOT_TRANSFERABLE = set((row.id for row in rows)) 
   else: 
      NOT_TRANSFERABLE = None 
   age_groups = {'children': (None, 15, 'mandatory_children', None), 'adolescents': (15, 18, 'mandatory_adolescents', None), 'adults': (18, None, 'mandatory_adults', 4)} 
   left = [stable.on((stable.id == ctable.status_id)), ptable.on((ptable.id == ctable.person_id)), rtable.on(((rtable.person_id == ptable.id) & (rtable.deleted != True))), utable.on((utable.id == rtable.shelter_unit_id))] 
   if transferability_complete: 
      tctable = atable.with_alias('transferability_complete') 
      tcjoin = tctable.on((((((((tctable.person_id == ctable.person_id) & tctable.type_id.belongs(transferability_complete)) & (tctable.deleted != True)) & (tctable.date != None)) & (tctable.date >= ONE_YEAR_AGO)) & (tctable.date <= TODAY)) & (tctable.status == COMPLETED))) 
      left.append(tcjoin) 
   if NOT_TRANSFERABLE: 
      cfjoin = cftable.on((((cftable.person_id == ctable.person_id) & cftable.flag_id.belongs(NOT_TRANSFERABLE)) & (cftable.deleted != True))) 
      left.append(cfjoin) 
   result = 0 
   for age_group in age_groups: 
      (min_age, max_age, appointment_flag, maximum_absence) = age_groups[age_group] 
      dob_query = (ptable.date_of_birth != None) 
      if max_age: 
         dob_min = (now - relativedelta(years=max_age)) 
         dob_query &= (ptable.date_of_birth > dob_min) 
      if min_age: 
         dob_max = (now - relativedelta(years=min_age)) 
         dob_query &= (ptable.date_of_birth <= dob_max) 
      case_query = ((ctable.deleted != True) & ((ctable.archived == False) | (ctable.archived == None))) 
      if OPEN: 
         case_query &= ctable.status_id.belongs(OPEN) 
      if site_id: 
         case_query &= (ctable.site_id == site_id) 
      case_query &= ((stable.is_not_transferable == False) | (stable.is_not_transferable == None)) 
      if NOT_TRANSFERABLE: 
         case_query &= (cftable.id == None) 
      case_query &= ((utable.id != None) & ((utable.transitory == False) | (utable.transitory == None))) 
      case_query &= dob_query 
      if transferability_complete: 
         case_query &= (tctable.id == None) 
      if (maximum_absence is not None): 
         if maximum_absence: 
            earliest_check_out_date = (now - relativedelta(days=maximum_absence)) 
            presence_query = ((rtable.registration_status == 2) | ((rtable.registration_status == 3) & (rtable.check_out_date > earliest_check_out_date))) 
         else: 
            presence_query = rtable.registration_status.belongs(2, 3) 
         case_query &= presence_query 
      cases = db(case_query).select(ctable.id, left=left) 
      case_ids = set((case.id for case in cases)) 
      if case_ids: 
         query = ctable.id.belongs(case_ids) 
         aleft = [] 
         if appointment_flag: 
            tquery = ((ttable[appointment_flag] == True) & (ttable.deleted != True)) 
            rows = db(tquery).select(ttable.id) 
            mandatory_appointments = [row.id for row in rows] 
         else: 
            mandatory_appointments = None 
         if mandatory_appointments: 
            for appointment_type_id in mandatory_appointments: 
               alias = ('appointments_%s' % appointment_type_id) 
               atable_ = atable.with_alias(alias) 
               join = atable_.on(((((atable_.person_id == ctable.person_id) & (atable_.type_id == appointment_type_id)) & (atable_.deleted != True)) & (((((atable_.status == COMPLETED) & (atable_.date != None)) & (atable_.date >= ONE_YEAR_AGO)) & (atable_.date <= TODAY)) | (atable_.status == NOT_REQUIRED)))) 
               aleft.append(join) 
               query &= (atable_.id != None) 
            cases = db(query).select(ctable.id, left=aleft) 
            case_ids = set((case.id for case in cases)) 
         success = db(ctable.id.belongs(case_ids)).update(transferable=True) 
         if success: 
            result += success 
   gtable = s3db.pr_group 
   mtable = s3db.pr_group_membership 
   query = (((gtable.group_type == 7) & (gtable.deleted != True)) & (ctable.id != None)) 
   left = [mtable.on(((mtable.group_id == gtable.id) & (mtable.deleted != True))), ctable.on(((ctable.person_id == mtable.person_id) & (ctable.transferable == True)))] 
   members = ctable.id.count() 
   rows = db(query).select(gtable.id, groupby=gtable.id, having=(members == 0), left=left) 
   group_ids = set((row.id for row in rows)) 
   open_case = ((ctable.archived != True) & (ctable.deleted != True)) 
   if OPEN: 
      open_case = (ctable.status_id.belongs(OPEN) & open_case) 
   if group_ids: 
      query &= (~ gtable.id.belongs(group_ids)) 
   left = [mtable.on(((mtable.group_id == gtable.id) & (mtable.deleted != True))), ctable.on((((ctable.person_id == mtable.person_id) & open_case) & ((ctable.transferable == False) | (ctable.transferable == None))))] 
   if transferability_complete: 
      left.append(tcjoin) 
      query &= (tctable.id == None) 
   rows = db(query).select(gtable.id, groupby=gtable.id, left=left) 
   group_ids |= set((row.id for row in rows)) 
   ftable = mtable.with_alias('family') 
   left = [mtable.on((((mtable.person_id == ctable.person_id) & mtable.group_id.belongs(group_ids)) & (mtable.deleted != True))), gtable.on(((((ftable.person_id == ctable.person_id) & (ftable.deleted != True)) & (gtable.id == ftable.group_id)) & (gtable.group_type == 7)))] 
   query = ((mtable.id == None) & (ctable.deleted != True)) 
   families = gtable.id.count() 
   required = ((families > 0) | (ctable.transferable == True)) 
   rows = db(query).select(ctable.id, groupby=ctable.id, having=required, left=left) 
   case_ids = set((row.id for row in rows)) 
   if case_ids: 
      db(ctable.id.belongs(case_ids)).update(household_transferable=True) 
   return result"," 'Update transferability. 
 :param site_id: site_id of the cases 
 :return: number of cases updated'","'Update transferability status of all cases, to be called either 
 from scheduler task or manually through custom controller. 
 @param site_id: the site to check for transferability of cases'"
"def project_indicator_progress_report(r, **attr): 
    if ((r.representation == 'html') and (r.name == 'project')): 
      T = current.T 
      db = current.db 
      s3db = current.s3db 
      project_id = r.id 
      item = TABLE(_class='project_indicator_progress_report') 
      output = dict(item=item) 
      output['title'] = T('Monthly   Progress   by   Indicator') 
      output['subtitle'] = ('%s:   %s' % (T('Project'), r.record.name)) 
      if ('rheader' in attr): 
         rheader = attr['rheader'](r) 
         if rheader: 
            output['rheader'] = rheader 
      current.response.view = 'simple.html' 
      return output 
   else: 
      raise HTTP(405, current.ERROR.BAD_METHOD)", 'Project Indicator Progress Report','@ToDo: Display the Progress of a Project'
"def find_module(module, paths=None): 
    parts = module.split('.') 
   while parts: 
      part = parts.pop(0) 
      (f, path, (suffix, mode, kind)) = info = imp.find_module(part, paths) 
      if (kind == PKG_DIRECTORY): 
         parts = (parts or ['__init__']) 
         paths = [path] 
      elif parts: 
         raise ImportError((""Can't   find   %r   in   %s"" % (parts, module))) 
   return info"," 'Find the module corresponding to the given module name. 
 If no module is found, raise ImportError. 
 The path to the module is returned as a tuple (module, path). 
 If the module was not found, then the path is None.'","'Just like \'imp.find_module()\', but with package support'"
"def make_layout(doc, meta, format_data=None): 
    (layout, page) = ([], []) 
   layout.append(page) 
   if format_data: 
      if (format_data[0].get(u'fieldname') == u'print_heading_template'): 
         doc.print_heading_template = format_data[0].get(u'options') 
         format_data = format_data[1:] 
   def get_new_section(): 
      return {u'columns': [], u'has_data': False} 
   def append_empty_field_dict_to_page_column(page): 
      u'   append   empty   columns   dict   to   page   layout   ' 
      if (not page[(-1)][u'columns']): 
         page[(-1)][u'columns'].append({u'fields': []}) 
   for df in (format_data or meta.fields): 
      if format_data: 
         df = frappe._dict(df) 
         if df.fieldname: 
            original = meta.get_field(df.fieldname) 
            if original: 
               newdf = original.as_dict() 
               newdf.update(df) 
               df = newdf 
         df.print_hide = 0 
      if ((df.fieldtype == u'Section   Break') or (page == [])): 
         if (len(page) > 1): 
            if (page[(-1)][u'has_data'] == False): 
               del page[(-1)] 
         section = get_new_section() 
         if ((df.fieldtype == u'Section   Break') and df.label): 
            section[u'label'] = df.label 
         page.append(section) 
      elif (df.fieldtype == u'Column   Break'): 
         page[(-1)][u'columns'].append({u'fields': []}) 
      else: 
         append_empty_field_dict_to_page_column(page) 
      if ((df.fieldtype == u'HTML') and df.options): 
         doc.set(df.fieldname, True) 
      if (is_visible(df, doc) and has_value(df, doc)): 
         append_empty_field_dict_to_page_column(page) 
         page[(-1)][u'columns'][(-1)][u'fields'].append(df) 
         page[(-1)][u'has_data'] = True 
         if (df.fieldtype == u'Table'): 
            df.rows = [] 
            df.start = 0 
            df.end = None 
            for (i, row) in enumerate(doc.get(df.fieldname)): 
               if row.get(u'page_break'): 
                  df.end = i 
                  page = [get_new_section()] 
                  layout.append(page) 
                  append_empty_field_dict_to_page_column(page) 
                  df = copy.copy(df) 
                  df.start = i 
                  df.end = None 
                  page[(-1)][u'columns'][(-1)][u'fields'].append(df) 
   return layout"," 'Returns the layout of the report as a list of lists. 
 :param doc: docstring of the report 
 :param meta: meta data of the report 
 :param format_data: format data of the report 
 :return: layout of the report'","'Builds a hierarchical layout object from the fields list to be rendered 
 by `standard.html` 
 :param doc: Document to be rendered. 
 :param meta: Document meta object (doctype). 
 :param format_data: Fields sequence and properties defined by Print Format Builder.'"
"def create_rpc(deadline=None, callback=None): 
    if (deadline is None): 
      deadline = get_default_fetch_deadline() 
   return apiproxy_stub_map.UserRPC('urlfetch', deadline, callback)"," 'Create a new RPC. 
 :param deadline: The deadline for the request. 
 :param callback: A function to be called when the request is done. 
 :returns: A new RPC. 
 :rtype: UserRPC'","'Creates an RPC object for use with the urlfetch API. 
 Args: 
 deadline: Optional deadline in seconds for the operation; the default 
 is a system-specific deadline (typically 5 seconds). 
 callback: Optional callable to invoke on completion. 
 Returns: 
 An apiproxy_stub_map.UserRPC object specialized for this service.'"
"def get_default_access_key_id(): 
    access_key_id_script = AWS_ACCOUNTS['default'].ACCESS_KEY_ID_SCRIPT.get() 
   return (access_key_id_script or get_s3a_access_key())"," 'Get the default access key ID for S3A. 
 This is the access key ID that is used by the default S3A 
 client. 
 :return: The default access key ID. 
 :rtype: str'","'Attempt to set AWS access key ID from script, else core-site, else None'"
"def cache_relation(descriptor, timeout=None): 
    rel = descriptor.related 
   related_name = ('%s_cache' % rel.field.related_query_name()) 
   @property 
   def get(self): 
      '\n                        Returns   the   cached   value   of   the   related   model   if   found\n                        in   the   cache.   Otherwise   gets   and   caches   the   related   model.\n                        ' 
      try: 
         return getattr(self, descriptor.cache_name) 
      except AttributeError: 
         pass 
      try: 
         return getattr(self, ('_%s_cache' % related_name)) 
      except AttributeError: 
         pass 
      instance = get_instance(rel.model, self.pk, timeout) 
      setattr(self, ('_%s_cache' % related_name), instance) 
      return instance 
   setattr(rel.parent_model, related_name, get) 
   def clear(self): 
      '\n                        Clears   the   cache   of   all   related   models   of   self.\n                        ' 
      delete_instance(rel.model, self) 
   @classmethod 
   def clear_pk(cls, *instances_or_pk): 
      '\n                        Clears   the   cache   of   all   related   models   of\n                        the   provided   instances_or_pk.\n                        ' 
      delete_instance(rel.model, *instances_or_pk) 
   def clear_cache(sender, instance, *args, **kwargs): 
      '\n                        Clears   the   cache   of   all   related   models   of   the\n                        given   instance.\n                        ' 
      delete_instance(rel.model, instance) 
   setattr(rel.parent_model, ('%s_clear' % related_name), clear) 
   setattr(rel.parent_model, ('%s_clear_pk' % related_name), clear_pk) 
   post_save.connect(clear_cache, sender=rel.model, weak=False) 
   post_delete.connect(clear_cache, sender=rel.model, weak=False)"," 'Caches the related model of the given descriptor. 
 If the related model is not cached, it is retrieved from the database and 
 cached. 
 The cache is cleared when the related model is deleted from the database. 
 The cache is cleared for all related models when the instance is deleted 
 from the database. 
 The cache is cleared for all related models when the instance is saved 
 to the database. 
 This is a convenience method that can be used to cache related models 
 of a model. 
 :param descriptor: the descriptor to cache 
 :param timeout: the timeout to use for caching related models'","'Adds utility methods to a model to obtain related 
 model instances via a cache.'"
"def tuplize(seq): 
    if isinstance(seq, (list, tuple)): 
      return tuple((tuplize(i) for i in seq)) 
   return seq"," 'Tupleize a sequence. 
 :param seq: 
 :type seq: 
 :return: 
 :rtype: 
 :seealso: 
 :meth:`~.tupleize`'",'Turn all nested sequences to tuples in given sequence.'
"@conf.commands.register 
 def tshark(*args, **kargs): 
    sniff(prn=(lambda x: x.display()), *args, **kargs)"," 'Capture traffic from a network interface. 
 Capture traffic from a network interface. 
 The ``*args`` are the network interfaces to capture from. 
 The ``**kargs`` are the arguments to ``sniff``. 
 .. versionadded:: 0.1.0'","'Sniff packets and print them calling pkt.show(), a bit like text wireshark'"
"def make_sparse_random_conv2D(num_nonzero, input_space, output_space, kernel_shape, pad=0, kernel_stride=(1, 1), border_mode='valid', message='', rng=None, partial_sum=None): 
    rng = make_np_rng(rng, default_sparse_seed, which_method=['randn', 'randint']) 
   W = np.zeros((input_space.num_channels, kernel_shape[0], kernel_shape[1], output_space.num_channels)) 
   def random_coord(): 
      return [rng.randint(dim) for dim in W.shape[0:3]] 
   for o in xrange(output_space.num_channels): 
      for i in xrange(num_nonzero): 
         (ch, r, c) = random_coord() 
         while (W[(ch, r, c, o)] != 0): 
            (ch, r, c) = random_coord() 
         W[(ch, r, c, o)] = rng.randn() 
   W = sharedX(W) 
   return Conv2D(filters=W, input_axes=input_space.axes, output_axes=output_space.axes, kernel_stride=kernel_stride, pad=pad, message=message, partial_sum=partial_sum)"," 'Make a random sparse convolution. 
 Parameters 
 num_nonzero : int 
 Number of nonzero weights in the convolution. 
 input_space : InputSpace 
 Input space. 
 output_space : OutputSpace 
 Output space. 
 kernel_shape : int 
 Convolution kernel size. 
 pad : int 
 Padding. 
 kernel_stride : int 
 Convolution stride. 
 border_mode : string 
 Border mode. 
 message : string 
 Message to send to the constructor. 
 rng : np.random.RandomState 
 Random number generator. 
 partial_sum : bool 
 Whether to return a partial sum of the weights. 
 Returns 
 W : Weights 
 Convolution weights. 
 Examples 
 >>> from theano.tensor.sharedvar import sharedX 
 >>> from theano.tensor.sparse import make_sparse_random_conv2D 
 >>> input_space = InputSpace(shape=(2, 3, 3, 3)) 
 >>> output_space = OutputSpace(shape=(2, 3, 3, ","'.. todo:: 
 WRITEME properly 
 Creates a Conv2D with random kernels, where the randomly initialized 
 values are sparse'"
"def infixNotation(baseExpr, opList, lpar=Suppress('('), rpar=Suppress(')')): 
    ret = Forward() 
   lastExpr = (baseExpr | ((lpar + ret) + rpar)) 
   for (i, operDef) in enumerate(opList): 
      (opExpr, arity, rightLeftAssoc, pa) = (operDef + (None,))[:4] 
      if (arity == 3): 
         if ((opExpr is None) or (len(opExpr) != 2)): 
            raise ValueError('if   numterms=3,   opExpr   must   be   a   tuple   or   list   of   two   expressions') 
         (opExpr1, opExpr2) = opExpr 
      thisExpr = Forward() 
      if (rightLeftAssoc == opAssoc.LEFT): 
         if (arity == 1): 
            matchExpr = (FollowedBy((lastExpr + opExpr)) + Group((lastExpr + OneOrMore(opExpr)))) 
         elif (arity == 2): 
            if (opExpr is not None): 
               matchExpr = (FollowedBy(((lastExpr + opExpr) + lastExpr)) + Group((lastExpr + OneOrMore((opExpr + lastExpr))))) 
            else: 
               matchExpr = (FollowedBy((lastExpr + lastExpr)) + Group((lastExpr + OneOrMore(lastExpr)))) 
         elif (arity == 3): 
            matchExpr = (FollowedBy(((((lastExpr + opExpr1) + lastExpr) + opExpr2) + lastExpr)) + Group(((((lastExpr + opExpr1) + lastExpr) + opExpr2) + lastExpr))) 
         else: 
            raise ValueError('operator   must   be   unary   (1),   binary   (2),   or   ternary   (3)') 
      elif (rightLeftAssoc == opAssoc.RIGHT): 
         if (arity == 1): 
            if (not isinstance(opExpr, Optional)): 
               opExpr = Optional(opExpr) 
            matchExpr = (FollowedBy((opExpr.expr + thisExpr)) + Group((opExpr + thisExpr))) 
         elif (arity == 2): 
            if (opExpr is not None): 
               matchExpr = (FollowedBy(((lastExpr + opExpr) + thisExpr)) + Group((lastExpr + OneOrMore((opExpr + thisExpr))))) 
            else: 
               matchExpr = (FollowedBy((lastExpr + thisExpr)) + Group((lastExpr + OneOrMore(thisExpr)))) 
         elif (arity == 3): 
            matchExpr = (FollowedBy(((((lastExpr + opExpr1) + thisExpr) + opExpr2) + thisExpr)) + Group(((((lastExpr + opExpr1) + thisExpr) + opExpr2) + thisExpr))) 
         else: 
            raise ValueError('operator   must   be   unary   (1),   binary   (2),   or   ternary   (3)') 
      else: 
         raise ValueError('operator   must   indicate   right   or   left   associativity') 
      if pa: 
         matchExpr.setParseAction(pa) 
      thisExpr <<= (matchExpr | lastExpr) 
      lastExpr = thisExpr 
   ret <<= lastExpr 
   return ret"," 'Infix notation parser. 
 This parser is a specialized form of the general parser described in 
 :func:`parsimonious.Parser` that allows for the parsing of infix notation 
 expressions. 
 The ``baseExpr`` is the base expression from which the infix notation 
 expressions are formed.  The ``opList`` is a list of tuples, where each 
 tuple contains the operator name, the operator precedence, and the 
 associativity of the operator.  The ``lpar`` and ``rpar`` are the 
 parentheses to use for the opening and closing of the infix notation 
 expressions. 
 The parser returns a Forward() expression that can be used as a 
 ``parse_func`` in the ``parsimonious.Parser`` class. 
 Examples:: 
 >>> from parsimonious import infixNotation 
 >>> from parsimonious import parse 
 >>> expr = infixNotation(baseExpr=OneOrMore(OneOrMore(OneOrMore(OneOrMore(OneOrMore(OneOrMore(OneOrMore(OneOrMore(OneOrMore(OneOrMore(One","'Helper method for constructing grammars of expressions made up of 
 operators working in a precedence hierarchy.  Operators may be unary or 
 binary, left- or right-associative.  Parse actions can also be attached 
 to operator expressions. 
 Parameters: 
 - baseExpr - expression representing the most basic element for the nested 
 - opList - list of tuples, one for each operator precedence level in the 
 expression grammar; each tuple is of the form 
 (opExpr, numTerms, rightLeftAssoc, parseAction), where: 
 - opExpr is the pyparsing expression for the operator; 
 may also be a string, which will be converted to a Literal; 
 if numTerms is 3, opExpr is a tuple of two expressions, for the 
 two operators separating the 3 terms 
 - numTerms is the number of terms for this operator (must 
 be 1, 2, or 3) 
 - rightLeftAssoc is the indicator whether the operator is 
 right or left associative, using the pyparsing-defined 
 constants C{opAssoc.RIGHT} and C{opAssoc.LEFT}. 
 - parseAction is the parse action to be associated with 
 expressions matching this operator expression (the 
 parse action tuple member may be omitted) 
 - lpar - expression for matching left-parentheses (default=Suppress(\'(\')) 
 - rpar - expression for matching right-parentheses (default=Suppress(\')\'))'"
"def get_account_created(name): 
    ret = _get_account_policy_data_value(name, 'creationTime') 
   unix_timestamp = salt.utils.mac_utils.parse_return(ret) 
   date_text = _convert_to_datetime(unix_timestamp) 
   return date_text"," 'Get the time the account was created. 
 :param name: The name of the account 
 :returns: The time the account was created in the form of a 
 datetime.datetime object'","'Get the date/time the account was created 
 :param str name: the username of the account 
 :return: the date/time the account was created (yyyy-mm-dd hh:mm:ss) 
 :rtype: str 
 :raises: CommandExecutionError on user not found or any other unknown error 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' shadow.get_account_created admin'"
"@commands(u'iplookup', u'ip') 
 @example(u'.ip   8.8.8.8', u'[IP/Host   Lookup]   Hostname:   google-public-dns-a.google.com   |   Location:   United   States   |   Region:   CA   |   ISP:   AS15169   Google   Inc.', re=True, ignore=u'Downloading   GeoIP   database,   please   wait...') 
 def ip(bot, trigger): 
    if (not trigger.group(2)): 
      return bot.reply(u'No   search   term.') 
   query = trigger.group(2) 
   db_path = _find_geoip_db(bot) 
   if (db_path is False): 
      LOGGER.error(u""Can't   find   (or   download)   usable   GeoIP   database"") 
      bot.say(u""Sorry,   I   don't   have   a   GeoIP   database   to   use   for   this   lookup"") 
      return False 
   geolite_city_filepath = os.path.join(_find_geoip_db(bot), u'GeoLiteCity.dat') 
   geolite_ASN_filepath = os.path.join(_find_geoip_db(bot), u'GeoIPASNum.dat') 
   gi_city = pygeoip.GeoIP(geolite_city_filepath) 
   gi_org = pygeoip.GeoIP(geolite_ASN_filepath) 
   host = socket.getfqdn(query) 
   response = (u'[IP/Host   Lookup]   Hostname:   %s' % host) 
   try: 
      response += (u'   |   Location:   %s' % gi_city.country_name_by_name(query)) 
   except AttributeError: 
      response += u'   |   Location:   Unknown' 
   except socket.gaierror: 
      return bot.say(u'[IP/Host   Lookup]   Unable   to   resolve   IP/Hostname') 
   region_data = gi_city.region_by_name(query) 
   try: 
      region = region_data[u'region_code'] 
   except KeyError: 
      region = region_data[u'region_name'] 
   if region: 
      response += (u'   |   Region:   %s' % region) 
   isp = gi_org.org_by_name(query) 
   response += (u'   |   ISP:   %s' % isp) 
   bot.say(response)"," 'Lookup an IP address or hostname. 
 :param trigger: The trigger that called this command. 
 :type trigger: :class:`~twisted.words.util.RemainingArguments` 
 :param bot: The bot instance. 
 :type bot: :class:`~twisted.words.twistd.service.BotService` 
 :returns: A :class:`~twisted.words.twistd.service.IServiceMixin` instance.'",'IP Lookup tool'
"def layer_gpx(): 
    tablename = ('%s_%s' % (module, resourcename)) 
   s3db.table(tablename) 
   type = 'GPX' 
   LAYERS = T((TYPE_LAYERS_FMT % type)) 
   ADD_NEW_LAYER = T((ADD_NEW_TYPE_LAYER_FMT % type)) 
   EDIT_LAYER = T((EDIT_TYPE_LAYER_FMT % type)) 
   LIST_LAYERS = T((LIST_TYPE_LAYERS_FMT % type)) 
   NO_LAYERS = T((NO_TYPE_LAYERS_FMT % type)) 
   s3.crud_strings[tablename] = Storage(label_create=ADD_LAYER, title_display=LAYER_DETAILS, title_list=LAYERS, title_update=EDIT_LAYER, label_list_button=LIST_LAYERS, label_delete_button=DELETE_LAYER, msg_record_created=LAYER_ADDED, msg_record_modified=LAYER_UPDATED, msg_record_deleted=LAYER_DELETED, msg_list_empty=NO_LAYERS) 
   def prep(r): 
      if r.interactive: 
         if (r.component_name == 'config'): 
            ltable = s3db.gis_layer_config 
            ltable.base.writable = ltable.base.readable = False 
            if (r.method != 'update'): 
               table = r.table 
               query = ((ltable.layer_id == table.layer_id) & (table.id == r.id)) 
               rows = db(query).select(ltable.config_id) 
               ltable.config_id.requires = IS_ONE_OF(db, 'gis_config.id', '%(name)s', not_filterby='config_id', not_filter_opts=[row.config_id for row in rows]) 
      return True 
   s3.prep = prep 
   def postp(r, output): 
      if (r.interactive and (r.method != 'import')): 
         if (not r.component): 
            inject_enable(output) 
      return output 
   s3.postp = postp 
   output = s3_rest_controller(rheader=s3db.gis_rheader) 
   return output", 'Gpx layer controller','RESTful CRUD controller'
"def _stub_islink(path): 
    return False"," 'Returns True if the path is a symbolic link, False otherwise.'","'Always return \'false\' if the operating system does not support symlinks. 
 @param path: a path string. 
 @type path: L{str} 
 @return: false'"
"def generate_py(bits, randfunc, progress_func=None, e=65537): 
    obj = RSAobj() 
   obj.e = long(e) 
   if progress_func: 
      progress_func('p,q\n') 
   p = q = 1L 
   while (number.size((p * q)) < bits): 
      p = pubkey.getStrongPrime((bits >> 1), obj.e, 1e-12, randfunc) 
      q = pubkey.getStrongPrime((bits - (bits >> 1)), obj.e, 1e-12, randfunc) 
   if (p > q): 
      (p, q) = (q, p) 
   obj.p = p 
   obj.q = q 
   if progress_func: 
      progress_func('u\n') 
   obj.u = pubkey.inverse(obj.p, obj.q) 
   obj.n = (obj.p * obj.q) 
   if progress_func: 
      progress_func('d\n') 
   obj.d = pubkey.inverse(obj.e, ((obj.p - 1) * (obj.q - 1))) 
   assert (bits <= (1 + obj.size())), 'Generated   key   is   too   small' 
   return obj"," 'Generate an RSA key. 
 :param bits: the number of bits in the modulus 
 :type bits: int 
 :param randfunc: a function that returns a random number 
 :type randfunc: function 
 :param progress_func: a function to be called periodically 
 :type progress_func: function 
 :param e: the public exponent 
 :type e: int 
 :returns: an RSA obj 
 :rtype: RSAobj'","'generate(bits:int, randfunc:callable, progress_func:callable, e:int) 
 Generate an RSA key of length \'bits\', public exponent \'e\'(which must be 
 odd), using \'randfunc\' to get random data and \'progress_func\', 
 if present, to display the progress of the key generation.'"
"@register.tag(u'filter') 
 def do_filter(parser, token): 
    (_, rest) = token.contents.split(None, 1) 
   filter_expr = parser.compile_filter((u'var|%s' % rest)) 
   for (func, unused) in filter_expr.filters: 
      filter_name = getattr(func, u'_filter_name', None) 
      if (filter_name in (u'escape', u'safe')): 
         raise TemplateSyntaxError((u'""filter   %s""   is   not   permitted.      Use   the   ""autoescape""   tag   instead.' % filter_name)) 
   nodelist = parser.parse((u'endfilter',)) 
   parser.delete_first_token() 
   return FilterNode(filter_expr, nodelist)"," 'Filter tag. 
 Example: 
 {% filter var|lower %} 
 {% filter var|escape %} 
 {% filter var|safe %} 
 {% filter var|escape|safe %} 
 {% filter var|escape|safe|lower %} 
 {% filter var|escape|safe|lower|upper %} 
 {% filter var|escape|safe|lower|upper|title %} 
 {% filter var|escape|safe|lower|upper|title|urlencode %} 
 {% filter var|escape|safe|lower|upper|title|urlencode|json %} 
 {% filter var|escape|safe|lower|upper|title|urlencode|json|html %} 
 {% filter var|escape|safe|lower|upper|title|urlencode|json|html|urlencode %} 
 {% filter var|escape|safe|lower|upper|title|urlencode|json|html|urlencode|json %} 
 {% filter var|escape|safe|lower|upper|title|urlencode|json|html|urlencode|json|html|urlencode %} 
 {% filter var|escape|safe|lower|upper|title|urlencode|json|html","'Filters the contents of the block through variable filters. 
 Filters can also be piped through each other, and they can have 
 arguments -- just like in variable syntax. 
 Sample usage:: 
 {% filter force_escape|lower %} 
 This text will be HTML-escaped, and will appear in lowercase. 
 {% endfilter %} 
 Note that the ``escape`` and ``safe`` filters are not acceptable arguments. 
 Instead, use the ``autoescape`` tag to manage autoescaping for blocks of 
 template code.'"
"def reshape_text(buffer, from_row, to_row): 
    lines = buffer.text.splitlines(True) 
   lines_before = lines[:from_row] 
   lines_after = lines[(to_row + 1):] 
   lines_to_reformat = lines[from_row:(to_row + 1)] 
   if lines_to_reformat: 
      length = re.search(u'^\\s*', lines_to_reformat[0]).end() 
      indent = lines_to_reformat[0][:length].replace(u'\n', u'') 
      words = u''.join(lines_to_reformat).split() 
      width = ((buffer.text_width or 80) - len(indent)) 
      reshaped_text = [indent] 
      current_width = 0 
      for w in words: 
         if current_width: 
            if (((len(w) + current_width) + 1) > width): 
               reshaped_text.append(u'\n') 
               reshaped_text.append(indent) 
               current_width = 0 
            else: 
               reshaped_text.append(u'   ') 
               current_width += 1 
         reshaped_text.append(w) 
         current_width += len(w) 
      if (reshaped_text[(-1)] != u'\n'): 
         reshaped_text.append(u'\n') 
      buffer.document = Document(text=u''.join(((lines_before + reshaped_text) + lines_after)), cursor_position=len(u''.join((lines_before + reshaped_text))))"," 'Reshape a text buffer to fit in a row of the given size. 
 :param buffer: The text buffer to be reshaped. 
 :param from_row: The row to start reshaping from. 
 :param to_row: The row to end reshaping at. 
 :returns: The text buffer reshaped. 
 :rtype: TextBuffer'","'Reformat text, taking the width into account. 
 `to_row` is included. 
 (Vi \'gq\' operator.)'"
"def _not_a_knot(x, k): 
    x = np.asarray(x) 
   if ((k % 2) != 1): 
      raise ValueError(('Odd   degree   for   now   only.   Got   %s.' % k)) 
   m = ((k - 1) // 2) 
   t = x[(m + 1):((- m) - 1)] 
   t = np.r_[(((x[0],) * (k + 1)), t, ((x[(-1)],) * (k + 1)))] 
   return t", 'Return a knot sequence for a given knot.',"'Given data x, construct the knot vector w/ not-a-knot BC. 
 cf de Boor, XIII(12).'"
"def _defaultFetcher(url): 
    try: 
      r = urlfetch.fetch(url, method=urlfetch.GET) 
   except urlfetch.Error as e: 
      log.warn((u'Error   opening   url=%r:   %s' % (url, e)), error=IOError) 
   else: 
      if (r.status_code == 200): 
         mimetype = 'application/octet-stream' 
         try: 
            (mimetype, params) = cgi.parse_header(r.headers['content-type']) 
            encoding = params['charset'] 
         except KeyError: 
            encoding = None 
         if (mimetype != u'text/css'): 
            log.error((u'Expected   ""text/css""   mime   type   for   url   %r   but   found:   %r' % (url, mimetype)), error=ValueError) 
         return (encoding, r.content) 
      else: 
         log.warn((u'Error   opening   url=%r:   HTTP   status   %s' % (url, r.status_code)), error=IOError)"," 'Fetch a URL and return the encoding and content. 
 :param url: URL to fetch. 
 :return: (encoding, content).'","'uses GoogleAppEngine (GAE) 
 fetch(url, payload=None, method=GET, headers={}, allow_truncated=False) 
 Response 
 content 
 The body content of the response. 
 content_was_truncated 
 True if the allow_truncated parameter to fetch() was True and 
 the response exceeded the maximum response size. In this case, 
 the content attribute contains the truncated response. 
 status_code 
 The HTTP status code. 
 headers 
 The HTTP response headers, as a mapping of names to values. 
 Exceptions 
 exception InvalidURLError() 
 The URL of the request was not a valid URL, or it used an 
 unsupported method. Only http and https URLs are supported. 
 exception DownloadError() 
 There was an error retrieving the data. 
 This exception is not raised if the server returns an HTTP 
 error code: In that case, the response data comes back intact, 
 including the error code. 
 exception ResponseTooLargeError() 
 The response data exceeded the maximum allowed size, and the 
 allow_truncated parameter passed to fetch() was False.'"
"def newer(source, target): 
    if (not os.path.exists(source)): 
      raise DistutilsFileError((""file   '%s'   does   not   exist"" % os.path.abspath(source))) 
   if (not os.path.exists(target)): 
      return 1 
   from stat import ST_MTIME 
   mtime1 = os.stat(source)[ST_MTIME] 
   mtime2 = os.stat(target)[ST_MTIME] 
   return (mtime1 > mtime2)"," 'Check if file `source` is newer than `target` 
 :param source: the source file 
 :type source: str 
 :param target: the target file 
 :type target: str 
 :return: 0 if `source` is newer than `target`, 1 otherwise'","'Return true if \'source\' exists and is more recently modified than 
 \'target\', or if \'source\' exists and \'target\' doesn\'t.  Return false if 
 both exist and \'target\' is the same age or younger than \'source\'. 
 Raise DistutilsFileError if \'source\' does not exist.'"
"def restore_asset_from_trashcan(location): 
    trash = contentstore('trashcan') 
   store = contentstore() 
   loc = StaticContent.get_location_from_path(location) 
   content = trash.find(loc) 
   store.save(content) 
   if (content.thumbnail_location is not None): 
      try: 
         thumbnail_content = trash.find(content.thumbnail_location) 
         store.save(thumbnail_content) 
      except Exception: 
         pass"," 'Restore a deleted asset from the trashcan. 
 :param location: The location of the asset to restore. 
 :type location: str 
 :return: The restored asset. 
 :rtype: StaticContent'",'This method will restore an asset which got soft deleted and put back in the original course'
"def ToCanonicalJSON(dict, indent=False): 
    return json.dumps(dict, sort_keys=True, indent=indent)"," 'Returns a JSON string representation of a dict, with sort_keys=True 
 and indent=True if requested. 
 :param dict: The dict to be converted to JSON. 
 :param indent: If True, the output will be indented.'","'Convert ""dict"" to a canonical JSON string. Sort keys so that output 
 ordering is always the same.'"
"def url_decode_stream(stream, charset='utf-8', decode_keys=False, include_empty=True, errors='replace', separator='&', cls=None, limit=None, return_iterator=False): 
    from werkzeug.wsgi import make_chunk_iter 
   if return_iterator: 
      cls = (lambda x: x) 
   elif (cls is None): 
      cls = MultiDict 
   pair_iter = make_chunk_iter(stream, separator, limit) 
   return cls(_url_decode_impl(pair_iter, charset, decode_keys, include_empty, errors))"," 'Decode a stream of URL-encoded data. 
 :param stream: A stream of data. 
 :param charset: The charset to use for decoding. 
 :param decode_keys: If True, decode the key-value pairs. 
 :param include_empty: If True, include empty key-value pairs. 
 :param errors: The errors to raise when decoding fails. 
 :param separator: The separator to use when splitting the stream. 
 :param cls: The class to use for the returned dict. 
 :param limit: The maximum number of key-value pairs to include. 
 :param return_iterator: If True, return an iterator instead of a dict. 
 :return: A dictionary of decoded key-value pairs. 
 :rtype: dict or iterator[dict]'","'Works like :func:`url_decode` but decodes a stream.  The behavior 
 of stream and limit follows functions like 
 :func:`~werkzeug.wsgi.make_line_iter`.  The generator of pairs is 
 directly fed to the `cls` so you can consume the data while it\'s 
 parsed. 
 .. versionadded:: 0.8 
 :param stream: a stream with the encoded querystring 
 :param charset: the charset of the query string.  If set to `None` 
 no unicode decoding will take place. 
 :param decode_keys: Used on Python 2.x to control whether keys should 
 be forced to be unicode objects.  If set to `True`, 
 keys will be unicode in all cases. Otherwise, they 
 remain `str` if they fit into ASCII. 
 :param include_empty: Set to `False` if you don\'t want empty values to 
 appear in the dict. 
 :param errors: the decoding error behavior. 
 :param separator: the pair separator to be used, defaults to ``&`` 
 :param cls: an optional dict class to use.  If this is not specified 
 or `None` the default :class:`MultiDict` is used. 
 :param limit: the content length of the URL data.  Not necessary if 
 a limited stream is provided. 
 :param return_iterator: if set to `True` the `cls` argument is ignored 
 and an iterator over all decoded pairs is 
 returned'"
"def delete_subnet(subnet, profile=None): 
    conn = _auth(profile) 
   return conn.delete_subnet(subnet)"," 'Delete a subnet. 
 :param subnet: The subnet to delete. 
 :type subnet: :class:`Subnet` 
 :param profile: The profile to use. 
 :type profile: :class:`Profile` 
 :returns: The response from the server.'","'Deletes the specified subnet 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' neutron.delete_subnet subnet-name 
 salt \'*\' neutron.delete_subnet subnet-name profile=openstack1 
 :param subnet: ID or name of subnet to delete 
 :param profile: Profile to build on (Optional) 
 :return: True(Succeed) or False'"
"@_ensure_exists 
 def pid(name): 
    try: 
      return int(info(name).get('PID')) 
   except (TypeError, ValueError) as exc: 
      raise CommandExecutionError(""Unable   to   get   PID   for   container   '{0}':   {1}"".format(name, exc))"," 'Get the PID of a container. 
 :param name: The name of the container. 
 :returns: The PID of the container.'","'Returns the PID of a container 
 name 
 Container name 
 CLI Example: 
 .. code-block:: bash 
 salt myminion nspawn.pid arch1'"
"def _get_used_lun_id_counter(mapping): 
    used_luns = _get_used_lun_ids_for_mappings(mapping) 
   used_lun_id_counter = collections.Counter(used_luns) 
   return used_lun_id_counter", 'Returns a counter of used lun IDs for a given mapping.','Returns used LUN IDs with count as a dictionary.'
"def _adapt_mismatch(original, matchee): 
    marker = object() 
   if (getattr(original, 'mismatched', marker) is marker): 
      return mismatch(matchee, original.describe(), original.get_details()) 
   return original"," 'Adapt a mismatch to match the new matchee. 
 :param original: The original mismatch. 
 :param matchee: The new matchee. 
 :returns: A new mismatch object.'","'If ``original`` doesn\'t already store ``matchee`` then return a new 
 one that has it stored.'"
"def top(**kwargs): 
    if ('id' not in kwargs['opts']): 
      return {} 
   cmd = '{0}   {1}'.format(__opts__['master_tops']['ext_nodes'], kwargs['opts']['id']) 
   ndata = yaml.safe_load(subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).communicate()[0]) 
   if (not ndata): 
      log.info('master_tops   ext_nodes   call   did   not   return   any   data') 
   ret = {} 
   if ('environment' in ndata): 
      env = ndata['environment'] 
   else: 
      env = 'base' 
   if ('classes' in ndata): 
      if isinstance(ndata['classes'], dict): 
         ret[env] = list(ndata['classes']) 
      elif isinstance(ndata['classes'], list): 
         ret[env] = ndata['classes'] 
      else: 
         return ret 
   else: 
      log.info('master_tops   ext_nodes   call   did   not   have   a   dictionary   with   a   ""classes""   key.') 
   return ret", 'Returns the top class for a given node','Run the command configured'
"def test_attributes(mixin_cols): 
    m = mixin_cols['m'] 
   m.info.name = 'a' 
   assert (m.info.name == 'a') 
   m.info.description = 'a' 
   assert (m.info.description == 'a') 
   if isinstance(m, (u.Quantity, coordinates.SkyCoord, time.Time)): 
      with pytest.raises(AttributeError): 
         m.info.unit = u.m 
   else: 
      m.info.unit = u.m 
      assert (m.info.unit is u.m) 
   m.info.format = 'a' 
   assert (m.info.format == 'a') 
   m.info.meta = {'a': 1} 
   assert (m.info.meta == {'a': 1}) 
   with pytest.raises(AttributeError): 
      m.info.bad_attr = 1 
   with pytest.raises(AttributeError): 
      m.info.bad_attr"," 'Test attributes of mixin. 
 Parameters 
 mixin_cols : dict 
 A dictionary with the mixin attributes. 
 Returns 
 None'",'Required attributes for a column can be set.'
"def reinitialize_command(self, command, reinit_subcommands): 
    cmd_obj = _DISTUTILS_REINIT(self, command, reinit_subcommands) 
   options = self.command_options.get(command) 
   if options: 
      self._set_command_options(cmd_obj, options) 
   return cmd_obj"," 'Reinitialize the command object and its subcommands. 
 :param command: the command to reinitialize 
 :param reinit_subcommands: a list of subcommands to reinitialize 
 :return: the reinitialized command object'","'Monkeypatch the original version from distutils. 
 It\'s supposed to match the behavior of Distribution.get_command_obj() 
 This fixes issues with \'pip install -e\' and \'./setup.py test\' not 
 respecting the setup.cfg configuration directives for the build_ext 
 command.'"
"def _zpkbilinear(z, p, k, fs): 
    z = atleast_1d(z) 
   p = atleast_1d(p) 
   degree = _relative_degree(z, p) 
   fs2 = (2 * fs) 
   z_z = ((fs2 + z) / (fs2 - z)) 
   p_z = ((fs2 + p) / (fs2 - p)) 
   z_z = append(z_z, (- ones(degree))) 
   k_z = (k * real((prod((fs2 - z)) / prod((fs2 - p))))) 
   return (z_z, p_z, k_z)"," 'Returns the zpk bilinear form of the given polynomials. 
 Parameters 
 z : 1d array 
 Polynomial in z. 
 p : 1d array 
 Polynomial in p. 
 k : 1d array 
 Polynomial in k. 
 fs : float 
 Sampling frequency in Hz. 
 Returns 
 z_z : 1d array 
 z_z = (1/fs) * z * (1/fs) + k 
 p_z : 1d array 
 p_z = (1/fs) * p * (1/fs) + k 
 k_z : 1d array 
 k_z = (1/fs) * k * (1/fs) 
 Notes 
 This function is used in the implementation of the bilinear 
 transforms. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Bilinear_transform 
 Examples 
 >>> from scipy.signal import _zpkbilinear 
 >>> _zpkbilinear([1, 2, 3],","'Return a digital filter from an analog one using a bilinear transform. 
 Transform a set of poles and zeros from the analog s-plane to the digital 
 z-plane using Tustin\'s method, which substitutes ``(z-1) / (z+1)`` for 
 ``s``, maintaining the shape of the frequency response. 
 Parameters 
 z : array_like 
 Zeros of the analog IIR filter transfer function. 
 p : array_like 
 Poles of the analog IIR filter transfer function. 
 k : float 
 System gain of the analog IIR filter transfer function. 
 fs : float 
 Sample rate, as ordinary frequency (e.g. hertz). No prewarping is 
 done in this function. 
 Returns 
 z : ndarray 
 Zeros of the transformed digital filter transfer function. 
 p : ndarray 
 Poles of the transformed digital filter transfer function. 
 k : float 
 System gain of the transformed digital filter.'"
"def pytest_configure(config): 
    if (config.getoption('gae_sdk') is not None): 
      set_up_gae_environment(config.getoption('gae_sdk'))", 'Set up the GAE environment if the GAE_SDK environment variable is set.','Configures the App Engine SDK imports on py.test startup.'
"def _date_year(release): 
    try: 
      date = release['ReleaseDate'] 
   except TypeError: 
      date = '' 
   if (date is not None): 
      year = date[:4] 
   else: 
      year = '' 
   return (date, year)"," 'Returns the year of a release. 
 If the release date is a string, the year is returned. 
 If the release date is a datetime object, the year is returned as the 
 year of the date. 
 If the release date is None, the year is returned as an empty string.'",'Extract release date and year from database row'
"def FindEndOfExpressionInLine(line, startpos, depth, startchar, endchar): 
    for i in xrange(startpos, len(line)): 
      if (line[i] == startchar): 
         depth += 1 
      elif (line[i] == endchar): 
         depth -= 1 
         if (depth == 0): 
            return ((i + 1), 0) 
   return ((-1), depth)"," 'Find the end of the expression in a line of code. 
 The function returns the position of the end of the expression in the 
 line, and the depth of the expression. 
 The depth is the number of times the expression has been nested. 
 The function assumes that the expression starts at the startpos position 
 in the line. 
 Examples: 
 >>> FindEndOfExpressionInLine(\'a = b\', 0, 0, \'a\', \'b\') 
 (2, 1) 
 >>> FindEndOfExpressionInLine(\'a = b\', 0, 0, \'a\', \'b\') 
 (2, 1) 
 >>> FindEndOfExpressionInLine(\'a = b\', 0, 0, \'a\', \'b\') 
 (2, 1) 
 >>> FindEndOfExpressionInLine(\'a = b\', 0, 0, \'a\', \'b\') 
 (2, 1) 
 >>> FindEndOfExpressionInLine(\'a = b\', ","'Find the position just after the matching endchar. 
 Args: 
 line: a CleansedLines line. 
 startpos: start searching at this position. 
 depth: nesting level at startpos. 
 startchar: expression opening character. 
 endchar: expression closing character. 
 Returns: 
 On finding matching endchar: (index just after matching endchar, 0) 
 Otherwise: (-1, new depth at end of this line)'"
"def expand_login_view(login_view): 
    if login_view.startswith(('https://', 'http://', '/')): 
      return login_view 
   else: 
      return url_for(login_view)", 'Expand login view url if it is not a full url',"'Returns the url for the login view, expanding the view name to a url if 
 needed. 
 :param login_view: The name of the login view or a URL for the login view. 
 :type login_view: str'"
"def varOr(population, toolbox, lambda_, cxpb, mutpb): 
    assert ((cxpb + mutpb) <= 1.0), 'The   sum   of   the   crossover   and   mutation   probabilities   must   be   smaller   or   equal   to   1.0.' 
   offspring = [] 
   for _ in xrange(lambda_): 
      op_choice = random.random() 
      if (op_choice < cxpb): 
         (ind1, ind2) = map(toolbox.clone, random.sample(population, 2)) 
         (ind1, ind2) = toolbox.mate(ind1, ind2) 
         del ind1.fitness.values 
         offspring.append(ind1) 
      elif (op_choice < (cxpb + mutpb)): 
         ind = toolbox.clone(random.choice(population)) 
         (ind,) = toolbox.mutate(ind) 
         del ind.fitness.values 
         offspring.append(ind) 
      else: 
         offspring.append(random.choice(population)) 
   return offspring"," 'Generate a population of offspring using crossover and mutation. 
 The population is generated by randomly choosing a crossover probability 
 and a mutation probability, then choosing a crossover or mutation operation 
 and applying it to the population. 
 Parameters 
 population : population of individuals 
 toolbox : a population toolbox 
 lambda_ : a crossover probability 
 cxpb : a crossover probability 
 mutpb : a mutation probability 
 Returns 
 offspring : a population of individuals 
 Examples 
 >>> from nupic.framework.ops import toolbox 
 >>> from nupic.framework.ops import varOr 
 >>> toolbox = toolbox.Toolbox() 
 >>> toolbox.addOperators(toolbox.Crossover(0.2)) 
 >>> toolbox.addOperators(toolbox.Mutation(0.2)) 
 >>> offspring = varOr(toolbox.population, toolbox, 0.2, 0.2, 0.2) 
 >>> for ind in offspring: 
 ...    ","'Part of an evolutionary algorithm applying only the variation part 
 (crossover, mutation **or** reproduction). The modified individuals have 
 their fitness invalidated. The individuals are cloned so returned 
 population is independent of the input population. 
 :param population: A list of individuals to vary. 
 :param toolbox: A :class:`~deap.base.Toolbox` that contains the evolution 
 operators. 
 :param lambda\_: The number of children to produce 
 :param cxpb: The probability of mating two individuals. 
 :param mutpb: The probability of mutating an individual. 
 :returns: The final population 
 :returns: A class:`~deap.tools.Logbook` with the statistics of the 
 evolution 
 The variation goes as follow. On each of the *lambda_* iteration, it 
 selects one of the three operations; crossover, mutation or reproduction. 
 In the case of a crossover, two individuals are selected at random from 
 the parental population :math:`P_\mathrm{p}`, those individuals are cloned 
 using the :meth:`toolbox.clone` method and then mated using the 
 :meth:`toolbox.mate` method. Only the first child is appended to the 
 offspring population :math:`P_\mathrm{o}`, the second child is discarded. 
 In the case of a mutation, one individual is selected at random from 
 :math:`P_\mathrm{p}`, it is cloned and then mutated using using the 
 :meth:`toolbox.mutate` method. The resulting mutant is appended to 
 :math:`P_\mathrm{o}`. In the case of a reproduction, one individual is 
 selected at random from :math:`P_\mathrm{p}`, cloned and appended to 
 :math:`P_\mathrm{o}`. 
 This variation is named *Or* beceause an offspring will never result from 
 both operations crossover and mutation. The sum of both probabilities 
 shall be in :math:`[0, 1]`, the reproduction probability is 
 1 - *cxpb* - *mutpb*.'"
"def _escape(value): 
    if isinstance(value, (list, tuple)): 
      value = u','.join(value) 
   elif isinstance(value, (date, datetime)): 
      value = value.isoformat() 
   elif isinstance(value, bool): 
      value = str(value).lower() 
   if isinstance(value, string_types): 
      try: 
         return value.encode(u'utf-8') 
      except UnicodeDecodeError: 
         pass 
   return str(value)"," 'Returns a string representation of the value. 
 If the value is a string, it is encoded using utf-8. 
 Otherwise, the value is returned unchanged. 
 :param value: The value to encode. 
 :type value: Any 
 :return: The encoded string. 
 :rtype: str'","'Escape a single value of a URL string or a query parameter. If it is a list 
 or tuple, turn it into a comma-separated string first.'"
"def api_validate(response_type=None, add_api_type_doc=False): 
    def wrap(response_function): 
      def _api_validate(*simple_vals, **param_vals): 
         def val(fn): 
            @wraps(fn) 
            def newfn(self, *a, **env): 
               renderstyle = request.params.get('renderstyle') 
               if renderstyle: 
                  c.render_style = api_type(renderstyle) 
               elif (not c.extension): 
                  c.render_style = api_type(response_type) 
               if ((response_type == 'html') and (not (request.params.get('api_type') == 'json'))): 
                  responder = JQueryResponse() 
               else: 
                  responder = JsonResponse() 
               response.content_type = responder.content_type 
               try: 
                  kw = _make_validated_kw(fn, simple_vals, param_vals, env) 
                  return response_function(self, fn, responder, simple_vals, param_vals, *a, **kw) 
               except UserRequiredException: 
                  responder.send_failure(errors.USER_REQUIRED) 
                  return self.api_wrapper(responder.make_response()) 
               except VerifiedUserRequiredException: 
                  responder.send_failure(errors.VERIFIED_USER_REQUIRED) 
                  return self.api_wrapper(responder.make_response()) 
            extra_param_vals = {} 
            if add_api_type_doc: 
               extra_param_vals = {'api_type': 'the   string   `json`'} 
            set_api_docs(newfn, simple_vals, param_vals, extra_param_vals) 
            newfn.handles_csrf = _validators_handle_csrf(simple_vals, param_vals) 
            return newfn 
         return val 
      return _api_validate 
   return wrap"," 'Wraps the given function to validate the request and 
 return a valid response.'","'Factory for making validators for API calls, since API calls come 
 in two flavors: responsive and unresponsive.  The machinary 
 associated with both is similar, and the error handling identical, 
 so this function abstracts away the kw validation and creation of 
 a Json-y responder object.'"
"def build_lcms_70(compiler): 
    if (compiler['platform'] == 'x64'): 
      return '' 
   'Build   LCMS   on   VC2008.   This   version   is   only   32bit/Win32' 
   return ('\nrem   Build   lcms2\nsetlocal\nrd   /S   /Q   %%LCMS%%\\Lib\nrd   /S   /Q   %%LCMS%%\\Projects\\VC%(vc_version)s\\Release\n%%MSBUILD%%   %%LCMS%%\\Projects\\VC%(vc_version)s\\lcms2.sln      /t:Clean   /p:Configuration=""Release""   /p:Platform=Win32   /m\n%%MSBUILD%%   %%LCMS%%\\Projects\\VC%(vc_version)s\\lcms2.sln   /t:lcms2_static   /p:Configuration=""Release""   /p:Platform=Win32   /m\nxcopy   /Y   /E   /Q   %%LCMS%%\\include   %%INCLIB%%\ncopy   /Y   /B   %%LCMS%%\\Projects\\VC%(vc_version)s\\Release\\*.lib   %%INCLIB%%\nendlocal\n' % compiler)", 'Build lcms2 on VC2008. This version is only 32bit/Win32','Link error here on x64'
"def _parse_date_rfc822(dt): 
    try: 
      m = _rfc822_match(dt.lower()).groupdict(0) 
   except AttributeError: 
      return None 
   return _parse_date_group_rfc822(m)", 'Parse a date from a RFC 822 date string.',"'Parse RFC 822 dates and times, with one minor 
 difference: years may be 4DIGIT or 2DIGIT. 
 http://tools.ietf.org/html/rfc822#section-5'"
"def create_image(ami_name, instance_id=None, instance_name=None, tags=None, region=None, key=None, keyid=None, profile=None, description=None, no_reboot=False, dry_run=False, filters=None): 
    instances = find_instances(instance_id=instance_id, name=instance_name, tags=tags, region=region, key=key, keyid=keyid, profile=profile, return_objs=True, filters=filters) 
   if (not instances): 
      log.error('Source   instance   not   found') 
      return False 
   if (len(instances) > 1): 
      log.error('Multiple   instances   found,   must   match   exactly   only   one   instance   to   create   an   image   from') 
      return False 
   instance = instances[0] 
   try: 
      return instance.create_image(ami_name, description=description, no_reboot=no_reboot, dry_run=dry_run) 
   except boto.exception.BotoServerError as exc: 
      log.error(exc) 
      return False"," 'Creates an image from a source instance. 
 :param ami_name: The name of the image to create. 
 :param instance_id: The ID of the instance to create the image from. 
 :param instance_name: The name of the instance to create the image from. 
 :param tags: A list of tags to apply to the image. 
 :param region: The region to create the image in. 
 :param key: The access key to use to create the image. 
 :param keyid: The access key ID to use to create the image. 
 :param profile: The profile to use to create the image. 
 :param description: A description for the image. 
 :param no_reboot: Whether to force a reboot of the instance after the image is created. 
 :param dry_run: Whether to perform a dry run of the operation. 
 :param filters: A dict of filters to apply to the operation. 
 :returns: The ID of the image created. 
 :rtype: str'","'Given instance properties that define exactly one instance, create AMI and return AMI-id. 
 CLI Examples: 
 .. code-block:: bash 
 salt myminion boto_ec2.create_instance ami_name instance_name=myinstance 
 salt myminion boto_ec2.create_instance another_ami_name tags=\'{""mytag"": ""value""}\' description=\'this is my ami\''"
"def sentence_chrf(reference, hypothesis, min_len=1, max_len=6, beta=3.0): 
    return corpus_chrf([reference], [hypothesis], min_len, max_len, beta=beta)"," 'Compute a sentence Chrf distance between two sentences. 
 The sentence Chrf distance between two sentences is defined as: 
 .. math:: 
 \text{SentenceChrf}(R, H) = \frac{1}{2} \log \frac{\text{Chrf}(R, H)}{\text{Chrf}(R, R)} 
 where Chrf is the Chrf distance between two sentences. 
 Parameters 
 reference : str 
 The reference sentence. 
 hypothesis : str 
 The hypothesis sentence. 
 min_len : int 
 The minimum length of the reference sentence. 
 max_len : int 
 The maximum length of the hypothesis sentence. 
 beta : float 
 The beta parameter for the Chrf distance. 
 Returns 
 distance : float 
 The sentence Chrf distance between the reference and hypothesis sentences. 
 Examples 
 >>> from nltk.corpus import sentence_chrf 
 >>> from nltk.corpus import brown 
 >>> reference = brown.words(10) 
 >>> hypothesis = brown.words(10) 
 >>> sentence_chrf(reference, hypothesis)","'Calculates the sentence level CHRF (Character n-gram F-score) described in 
 - Maja Popovic. 2015. CHRF: Character n-gram F-score for Automatic MT Evaluation. 
 In Proceedings of the 10th Workshop on Machine Translation. 
 http://www.statmt.org/wmt15/pdf/WMT49.pdf 
 - Maja Popovic. 2016. CHRF Deconstructed: Î² Parameters and n-gram Weights. 
 In Proceedings of the 1st Conference on Machine Translation. 
 http://www.statmt.org/wmt16/pdf/W16-2341.pdf 
 Unlike multi-reference BLEU, CHRF only supports a single reference. 
 An example from the original BLEU paper 
 http://www.aclweb.org/anthology/P02-1040.pdf 
 >>> ref1 = str(\'It is a guide to action that ensures that the military \' 
 ...            \'will forever heed Party commands\').split() 
 >>> hyp1 = str(\'It is a guide to action which ensures that the military \' 
 ...            \'always obeys the commands of the party\').split() 
 >>> hyp2 = str(\'It is to insure the troops forever hearing the activity \' 
 ...            \'guidebook that party direct\').split() 
 >>> sentence_chrf(ref1, hyp1) # doctest: +ELLIPSIS 
 0.6768... 
 >>> sentence_chrf(ref1, hyp2) # doctest: +ELLIPSIS 
 0.4201... 
 The infamous ""the the the ... "" example 
 >>> ref = \'the cat is on the mat\'.split() 
 >>> hyp = \'the the the the the the the\'.split() 
 >>> sentence_chrf(ref, hyp)  # doctest: +ELLIPSIS 
 0.2530... 
 An example to show that this function allows users to use strings instead of 
 tokens, i.e. list(str) as inputs. 
 >>> ref1 = str(\'It is a guide to action that ensures that the military \' 
 ...            \'will forever heed Party commands\') 
 >>> hyp1 = str(\'It is a guide to action which ensures that the military \' 
 ...            \'always obeys the commands of the party\') 
 >>> sentence_chrf(ref1, hyp1) # doctest: +ELLIPSIS 
 0.6768... 
 >>> type(ref1) == type(hyp1) == str 
 True 
 >>> sentence_chrf(ref1.split(), hyp1.split()) # doctest: +ELLIPSIS 
 0.6768... 
 To skip the unigrams and only use 2- to 3-grams: 
 >>> sentence_chrf(ref1, hyp1, min_len=2, max_len=3) # doctest: +ELLIPSIS 
 0.7018... 
 :param references: reference sentence 
 :type references: list(str) / str 
 :param hypothesis: a hypothesis sentence 
 :type hypothesis: list(str) / str 
 :param min_len: The minimum order of n-gram this function should extract. 
 :type min_len: int 
 :param max_len: The maximum order of n-gram this function should extract. 
 :type max_len: int 
 :param beta: the parameter to assign more importance to recall over precision 
 :type beta: float 
 :return: the sentence level CHRF score. 
 :rtype: float'"
"def check_print_compat(): 
    return (not ((os.name == 'nt') and version_check('5.3.0', operator.lt)))", 'Check for Python 3.3+ print() compatibility.','Check if printing should work in the given Qt version.'
"def run_script(scriptfile): 
    try: 
      f = open(scriptfile, mode='r') 
   except Exception: 
      return 
   mpstate.console.writeln(('Running   script   %s' % scriptfile)) 
   for line in f: 
      line = line.strip() 
      if ((line == '') or line.startswith('#')): 
         continue 
      if line.startswith('@'): 
         line = line[1:] 
      else: 
         mpstate.console.writeln(('->   %s' % line)) 
      process_stdin(line) 
   f.close()", 'Run a script from stdin.','run a script file'
"def add_cohort(course_key, name, assignment_type): 
    log.debug('Adding   cohort   %s   to   %s', name, course_key) 
   if is_cohort_exists(course_key, name): 
      raise ValueError(_('You   cannot   create   two   cohorts   with   the   same   name')) 
   try: 
      course = courses.get_course_by_id(course_key) 
   except Http404: 
      raise ValueError('Invalid   course_key') 
   cohort = CourseCohort.create(cohort_name=name, course_id=course.id, assignment_type=assignment_type).course_user_group 
   tracker.emit('edx.cohort.creation_requested', {'cohort_name': cohort.name, 'cohort_id': cohort.id}) 
   return cohort"," 'Adds a cohort to a course. 
 :param course_key: The course key of the course the cohort will be added to. 
 :type course_key: unicode 
 :param name: The name of the cohort. 
 :type name: unicode 
 :param assignment_type: The assignment type to use for this cohort. 
 :type assignment_type: str'","'Add a cohort to a course.  Raises ValueError if a cohort of the same name already 
 exists.'"
"def delete_affinity_group(kwargs=None, conn=None, call=None): 
    if (call != 'function'): 
      raise SaltCloudSystemExit('The   delete_affinity_group   function   must   be   called   with   -f   or   --function.') 
   if (kwargs is None): 
      kwargs = {} 
   if ('name' not in kwargs): 
      raise SaltCloudSystemExit('A   name   must   be   specified   as   ""name""') 
   if (not conn): 
      conn = get_conn() 
   try: 
      conn.delete_affinity_group(kwargs['name']) 
      return {'Success': 'The   affinity   group   was   successfully   deleted'} 
   except AzureMissingResourceHttpError as exc: 
      raise SaltCloudSystemExit('{0}:   {1}'.format(kwargs['name'], exc.message))", 'Delete an affinity group.',"'.. versionadded:: 2015.8.0 
 Delete a specific affinity group associated with the account 
 CLI Examples: 
 .. code-block:: bash 
 salt-cloud -f delete_affinity_group my-azure name=my_affinity_group'"
"def _sqrtdenest_rec(expr): 
    from sympy.simplify.radsimp import radsimp, rad_rationalize, split_surds 
   if (not expr.is_Pow): 
      return sqrtdenest(expr) 
   if (expr.base < 0): 
      return (sqrt((-1)) * _sqrtdenest_rec(sqrt((- expr.base)))) 
   (g, a, b) = split_surds(expr.base) 
   a = (a * sqrt(g)) 
   if (a < b): 
      (a, b) = (b, a) 
   c2 = _mexpand(((a ** 2) - (b ** 2))) 
   if (len(c2.args) > 2): 
      (g, a1, b1) = split_surds(c2) 
      a1 = (a1 * sqrt(g)) 
      if (a1 < b1): 
         (a1, b1) = (b1, a1) 
      c2_1 = _mexpand(((a1 ** 2) - (b1 ** 2))) 
      c_1 = _sqrtdenest_rec(sqrt(c2_1)) 
      d_1 = _sqrtdenest_rec(sqrt((a1 + c_1))) 
      (num, den) = rad_rationalize(b1, d_1) 
      c = _mexpand(((d_1 / sqrt(2)) + (num / (den * sqrt(2))))) 
   else: 
      c = _sqrtdenest1(sqrt(c2)) 
   if (sqrt_depth(c) > 1): 
      raise SqrtdenestStopIteration 
   ac = (a + c) 
   if (len(ac.args) >= len(expr.args)): 
      if (count_ops(ac) >= count_ops(expr.base)): 
         raise SqrtdenestStopIteration 
   d = sqrtdenest(sqrt(ac)) 
   if (sqrt_depth(d) > 1): 
      raise SqrtdenestStopIteration 
   (num, den) = rad_rationalize(b, d) 
   r = ((d / sqrt(2)) + (num / (den * sqrt(2)))) 
   r = radsimp(r) 
   return _mexpand(r)"," 'Recursive algorithm for simplifying square roots 
 This is a generalization of the algorithm used by 
 `sqrtdenest1` to handle square roots of polynomials with 
 multiple terms. 
 The algorithm is based on the following observation: 
 If `sqrt(a) == sqrt(b)` and `sqrt(a) == sqrt(c)` then 
 `sqrt(a + b) == sqrt(c)` and 
 `sqrt(a - b) == sqrt(c)` 
 This is used to factor out the square root of a term 
 that is common to two or more terms. 
 The algorithm is initialized with the square root of the 
 polynomial and then recursively calls itself on the 
 remaining terms. 
 The algorithm terminates when the number of operations 
 in the result is less than or equal to the number of 
 operations in the input. 
 Examples 
 >>> from sympy.simplify.radsimp import _sqrtdenest_rec 
 >>> from sympy import sqrt, Symbol, I, cos, sin, exp, pi 
 >>> x = Symbol(\'x\') 
 >>> _sqrt","'Helper that denests the square root of three or more surds. 
 It returns the denested expression; if it cannot be denested it 
 throws SqrtdenestStopIteration 
 Algorithm: expr.base is in the extension Q_m = Q(sqrt(r_1),..,sqrt(r_k)); 
 split expr.base = a + b*sqrt(r_k), where `a` and `b` are on 
 Q_(m-1) = Q(sqrt(r_1),..,sqrt(r_(k-1))); then a**2 - b**2*r_k is 
 on Q_(m-1); denest sqrt(a**2 - b**2*r_k) and so on. 
 See [1], section 6. 
 Examples 
 >>> from sympy import sqrt 
 >>> from sympy.simplify.sqrtdenest import _sqrtdenest_rec 
 >>> _sqrtdenest_rec(sqrt(-72*sqrt(2) + 158*sqrt(5) + 498)) 
 -sqrt(10) + sqrt(2) + 9 + 9*sqrt(5) 
 >>> w=-6*sqrt(55)-6*sqrt(35)-2*sqrt(22)-2*sqrt(14)+2*sqrt(77)+6*sqrt(10)+65 
 >>> _sqrtdenest_rec(sqrt(w)) 
 -sqrt(11) - sqrt(7) + sqrt(2) + 3*sqrt(5)'"
"def user_pre_save(sender, instance, **kw): 
    if instance.id: 
      user = User.objects.get(id=instance.id) 
      if (user.username != instance.username): 
         questions = Question.objects.filter((Q(creator=instance) | Q(answers__creator=instance))).only('id').distinct() 
         for q in questions: 
            q.index_later()", 'Pre-save hook to index questions that the user has created or answered.',"'When a user\'s username is changed, we must reindex the questions 
 they participated in.'"
"def peakDetection(mX, t): 
    thresh = np.where((mX[1:(-1)] > t), mX[1:(-1)], 0) 
   next_minor = np.where((mX[1:(-1)] > mX[2:]), mX[1:(-1)], 0) 
   prev_minor = np.where((mX[1:(-1)] > mX[:(-2)]), mX[1:(-1)], 0) 
   ploc = ((thresh * next_minor) * prev_minor) 
   ploc = (ploc.nonzero()[0] + 1) 
   return ploc"," 'Detect peaks in the mX array. 
 Parameters 
 mX : ndarray 
 The mX array. 
 t : float 
 The threshold value for the peaks. 
 Returns 
 ploc : ndarray 
 The locations of the peaks. 
 Notes 
 This function is used to determine the peaks in the mX array. 
 It is assumed that the array is sorted in ascending order. 
 The function detects the peaks by comparing the values in the 
 array to the threshold value t. 
 The function returns the locations of the peaks in the array. 
 The locations are the indices of the array. 
 Examples 
 >>> import numpy as np 
 >>> import scipy.signal as sig 
 >>> mX = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 
 >>> t = 2 
 >>> ploc = sig.peakDetection(mX, t) 
 >>> ploc 
 array([0, 1, 2, 3,","'Detect spectral peak locations 
 mX: magnitude spectrum, t: threshold 
 returns ploc: peak locations'"
"def parse_encoding(fp): 
    pos = fp.tell() 
   fp.seek(0) 
   try: 
      line1 = fp.readline() 
      has_bom = line1.startswith(codecs.BOM_UTF8) 
      if has_bom: 
         line1 = line1[len(codecs.BOM_UTF8):] 
      m = PYTHON_MAGIC_COMMENT_re.match(line1) 
      if (not m): 
         try: 
            import parser 
            parser.suite(line1) 
         except (ImportError, SyntaxError): 
            pass 
         else: 
            line2 = fp.readline() 
            m = PYTHON_MAGIC_COMMENT_re.match(line2) 
      if has_bom: 
         if m: 
            raise SyntaxError('python   refuses   to   compile   code   with   both   a   UTF8   byte-order-mark   and   a   magic   encoding   comment') 
         return 'utf_8' 
      elif m: 
         return m.group(1) 
      else: 
         return None 
   finally: 
      fp.seek(pos)"," 'Parse the encoding of a file. 
 The file is read up to the first line, and the encoding is determined 
 by looking for a magic comment. 
 The magic comment is in the form of a single line containing 
 ""encoding:<encoding>"" or ""encoding:<encoding>,charmap:<charmap>"" 
 where <encoding> is a valid Python string. 
 If the magic comment is not found, the file is assumed to be 
 encoded in UTF-8. 
 The encoding is returned as a string.'","'Deduce the encoding of a source file from magic comment. 
 It does this in the same way as the `Python interpreter`__ 
 .. __: http://docs.python.org/ref/encodings.html 
 The ``fp`` argument should be a seekable file object. 
 (From Jeff Dairiki)'"
"def _allow_CTRL_C_posix(): 
    signal.signal(signal.SIGINT, signal.default_int_handler)", 'Allow CTRL-C to be used to interrupt a running process.','Take CTRL+C into account (SIGINT).'
"def dir(suffix='', prefix='tmp', parent=None): 
    return tempfile.mkdtemp(suffix, prefix, parent)"," 'Create a temporary directory. 
 :param suffix: 
 :param prefix: 
 :param parent: 
 :return: 
 :rtype: 
 :raises: 
 :param prefix: 
 :param parent: 
 :return: 
 :rtype: 
 :raises: 
 :param suffix: 
 :param prefix: 
 :param parent: 
 :return: 
 :rtype: 
 :raises: 
 :param suffix: 
 :param prefix: 
 :param parent: 
 :return: 
 :rtype: 
 :raises: 
 :param prefix: 
 :param parent: 
 :return: 
 :rtype: 
 :raises: 
 :param prefix: 
 :param parent: 
 :return: 
 :rtype: 
 :raises: 
 :param prefix: 
 :param parent: 
 :return: 
 :rtype: 
 :raises: 
 :param suffix: 
 :param prefix: 
 :param parent: 
 :return: 
 :rtype:","'Create a temporary directory 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' temp.dir 
 salt \'*\' temp.dir prefix=\'mytemp-\' parent=\'/var/run/\''"
"def backup_dir(dir, ext='.bak'): 
    n = 1 
   extension = ext 
   while os.path.exists((dir + extension)): 
      n += 1 
      extension = (ext + str(n)) 
   return (dir + extension)"," 'Returns a backup directory name, with a unique extension.'","'Figure out the name of a directory to back up the given dir to 
 (adding .bak, .bak2, etc)'"
"@memoized 
 def cyclic_pattern(size=None, start=None, charset_type=None): 
    charset = config.Option.get('p_charset') 
   if (not charset): 
      charset = cyclic_pattern_charset(charset) 
   else: 
      charset = ''.join(set(charset)) 
   if (start is None): 
      start = 0 
   if (size is None): 
      size = 65536 
   size += start 
   pattern = de_bruijn(charset, 3, size) 
   return pattern[start:size].encode('utf-8')"," 'Generate a cyclic pattern. 
 This is a function that generates a cyclic pattern of the given size 
 and start position. 
 The pattern is generated using a De Bruijn sequence and the given 
 charset. 
 :param size: The size of the pattern. 
 :param start: The start position of the pattern. 
 :param charset_type: The type of the charset to use. 
 :type charset_type: str 
 :return: The cyclic pattern. 
 :rtype: str'","'Generate a cyclic pattern 
 Args: 
 - size: size of generated pattern (Int) 
 - start: the start offset of the generated pattern (Int) 
 - charset_type: charset type 
 0: basic (0-9A-za-z) 
 1: extended (default) 
 2: maximum (almost printable chars) 
 Returns: 
 - pattern text (byte string) (str in Python 2; bytes in Python 3)'"
"def Bar(xs, ys, **options): 
    options = _UnderrideColor(options) 
   options = _Underride(options, linewidth=0, alpha=0.6) 
   pyplot.bar(xs, ys, **options)"," 'Plot a bar chart with the given data. 
 Parameters 
 xs : array-like 
 The x-values of the bars. 
 ys : array-like 
 The y-values of the bars. 
 options : dict 
 Additional options for the plot. 
 See Also 
 pyplot.bar'","'Plots a line. 
 Args: 
 xs: sequence of x values 
 ys: sequence of y values 
 options: keyword args passed to pyplot.bar'"
"def dump(obj, fp, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None, use_decimal=True, namedtuple_as_object=True, tuple_as_array=True, bigint_as_string=False, sort_keys=False, item_sort_key=None, for_json=False, ignore_nan=False, int_as_string_bitcount=None, iterable_as_array=False, **kw): 
    if ((not skipkeys) and ensure_ascii and check_circular and allow_nan and (cls is None) and (indent is None) and (separators is None) and (encoding == 'utf-8') and (default is None) and use_decimal and namedtuple_as_object and tuple_as_array and (not iterable_as_array) and (not bigint_as_string) and (not sort_keys) and (not item_sort_key) and (not for_json) and (not ignore_nan) and (int_as_string_bitcount is None) and (not kw)): 
      iterable = _default_encoder.iterencode(obj) 
   else: 
      if (cls is None): 
         cls = JSONEncoder 
      iterable = cls(skipkeys=skipkeys, ensure_ascii=ensure_ascii, check_circular=check_circular, allow_nan=allow_nan, indent=indent, separators=separators, encoding=encoding, default=default, use_decimal=use_decimal, namedtuple_as_object=namedtuple_as_object, tuple_as_array=tuple_as_array, iterable_as_array=iterable_as_array, bigint_as_string=bigint_as_string, sort_keys=sort_keys, item_sort_key=item_sort_key, for_json=for_json, ignore_nan=ignore_nan, int_as_string_bitcount=int_as_string_bitcount, **kw).iterencode(obj) 
   for chunk in iterable: 
      fp.write(chunk)"," 'Dump a Python object to a file-like object. 
 :param obj: The object to dump. 
 :param fp: The file-like object to dump to. 
 :param skipkeys: Whether to skip keys when dumping a dict. 
 :param ensure_ascii: Whether to ensure that all bytes are ASCII. 
 :param check_circular: Whether to check for circular references. 
 :param allow_nan: Whether to allow None values. 
 :param cls: The class to use for the dict. 
 :param indent: The number of spaces to indent the dict output. 
 :param separators: The separators to use for the dict output. 
 :param encoding: The encoding to use for the dict output. 
 :param default: The default value to use for missing keys. 
 :param use_decimal: Whether to use decimal.Decimal for numeric values. 
 :param namedtuple_as_object: Whether to use namedtuples as objects. 
 :param tuple_as_array: Whether to use tuples as arrays. 
 :param bigint_as_string: Whether to use bigint","'Serialize ``obj`` as a JSON formatted stream to ``fp`` (a 
 ``.write()``-supporting file-like object). 
 If *skipkeys* is true then ``dict`` keys that are not basic types 
 (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``) 
 will be skipped instead of raising a ``TypeError``. 
 If *ensure_ascii* is false, then the some chunks written to ``fp`` 
 may be ``unicode`` instances, subject to normal Python ``str`` to 
 ``unicode`` coercion rules. Unless ``fp.write()`` explicitly 
 understands ``unicode`` (as in ``codecs.getwriter()``) this is likely 
 to cause an error. 
 If *check_circular* is false, then the circular reference check 
 for container types will be skipped and a circular reference will 
 result in an ``OverflowError`` (or worse). 
 If *allow_nan* is false, then it will be a ``ValueError`` to 
 serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) 
 in strict compliance of the original JSON specification, instead of using 
 the JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``). See 
 *ignore_nan* for ECMA-262 compliant behavior. 
 If *indent* is a string, then JSON array elements and object members 
 will be pretty-printed with a newline followed by that string repeated 
 for each level of nesting. ``None`` (the default) selects the most compact 
 representation without any newlines. For backwards compatibility with 
 versions of simplejson earlier than 2.1.0, an integer is also accepted 
 and is converted to a string with that many spaces. 
 If specified, *separators* should be an 
 ``(item_separator, key_separator)`` tuple.  The default is ``(\', \', \': \')`` 
 if *indent* is ``None`` and ``(\',\', \': \')`` otherwise.  To get the most 
 compact JSON representation, you should specify ``(\',\', \':\')`` to eliminate 
 whitespace. 
 *encoding* is the character encoding for str instances, default is UTF-8. 
 *default(obj)* is a function that should return a serializable version 
 of obj or raise ``TypeError``. The default simply raises ``TypeError``. 
 If *use_decimal* is true (default: ``True``) then decimal.Decimal 
 will be natively serialized to JSON with full precision. 
 If *namedtuple_as_object* is true (default: ``True``), 
 :class:`tuple` subclasses with ``_asdict()`` methods will be encoded 
 as JSON objects. 
 If *tuple_as_array* is true (default: ``True``), 
 :class:`tuple` (and subclasses) will be encoded as JSON arrays. 
 If *iterable_as_array* is true (default: ``False``), 
 any object not in the above table that implements ``__iter__()`` 
 will be encoded as a JSON array. 
 If *bigint_as_string* is true (default: ``False``), ints 2**53 and higher 
 or lower than -2**53 will be encoded as strings. This is to avoid the 
 rounding that happens in Javascript otherwise. Note that this is still a 
 lossy operation that will not round-trip correctly and should be used 
 sparingly. 
 If *int_as_string_bitcount* is a positive number (n), then int of size 
 greater than or equal to 2**n or lower than or equal to -2**n will be 
 encoded as strings. 
 If specified, *item_sort_key* is a callable used to sort the items in 
 each dictionary. This is useful if you want to sort items other than 
 in alphabetical order by key. This option takes precedence over 
 *sort_keys*. 
 If *sort_keys* is true (default: ``False``), the output of dictionaries 
 will be sorted by item. 
 If *for_json* is true (default: ``False``), objects with a ``for_json()`` 
 method will use the return value of that method for encoding as JSON 
 instead of the object. 
 If *ignore_nan* is true (default: ``False``), then out of range 
 :class:`float` values (``nan``, ``inf``, ``-inf``) will be serialized as 
 ``null`` in compliance with the ECMA-262 specification. If true, this will 
 override *allow_nan*. 
 To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the 
 ``.default()`` method to serialize additional types), specify it with 
 the ``cls`` kwarg. NOTE: You should use *default* or *for_json* instead 
 of subclassing whenever possible.'"
"def uslugify_encoded(text, sep): 
    if (text is None): 
      return u'' 
   tag_id = RE_TAGS.sub(u'', unicodedata.normalize(u'NFKD', text)).lower() 
   tag_id = RE_WORD.sub(u'', tag_id).replace(u'   ', sep) 
   return quote(tag_id.encode(u'utf-8'))"," 'Convert a unicode string into a URL-encoded string. 
 :param text: A unicode string 
 :param sep: A separator character, default is \'-\' 
 :type text: unicode 
 :type sep: unicode 
 :return: A unicode string'",'Custom slugify (percent encoded).'
"def find_email(string, unique=True): 
    string = u(string).replace(u'\u2024', '.') 
   matches = [] 
   for m in RE_EMAIL.finditer(string): 
      s = m.group(0) 
      if ((not unique) or (s not in matches)): 
         matches.append(s) 
   return matches"," 'Returns a list of email addresses found in a string. 
 :param string: 
 :type string: 
 :param unique: 
 :type unique: 
 :return: 
 :rtype:'",'Returns a list of e-mail addresses parsed from the string.'
"def record_usage(key_prefix, time_slice): 
    key = _make_ratelimit_cache_key(key_prefix, time_slice) 
   try: 
      g.ratelimitcache.add(key, 0, time=time_slice.remaining) 
      try: 
         return g.ratelimitcache.incr(key) 
      except pylibmc.NotFound: 
         now = int(time.time()) 
         if (now < time_slice.end): 
            g.ratelimitcache.add(key, 1, time=((time_slice.end - now) + 1)) 
            g.stats.simple_event('ratelimit.eviction') 
         return 1 
   except pylibmc.Error as e: 
      raise RatelimitError(e)"," 'Record usage of the ratelimit for the given key. 
 :param key_prefix: the key prefix to record usage for 
 :type key_prefix: str 
 :param time_slice: the time slice to record usage for 
 :type time_slice: :class:`~.time.TimeSlice` 
 :raises: :class:`~.errors.RatelimitError` 
 :returns: the number of remaining hits for the key'","'Record usage of a ratelimit for the specified time slice. 
 The total usage (including this one) of the ratelimit is returned or 
 RatelimitError is raised if something went wrong during the process.'"
"def setup_platform(hass, config, add_devices, discovery_info=None): 
    try: 
      bbox_data = BboxData() 
      bbox_data.update() 
   except requests.exceptions.HTTPError as error: 
      _LOGGER.error(error) 
      return False 
   name = config.get(CONF_NAME) 
   sensors = [] 
   for variable in config[CONF_MONITORED_VARIABLES]: 
      sensors.append(BboxSensor(bbox_data, variable, name)) 
   add_devices(sensors)", 'Create a Bbox sensor for each monitored variable.','Set up the Bbox sensor.'
"@require_POST 
 @login_required 
 def watch_forum(request, forum_slug): 
    forum = get_object_or_404(Forum, slug=forum_slug) 
   if (not forum.allows_viewing_by(request.user)): 
      raise Http404 
   if (request.POST.get('watch') == 'yes'): 
      NewThreadEvent.notify(request.user, forum) 
      statsd.incr('forums.watches.forum') 
   else: 
      NewThreadEvent.stop_notifying(request.user, forum) 
   return HttpResponseRedirect(reverse('forums.threads', args=[forum_slug]))", 'Update whether a user is watching a forum.','Watch/unwatch a forum (based on \'watch\' POST param).'
"def overrides_disabled(): 
    return bool(_OVERRIDES_DISABLED.disabled)", 'Returns True if overrides are disabled.',"'Checks to see whether overrides are disabled in the current context. 
 Returns a boolean value.  See `disable_overrides`.'"
"def H_from_ransac(fp, tp, model, maxiter=1000, match_theshold=10): 
    import ransac 
   data = vstack((fp, tp)) 
   (H, ransac_data) = ransac.ransac(data.T, model, 4, maxiter, match_theshold, 10, return_all=True) 
   return (H, ransac_data['inliers'])", 'Generate H from ransac',"'Robust estimation of homography H from point 
 correspondences using RANSAC (ransac.py from 
 http://www.scipy.org/Cookbook/RANSAC). 
 input: fp,tp (3*n arrays) points in hom. coordinates.'"
"def shorten_string(string, max_width): 
    string_len = len(string) 
   if (string_len <= max_width): 
      return string 
   visible = ((max_width - 16) - int(log10(string_len))) 
   if (not isinstance(string, unistr)): 
      visstring = unistr(string[:visible], errors='ignore') 
   else: 
      visstring = string[:visible] 
   return u''.join((visstring, u'...(and   ', unistr((string_len - visible)), u'   more)'))"," 'Return a shortened string with the first `max_width` characters, 
 followed by a ellipsis, and the remaining characters. 
 If the string is too long, the ellipsis is followed by a number 
 indicating how many characters are missing. 
 If the string is not unicode, it is converted to unicode before 
 shortening.'","'make limited length string in form: 
 ""the string is very lo...(and 15 more)""'"
"def test_mouse_key_events(): 
    me = MouseEvent('mouse_press') 
   for fun in (me.pos, me.button, me.buttons, me.modifiers, me.delta, me.press_event, me.last_event, me.is_dragging): 
      fun 
   me.drag_events() 
   me._forget_last_event() 
   me.trail() 
   ke = KeyEvent('key_release') 
   ke.key 
   ke.text 
   ke.modifiers", 'Test mouse and key events','Test mouse and key events'
"def init_native(): 
    init_subsystem(Native.Factory) 
   return Native.Factory.global_instance().create()", 'Initialize the native subsystem.','Initialize and return the `Native` subsystem.'
"def run_supervised_learning(predictor_fp, response_fp, response_name, ntree=1000, errortype='oob', output_dir='.', verbose=False, HALT_EXEC=False): 
    rsl = RSupervisedLearner(HALT_EXEC=HALT_EXEC) 
   rsl.Parameters['-m'].on(response_fp) 
   rsl.Parameters['-c'].on(response_name) 
   rsl.Parameters['-n'].on(str(ntree)) 
   rsl.Parameters['-o'].on(output_dir) 
   rsl.Parameters['-e'].on(errortype) 
   if verbose: 
      rsl.Parameters['-v'].on() 
   app_result = rsl(predictor_fp) 
   remove(join(output_dir, (splitext(split(predictor_fp)[1])[0] + '.txt'))) 
   return app_result"," 'Runs a supervised learning algorithm on a predictor and response. 
 Parameters 
 predictor_fp : str 
 Path to the predictor. 
 response_fp : str 
 Path to the response. 
 response_name : str 
 Name of the response. 
 ntree : int 
 Number of trees to be trained. 
 errortype : str 
 Type of error to be used. 
 output_dir : str 
 Path to the directory where the output will be saved. 
 verbose : bool 
 If True, prints the status of the algorithm. 
 HALT_EXEC : bool 
 If True, halts the algorithm when the error is not improving. 
 Returns 
 app_result : str 
 Path to the output file. 
 Notes 
 This function uses the RSupervisedLearner class. 
 Examples 
 >>> import nltk 
 >>> from nltk.classify import RSupervisedLearner 
 >>> from nltk.classify.util import run_supervised_learning 
 >>> data = nltk.corpus.words.words()","'Run supervised learning (random forests here) 
 predictor_fp: path to otu table 
 response_fp: path to metadata table 
 response_name: Column header for gradient variable in metadata table 
 ntree: Number of trees in forest 
 errortype: method for estimating generalization error 
 output_dir: output directory 
 verbose: print verbose output 
 output_dir: directory where output should be written (default \'.\') 
 HALT_EXEC: halt just before running the formatdb command and'"
"def test_no_truncate_using_compare(): 
    w = wcs.WCS(naxis=3) 
   w.wcs.crval = [240.9303333333, 50, 212345678000.0] 
   w.wcs.cdelt = [0.001, 0.001, 100000000.0] 
   w.wcs.ctype = [u'RA---TAN', u'DEC--TAN', u'FREQ'] 
   w.wcs.set() 
   w2 = wcs.WCS(w.to_header()) 
   w.wcs.compare(w2.wcs)", 'Test that the WCS is not truncated.',"'Regression test for https://github.com/astropy/astropy/issues/4612 
 This one uses WCS.wcs.compare and some slightly different values'"
"def _synthesize(browser, update_tryorder=1): 
    cmd = browser.split()[0] 
   if (not _iscommand(cmd)): 
      return [None, None] 
   name = os.path.basename(cmd) 
   try: 
      command = _browsers[name.lower()] 
   except KeyError: 
      return [None, None] 
   controller = command[1] 
   if (controller and (name.lower() == controller.basename)): 
      import copy 
      controller = copy.copy(controller) 
      controller.name = browser 
      controller.basename = os.path.basename(browser) 
      register(browser, None, controller, update_tryorder) 
      return [None, controller] 
   return [None, None]"," 'Synthesize a controller from a browser command. 
 If the command is not a recognized command, return None. 
 Otherwise, if the command is recognized, and the controller is 
 registered, return None.  Otherwise, create a new controller. 
 :param browser: The command to synthesize a controller for 
 :param update_tryorder: If True, update the tryorder of the controller 
 :return: A tuple of None, a controller'","'Attempt to synthesize a controller base on existing controllers. 
 This is useful to create a controller when a user specifies a path to 
 an entry in the BROWSER environment variable -- we can copy a general 
 controller to operate using a specific installation of the desired 
 browser in this way. 
 If we can\'t create a controller in this way, or if there is no 
 executable for the requested browser, return [None, None].'"
"def timeout(reactor, deferred, timeout_sec, reason=None): 
    def _timeout(): 
      deferred.cancel() 
   delayed_timeout = reactor.callLater(timeout_sec, _timeout) 
   if (reason is not None): 
      def maybe_replace_reason(passthrough): 
         if delayed_timeout.active(): 
            return passthrough 
         return Failure(reason) 
      deferred.addErrback(maybe_replace_reason) 
   def abort_timeout(passthrough): 
      if delayed_timeout.active(): 
         delayed_timeout.cancel() 
      return passthrough 
   deferred.addBoth(abort_timeout) 
   return deferred"," 'Schedule a timeout. 
 :param reactor: The reactor to schedule the timeout with. 
 :param deferred: The deferred to schedule the timeout on. 
 :param timeout_sec: The amount of time to wait before the timeout. 
 :param reason: A reason to cancel the timeout. 
 :returns: The deferred that will be cancelled when the timeout expires.'","'Adds a timeout to an existing deferred.  If the timeout expires before the 
 deferred expires, then the deferred is cancelled. 
 :param IReactorTime reactor: The reactor implementation to schedule the 
 timeout. 
 :param Deferred deferred: The deferred to cancel at a later point in time. 
 :param float timeout_sec: The number of seconds to wait before the deferred 
 should time out. 
 :param Exception reason: An exception used to create a Failure with which 
 to fire the Deferred if the timeout is encountered.  If not given, 
 ``deferred`` retains its original failure behavior. 
 :return: The updated deferred.'"
"def _row_from_json(row, schema): 
    row_data = [] 
   for (field, cell) in zip(schema, row['f']): 
      converter = _CELLDATA_FROM_JSON[field.field_type] 
      if (field.mode == 'REPEATED'): 
         row_data.append([converter(item['v'], field) for item in cell['v']]) 
      else: 
         row_data.append(converter(cell['v'], field)) 
   return tuple(row_data)"," 'Convert a row from a JSON object into a row of values for a table. 
 :param row: A row from a JSON object. 
 :param schema: The schema for the table. 
 :return: A row of values for the table. 
 :rtype: list'","'Convert JSON row data to row with appropriate types. 
 :type row: dict 
 :param row: A JSON response row to be converted. 
 :type schema: tuple 
 :param schema: A tuple of 
 :class:`~google.cloud.bigquery.schema.SchemaField`. 
 :rtype: tuple 
 :returns: A tuple of data converted to native types.'"
"def tree_from_cix(cix): 
    if isinstance(cix, unicode): 
      cix = cix.encode('UTF-8', 'xmlcharrefreplace') 
   tree = ET.XML(cix) 
   version = tree.get('version') 
   if (version == CIX_VERSION): 
      return tree 
   elif (version == '0.1'): 
      return tree_2_0_from_tree_0_1(tree) 
   else: 
      raise CodeIntelError(('unknown   CIX   version:   %r' % version))"," 'Convert a CIX document to a tree. 
 :param cix: the CIX document. 
 :type cix: string 
 :return: the tree. 
 :rtype: ET.Element'","'Return a (ci)tree for the given CIX content. 
 Raises pyexpat.ExpatError if the CIX content could not be parsed.'"
"@register.filter 
 @stringfilter 
 def issue_status_icon(status): 
    if (status == BaseComment.OPEN): 
      return u'rb-icon-issue-open' 
   elif (status == BaseComment.RESOLVED): 
      return u'rb-icon-issue-resolved' 
   elif (status == BaseComment.DROPPED): 
      return u'rb-icon-issue-dropped' 
   else: 
      raise ValueError((u'Unknown   comment   issue   status   ""%s""' % status))"," 'Returns the icon to use for the given issue status. 
 :param status: The status to return the icon for. 
 :type status: str 
 :return: The icon to use for the given status. 
 :rtype: str'","'Return an icon name for the issue status. 
 Args: 
 status (unicode): 
 The stored issue status for the comment. 
 Returns: 
 unicode: The icon name for the issue status.'"
"def test_integer_sequence_generator(): 
    floatX = theano.config.floatX 
   rng = numpy.random.RandomState(1234) 
   readout_dim = 5 
   feedback_dim = 3 
   dim = 20 
   batch_size = 30 
   n_steps = 10 
   transition = GatedRecurrent(dim=dim, activation=Tanh(), weights_init=Orthogonal()) 
   generator = SequenceGenerator(Readout(readout_dim=readout_dim, source_names=['states'], emitter=SoftmaxEmitter(theano_seed=1234), feedback_brick=LookupFeedback(readout_dim, feedback_dim)), transition, weights_init=IsotropicGaussian(0.1), biases_init=Constant(0), seed=1234) 
   generator.initialize() 
   y = tensor.lmatrix('y') 
   mask = tensor.matrix('mask') 
   costs = generator.cost_matrix(y, mask) 
   assert (costs.ndim == 2) 
   costs_fun = theano.function([y, mask], [costs]) 
   y_test = rng.randint(readout_dim, size=(n_steps, batch_size)) 
   m_test = numpy.ones((n_steps, batch_size), dtype=floatX) 
   costs_val = costs_fun(y_test, m_test)[0] 
   assert (costs_val.shape == (n_steps, batch_size)) 
   assert_allclose(costs_val.sum(), 482.827, rtol=1e-05) 
   cost = generator.cost(y, mask) 
   assert (cost.ndim == 0) 
   cost_val = theano.function([y, mask], [cost])(y_test, m_test) 
   assert_allclose(cost_val, 16.0942, rtol=1e-05) 
   cg = ComputationGraph([cost]) 
   var_filter = VariableFilter(roles=[AUXILIARY]) 
   aux_var_name = '_'.join([generator.name, generator.cost.name, 'per_sequence_element']) 
   cost_per_el = [el for el in var_filter(cg.variables) if (el.name == aux_var_name)][0] 
   assert (cost_per_el.ndim == 0) 
   cost_per_el_val = theano.function([y, mask], [cost_per_el])(y_test, m_test) 
   assert_allclose(cost_per_el_val, 1.60942, rtol=1e-05) 
   (states, outputs, costs) = generator.generate(iterate=True, batch_size=batch_size, n_steps=n_steps) 
   cg = ComputationGraph(((states + outputs) + costs)) 
   (states_val, outputs_val, costs_val) = theano.function([], [states, outputs, costs], updates=cg.updates)() 
   assert (states_val.shape == (n_steps, batch_size, dim)) 
   assert (outputs_val.shape == (n_steps, batch_size)) 
   assert (outputs_val.dtype == 'int64') 
   assert (costs_val.shape == (n_steps, batch_size)) 
   assert_allclose(states_val.sum(), (-17.854), rtol=1e-05) 
   assert_allclose(costs_val.sum(), 482.868, rtol=1e-05) 
   assert (outputs_val.sum() == 629) 
   cost1 = costs_fun([[1], [2]], [[1], [1]])[0] 
   cost2 = costs_fun([[3, 1], [4, 2], [2, 0]], [[1, 1], [1, 1], [1, 0]])[0] 
   assert_allclose(cost1.sum(), cost2[:, 1].sum(), rtol=1e-05)", 'Test a sequence generator with a single recurrent layer.',"'Test a sequence generator with integer outputs. 
 Such sequence generators can be used to e.g. model language.'"
"def remove_trailing_string(content, trailing): 
    if (content.endswith(trailing) and (content != trailing)): 
      return content[:(- len(trailing))] 
   return content", 'Remove trailing string from content if it is there.',"'Strip trailing component `trailing` from `content` if it exists. 
 Used when generating names from view classes.'"
"def exit_if_empty(): 
    state = twill.get_browser() 
   form = state.get_form('1') 
   if (not form): 
      print 'No   messages;   exiting.' 
      raise SystemExit", 'Exit if there are no messages.',"'>> exit_if_empty 
 Exit the script currently running, if there are no deferred messages 
 on the current page.'"
"def instances_by_name(name_filter): 
    return [o for o in gc.get_objects() if (name_filter == typename(o))]", 'Returns a list of instances with the given name.',"'Return the list of objects that exactly match the given 
 name_filter.'"
"def RenderParetoCdf(xmin, alpha, low, high, n=50): 
    if (low < xmin): 
      low = xmin 
   xs = np.linspace(low, high, n) 
   ps = (1 - ((xs / xmin) ** (- alpha))) 
   return (xs, ps)"," 'Returns the Pareto distribution cdf for a given set of parameters. 
 Parameters 
 xmin : float 
 Minimum of the distribution. 
 alpha : float 
 Power of the distribution. 
 low : float 
 Lower bound of the distribution. 
 high : float 
 Upper bound of the distribution. 
 n : int 
 Number of points to generate. 
 Returns 
 (x, p) : tuple 
 x : array 
 The values of the distribution. 
 p : array 
 The probabilities of the distribution.'","'Generates sequences of xs and ps for a Pareto CDF. 
 xmin: parameter 
 alpha: parameter 
 low: float 
 high: float 
 n: number of points to render 
 returns: numpy arrays (xs, ps)'"
"def _check_apt(): 
    if (not HAS_APT): 
      raise CommandExecutionError(""Error:   'python-apt'   package   not   installed"")", 'Check if apt is installed','Abort if python-apt is not installed'
"def __update_loaders(z): 
    non_local['loaders'] = [] 
   for filename in z.namelist(): 
      if (not isinstance(filename, str_cls)): 
         filename = filename.decode('utf-8') 
      non_local['loaders'].append(filename)", 'Update the loaders list with the new files in the zipped archive.',"'Updates the cached list of loaders from a zipfile. The loader_lock MUST 
 be held when calling this function. 
 :param z: 
 The zipfile.ZipFile object to list the files in'"
"def word_ids_to_words(data, id_to_word): 
    return [id_to_word[i] for i in data]", 'Convert a list of word ids to a list of words.',"'Given a context (ids) in list format and the vocabulary, 
 Returns a list of words to represent the context. 
 Parameters 
 data : a list of integer 
 the context in list format 
 id_to_word : a dictionary 
 mapping id to unique word. 
 Returns 
 A list of string or byte to represent the context. 
 Examples 
 >>> see words_to_word_ids'"
"def obrientransform(*args): 
    TINY = np.sqrt(np.finfo(float).eps) 
   arrays = [] 
   for arg in args: 
      a = np.asarray(arg) 
      n = len(a) 
      mu = np.mean(a) 
      sq = ((a - mu) ** 2) 
      sumsq = sq.sum() 
      t = (((((n - 1.5) * n) * sq) - (0.5 * sumsq)) / ((n - 1) * (n - 2))) 
      var = (sumsq / (n - 1)) 
      if (abs((var - np.mean(t))) > TINY): 
         raise ValueError('Lack   of   convergence   in   obrientransform.') 
      arrays.append(t) 
   return np.array(arrays)"," 'Obrient transform of a sequence of values. 
 Parameters 
 *args* 
 Sequence of values. 
 Returns 
 *array* 
 Sequence of values transformed by the obrient transform. 
 Notes 
 The obrient transform is a nonlinear transformation of the form 
 `x -> (x - mu) / sqrt(var + t)` where `t` is a sequence of values 
 computed from the sequence of values `x`. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Obrient_transformation 
 .. [2] http://www.mathworks.com/matlabcentral/fileexchange/20021-obrient-transform-in-matlab 
 .. [3] http://www.mathworks.com/matlabcentral/fileexchange/20021-obrient-transform-in-matlab 
 .. [4] http://en.wikipedia.org/wiki/Nonlinear_regression 
 .. [5] http://en.wikipedia.org/wiki/Least_squares_approximation'","'Computes the O\'Brien transform on input data (any number of arrays). 
 Used to test for homogeneity of variance prior to running one-way stats. 
 Each array in ``*args`` is one level of a factor. 
 If `f_oneway` is run on the transformed data and found significant, 
 the variances are unequal.  From Maxwell and Delaney [1]_, p.112. 
 Parameters 
 args : tuple of array_like 
 Any number of arrays. 
 Returns 
 obrientransform : ndarray 
 Transformed data for use in an ANOVA.  The first dimension 
 of the result corresponds to the sequence of transformed 
 arrays.  If the arrays given are all 1-D of the same length, 
 the return value is a 2-D array; otherwise it is a 1-D array 
 of type object, with each element being an ndarray. 
 References 
 .. [1] S. E. Maxwell and H. D. Delaney, ""Designing Experiments and 
 Analyzing Data: A Model Comparison Perspective"", Wadsworth, 1990. 
 Examples 
 We\'ll test the following data sets for differences in their variance. 
 >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10] 
 >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15] 
 Apply the O\'Brien transform to the data. 
 >>> from scipy.stats import obrientransform 
 >>> tx, ty = obrientransform(x, y) 
 Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the 
 transformed data. 
 >>> from scipy.stats import f_oneway 
 >>> F, p = f_oneway(tx, ty) 
 >>> p 
 0.1314139477040335 
 If we require that ``p < 0.05`` for significance, we cannot conclude 
 that the variances are different.'"
"def test_install_package_with_target(script): 
    target_dir = (script.scratch_path / 'target') 
   result = script.pip_install_local('-t', target_dir, 'simple==1.0') 
   assert (((Path('scratch') / 'target') / 'simple') in result.files_created), str(result) 
   result = script.pip_install_local('-t', target_dir, 'simple==1.0', expect_stderr=True) 
   assert (not (((Path('scratch') / 'target') / 'simple') in result.files_updated)) 
   result = script.pip_install_local('--upgrade', '-t', target_dir, 'simple==2.0') 
   assert (((Path('scratch') / 'target') / 'simple') in result.files_updated), str(result) 
   egg_folder = (((Path('scratch') / 'target') / 'simple-2.0-py%s.egg-info') % pyversion) 
   assert (egg_folder in result.files_created), str(result) 
   result = script.pip_install_local('-t', target_dir, 'singlemodule==0.0.0') 
   singlemodule_py = ((Path('scratch') / 'target') / 'singlemodule.py') 
   assert (singlemodule_py in result.files_created), str(result) 
   result = script.pip_install_local('-t', target_dir, 'singlemodule==0.0.1', '--upgrade') 
   assert (singlemodule_py in result.files_updated), str(result)", 'Test installing packages with target','Test installing a package using pip install --target'
"def occur_check(var, x): 
    if (var == x): 
      return True 
   elif isinstance(x, Compound): 
      return occur_check(var, x.args) 
   elif is_args(x): 
      if any((occur_check(var, xi) for xi in x)): 
         return True 
   return False", 'Checks whether var occurs in x.','var occurs in subtree owned by x?'
"def predecessor(G, source, target=None, cutoff=None, return_seen=None): 
    if (source not in G): 
      raise nx.NodeNotFound('Source   {}   not   in   G'.format(source)) 
   level = 0 
   nextlevel = [source] 
   seen = {source: level} 
   pred = {source: []} 
   while nextlevel: 
      level = (level + 1) 
      thislevel = nextlevel 
      nextlevel = [] 
      for v in thislevel: 
         for w in G[v]: 
            if (w not in seen): 
               pred[w] = [v] 
               seen[w] = level 
               nextlevel.append(w) 
            elif (seen[w] == level): 
               pred[w].append(v) 
      if (cutoff and (cutoff <= level)): 
         break 
   if (target is not None): 
      if return_seen: 
         if (not (target in pred)): 
            return ([], (-1)) 
         return (pred[target], seen[target]) 
      else: 
         if (not (target in pred)): 
            return [] 
         return pred[target] 
   elif return_seen: 
      return (pred, seen) 
   else: 
      return pred"," 'Returns the predecessors of the source node in the graph. 
 Parameters 
 G : NetworkX graph 
 The graph. 
 source : node 
 The source node. 
 target : node, optional 
 The target node. 
 cutoff : int, optional 
 The maximum level to search. 
 return_seen : bool, optional 
 If True, return the predecessors and the level they were found at. 
 Returns 
 predecessors : list of nodes 
 The predecessors of the source node. 
 level : int 
 The level of the source node. 
 Examples 
 >>> from networkx.algorithms.common import predecessor 
 >>> G = nx.DiGraph() 
 >>> G.add_edge(0, 1) 
 >>> G.add_edge(0, 2) 
 >>> G.add_edge(1, 3) 
 >>> G.add_edge(2, 3) 
 >>> predecessor(G, 0) 
 ([1, 2], 1) 
 >>> predecessor(G, 1)","'Returns dictionary of predecessors for the path from source to all nodes in G. 
 Parameters 
 G : NetworkX graph 
 source : node label 
 Starting node for path 
 target : node label, optional 
 Ending node for path. If provided only predecessors between 
 source and target are returned 
 cutoff : integer, optional 
 Depth to stop the search. Only paths of length <= cutoff are returned. 
 Returns 
 pred : dictionary 
 Dictionary, keyed by node, of predecessors in the shortest path. 
 Examples 
 >>> G = nx.path_graph(4) 
 >>> list(G) 
 [0, 1, 2, 3] 
 >>> nx.predecessor(G, 0) 
 {0: [], 1: [0], 2: [1], 3: [2]}'"
"def _fingerprint(public_key): 
    try: 
      if six.PY2: 
         raw_key = public_key.decode('base64') 
      else: 
         raw_key = base64.b64decode(public_key, validate=True) 
   except binascii.Error: 
      return None 
   ret = hashlib.md5(raw_key).hexdigest() 
   chunks = [ret[i:(i + 2)] for i in range(0, len(ret), 2)] 
   return ':'.join(chunks)", 'Fingerprint a public key.',"'Return a public key fingerprint based on its base64-encoded representation 
 The fingerprint string is formatted according to RFC 4716 (ch.4), that is, 
 in the form ""xx:xx:...:xx"" 
 If the key is invalid (incorrect base64 string), return None'"
"def get_cli_body_ssh(command, response, module): 
    if ('^' == response[0]): 
      body = [] 
   elif ('running' in command): 
      body = response 
   else: 
      if (command in response[0]): 
         response = [response[0].split(command)[1]] 
      try: 
         body = [json.loads(response[0])] 
      except ValueError: 
         module.fail_json(msg='Command   does   not   support   JSON   output', command=command) 
   return body", 'Get the body of the ssh command',"'Get response for when transport=cli.  This is kind of a hack and mainly 
 needed because these modules were originally written for NX-API.  And 
 not every command supports ""| json"" when using cli/ssh.'"
"def hash_(attrs=None, where=None): 
    return _osquery_cmd(table='hash', attrs=attrs, where=where)", 'Returns a hash of the given attributes.',"'Return hash information from osquery 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' osquery.hash'"
"def load(): 
    data = _get_data() 
   return du.process_recarray(data, endog_idx=0, dtype=float)", 'Load the data from the file.',"'Load the strikes data and return a Dataset class instance. 
 Returns 
 Dataset instance: 
 See DATASET_PROPOSAL.txt for more information.'"
"def url_unquote_plus(s, charset='utf-8', errors='replace'): 
    if isinstance(s, unicode): 
      s = s.encode(charset) 
   return _decode_unicode(_unquote_plus(s), charset, errors)"," 'Unquotes a URL-encoded string. 
 :param s: The string to unquote. 
 :param charset: The character encoding to use. 
 :param errors: The errors to raise on decoding errors. 
 :return: The decoded string.'","'URL decode a single string with the given decoding and decode 
 a ""+"" to whitespace. 
 Per default encoding errors are ignored.  If you want a different behavior 
 you can set `errors` to ``\'replace\'`` or ``\'strict\'``.  In strict mode a 
 `HTTPUnicodeError` is raised. 
 :param s: the string to unquote. 
 :param charset: the charset to be used. 
 :param errors: the error handling for the charset decoding.'"
"def createMemoryWorker(): 
    def perform(): 
      if (not worker._pending): 
         return False 
      if (worker._pending[0] is NoMoreWork): 
         return False 
      worker._pending.pop(0)() 
      return True 
   worker = MemoryWorker() 
   return (worker, perform)"," 'Create a worker that uses a memory queue to store pending work. 
 :return: A tuple of (worker, perform), where perform is a function that 
 returns True if the worker is ready to be run, False otherwise.'","'Create an L{IWorker} that does nothing but defer work, to be performed 
 later. 
 @return: a worker that will enqueue work to perform later, and a callable 
 that will perform one element of that work. 
 @rtype: 2-L{tuple} of (L{IWorker}, L{callable})'"
"def is_master_node(client): 
    my_node_id = list(client.nodes.info('_local')['nodes'])[0] 
   master_node_id = client.cluster.state(metric='master_node')['master_node'] 
   return (my_node_id == master_node_id)"," 'Returns true if the node is the master node, false otherwise.'","'Return `True` if the connected client node is the elected master node in 
 the Elasticsearch cluster, otherwise return `False`. 
 :arg client: An :class:`elasticsearch.Elasticsearch` client object 
 :rtype: bool'"
"def get_config_vars(*args): 
    global _config_vars 
   if (_config_vars is None): 
      func = globals().get(('_init_' + os.name)) 
      if func: 
         func() 
      else: 
         _config_vars = {} 
      _config_vars['prefix'] = PREFIX 
      _config_vars['exec_prefix'] = EXEC_PREFIX 
      if (sys.platform == 'darwin'): 
         import _osx_support 
         _osx_support.customize_config_vars(_config_vars) 
   if args: 
      vals = [] 
      for name in args: 
         vals.append(_config_vars.get(name)) 
      return vals 
   else: 
      return _config_vars"," 'Returns a dictionary of configuration variables. 
 If `args` is given, returns a list of configuration variables 
 matching the given names.  Otherwise, returns the global variable 
 `_config_vars`.'","'With no arguments, return a dictionary of all configuration 
 variables relevant for the current platform.  Generally this includes 
 everything needed to build extensions and install both pure modules and 
 extensions.  On Unix, this means every variable defined in Python\'s 
 installed Makefile; on Windows and Mac OS it\'s a much smaller set. 
 With arguments, return a list of values that result from looking up 
 each argument in the configuration variable dictionary.'"
"@frappe.whitelist() 
 def remove(doctype, name, assign_to): 
    try: 
      todo = frappe.db.get_value(u'ToDo', {u'reference_type': doctype, u'reference_name': name, u'owner': assign_to, u'status': u'Open'}) 
      if todo: 
         todo = frappe.get_doc(u'ToDo', todo) 
         todo.status = u'Closed' 
         todo.save(ignore_permissions=True) 
         notify_assignment(todo.assigned_by, todo.owner, todo.reference_type, todo.reference_name) 
   except frappe.DoesNotExistError: 
      pass 
   if frappe.get_meta(doctype).get_field(u'assigned_to'): 
      frappe.db.set_value(doctype, name, u'assigned_to', None) 
   return get({u'doctype': doctype, u'name': name})"," 'Remove a ToDo item. 
 :param doctype: Reference to the ToDo item 
 :param name: Reference to the ToDo item 
 :param assign_to: The user who will be notified when the ToDo item is closed'",'remove from todo'
"def create_apppool(name): 
    pscmd = [] 
   current_apppools = list_apppools() 
   apppool_path = 'IIS:\\AppPools\\{0}'.format(name) 
   if (name in current_apppools): 
      _LOG.debug(""Application   pool   '%s'   already   present."", name) 
      return True 
   pscmd.append(""New-Item   -Path   '{0}'"".format(apppool_path)) 
   cmd_ret = _srvmgr(str().join(pscmd)) 
   if (cmd_ret['retcode'] == 0): 
      _LOG.debug('Application   pool   created   successfully:   %s', name) 
      return True 
   _LOG.error('Unable   to   create   application   pool:   %s', name) 
   return False"," 'Create an application pool. 
 :param name: name of the application pool to create 
 :type name: str 
 :return: True if the application pool was created successfully, False otherwise'","'Create an IIS application pool. 
 .. note: 
 This function only validates against the application pool name, and will return 
 True even if the application pool already exists with a different configuration. 
 It will not modify the configuration of an existing application pool. 
 :param str name: The name of the IIS application pool. 
 :return: A boolean representing whether all changes succeeded. 
 :rtype: bool 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' win_iis.create_apppool name=\'MyTestPool\''"
"def do_baremetal_node_list(cs, _args): 
    _emit_deprecation_warning('baremetal-node-list') 
   nodes = cs.baremetal.list() 
   _print_baremetal_nodes_list(nodes)", 'List baremetal nodes.','DEPRECATED: Print list of available baremetal nodes.'
"def fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1): 
    options = {'xatol': xtol, 'maxiter': maxfun, 'disp': disp} 
   res = _minimize_scalar_bounded(func, (x1, x2), args, **options) 
   if full_output: 
      return (res['x'], res['fun'], res['status'], res['nfev']) 
   else: 
      return res['x']"," 'Finds the minimum of a function bounded by two values. 
 Parameters 
 func : callable 
 A function that takes the arguments ``(x1, x2)`` and returns a 
 scalar. 
 x1 : scalar 
 The lower bound of the search interval. 
 x2 : scalar 
 The upper bound of the search interval. 
 args : tuple of scalars 
 Additional arguments to pass to the function ``func``. 
 xtol : scalar 
 The tolerance for the function value. 
 maxfun : int 
 The maximum number of function evaluations. 
 full_output : int 
 0: Return only the minimum value of the function. 
 1: Return the minimum value of the function and the number of function 
 evaluations. 
 disp : int 
 0: Do not print any output. 
 1: Print all output. 
 Returns 
 x : scalar 
 The minimum value of the function. 
 fun : scalar 
 The value of the function at the minimum. 
 status : int 
 The status of the minimization. 
 nfev : int","'Bounded minimization for scalar functions. 
 Parameters 
 func : callable f(x,*args) 
 Objective function to be minimized (must accept and return scalars). 
 x1, x2 : float or array scalar 
 The optimization bounds. 
 args : tuple, optional 
 Extra arguments passed to function. 
 xtol : float, optional 
 The convergence tolerance. 
 maxfun : int, optional 
 Maximum number of function evaluations allowed. 
 full_output : bool, optional 
 If True, return optional outputs. 
 disp : int, optional 
 If non-zero, print messages. 
 0 : no message printing. 
 1 : non-convergence notification messages only. 
 2 : print a message on convergence too. 
 3 : print iteration results. 
 Returns 
 xopt : ndarray 
 Parameters (over given interval) which minimize the 
 objective function. 
 fval : number 
 The function value at the minimum point. 
 ierr : int 
 An error flag (0 if converged, 1 if maximum number of 
 function calls reached). 
 numfunc : int 
 The number of function calls made. 
 See also 
 minimize_scalar: Interface to minimization algorithms for scalar 
 univariate functions. See the \'Bounded\' `method` in particular. 
 Notes 
 Finds a local minimizer of the scalar function `func` in the 
 interval x1 < xopt < x2 using Brent\'s method.  (See `brent` 
 for auto-bracketing).'"
"def primary_key_value(instance, as_string=False): 
    result = getattr(instance, primary_key_for(instance)) 
   if (not as_string): 
      return result 
   try: 
      return str(result) 
   except UnicodeEncodeError: 
      return url_quote_plus(result.encode('utf-8'))"," 'Return the primary key value of an instance. 
 :param instance: An instance of the model. 
 :param as_string: If True, the value will be returned as a string. 
 :return: The primary key value of the instance. 
 :rtype: str'","'Returns the value of the primary key field of the specified `instance` 
 of a SQLAlchemy model. 
 This essentially a convenience function for:: 
 getattr(instance, primary_key_for(instance)) 
 If `as_string` is ``True``, try to coerce the return value to a string.'"
"def between(expr, lower_bound, upper_bound): 
    expr = _literal_as_binds(expr) 
   return expr.between(lower_bound, upper_bound)"," 'Returns a new expression that is `expr` between `lower_bound` and 
 `upper_bound`. 
 >>> from sympy.core.expr import between 
 >>> between(2, 1, 3) 
 2 
 >>> between(2, 1, 3) == between(2, 1, 3) 
 True 
 >>> between(2, 1, 3) != between(2, 1, 3) 
 False 
 >>> between(2, 1, 3) == between(2, 1, 3, 2, 1, 3) 
 True 
 >>> between(2, 1, 3) != between(2, 1, 3, 2, 1, 3) 
 False 
 >>> between(2, 1, 3) == between(2, 1, 3, 2, 1, 3, 2, 1, 3) 
 True 
 >>> between(2, 1, 3) != between(2, 1, 3, 2, 1, ","'Produce a ``BETWEEN`` predicate clause. 
 E.g.:: 
 from sqlalchemy import between 
 stmt = select([users_table]).where(between(users_table.c.id, 5, 7)) 
 Would produce SQL resembling:: 
 SELECT id, name FROM user WHERE id BETWEEN :id_1 AND :id_2 
 The :func:`.between` function is a standalone version of the 
 :meth:`.ColumnElement.between` method available on all 
 SQL expressions, as in:: 
 stmt = select([users_table]).where(users_table.c.id.between(5, 7)) 
 All arguments passed to :func:`.between`, including the left side 
 column expression, are coerced from Python scalar values if a 
 the value is not a :class:`.ColumnElement` subclass.   For example, 
 three fixed values can be compared as in:: 
 print(between(5, 3, 7)) 
 Which would produce:: 
 :param_1 BETWEEN :param_2 AND :param_3 
 :param expr: a column expression, typically a :class:`.ColumnElement` 
 instance or alternatively a Python scalar expression to be coerced 
 into a column expression, serving as the left side of the ``BETWEEN`` 
 expression. 
 :param lower_bound: a column or Python scalar expression serving as the lower 
 bound of the right side of the ``BETWEEN`` expression. 
 :param upper_bound: a column or Python scalar expression serving as the 
 upper bound of the right side of the ``BETWEEN`` expression. 
 .. seealso:: 
 :meth:`.ColumnElement.between`'"
"def _windows_commondata_path(): 
    import ctypes 
   from ctypes import wintypes, windll 
   CSIDL_COMMON_APPDATA = 35 
   _SHGetFolderPath = windll.shell32.SHGetFolderPathW 
   _SHGetFolderPath.argtypes = [wintypes.HWND, ctypes.c_int, wintypes.HANDLE, wintypes.DWORD, wintypes.LPCWSTR] 
   path_buf = wintypes.create_unicode_buffer(wintypes.MAX_PATH) 
   _SHGetFolderPath(0, CSIDL_COMMON_APPDATA, 0, 0, path_buf) 
   return path_buf.value", 'Returns the path to the common data folder for Windows.',"'Return the common appdata path, using ctypes 
 From http://stackoverflow.com/questions/626796/    how-do-i-find-the-windows-common-application-data-folder-using-python'"
"def get_all_launch_configurations(region=None, key=None, keyid=None, profile=None): 
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
   try: 
      return conn.get_all_launch_configurations() 
   except boto.exception.BotoServerError as e: 
      log.error(e) 
      return []"," 'Get all launch configurations. 
 :param region: region name 
 :param key: AWS access key ID 
 :param keyid: AWS access key ID 
 :param profile: AWS profile name 
 :returns: list of launch configurations'","'Fetch and return all Launch Configuration with details. 
 CLI example:: 
 salt myminion boto_asg.get_all_launch_configurations'"
"def test_conflicting_path(tmpdir, mocked_aws_cf_simple): 
    with tmpdir.as_cwd(): 
      tmpdir.join('config.yaml').write(mocked_aws_cf_simple) 
      assert (main(['create']) == 0) 
      assert (main(['create']) == 1)"," 'Test that when config.yaml is present and conflicts with the default, 
 the user is prompted to choose the config to use.'","'Ensure default cluster info path is never overwritten 
 by launching successive clusters'"
"def test_StepDict_raise_StepLoadingError_if_load_first_argument_is_not_a_regex(): 
    steps = StepDict() 
   test_load = (lambda : steps.load('an   invalid   regex;)', (lambda : ''))) 
   assert_raises(StepLoadingError, test_load)"," 'Test that a StepLoadingError is raised if the load_first argument is not a 
 regex.'","'lettuce.STEP_REGISTRY.load(step, func) should raise an error if step is not a regex'"
"def _conf(family='ipv4'): 
    if (__grains__['os_family'] == 'RedHat'): 
      if (family == 'ipv6'): 
         return '/etc/sysconfig/ip6tables' 
      else: 
         return '/etc/sysconfig/iptables' 
   elif (__grains__['os_family'] == 'Arch'): 
      if (family == 'ipv6'): 
         return '/etc/iptables/ip6tables.rules' 
      else: 
         return '/etc/iptables/iptables.rules' 
   elif (__grains__['os_family'] == 'Debian'): 
      if (family == 'ipv6'): 
         return '/etc/iptables/rules.v6' 
      else: 
         return '/etc/iptables/rules.v4' 
   elif (__grains__['os'] == 'Gentoo'): 
      if (family == 'ipv6'): 
         return '/var/lib/ip6tables/rules-save' 
      else: 
         return '/var/lib/iptables/rules-save' 
   elif (__grains__['os_family'] == 'SUSE'): 
      return '/etc/sysconfig/scripts/SuSEfirewall2-custom' 
   else: 
      raise SaltException((('Saving   iptables   to   file   is   not' + '   supported   on   {0}.'.format(__grains__['os'])) + '   Please   file   an   issue   with   SaltStack'))"," 'Returns the path to the iptables file that should be used to save 
 the iptables rules. 
 This function is platform dependent. 
 :param family: ipv4 or ipv6 
 :return: path to the iptables file'",'Some distros have a specific location for config files'
"def prelu(layer, **kwargs): 
    nonlinearity = getattr(layer, 'nonlinearity', None) 
   if (nonlinearity is not None): 
      layer.nonlinearity = nonlinearities.identity 
   return ParametricRectifierLayer(layer, **kwargs)"," 'Applies a prelu nonlinearity to a layer. 
 Parameters 
 layer : Layer 
 The layer to be transformed. 
 kwargs : dict 
 Additional keyword arguments are passed to the 
 :py:meth:`~theano.tensor.op.ParametricRectifierLayer` constructor. 
 Returns 
 layer : Layer 
 The layer with the prelu nonlinearity applied. 
 Examples 
 >>> x = T.matrix() 
 >>> x.tag.test = True 
 >>> layer = nn.Dense(2, nonlinearity=nn.relu) 
 >>> prelu(layer, nonlinearity=nn.prelu) 
 >>> x.tag.test 
 True'","'Convenience function to apply parametric rectify to a given layer\'s output. 
 Will set the layer\'s nonlinearity to identity if there is one and will 
 apply the parametric rectifier instead. 
 Parameters 
 layer: a :class:`Layer` instance 
 The `Layer` instance to apply the parametric rectifier layer to; 
 note that it will be irreversibly modified as specified above 
 **kwargs 
 Any additional keyword arguments are passed to the 
 :class:`ParametericRectifierLayer` 
 Examples 
 Note that this function modifies an existing layer, like this: 
 >>> from lasagne.layers import InputLayer, DenseLayer, prelu 
 >>> layer = InputLayer((32, 100)) 
 >>> layer = DenseLayer(layer, num_units=200) 
 >>> layer = prelu(layer) 
 In particular, :func:`prelu` can *not* be passed as a nonlinearity.'"
"def setup(templates, *args, **kwargs): 
    test_once = kwargs.get('test_once', False) 
   for arg in args: 
      templates.update(arg) 
   templates['inclusion.html'] = '{{   result   }}' 
   loaders = [('django.template.loaders.cached.Loader', [('django.template.loaders.locmem.Loader', templates)])] 
   def decorator(func): 
      @override_settings(TEMPLATES=None) 
      @functools.wraps(func) 
      def inner(self): 
         libraries = getattr(self, 'libraries', {}) 
         self.engine = Engine(libraries=libraries, loaders=loaders) 
         func(self) 
         if test_once: 
            return 
         func(self) 
         self.engine = Engine(libraries=libraries, loaders=loaders, string_if_invalid='INVALID') 
         func(self) 
         func(self) 
         self.engine = Engine(debug=True, libraries=libraries, loaders=loaders) 
         func(self) 
         func(self) 
      return inner 
   return decorator"," 'Set up a template loader for the given templates. 
 The loader will be used for rendering the template, as well as 
 for rendering the test results. 
 The template loader is setup with the given templates and a 
 dictionary of options. 
 The test once option is used to determine if the template should 
 be rendered once or twice. 
 :param templates: The template dictionary to be used. 
 :param test_once: If True, only render the template once. 
 :param kwargs: Additional options to pass to the template loader. 
 :return: The decorator that will be used to decorate the template 
 renderer.'","'Runs test method multiple times in the following order: 
 debug       cached      string_if_invalid 
 False       False 
 False       True 
 False       False       INVALID 
 False       True        INVALID 
 True        False 
 True        True'"
"def get_base_dirs(): 
    if options['basedirlist']: 
      return options['basedirlist'] 
   if os.environ.get('MPLBASEDIRLIST'): 
      return os.environ.get('MPLBASEDIRLIST').split(os.pathsep) 
   win_bases = ['win32_static'] 
   if os.getenv('CONDA_DEFAULT_ENV'): 
      win_bases.append(os.path.join(os.getenv('CONDA_DEFAULT_ENV'), 'Library')) 
   basedir_map = {'win32': win_bases, 'darwin': ['/usr/local/', '/usr', '/usr/X11', '/opt/X11', '/opt/local'], 'sunos5': [(os.getenv('MPLIB_BASE') or '/usr/local')], 'gnu0': ['/usr'], 'aix5': ['/usr/local']} 
   return basedir_map.get(sys.platform, ['/usr/local', '/usr'])"," 'Return a list of paths to search for MPL binaries. 
 This function will return a list of paths to search for MPL binaries. 
 This list is used to determine the MPL_BINDIR environment variable, which 
 is used to determine the default path to look for MPL binaries. 
 The default value is ``['/usr/local', '/usr']``. 
 If ``basedirlist`` is set in the configuration file, it will be used instead. 
 If ``MPLBASEDIRLIST`` is set in the environment, it will be used instead. 
 If ``CONDA_DEFAULT_ENV`` is set, it will be used to determine the default 
 path to look for MPL binaries. 
 :returns: a list of paths to search for MPL binaries. 
 :rtype: list of str'",'Returns a list of standard base directories on this platform.'
"def ci(a, which=95, axis=None): 
    p = ((50 - (which / 2)), (50 + (which / 2))) 
   return percentiles(a, p, axis)"," 'Compute the confidence interval for the given axis. 
 Parameters 
 a : array 
 Data to be analyzed. 
 which : int 
 Confidence level. 
 axis : int 
 Axis to compute the confidence interval for. 
 Returns 
 ci : array 
 The confidence interval. 
 Examples 
 >>> from scipy import stats 
 >>> stats.ci(numpy.arange(10)) 
 (0.0, 1.0) 
 >>> stats.ci(numpy.arange(10), 95) 
 (0.0, 1.0) 
 >>> stats.ci(numpy.arange(10), 95, 0) 
 (0.0, 1.0) 
 >>> stats.ci(numpy.arange(10), 95, 1) 
 (0.0, 1.0) 
 >>> stats.ci(numpy.arange(10), 95, 2) 
 (0.0, 1.0) 
 >>> stats.ci(numpy.arange(10),",'Return a percentile range from an array of values.'
"def zpk2sos(z, p, k, pairing='nearest'): 
    valid_pairings = ['nearest', 'keep_odd'] 
   if (pairing not in valid_pairings): 
      raise ValueError(('pairing   must   be   one   of   %s,   not   %s' % (valid_pairings, pairing))) 
   if (len(z) == len(p) == 0): 
      return array([[k, 0.0, 0.0, 1.0, 0.0, 0.0]]) 
   p = np.concatenate((p, np.zeros(max((len(z) - len(p)), 0)))) 
   z = np.concatenate((z, np.zeros(max((len(p) - len(z)), 0)))) 
   n_sections = ((max(len(p), len(z)) + 1) // 2) 
   sos = zeros((n_sections, 6)) 
   if (((len(p) % 2) == 1) and (pairing == 'nearest')): 
      p = np.concatenate((p, [0.0])) 
      z = np.concatenate((z, [0.0])) 
   assert (len(p) == len(z)) 
   z = np.concatenate(_cplxreal(z)) 
   p = np.concatenate(_cplxreal(p)) 
   p_sos = np.zeros((n_sections, 2), np.complex128) 
   z_sos = np.zeros_like(p_sos) 
   for si in range(n_sections): 
      p1_idx = np.argmin(np.abs((1 - np.abs(p)))) 
      p1 = p[p1_idx] 
      p = np.delete(p, p1_idx) 
      if (np.isreal(p1) and (np.isreal(p).sum() == 0)): 
         z1_idx = _nearest_real_complex_idx(z, p1, 'real') 
         z1 = z[z1_idx] 
         z = np.delete(z, z1_idx) 
         p2 = z2 = 0 
      else: 
         if ((not np.isreal(p1)) and (np.isreal(z).sum() == 1)): 
            z1_idx = _nearest_real_complex_idx(z, p1, 'complex') 
            assert (not np.isreal(z[z1_idx])) 
         else: 
            z1_idx = np.argmin(np.abs((p1 - z))) 
         z1 = z[z1_idx] 
         z = np.delete(z, z1_idx) 
         if (not np.isreal(p1)): 
            if (not np.isreal(z1)): 
               p2 = p1.conj() 
               z2 = z1.conj() 
            else: 
               p2 = p1.conj() 
               z2_idx = _nearest_real_complex_idx(z, p1, 'real') 
               z2 = z[z2_idx] 
               assert np.isreal(z2) 
               z = np.delete(z, z2_idx) 
         else: 
            if (not np.isreal(z1)): 
               z2 = z1.conj() 
               p2_idx = _nearest_real_complex_idx(p, z1, 'real') 
               p2 = p[p2_idx] 
               assert np.isreal(p2) 
            else: 
               idx = np.where(np.isreal(p))[0] 
               assert (len(idx) > 0) 
               p2_idx = idx[np.argmin(np.abs((np.abs(p[idx]) - 1)))] 
               p2 = p[p2_idx] 
               assert np.isreal(p2) 
               z2_idx = _nearest_real_complex_idx(z, p2, 'real') 
               z2 = z[z2_idx] 
               assert np.isreal(z2) 
               z = np.delete(z, z2_idx) 
            p = np.delete(p, p2_idx) 
      p_sos[si] = [p1, p2] 
      z_sos[si] = [z1, z2] 
   assert (len(p) == len(z) == 0) 
   del p, z 
   p_sos = np.reshape(p_sos[::(-1)], (n_sections, 2)) 
   z_sos = np.reshape(z_sos[::(-1)], (n_sections, 2)) 
   gains = np.ones(n_sections) 
   gains[0] = k 
   for si in range(n_sections): 
      x = zpk2tf(z_sos[si], p_sos[si], gains[si]) 
      sos[si] = np.concatenate(x) 
   return sos"," 'Convert zpk to SOS. 
 Parameters 
 z : array 
 zpk coefficients 
 p : array 
 pqr coefficients 
 k : float 
 gain 
 pairing : \'nearest\' or \'keep_odd\'. 
 Returns 
 sos : array 
 SOS coefficients 
 Examples 
 >>> from sympy.abc import x 
 >>> from sympy.physics.electrical.circuits import zpk2sos 
 >>> z = [1, 2, 3] 
 >>> p = [1, 2] 
 >>> k = 2 
 >>> zpk2sos(z, p, k) 
 array([[2, 1, 0, 1, 0, 0]])'","'Return second-order sections from zeros, poles, and gain of a system 
 Parameters 
 z : array_like 
 Zeros of the transfer function. 
 p : array_like 
 Poles of the transfer function. 
 k : float 
 System gain. 
 pairing : {\'nearest\', \'keep_odd\'}, optional 
 The method to use to combine pairs of poles and zeros into sections. 
 See Notes below. 
 Returns 
 sos : ndarray 
 Array of second-order filter coefficients, with shape 
 ``(n_sections, 6)``. See `sosfilt` for the SOS filter format 
 specification. 
 See Also 
 sosfilt 
 Notes 
 The algorithm used to convert ZPK to SOS format is designed to 
 minimize errors due to numerical precision issues. The pairing 
 algorithm attempts to minimize the peak gain of each biquadratic 
 section. This is done by pairing poles with the nearest zeros, starting 
 with the poles closest to the unit circle. 
 *Algorithms* 
 The current algorithms are designed specifically for use with digital 
 filters. (The output coefficents are not correct for analog filters.) 
 The steps in the ``pairing=\'nearest\'`` and ``pairing=\'keep_odd\'`` 
 algorithms are mostly shared. The ``nearest`` algorithm attempts to 
 minimize the peak gain, while ``\'keep_odd\'`` minimizes peak gain under 
 the constraint that odd-order systems should retain one section 
 as first order. The algorithm steps and are as follows: 
 As a pre-processing step, add poles or zeros to the origin as 
 necessary to obtain the same number of poles and zeros for pairing. 
 If ``pairing == \'nearest\'`` and there are an odd number of poles, 
 add an additional pole and a zero at the origin. 
 The following steps are then iterated over until no more poles or 
 zeros remain: 
 1. Take the (next remaining) pole (complex or real) closest to the 
 unit circle to begin a new filter section. 
 2. If the pole is real and there are no other remaining real poles [#]_, 
 add the closest real zero to the section and leave it as a first 
 order section. Note that after this step we are guaranteed to be 
 left with an even number of real poles, complex poles, real zeros, 
 and complex zeros for subsequent pairing iterations. 
 3. Else: 
 1. If the pole is complex and the zero is the only remaining real 
 zero*, then pair the pole with the *next* closest zero 
 (guaranteed to be complex). This is necessary to ensure that 
 there will be a real zero remaining to eventually create a 
 first-order section (thus keeping the odd order). 
 2. Else pair the pole with the closest remaining zero (complex or 
 real). 
 3. Proceed to complete the second-order section by adding another 
 pole and zero to the current pole and zero in the section: 
 1. If the current pole and zero are both complex, add their 
 conjugates. 
 2. Else if the pole is complex and the zero is real, add the 
 conjugate pole and the next closest real zero. 
 3. Else if the pole is real and the zero is complex, add the 
 conjugate zero and the real pole closest to those zeros. 
 4. Else (we must have a real pole and real zero) add the next 
 real pole closest to the unit circle, and then add the real 
 zero closest to that pole. 
 .. [#] This conditional can only be met for specific odd-order inputs 
 with the ``pairing == \'keep_odd\'`` method. 
 .. versionadded:: 0.16.0 
 Examples 
 Design a 6th order low-pass elliptic digital filter for a system with a 
 sampling rate of 8000 Hz that has a pass-band corner frequency of 
 1000 Hz.  The ripple in the pass-band should not exceed 0.087 dB, and 
 the attenuation in the stop-band should be at least 90 dB. 
 In the following call to `signal.ellip`, we could use ``output=\'sos\'``, 
 but for this example, we\'ll use ``output=\'zpk\'``, and then convert to SOS 
 format with `zpk2sos`: 
 >>> from scipy import signal 
 >>> z, p, k = signal.ellip(6, 0.087, 90, 1000/(0.5*8000), output=\'zpk\') 
 Now convert to SOS format. 
 >>> sos = signal.zpk2sos(z, p, k) 
 The coefficients of the numerators of the sections: 
 >>> sos[:, :3] 
 array([[ 0.0014154 ,  0.00248707,  0.0014154 ], 
 [ 1.        ,  0.72965193,  1.        ], 
 [ 1.        ,  0.17594966,  1.        ]]) 
 The symmetry in the coefficients occurs because all the zeros are on the 
 unit circle. 
 The coefficients of the denominators of the sections: 
 >>> sos[:, 3:] 
 array([[ 1.        , -1.32543251,  0.46989499], 
 [ 1.        , -1.26117915,  0.6262586 ], 
 [ 1.        , -1.25707217,  0.86199667]]) 
 The next example shows the effect of the `pairing` option.  We have a 
 system with three poles and three zeros, so the SOS array will have 
 shape (2, 6).  The means there is, in effect, an extra pole and an extra 
 zero at the origin in the SOS representation. 
 >>> z1 = np.array([-1, -0.5-0.5j, -0.5+0.5j]) 
 >>> p1 = np.array([0.75, 0.8+0.1j, 0.8-0.1j]) 
 With ``pairing=\'nearest\'`` (the default), we obtain 
 >>> signal.zpk2sos(z1, p1, 1) 
 array([[ 1.  ,  1.  ,  0.5 ,  1.  , -0.75,  0.  ], 
 [ 1.  ,  1.  ,  0.  ,  1.  , -1.6 ,  0.65]]) 
 The first section has the zeros {-0.5-0.05j, -0.5+0.5j} and the poles 
 {0, 0.75}, and the second section has the zeros {-1, 0} and poles 
 {0.8+0.1j, 0.8-0.1j}.  Note that the extra pole and zero at the origin 
 have been assigned to different sections. 
 With ``pairing=\'keep_odd\'``, we obtain: 
 >>> signal.zpk2sos(z1, p1, 1, pairing=\'keep_odd\') 
 array([[ 1.  ,  1.  ,  0.  ,  1.  , -0.75,  0.  ], 
 [ 1.  ,  1.  ,  0.5 ,  1.  , -1.6 ,  0.65]]) 
 The extra pole and zero at the origin are in the same section. 
 The first section is, in effect, a first-order section.'"
"def test_to(): 
    with pytest.raises(falcon.http_status.HTTPStatus) as redirect: 
      hug.redirect.to('/') 
   assert ('302' in redirect.value.status)", 'Testing the redirect to /','Test that the base redirect to function works as expected'
"def get_load(jid): 
    jid_dir = salt.utils.jid.jid_dir(jid, _job_dir(), __opts__['hash_type']) 
   load_fn = os.path.join(jid_dir, LOAD_P) 
   if ((not os.path.exists(jid_dir)) or (not os.path.exists(load_fn))): 
      return {} 
   serial = salt.payload.Serial(__opts__) 
   ret = serial.load(salt.utils.fopen(os.path.join(jid_dir, LOAD_P), 'rb')) 
   minions_cache = [os.path.join(jid_dir, MINIONS_P)] 
   minions_cache.extend(glob.glob(os.path.join(jid_dir, SYNDIC_MINIONS_P.format('*')))) 
   all_minions = set() 
   for minions_path in minions_cache: 
      log.debug('Reading   minion   list   from   %s', minions_path) 
      try: 
         all_minions.update(serial.load(salt.utils.fopen(minions_path, 'rb'))) 
      except IOError as exc: 
         salt.utils.files.process_read_exception(exc, minions_path) 
   if all_minions: 
      ret['Minions'] = sorted(all_minions) 
   return ret"," 'Return a dictionary of data that is useful to the salt-master. 
 :param jid: The job ID to retrieve data for 
 :return: A dictionary of data that is useful to the salt-master'",'Return the load data that marks a specified jid'
"def parse_features(feature_files, language=None): 
    scenario_collector = FeatureScenarioLocationCollector() 
   features = [] 
   for location in feature_files: 
      if (not isinstance(location, FileLocation)): 
         assert isinstance(location, string_types) 
         location = FileLocation(os.path.normpath(location)) 
      if (location.filename == scenario_collector.filename): 
         scenario_collector.add_location(location) 
         continue 
      elif scenario_collector.feature: 
         current_feature = scenario_collector.build_feature() 
         features.append(current_feature) 
         scenario_collector.clear() 
      assert isinstance(location, FileLocation) 
      filename = os.path.abspath(location.filename) 
      feature = parser.parse_file(filename, language=language) 
      if feature: 
         scenario_collector.feature = feature 
         scenario_collector.add_location(location) 
   if scenario_collector.feature: 
      current_feature = scenario_collector.build_feature() 
      features.append(current_feature) 
   return features"," 'Parse feature files and return a list of features. 
 :param feature_files: A list of feature files to parse. 
 :param language: The language of the feature files. 
 :return: A list of features. 
 :rtype: list(Feature)'","'Parse feature files and return list of Feature model objects. 
 Handles: 
 * feature file names, ala ""alice.feature"" 
 * feature file locations, ala: ""alice.feature:10"" 
 :param feature_files: List of feature file names to parse. 
 :param language:      Default language to use. 
 :return: List of feature objects.'"
"def get_credential(tenant_id, credential_id): 
    session = db.get_session() 
   try: 
      cred = session.query(l2network_models.Credential).filter_by(tenant_id=tenant_id).filter_by(credential_id=credential_id).one() 
      return cred 
   except exc.NoResultFound: 
      raise c_exc.CredentialNotFound(credential_id=credential_id, tenant_id=tenant_id)", 'Get a credential by tenant_id and credential_id.','Lists the creds for given a cred_id and tenant_id'
"@CELERY_APP.task 
 def send_ccx_course_published(course_key): 
    course_key = CourseLocator.from_string(course_key) 
   for ccx in CustomCourseForEdX.objects.filter(course_id=course_key): 
      try: 
         ccx_key = CCXLocator.from_course_locator(course_key, unicode(ccx.id)) 
      except InvalidKeyError: 
         log.info('Attempt   to   publish   course   with   deprecated   id.   Course:   %s.   CCX:   %s', course_key, ccx.id) 
         continue 
      responses = SignalHandler.course_published.send(sender=ccx, course_key=ccx_key) 
      for (rec, response) in responses: 
         log.info('Signal   fired   when   course   is   published.   Receiver:   %s.   Response:   %s', rec, response)", 'Send signal when course is published.',"'Find all CCX derived from this course, and send course published event for them.'"
"def inverse_hankel_transform(F, k, r, nu, **hints): 
    return InverseHankelTransform(F, k, r, nu).doit(**hints)"," 'Inverse Hankel transform of F(k, r, nu) to F(k, r, nu) 
 This is a generalization of the inverse Hankel transform of 
 F(k, r, nu) to F(k, r, nu). 
 Examples 
 >>> from sympy import inverse_hankel_transform, symbols 
 >>> from sympy.abc import k, r, nu 
 >>> F = symbols(\'F\', k, r, nu) 
 >>> inverse_hankel_transform(F, k, r, nu) 
 F(k, r, nu) 
 >>> inverse_hankel_transform(F, k, r, nu, k=2) 
 F(2, r, nu) 
 >>> inverse_hankel_transform(F, k, r, nu, k=2, r=3) 
 F(2, 3, nu) 
 >>> inverse_hankel_transform(F, k, r, nu, k=2, r=3, nu=4) 
 F(2, 3, 4)'","'Compute the inverse Hankel transform of `F` defined as 
 .. math:: f(r) = \int_{0}^\infty F_\nu(k) J_\nu(k r) k \mathrm{d} k. 
 If the transform cannot be computed in closed form, this 
 function returns an unevaluated :class:`InverseHankelTransform` object. 
 For a description of possible hints, refer to the docstring of 
 :func:`sympy.integrals.transforms.IntegralTransform.doit`. 
 Note that for this transform, by default ``noconds=True``. 
 >>> from sympy import hankel_transform, inverse_hankel_transform, gamma 
 >>> from sympy import gamma, exp, sinh, cosh 
 >>> from sympy.abc import r, k, m, nu, a 
 >>> ht = hankel_transform(1/r**m, r, k, nu) 
 >>> ht 
 2*2**(-m)*k**(m - 2)*gamma(-m/2 + nu/2 + 1)/gamma(m/2 + nu/2) 
 >>> inverse_hankel_transform(ht, k, r, nu) 
 r**(-m) 
 >>> ht = hankel_transform(exp(-a*r), r, k, 0) 
 >>> ht 
 a/(k**3*(a**2/k**2 + 1)**(3/2)) 
 >>> inverse_hankel_transform(ht, k, r, 0) 
 exp(-a*r) 
 See Also 
 fourier_transform, inverse_fourier_transform 
 sine_transform, inverse_sine_transform 
 cosine_transform, inverse_cosine_transform 
 hankel_transform 
 mellin_transform, laplace_transform'"
"def iterable(obj): 
    try: 
      iter(obj) 
   except TypeError: 
      return False 
   return True"," 'Return True if obj is iterable, False otherwise.'",'return true if *obj* is iterable'
"def normalize(a, axis=None): 
    a_sum = a.sum(axis) 
   if (axis and (a.ndim > 1)): 
      a_sum[(a_sum == 0)] = 1 
      shape = list(a.shape) 
      shape[axis] = 1 
      a_sum.shape = shape 
   a /= a_sum"," 'Normalize the array a along axis. 
 If axis is None, normalize the array a. 
 If axis is specified, normalize the array along axis. 
 Parameters 
 a : ndarray 
 Input array. 
 axis : int, optional 
 Axis to normalize. 
 Returns 
 a : ndarray 
 Normalized array. 
 Examples 
 >>> from sympy.physics.vector import normalize 
 >>> from sympy.physics.vector import cross 
 >>> a = [1, 2, 3] 
 >>> normalize(a) 
 [0.33333333, 0.66666667, 1.0] 
 >>> normalize([1, 2, 3]) 
 [0.5, 0.5, 1.0] 
 >>> normalize([1, 2, 3], 1) 
 [0.5, 0.5, 1.0] 
 >>> normalize([1, 2, 3], 1) 
 [0.5, 0","'Normalizes the input array so that it sums to 1. 
 Parameters 
 a : array 
 Non-normalized input data. 
 axis : int 
 Dimension along which normalization is performed. 
 Notes 
 Modifies the input **inplace**.'"
"def krackhardt_kite_graph(create_using=None): 
    description = ['adjacencylist', 'Krackhardt   Kite   Social   Network', 10, [[2, 3, 4, 6], [1, 4, 5, 7], [1, 4, 6], [1, 2, 3, 5, 6, 7], [2, 4, 7], [1, 3, 4, 7, 8], [2, 4, 5, 6, 8], [6, 7, 9], [8, 10], [9]]] 
   G = make_small_undirected_graph(description, create_using) 
   return G"," 'Create a graph that is a kite graph. 
 This is a graph that is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
 The graph is a kite graph. 
","'Return the Krackhardt Kite Social Network. 
 A 10 actor social network introduced by David Krackhardt 
 to illustrate: degree, betweenness, centrality, closeness, etc. 
 The traditional labeling is: 
 Andre=1, Beverley=2, Carol=3, Diane=4, 
 Ed=5, Fernando=6, Garth=7, Heather=8, Ike=9, Jane=10.'"
"def save_load(jid, load, minions=None): 
    serv = _get_serv(ret=None) 
   if ('influxdb08' in serv.__module__): 
      req = [{'name': 'jids', 'columns': ['jid', 'load'], 'points': [[jid, json.dumps(load)]]}] 
   else: 
      req = [{'measurement': 'jids', 'tags': {'jid': jid}, 'fields': {'load': json.dumps(load)}}] 
   try: 
      serv.write_points(req) 
   except Exception as ex: 
      log.critical('Failed   to   store   load   with   InfluxDB   returner:   {0}'.format(ex))"," 'Save the load to InfluxDB. 
 :param jid: The jid to save. 
 :param load: The load to save. 
 :param minions: The minions to save. 
 :type jid: str 
 :type load: dict 
 :type minions: list[str]'",'Save the load to the specified jid'
"def is_effective_user(user_id_or_name): 
    euid = os.geteuid() 
   if (str(user_id_or_name) == str(euid)): 
      return True 
   effective_user_name = pwd.getpwuid(euid).pw_name 
   return (user_id_or_name == effective_user_name)"," 'Check if the user is an effective user. 
 The effective user is the user that is currently logged in. 
 :param user_id_or_name: The user\'s id or name. 
 :type user_id_or_name: str 
 :return: True if the user is an effective user, False otherwise.'",'Returns True if user_id_or_name is effective user (id/name).'
"def _match_abbrev(s, wordmap): 
    if (s in wordmap): 
      return s 
   else: 
      possibilities = [word for word in wordmap.keys() if word.startswith(s)] 
      if (len(possibilities) == 1): 
         return possibilities[0] 
      elif (not possibilities): 
         raise BadOptionError(s) 
      else: 
         possibilities.sort() 
         raise AmbiguousOptionError(s, possibilities)"," 'Return the abbreviation for the given word, or raise an exception. 
 :param s: The word to check 
 :param wordmap: The wordmap 
 :raises: BadOptionError or AmbiguousOptionError'","'_match_abbrev(s : string, wordmap : {string : Option}) -> string 
 Return the string key in \'wordmap\' for which \'s\' is an unambiguous 
 abbreviation.  If \'s\' is found to be ambiguous or doesn\'t match any of 
 \'words\', raise BadOptionError.'"
"def get_key_func(key_func): 
    if (key_func is not None): 
      if callable(key_func): 
         return key_func 
      else: 
         (key_func_module_path, key_func_name) = key_func.rsplit(u'.', 1) 
         key_func_module = import_module(key_func_module_path) 
         return getattr(key_func_module, key_func_name) 
   return default_key_func"," 'Returns the key function given the key function or None. 
 If the key function is a function, it is returned as-is. 
 Otherwise, the key function is imported and the function is returned.'","'Function to decide which key function to use. 
 Defaults to ``default_key_func``.'"
"def weight_boundary(graph, src, dst, n): 
    default = {'weight': 0.0, 'count': 0} 
   count_src = graph[src].get(n, default)['count'] 
   count_dst = graph[dst].get(n, default)['count'] 
   weight_src = graph[src].get(n, default)['weight'] 
   weight_dst = graph[dst].get(n, default)['weight'] 
   count = (count_src + count_dst) 
   return {'count': count, 'weight': (((count_src * weight_src) + (count_dst * weight_dst)) / count)}"," 'Return the weight of the edge between the two nodes. 
 :param graph: A graph 
 :param src: The source node 
 :param dst: The destination node 
 :param n: The node name 
 :return: The weight of the edge between the two nodes 
 :rtype: dict'","'Handle merging of nodes of a region boundary region adjacency graph. 
 This function computes the `""weight""` and the count `""count""` 
 attributes of the edge between `n` and the node formed after 
 merging `src` and `dst`. 
 Parameters 
 graph : RAG 
 The graph under consideration. 
 src, dst : int 
 The vertices in `graph` to be merged. 
 n : int 
 A neighbor of `src` or `dst` or both. 
 Returns 
 data : dict 
 A dictionary with the ""weight"" and ""count"" attributes to be 
 assigned for the merged node.'"
"def xstr(*args): 
    if _use_unicode: 
      return unicode(*args) 
   else: 
      return str(*args)", 'Returns a string representation of the arguments.','call str or unicode depending on current mode'
"def mark_as_titlepage(container, name, move_to_start=True): 
    ver = container.opf_version_parsed 
   if move_to_start: 
      for (item, q, linear) in container.spine_iter: 
         if (name == q): 
            break 
      if (not linear): 
         item.set(u'linear', u'yes') 
      if (item.getparent().index(item) > 0): 
         container.insert_into_xml(item.getparent(), item, 0) 
   if (ver.major < 3): 
      for ref in container.opf_xpath(u'//opf:guide/opf:reference[@type=""cover""]'): 
         ref.getparent().remove(ref) 
      for guide in get_guides(container): 
         container.insert_into_xml(guide, guide.makeelement(OPF(u'reference'), type=u'cover', href=container.name_to_href(name, container.opf_name))) 
   else: 
      container.apply_unique_properties(name, u'calibre:title-page') 
   container.dirty(container.opf_name)"," 'Mark a page as a title page. 
 If the container is a spine, move the item to the start of the spine. 
 :param container: The container to mark as title page. 
 :param name: The name of the title page. 
 :param move_to_start: If True, move the title page to the start of the spine. 
 :return: None'","'Mark the specified HTML file as the titlepage of the EPUB. 
 :param move_to_start: If True the HTML file is moved to the start of the spine'"
"def spew(trace_names=None, show_values=False): 
    sys.settrace(Spew(trace_names, show_values))"," 'Trace the Python stack. 
 :param trace_names: 
 :param show_values: 
 :type trace_names: 
 :type show_values: 
 :return: 
 :rtype: 
 :seealso: 
 :ref:`trace_stack`'","'Install a trace hook which writes incredibly detailed logs 
 about what code is being executed to stdout.'"
"def batch_det(a): 
    return BatchDet()(a)"," 'Detects faces in a batch of images. 
 Parameters 
 a : array of images 
 Returns 
 A list of lists of face detections.'","'Computes the determinant of a batch of square matrices. 
 Args: 
 a (Variable): Input array to compute the determinant for. 
 The first dimension should iterate over each matrix and be 
 of the batchsize. 
 Returns: 
 ~chainer.Variable: vector of determinants for every matrix 
 in the batch.'"
"def add_csrf(request, **kwargs): 
    d = dict(user=request.user, **kwargs) 
   d.update(csrf(request)) 
   return RequestContext(request, d)", 'Adds the csrf token to the context','Add CSRF to dictionary and wrap in a RequestContext (needed for context processor!).'
"def _get_content_range(start, end, total): 
    start = (start or 0) 
   end = ((end or total) - 1) 
   return ('bytes   %s-%s/%s' % (start, end, total))"," 'Get a Content-Range header for the given range. 
 :param start: start of the range. 
 :param end: end of the range. 
 :param total: total size of the file.'","'Returns a suitable Content-Range header: 
 >>> print(_get_content_range(None, 1, 4)) 
 bytes 0-0/4 
 >>> print(_get_content_range(1, 3, 4)) 
 bytes 1-2/4 
 >>> print(_get_content_range(None, None, 4)) 
 bytes 0-3/4'"
"def set_main_css(css_file): 
    assert css_file.endswith('.css') 
   new_css = css_file 
   app_globals.main_css = str(new_css)", 'Set the main css file for the application.','Sets the main_css.  The css_file must be of the form file.css'
"def isPointOfTableInLoop(loop, pointTable): 
    for point in loop: 
      if (point in pointTable): 
         return True 
   return False"," 'Returns True if the point is in the loop, False otherwise.'",'Determine if a point in the point table is in the loop.'
"def _compose_linear_fitting_data(mu, u): 
    for k in range((u['nterms'] - 1)): 
      k1 = (k + 1) 
      mu1n = np.power(mu[0], k1) 
      u['y'][k] = (u['w'][k] * (u['fn'][k1] - (mu1n * u['fn'][0]))) 
      for p in range((u['nfit'] - 1)): 
         u['M'][k][p] = (u['w'][k] * (np.power(mu[(p + 1)], k1) - mu1n))"," 'Compose the linear fitting data. 
 Parameters 
 mu : array 
 The initial values of the parameters. 
 u : dict 
 The fitting data. 
 Returns 
 u : dict 
 The fitting data.'",'Get the linear fitting data.'
"@slow_test 
 def test_io_evoked(): 
    tempdir = _TempDir() 
   ave = read_evokeds(fname, 0) 
   write_evokeds(op.join(tempdir, 'evoked-ave.fif'), ave) 
   ave2 = read_evokeds(op.join(tempdir, 'evoked-ave.fif'))[0] 
   assert_true(np.allclose(ave.data, ave2.data, atol=1e-16, rtol=0.001)) 
   assert_array_almost_equal(ave.times, ave2.times) 
   assert_equal(ave.nave, ave2.nave) 
   assert_equal(ave._aspect_kind, ave2._aspect_kind) 
   assert_equal(ave.kind, ave2.kind) 
   assert_equal(ave.last, ave2.last) 
   assert_equal(ave.first, ave2.first) 
   assert_true(repr(ave)) 
   ave2 = read_evokeds(fname_gz, 0) 
   assert_true(np.allclose(ave.data, ave2.data, atol=1e-16, rtol=1e-08)) 
   condition = 'Left   Auditory' 
   assert_raises(ValueError, read_evokeds, fname, condition, kind='stderr') 
   assert_raises(ValueError, read_evokeds, fname, condition, kind='standard_error') 
   ave3 = read_evokeds(fname, condition) 
   assert_array_almost_equal(ave.data, ave3.data, 19) 
   aves1 = read_evokeds(fname)[1::2] 
   aves2 = read_evokeds(fname, [1, 3]) 
   aves3 = read_evokeds(fname, ['Right   Auditory', 'Right   visual']) 
   write_evokeds(op.join(tempdir, 'evoked-ave.fif'), aves1) 
   aves4 = read_evokeds(op.join(tempdir, 'evoked-ave.fif')) 
   for aves in [aves2, aves3, aves4]: 
      for [av1, av2] in zip(aves1, aves): 
         assert_array_almost_equal(av1.data, av2.data) 
         assert_array_almost_equal(av1.times, av2.times) 
         assert_equal(av1.nave, av2.nave) 
         assert_equal(av1.kind, av2.kind) 
         assert_equal(av1._aspect_kind, av2._aspect_kind) 
         assert_equal(av1.last, av2.last) 
         assert_equal(av1.first, av2.first) 
         assert_equal(av1.comment, av2.comment) 
   fname2 = op.join(tempdir, 'test-bad-name.fif') 
   with warnings.catch_warnings(record=True) as w: 
      warnings.simplefilter('always') 
      write_evokeds(fname2, ave) 
      read_evokeds(fname2) 
   assert_naming(w, 'test_evoked.py', 2) 
   assert_raises(TypeError, Evoked, fname) 
   fname_ms = op.join(tempdir, 'test-ave.fif') 
   assert_true((ave.info['maxshield'] is False)) 
   ave.info['maxshield'] = True 
   ave.save(fname_ms) 
   assert_raises(ValueError, read_evokeds, fname_ms) 
   with warnings.catch_warnings(record=True) as w: 
      aves = read_evokeds(fname_ms, allow_maxshield=True) 
   assert_true(all((('Elekta' in str(ww.message)) for ww in w))) 
   assert_true(all(((ave.info['maxshield'] is True) for ave in aves))) 
   with warnings.catch_warnings(record=True) as w: 
      aves = read_evokeds(fname_ms, allow_maxshield='yes') 
   assert_equal(len(w), 0) 
   assert_true(all(((ave.info['maxshield'] is True) for ave in aves)))", 'Test the Evoked class.','Test IO for evoked data (fif + gz) with integer and str args.'
"def brightness_temperature(beam_area, disp): 
    beam = beam_area.to(si.sr).value 
   nu = disp.to(si.GHz, spectral()) 
   def convert_Jy_to_K(x_jybm): 
      factor = ((((2 * _si.k_B) * si.K) * (nu ** 2)) / (_si.c ** 2)).to(astrophys.Jy).value 
      return ((x_jybm / beam) / factor) 
   def convert_K_to_Jy(x_K): 
      factor = (astrophys.Jy / (((2 * _si.k_B) * (nu ** 2)) / (_si.c ** 2))).to(si.K).value 
      return ((x_K * beam) / factor) 
   return [(astrophys.Jy, si.K, convert_Jy_to_K, convert_K_to_Jy)]"," 'Converts brightness temperature to Kelvin and back. 
 Parameters 
 beam_area : `~astropy.units.Quantity` 
 The beam area in steradians. 
 disp : `~astropy.units.Quantity` 
 The dispersion in GHz. 
 Returns 
 A tuple of a function that converts brightness temperature to Kelvin, 
 and a function that converts brightness temperature to Jy. 
 Notes 
 The conversion is done in the following way: 
 * Convert the beam area to a quantity of steradians. 
 * Convert the dispersion to a quantity of GHz. 
 * Convert the brightness temperature to a quantity of Jy. 
 * Convert the brightness temperature to a quantity of Kelvin. 
 References 
 .. [1] https://en.wikipedia.org/wiki/Brightness_temperature 
 Examples 
 >>> from astropy.units import Quantity, Unit 
 >>> from astropy.io import fits 
 >>> from astropy.table import Table 
 >>> beam_area = Quantity(10, Unit(""sr"")) 
","'Defines the conversion between Jy/beam and ""brightness temperature"", 
 :math:`T_B`, in Kelvins.  The brightness temperature is a unit very 
 commonly used in radio astronomy.  See, e.g., ""Tools of Radio Astronomy"" 
 (Wilson 2009) eqn 8.16 and eqn 8.19 (these pages are available on `google 
 books 
 <http://books.google.com/books?id=9KHw6R8rQEMC&pg=PA179&source=gbs_toc_r&cad=4#v=onepage&q&f=false>`__). 
 :math:`T_B \equiv S_\nu / \left(2 k \nu^2 / c^2 \right)` 
 However, the beam area is essential for this computation: the brightness 
 temperature is inversely proportional to the beam area 
 Parameters 
 beam_area : Beam Area equivalent 
 Beam area in angular units, i.e. steradian equivalent 
 disp : `~astropy.units.Quantity` with spectral units 
 The observed `spectral` equivalent `~astropy.units.Unit` (e.g., 
 frequency or wavelength) 
 Examples 
 Arecibo C-band beam:: 
 >>> import numpy as np 
 >>> from astropy import units as u 
 >>> beam_sigma = 50*u.arcsec 
 >>> beam_area = 2*np.pi*(beam_sigma)**2 
 >>> freq = 5*u.GHz 
 >>> equiv = u.brightness_temperature(beam_area, freq) 
 >>> u.Jy.to(u.K, equivalencies=equiv)  # doctest: +FLOAT_CMP 
 3.526294429423223 
 >>> (1*u.Jy).to(u.K, equivalencies=equiv)  # doctest: +FLOAT_CMP 
 <Quantity 3.526294429423223 K> 
 VLA synthetic beam:: 
 >>> bmaj = 15*u.arcsec 
 >>> bmin = 15*u.arcsec 
 >>> fwhm_to_sigma = 1./(8*np.log(2))**0.5 
 >>> beam_area = 2.*np.pi*(bmaj*bmin*fwhm_to_sigma**2) 
 >>> freq = 5*u.GHz 
 >>> equiv = u.brightness_temperature(beam_area, freq) 
 >>> u.Jy.to(u.K, equivalencies=equiv)  # doctest: +FLOAT_CMP 
 217.2658703625732'"
"def datatype(dbtype, description): 
    dt = connection.introspection.get_field_type(dbtype, description) 
   if (type(dt) is tuple): 
      return dt[0] 
   else: 
      return dt"," 'Return the Python data type of a database field. 
 :param dbtype: The name of the database field. 
 :param description: The description of the database field. 
 :return: The Python data type of the database field. 
 :rtype: str or tuple'",'Helper to convert a data type into a string.'
"def get_exploration_components_from_dir(dir_path): 
    yaml_content = None 
   assets_list = [] 
   dir_path_array = dir_path.split('/') 
   while (dir_path_array[(-1)] == ''): 
      dir_path_array = dir_path_array[:(-1)] 
   dir_path_length = len(dir_path_array) 
   for (root, dirs, files) in os.walk(dir_path): 
      for directory in dirs: 
         if ((root == dir_path) and (directory != 'assets')): 
            raise Exception(('The   only   directory   in   %s   should   be   assets/' % dir_path)) 
      for filename in files: 
         filepath = os.path.join(root, filename) 
         if (root == dir_path): 
            if filepath.endswith('.DS_Store'): 
               continue 
            if (yaml_content is not None): 
               raise Exception(('More   than   one   non-asset   file   specified   for   %s' % dir_path)) 
            elif (not filepath.endswith('.yaml')): 
               raise Exception(('Found   invalid   non-asset   file   %s.   There   should   only   be   a   single   non-asset   file,   and   it   should   have   a   .yaml   suffix.' % filepath)) 
            else: 
               yaml_content = get_file_contents(filepath) 
         else: 
            filepath_array = filepath.split('/') 
            filename = '/'.join(filepath_array[(dir_path_length + 1):]) 
            assets_list.append((filename, get_file_contents(filepath, raw_bytes=True))) 
   if (yaml_content is None): 
      raise Exception(('No   yaml   file   specifed   for   %s' % dir_path)) 
   return (yaml_content, assets_list)"," 'Returns a tuple of the yaml content and the assets list of the exploration 
 specified in the directory path. 
 Args: 
 dir_path: The directory path of the exploration. 
 Returns: 
 A tuple of the yaml content and the assets list of the exploration.'","'Gets the (yaml, assets) from the contents of an exploration data dir. 
 Args: 
 dir_path: a full path to the exploration root directory. 
 Returns: 
 a 2-tuple, the first element of which is a yaml string, and the second 
 element of which is a list of (filepath, content) 2-tuples. The filepath 
 does not include the assets/ prefix. 
 Raises: 
 Exception: if the following condition doesn\'t hold: ""There is exactly one 
 file not in assets/, and this file has a .yaml suffix"".'"
"def _fit_dipoles(fun, min_dist_to_inner_skull, data, times, guess_rrs, guess_data, fwd_data, whitener, proj_op, ori, n_jobs): 
    from scipy.optimize import fmin_cobyla 
   (parallel, p_fun, _) = parallel_func(fun, n_jobs) 
   res = parallel((p_fun(min_dist_to_inner_skull, B, t, guess_rrs, guess_data, fwd_data, whitener, proj_op, fmin_cobyla, ori) for (B, t) in zip(data.T, times))) 
   pos = np.array([r[0] for r in res]) 
   amp = np.array([r[1] for r in res]) 
   ori = np.array([r[2] for r in res]) 
   gof = (np.array([r[3] for r in res]) * 100) 
   residual = np.array([r[4] for r in res]).T 
   return (pos, amp, ori, gof, residual)"," 'Fit the dipole model using the least-squares method. 
 Parameters 
 fun : function 
 Function to be minimized. 
 min_dist_to_inner_skull : float 
 Minimum distance to the inner skull. 
 data : ndarray 
 Array of dipole moment vectors. 
 times : ndarray 
 Array of times. 
 guess_rrs : ndarray 
 Array of guessed rrs. 
 guess_data : ndarray 
 Array of guessed data. 
 fwd_data : ndarray 
 Array of forward data. 
 whitener : ndarray 
 Array of whitening vectors. 
 proj_op : function 
 Function to project the data onto the whitening vectors. 
 ori : ndarray 
 Array of orientation vectors. 
 n_jobs : int 
 Number of jobs to run in parallel. 
 Returns 
 pos : ndarray 
 Array of dipole moment vectors. 
 amp : ndarray 
 Array of dipole moment amplitudes. 
 ori : ndarray","'Fit a single dipole to the given whitened, projected data.'"
"def copy_snapshot(kwargs=None, call=None): 
    if (call != 'function'): 
      log.error('The   copy_snapshot   function   must   be   called   with   -f   or   --function.') 
      return False 
   if ('source_region' not in kwargs): 
      log.error('A   source_region   must   be   specified   to   copy   a   snapshot.') 
      return False 
   if ('source_snapshot_id' not in kwargs): 
      log.error('A   source_snapshot_id   must   be   specified   to   copy   a   snapshot.') 
      return False 
   if ('description' not in kwargs): 
      kwargs['description'] = '' 
   params = {'Action': 'CopySnapshot'} 
   if ('source_region' in kwargs): 
      params['SourceRegion'] = kwargs['source_region'] 
   if ('source_snapshot_id' in kwargs): 
      params['SourceSnapshotId'] = kwargs['source_snapshot_id'] 
   if ('description' in kwargs): 
      params['Description'] = kwargs['description'] 
   log.debug(params) 
   data = aws.query(params, return_url=True, location=get_location(), provider=get_provider(), opts=__opts__, sigver='4') 
   return data", 'Copy a snapshot to another region.','Copy a snapshot'
"def scenario_tests_need_service_tags(physical_line, filename, previous_logical): 
    if (('tempest/scenario/' in filename) and ('/test_' in filename)): 
      if TEST_DEFINITION.match(physical_line): 
         if (not SCENARIO_DECORATOR.match(previous_logical)): 
            return (physical_line.find('def'), 'T104:   Scenario   tests   require   a   service   decorator')", 'Checks if a scenario test needs a service tag decorator.',"'Check that scenario tests have service tags 
 T104: Scenario tests require a services decorator'"
"def isclass(object): 
    return isinstance(object, (type, types.ClassType))"," 'Return True if the object is a class, False otherwise.'","'Return true if the object is a class. 
 Class objects provide these attributes: 
 __doc__         documentation string 
 __module__      name of module in which this class was defined'"
"@receiver(post_save, sender=UserLog) 
 def cull_records(sender, **kwargs): 
    if (settings.USER_LOG_MAX_RECORDS_PER_USER and kwargs['created']): 
      current_models = UserLog.objects.filter(user=kwargs['instance'].user, activity_type=kwargs['instance'].activity_type) 
      if (current_models.count() > settings.USER_LOG_MAX_RECORDS_PER_USER): 
         to_discard = current_models.order_by('start_datetime')[0:(current_models.count() - settings.USER_LOG_MAX_RECORDS_PER_USER)] 
         UserLog.objects.filter(pk__in=to_discard).delete()"," 'Cull old records for each user. 
 If the settings.USER_LOG_MAX_RECORDS_PER_USER is set, only the most recent 
 `settings.USER_LOG_MAX_RECORDS_PER_USER` records will be kept. 
 If the settings.USER_LOG_MAX_RECORDS_PER_USER is not set, all records will 
 be kept.'",'Listen in to see when videos become available.'
"def JarContents(jar_path): 
    with zipfile.ZipFile(jar_path) as jar: 
      for name in jar.namelist(): 
         (yield (name, jar.read(name)))"," 'Returns a generator of (name, contents) tuples. 
 :param jar_path: The path to the jar file. 
 :rtype: generator of (str, str)'","'Generates (name, contents) pairs for the given jar. 
 Each generated tuple consists of the relative name within the jar of an entry, 
 for example \'java/lang/Object.class\', and a str that is the corresponding 
 contents. 
 Args: 
 jar_path: a str that is the path to the jar. 
 Yields: 
 A (name, contents) pair.'"
"def color(columns=None, palette=None, bin=False, **kwargs): 
    if (palette is not None): 
      kwargs['palette'] = palette 
   kwargs['columns'] = columns 
   kwargs['bin'] = bin 
   return ColorAttr(**kwargs)"," 'Return a ColorAttr with the given options. 
 If ``palette`` is given, the ``ColorAttr`` will use it instead of 
 the default ``matplotlib`` palette. 
 If ``columns`` is given, the ``ColorAttr`` will use it instead of the 
 default ``matplotlib`` color table. 
 If ``bin`` is given, the ``ColorAttr`` will use it instead of the default 
 ``matplotlib`` ``True``. 
 Parameters 
 columns : int or list of int 
 The number of columns in the color table. 
 palette : str or list of str 
 The name of the palette or a list of palette names. 
 bin : bool 
 If ``True``, the color table will be binned. 
 Other parameters 
 See :ref:`matplotlib.colors.ColorMap` for the available parameters.'","'Produces a ColorAttr specification for coloring groups of data based on columns. 
 Args: 
 columns (str or list(str), optional): a column or list of columns for coloring 
 palette (list(str), optional): a list of colors to use for assigning to unique 
 values in `columns`. 
 **kwargs: any keyword, arg supported by :class:`AttrSpec` 
 Returns: 
 a `ColorAttr` object'"
"def import_from_cwd(module, imp=None, package=None): 
    if (imp is None): 
      imp = importlib.import_module 
   with cwd_in_path(): 
      return imp(module, package=package)", 'Return a module imported from the current working directory.',"'Import module, temporarily including modules in the current directory. 
 Modules located in the current directory has 
 precedence over modules located in `sys.path`.'"
"@staff_member_required 
 def dashboard(request, template_name=u'admin/dashboard.html'): 
    profile = Profile.objects.get_or_create(user=request.user)[0] 
   if (profile.extra_data is None): 
      profile.extra_data = {} 
      profile.save(update_fields=(u'extra_data',)) 
   profile_data = profile.extra_data 
   selected_primary_widgets = [] 
   unselected_primary_widgets = [] 
   primary_widget_selections = profile_data.get(u'primary_widget_selections') 
   if primary_widget_selections: 
      for p in primary_widgets: 
         if (primary_widget_selections[p.widget_id] == u'1'): 
            selected_primary_widgets.append(p) 
         else: 
            unselected_primary_widgets.append(p) 
   else: 
      selected_primary_widgets = primary_widgets 
      unselected_primary_widgets = None 
   selected_secondary_widgets = [] 
   unselected_secondary_widgets = [] 
   secondary_widget_selections = profile_data.get(u'secondary_widget_selections') 
   if secondary_widget_selections: 
      for s in secondary_widgets: 
         if (secondary_widget_selections[s.widget_id] == u'1'): 
            selected_secondary_widgets.append(s) 
         else: 
            unselected_secondary_widgets.append(s) 
   else: 
      selected_secondary_widgets = secondary_widgets 
      unselected_secondary_widgets = None 
   primary_widget_positions = profile_data.get(u'primary_widget_positions') 
   if primary_widget_positions: 
      sorted_primary_widgets = sorted(selected_primary_widgets, key=(lambda y: (primary_widget_positions[y.widget_id] or len(primary_widget_positions)))) 
   else: 
      sorted_primary_widgets = selected_primary_widgets 
   secondary_widget_positions = profile_data.get(u'secondary_widget_positions') 
   if secondary_widget_positions: 
      sorted_secondary_widgets = sorted(selected_secondary_widgets, key=(lambda y: (secondary_widget_positions[y.widget_id] or len(secondary_widget_positions)))) 
   else: 
      sorted_secondary_widgets = selected_secondary_widgets 
   return render_to_response(template_name, RequestContext(request, {u'primary_widgets': primary_widgets, u'root_path': (settings.SITE_ROOT + u'admin/db/'), u'secondary_widgets': secondary_widgets, u'selected_primary_widgets': sorted_primary_widgets, u'selected_secondary_widgets': sorted_secondary_widgets, u'support_data': serialize_support_data(request, True), u'title': _(u'Admin   Dashboard'), u'unselected_primary_widgets': unselected_primary_widgets, u'unselected_secondary_widgets': unselected_secondary_widgets}))", 'Render the dashboard page.',"'Display the administration dashboard. 
 This is the entry point to the admin site, containing news updates and 
 useful administration tasks.'"
"def dump_all(documents, stream=None, Dumper=Dumper, default_style=None, default_flow_style=None, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None, encoding='utf-8', explicit_start=None, explicit_end=None, version=None, tags=None): 
    getvalue = None 
   if (stream is None): 
      if (encoding is None): 
         from StringIO import StringIO 
      else: 
         from cStringIO import StringIO 
      stream = StringIO() 
      getvalue = stream.getvalue 
   dumper = Dumper(stream, default_style=default_style, default_flow_style=default_flow_style, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break, encoding=encoding, version=version, tags=tags, explicit_start=explicit_start, explicit_end=explicit_end) 
   dumper.open() 
   for data in documents: 
      dumper.represent(data) 
   dumper.close() 
   if getvalue: 
      return getvalue()"," 'Dump a list of documents to a stream. 
 :param documents: A list of documents to dump. 
 :param stream: A stream to dump to. 
 :param Dumper: A class that implements the :py:meth:`~yaml.representer.Representer.represent` method. 
 :param default_style: A style to use when no explicit style is given. 
 :param default_flow_style: A flow style to use when no explicit flow style is given. 
 :param canonical: A flag to use canonical representation. 
 :param indent: An integer to use for indentation. 
 :param width: An integer to use for line width. 
 :param allow_unicode: A flag to use unicode for strings. 
 :param line_break: A function to use for line breaking. 
 :param encoding: A string to use for encoding. 
 :param explicit_start: A flag to use explicit start tags. 
 :param explicit_end: A flag to use explicit end tags. 
 :param version: A string to use for the YAML version. 
 :param tags: A dict of","'Serialize a sequence of Python objects into a YAML stream. 
 If stream is None, return the produced string instead.'"
"def _compare_by_version(path1, path2): 
    if (path1.source == path2.source): 
      if (path1.source_version_num > path2.source_version_num): 
         return path1 
      else: 
         return path2 
   return None"," 'Compare two paths by version number. 
 If the source of the two paths are the same, then compare the 
 source version number.  If the source version numbers are the same, 
 then compare the source path.  If the source path is not the same, 
 then return None.'","'Returns the current/latest learned path. 
 Checks if given paths are from same source/peer and then compares their 
 version number to determine which path is received later. If paths are from 
 different source/peer return None.'"
"def graphviz_layout(G, prog='neato', root=None, args=''): 
    return pygraphviz_layout(G, prog=prog, root=root, args=args)", 'Return a string representing a Graphviz graph of G.',"'Create node positions for G using Graphviz. 
 Parameters 
 G : NetworkX graph 
 A graph created with NetworkX 
 prog : string 
 Name of Graphviz layout program 
 root : string, optional 
 Root node for twopi layout 
 args : string, optional 
 Extra arguments to Graphviz layout program 
 Returns : dictionary 
 Dictionary of x,y, positions keyed by node. 
 Examples 
 >>> G = nx.petersen_graph() 
 >>> pos = nx.nx_agraph.graphviz_layout(G) 
 >>> pos = nx.nx_agraph.graphviz_layout(G, prog=\'dot\') 
 Notes 
 This is a wrapper for pygraphviz_layout.'"
"def get_level_tags(): 
    level_tags = constants.DEFAULT_TAGS.copy() 
   level_tags.update(getattr(settings, 'MESSAGE_TAGS', {})) 
   return level_tags", 'Returns a dictionary of tags to levels.','Returns the message level tags.'
"def list_rows(dataset_name, table_name, project=None): 
    bigquery_client = bigquery.Client(project=project) 
   dataset = bigquery_client.dataset(dataset_name) 
   table = dataset.table(table_name) 
   if (not table.exists()): 
      print 'Table   {}:{}   does   not   exist.'.format(dataset_name, table_name) 
      return 
   table.reload() 
   rows = list(table.fetch_data(max_results=25)) 
   format_string = ('{!s:<16}   ' * len(table.schema)) 
   field_names = [field.name for field in table.schema] 
   print format_string.format(*field_names) 
   for row in rows: 
      print format_string.format(*row)", 'List rows from a BigQuery table.',"'Prints rows in the given table. 
 Will print 25 rows at most for brevity as tables can contain large amounts 
 of rows. 
 If no project is specified, then the currently active project is used.'"
"@environmentfilter 
 def do_first(environment, seq): 
    try: 
      return next(iter(seq)) 
   except StopIteration: 
      return environment.undefined('No   first   item,   sequence   was   empty.')"," 'Returns the first element of the sequence, or an error message if the sequence is empty.'",'Return the first item of a sequence.'
"def main(): 
    if (len(sys.argv) > 1): 
      writeOutput('   '.join(sys.argv[1:])) 
   else: 
      settings.startMainLoopFromConstructor(getNewRepository())", 'This is the main entry point for the program.','Display the feed dialog.'
"@asyncio.coroutine 
 @hook.command 
 def flirt(text, conn, nick, message): 
    target = text.strip() 
   if (not is_valid(target)): 
      return ""I   can't   attack   that."" 
   if is_self(conn, target): 
      target = nick 
   message('{},   {}'.format(target, random.choice(flirts)))", 'Flirt with a target.','<user> - flirts with <user>'
"def urlsplit(url, scheme='', allow_fragments=True): 
    allow_fragments = bool(allow_fragments) 
   key = (url, scheme, allow_fragments, type(url), type(scheme)) 
   cached = _parse_cache.get(key, None) 
   if cached: 
      return cached 
   if (len(_parse_cache) >= MAX_CACHE_SIZE): 
      clear_cache() 
   netloc = query = fragment = '' 
   i = url.find(':') 
   if (i > 0): 
      if (url[:i] == 'http'): 
         scheme = url[:i].lower() 
         url = url[(i + 1):] 
         if (url[:2] == '//'): 
            (netloc, url) = _splitnetloc(url, 2) 
            if ((('[' in netloc) and (']' not in netloc)) or ((']' in netloc) and ('[' not in netloc))): 
               raise ValueError('Invalid   IPv6   URL') 
         if (allow_fragments and ('#' in url)): 
            (url, fragment) = url.split('#', 1) 
         if ('?' in url): 
            (url, query) = url.split('?', 1) 
         v = SplitResult(scheme, netloc, url, query, fragment) 
         _parse_cache[key] = v 
         return v 
      for c in url[:i]: 
         if (c not in scheme_chars): 
            break 
      else: 
         try: 
            _testportnum = int(url[(i + 1):]) 
         except ValueError: 
            (scheme, url) = (url[:i].lower(), url[(i + 1):]) 
   if (url[:2] == '//'): 
      (netloc, url) = _splitnetloc(url, 2) 
      if ((('[' in netloc) and (']' not in netloc)) or ((']' in netloc) and ('[' not in netloc))): 
         raise ValueError('Invalid   IPv6   URL') 
   if (allow_fragments and (scheme in uses_fragment) and ('#' in url)): 
      (url, fragment) = url.split('#', 1) 
   if ((scheme in uses_query) and ('?' in url)): 
      (url, query) = url.split('?', 1) 
   v = SplitResult(scheme, netloc, url, query, fragment) 
   _parse_cache[key] = v 
   return v"," 'Parse an URL into its component parts. 
 The scheme is the name of the protocol, such as http or ftp. 
 The netloc is the host name or IP address. 
 The path is the path relative to the host, starting at the root. 
 The query is the query string, such as ?foo=bar. 
 The fragment is the fragment identifier, such as #fragment. 
 If allow_fragments is False, the fragment is ignored. 
 Raises ValueError if the URL is not valid. 
 >>> urlsplit(""http://www.python.org/"") 
 SplitResult(http, \'www.python.org\', \'/', \'', \'') 
 >>> urlsplit(""http://www.python.org/foo#bar"") 
 SplitResult(http, \'www.python.org\', \'/foo\', \'#bar\', \'') 
 >>> urlsplit(""http://www.python.org/foo#bar?baz=qux"") 
 SplitResult(http, \'www.python.org\', \'/foo\', \'#bar","'Parse a URL into 5 components: 
 <scheme>://<netloc>/<path>?<query>#<fragment> 
 Return a 5-tuple: (scheme, netloc, path, query, fragment). 
 Note that we don\'t break the components up in smaller bits 
 (e.g. netloc is a single string) and we don\'t expand % escapes.'"
"def bygroups(*args): 
    def callback(lexer, match, ctx=None): 
      for (i, action) in enumerate(args): 
         if (action is None): 
            continue 
         elif (type(action) is _TokenType): 
            data = match.group((i + 1)) 
            if data: 
               (yield (match.start((i + 1)), action, data)) 
         else: 
            if ctx: 
               ctx.pos = match.start((i + 1)) 
            for item in action(lexer, _PseudoMatch(match.start((i + 1)), match.group((i + 1))), ctx): 
               if item: 
                  (yield item) 
      if ctx: 
         ctx.pos = match.end() 
   return callback"," 'A decorator that groups tokens together. 
 Given a list of tuples, ``(group, callback)``, the decorator will 
 group tokens together and call the ``callback`` function for each group. 
 The first element of the tuple is the name of the group, the second 
 element is the callback function. 
 The callback function will be called with the following arguments: 
 * ``lexer``: The current lexer object. 
 * ``match``: The match object for the current group. 
 * ``ctx``: A context object that holds the current position of the 
 lexer. 
 :param args: A list of tuples, ``(group, callback)``, where ``group`` 
 is the name of the group and ``callback`` is the callback function. 
 :type args: list(tuple(str, callable)) 
 :return: A function decorator. 
 :rtype: function 
 :seealso: :func:`~.bygroup` 
 :seealso: :func:`~.bygroup_context`'",'Callback that yields multiple actions for each group in the match.'
"def describe_policy(policyName, region=None, key=None, keyid=None, profile=None): 
    try: 
      conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
      policy = conn.get_policy(policyName=policyName) 
      if policy: 
         keys = ('policyName', 'policyArn', 'policyDocument', 'defaultVersionId') 
         return {'policy': dict([(k, policy.get(k)) for k in keys])} 
      else: 
         return {'policy': None} 
   except ClientError as e: 
      err = salt.utils.boto3.get_error(e) 
      if (e.response.get('Error', {}).get('Code') == 'ResourceNotFoundException'): 
         return {'policy': None} 
      return {'error': salt.utils.boto3.get_error(e)}"," 'Describe a policy. 
 :param policyName: The name of the policy to describe. 
 :param region: The region to query. 
 :param key: The access key to use. 
 :param keyid: The access key ID to use. 
 :param profile: The profile to use. 
 :returns: A dictionary of the policy attributes. 
 :raises: :py:class:`boto3.exceptions.ClientError` if the policy cannot 
 be found.'","'Given a policy name describe its properties. 
 Returns a dictionary of interesting properties. 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_iot.describe_policy mypolicy'"
"def _gen_gce_as_policy(as_params): 
    asp_data = {} 
   asp_data['maxNumReplicas'] = as_params['max_instances'] 
   if ('min_instances' in as_params): 
      asp_data['minNumReplicas'] = as_params['min_instances'] 
   if ('cool_down_period' in as_params): 
      asp_data['coolDownPeriodSec'] = as_params['cool_down_period'] 
   if (('cpu_utilization' in as_params) and ('target' in as_params['cpu_utilization'])): 
      asp_data['cpuUtilization'] = {'utilizationTarget': as_params['cpu_utilization']['target']} 
   if (('load_balancing_utilization' in as_params) and ('target' in as_params['load_balancing_utilization'])): 
      asp_data['loadBalancingUtilization'] = {'utilizationTarget': as_params['load_balancing_utilization']['target']} 
   return asp_data", 'Generate GCE as policy data',"'Take Autoscaler params and generate GCE-compatible policy. 
 :param as_params: Dictionary in Ansible-playbook format 
 containing policy arguments. 
 :type as_params: ``dict`` 
 :return: GCE-compatible policy dictionary 
 :rtype: ``dict``'"
"def parse_atom(tokens, options): 
    token = tokens.current() 
   result = [] 
   if (token in '(['): 
      tokens.move() 
      (matching, pattern) = {'(': [')', Required], '[': [']', Optional]}[token] 
      result = pattern(*parse_expr(tokens, options)) 
      if (tokens.move() != matching): 
         raise tokens.error((""unmatched   '%s'"" % token)) 
      return [result] 
   elif (token == 'options'): 
      tokens.move() 
      return [OptionsShortcut()] 
   elif (token.startswith('--') and (token != '--')): 
      return parse_long(tokens, options) 
   elif (token.startswith('-') and (token not in ('-', '--'))): 
      return parse_shorts(tokens, options) 
   elif ((token.startswith('<') and token.endswith('>')) or token.isupper()): 
      return [Argument(tokens.move())] 
   else: 
      return [Command(tokens.move())]"," 'Parse an atom. 
 :param tokens: The parser. 
 :param options: The command line options. 
 :return: A list of arguments.'","'atom ::= \'(\' expr \')\' | \'[\' expr \']\' | \'options\' 
 | long | shorts | argument | command ;'"
"def __test_html(): 
    with open('test.rpt', 'r') as input_file: 
      data_text = input_file.read() 
   data = yaml.safe_load(data_text) 
   string_file = StringIO.StringIO() 
   _generate_html(data, string_file) 
   string_file.seek(0) 
   result = string_file.read() 
   with open('test.html', 'w') as output: 
      output.write(result)", 'Test the HTML generation of the report.',"'HTML generation test only used when called from the command line: 
 python ./highstate.py 
 Typical options for generating the report file: 
 highstate: 
 report_format: yaml 
 report_delivery: file 
 file_output: \'/srv/salt/_returners/test.rpt\''"
"def str2bin(value, classic_mode=True): 
    text = '' 
   for character in value: 
      if (text != ''): 
         text += '   ' 
      byte = ord(character) 
      text += byte2bin(byte, classic_mode) 
   return text"," 'Convert a string to binary. 
 :param value: The string to convert to binary. 
 :type value: str 
 :param classic_mode: If True, use the classic Python binary 
 format, otherwise use the new Python binary format. 
 :type classic_mode: bool'","'Convert binary string to binary numbers. 
 If classic_mode  is true (default value), reverse bits. 
 >>> str2bin(""\x03\xFF"") 
 \'00000011 11111111\' 
 >>> str2bin(""\x03\xFF"", False) 
 \'11000000 11111111\''"
"def from_files(job, form): 
    if form.textfile_use_local_files.data: 
      job.labels_file = form.textfile_local_labels_file.data.strip() 
   else: 
      flask.request.files[form.textfile_labels_file.name].save(os.path.join(job.dir(), utils.constants.LABELS_FILE)) 
      job.labels_file = utils.constants.LABELS_FILE 
   shuffle = bool(form.textfile_shuffle.data) 
   backend = form.backend.data 
   encoding = form.encoding.data 
   compression = form.compression.data 
   if form.textfile_use_local_files.data: 
      train_file = form.textfile_local_train_images.data.strip() 
   else: 
      flask.request.files[form.textfile_train_images.name].save(os.path.join(job.dir(), utils.constants.TRAIN_FILE)) 
      train_file = utils.constants.TRAIN_FILE 
   image_folder = form.textfile_train_folder.data.strip() 
   if (not image_folder): 
      image_folder = None 
   job.tasks.append(tasks.CreateDbTask(job_dir=job.dir(), input_file=train_file, db_name=utils.constants.TRAIN_DB, backend=backend, image_dims=job.image_dims, image_folder=image_folder, resize_mode=job.resize_mode, encoding=encoding, compression=compression, mean_file=utils.constants.MEAN_FILE_CAFFE, labels_file=job.labels_file, shuffle=shuffle)) 
   if form.textfile_use_val.data: 
      if form.textfile_use_local_files.data: 
         val_file = form.textfile_local_val_images.data.strip() 
      else: 
         flask.request.files[form.textfile_val_images.name].save(os.path.join(job.dir(), utils.constants.VAL_FILE)) 
         val_file = utils.constants.VAL_FILE 
      image_folder = form.textfile_val_folder.data.strip() 
      if (not image_folder): 
         image_folder = None 
      job.tasks.append(tasks.CreateDbTask(job_dir=job.dir(), input_file=val_file, db_name=utils.constants.VAL_DB, backend=backend, image_dims=job.image_dims, image_folder=image_folder, resize_mode=job.resize_mode, encoding=encoding, compression=compression, labels_file=job.labels_file, shuffle=shuffle)) 
   if form.textfile_use_test.data: 
      if form.textfile_use_local_files.data: 
         test_file = form.textfile_local_test_images.data.strip() 
      else: 
         flask.request.files[form.textfile_test_images.name].save(os.path.join(job.dir(), utils.constants.TEST_FILE)) 
         test_file = utils.constants.TEST_FILE 
      image_folder = form.textfile_test_folder.data.strip() 
      if (not image_folder): 
         image_folder = None 
      job.tasks.append(tasks.CreateDbTask(job_dir=job.dir(), input_file=test_file, db_name=utils.constants.TEST_DB, backend=backend, image_dims=job.image_dims, image_folder=image_folder, resize_mode=job.resize_mode, encoding=encoding, compression=compression, labels_file=job.labels_file, shuffle=shuffle))", 'Create db task from files','Add tasks for creating a dataset by reading textfiles'
"def debug(msg, *args, **kwargs): 
    if (len(root.handlers) == 0): 
      basicConfig() 
   root.debug(*((msg,) + args), **kwargs)", 'Log a message to the root logger.','Log a message with severity \'DEBUG\' on the root logger.'
"def greater_than_zero(): 
    return st.floats(min_value=0.0, allow_infinity=False).filter((lambda x: (x > 0.0)))", 'A function that returns a list of positive floats','A strategy that yields floats greater than zero.'
"def _int64_feature_list(values): 
    return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])", 'Create a feature list with int64 features.','Wrapper for inserting an int64 FeatureList into a SequenceExample proto.'
"def python_implementation(): 
    return _sys_version()[0]", 'Returns the Python implementation version.',"'Returns a string identifying the Python implementation. 
 Currently, the following implementations are identified: 
 \'CPython\' (C implementation of Python), 
 \'IronPython\' (.NET implementation of Python), 
 \'Jython\' (Java implementation of Python), 
 \'PyPy\' (Python implementation of Python).'"
"def construct_sort_part(model_cls, part): 
    assert part, 'part   must   be   a   field   name   and   +   or   -' 
   field = part[:(-1)] 
   assert field, 'field   is   missing' 
   direction = part[(-1)] 
   assert (direction in ('+', '-')), 'part   must   end   with   +   or   -' 
   is_ascending = (direction == '+') 
   case_insensitive = beets.config['sort_case_insensitive'].get(bool) 
   if (field in model_cls._sorts): 
      sort = model_cls._sorts[field](model_cls, is_ascending, case_insensitive) 
   elif (field in model_cls._fields): 
      sort = query.FixedFieldSort(field, is_ascending, case_insensitive) 
   else: 
      sort = query.SlowFieldSort(field, is_ascending, case_insensitive) 
   return sort"," 'Construct a sort part from a field name and a direction. 
 :param model_cls: The model class to sort. 
 :param part: A field name and a direction. 
 :returns: A query.Sort instance.'","'Create a `Sort` from a single string criterion. 
 `model_cls` is the `Model` being queried. `part` is a single string 
 ending in ``+`` or ``-`` indicating the sort.'"
"def add_resource(zone, resource_type, **kwargs): 
    return _resource('add', zone, resource_type, None, **kwargs)"," 'Add a resource to the zone. 
 :param zone: The zone to add the resource to. 
 :param resource_type: The type of resource to add. 
 :param kwargs: Keyword arguments to pass to the add method. 
 :returns: The added resource. 
 :rtype: :class:`~google.cloud.compute_v1.resources.compute.v1_compute.Instance`'","'Add a resource 
 zone : string 
 name of zone 
 resource_type : string 
 type of resource 
 **kwargs : string|int|... 
 resource properties 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' zonecfg.add_resource tallgeese rctl name=zone.max-locked-memory value=\'(priv=privileged,limit=33554432,action=deny)\''"
"def bits_str(s, endian='big', zero='0', one='1'): 
    return ''.join(bits(s, endian, zero, one))"," 'Return a string representing the binary representation of a 
 number. 
 Parameters 
 s : number 
 The number to convert. 
 endian : string, optional 
 The endianness of the string. 
 default : big 
 The default endianness. 
 zero : string, optional 
 The string to use for zero bits. 
 default : 0 
 The default zero string. 
 one : string, optional 
 The string to use for one bits. 
 default : 1 
 The default one string.'","'bits_str(s, endian = \'big\', zero = \'0\', one = \'1\') -> str 
 A wrapper around :func:`bits`, which converts the output into a string. 
 Examples: 
 >>> bits_str(511) 
 \'0000000111111111\' 
 >>> bits_str(""bits_str"", endian = ""little"") 
 \'0100011010010110001011101100111011111010110011100010111001001110\''"
"def CreateGRRTempFileVFS(directory=None, filename=None, lifetime=0, mode='w+b', suffix=''): 
    fd = CreateGRRTempFile(directory=directory, filename=filename, lifetime=lifetime, mode=mode, suffix=suffix) 
   pathspec = rdf_paths.PathSpec(path=fd.name, pathtype=rdf_paths.PathSpec.PathType.TMPFILE) 
   return (fd, pathspec)"," 'Create a temporary file in the given directory. 
 :param directory: The directory to create the file in. 
 :param filename: The name of the file to create. 
 :param lifetime: The number of seconds to keep the file around. 
 :param mode: The file mode to use. 
 :param suffix: The suffix to append to the filename. 
 :return: A tuple of the file descriptor and the pathspec. 
 :rtype: (int, rdf_paths.PathSpec)'","'Creates a GRR VFS temp file. 
 This function is analogous to CreateGRRTempFile but returns an open VFS handle 
 to the newly created file. Arguments are the same as for CreateGRRTempFile: 
 Args: 
 directory: string representing absolute directory where file should be 
 written. If None, use \'tmp\' under the directory we\'re running 
 from. 
 filename: The name of the file to use. Note that setting both filename and 
 directory name is not allowed. 
 lifetime: time in seconds before we should delete this tempfile. 
 mode: The mode to open the file. 
 suffix: optional suffix to use for the temp file 
 Returns: 
 An open file handle to the new file and the corresponding pathspec.'"
"def pylab_setup(): 
    if backend.startswith('module://'): 
      backend_name = backend[9:] 
   else: 
      backend_name = ('backend_' + backend) 
      backend_name = backend_name.lower() 
      backend_name = ('matplotlib.backends.%s' % backend_name.lower()) 
   backend_mod = __import__(backend_name, globals(), locals(), [backend_name]) 
   new_figure_manager = backend_mod.new_figure_manager 
   def do_nothing_show(*args, **kwargs): 
      frame = inspect.currentframe() 
      fname = frame.f_back.f_code.co_filename 
      if (fname in ('<stdin>', '<ipython   console>')): 
         warnings.warn((""\nYour   currently   selected   backend,   '%s'   does   not   support   show().\nPlease   select   a   GUI   backend   in   your   matplotlibrc   file   ('%s')\nor   with   matplotlib.use()"" % (backend, matplotlib.matplotlib_fname()))) 
   def do_nothing(*args, **kwargs): 
      pass 
   backend_version = getattr(backend_mod, 'backend_version', 'unknown') 
   show = getattr(backend_mod, 'show', do_nothing_show) 
   draw_if_interactive = getattr(backend_mod, 'draw_if_interactive', do_nothing) 
   if (backend.lower() in ['wx', 'wxagg']): 
      Toolbar = backend_mod.Toolbar 
      __all__.append('Toolbar') 
   matplotlib.verbose.report(('backend   %s   version   %s' % (backend, backend_version))) 
   return (new_figure_manager, draw_if_interactive, show)"," 'Create a new figure manager, show and draw methods. 
 This function is called by the backend to create the new figure manager 
 and show and draw methods. 
 Parameters 
 backend : str 
 Name of the backend to use. 
 Returns 
 tuple 
 The new figure manager, draw and show methods.'","'return new_figure_manager, draw_if_interactive and show for pylab'"
"@ensure_csrf_cookie 
 @cache_if_anonymous() 
 def course_about(request, course_id): 
    course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id) 
   if hasattr(course_key, 'ccx'): 
      return redirect(reverse('dashboard')) 
   with modulestore().bulk_operations(course_key): 
      permission = get_permission_for_course_about() 
      course = get_course_with_access(request.user, permission, course_key) 
      course_details = CourseDetails.populate(course) 
      modes = CourseMode.modes_for_course_dict(course_key) 
      if configuration_helpers.get_value('ENABLE_MKTG_SITE', settings.FEATURES.get('ENABLE_MKTG_SITE', False)): 
         return redirect(reverse('info', args=[course.id.to_deprecated_string()])) 
      registered = registered_for_course(course, request.user) 
      staff_access = bool(has_access(request.user, 'staff', course)) 
      studio_url = get_studio_url(course, 'settings/details') 
      if has_access(request.user, 'load', course): 
         course_target = reverse('info', args=[course.id.to_deprecated_string()]) 
      else: 
         course_target = reverse('about_course', args=[course.id.to_deprecated_string()]) 
      show_courseware_link = bool(((has_access(request.user, 'load', course) and has_access(request.user, 'view_courseware_with_prerequisites', course)) or settings.FEATURES.get('ENABLE_LMS_MIGRATION'))) 
      in_cart = False 
      reg_then_add_to_cart_link = '' 
      _is_shopping_cart_enabled = is_shopping_cart_enabled() 
      if _is_shopping_cart_enabled: 
         if request.user.is_authenticated(): 
            cart = shoppingcart.models.Order.get_cart_for_user(request.user) 
            in_cart = (shoppingcart.models.PaidCourseRegistration.contained_in_order(cart, course_key) or shoppingcart.models.CourseRegCodeItem.contained_in_order(cart, course_key)) 
         reg_then_add_to_cart_link = '{reg_url}?course_id={course_id}&enrollment_action=add_to_cart'.format(reg_url=reverse('register_user'), course_id=urllib.quote(str(course_id))) 
      ecomm_service = EcommerceService() 
      ecommerce_checkout = ecomm_service.is_enabled(request.user) 
      ecommerce_checkout_link = '' 
      ecommerce_bulk_checkout_link = '' 
      professional_mode = None 
      is_professional_mode = ((CourseMode.PROFESSIONAL in modes) or (CourseMode.NO_ID_PROFESSIONAL_MODE in modes)) 
      if (ecommerce_checkout and is_professional_mode): 
         professional_mode = (modes.get(CourseMode.PROFESSIONAL, '') or modes.get(CourseMode.NO_ID_PROFESSIONAL_MODE, '')) 
         if professional_mode.sku: 
            ecommerce_checkout_link = ecomm_service.checkout_page_url(professional_mode.sku) 
         if professional_mode.bulk_sku: 
            ecommerce_bulk_checkout_link = ecomm_service.checkout_page_url(professional_mode.bulk_sku) 
      registration_price = CourseMode.min_course_price_for_currency(course_key, settings.PAID_COURSE_REGISTRATION_CURRENCY[0]) 
      course_price = get_cosmetic_display_price(course, registration_price) 
      can_add_course_to_cart = (_is_shopping_cart_enabled and registration_price and (not ecommerce_checkout_link)) 
      can_enroll = bool(has_access(request.user, 'enroll', course)) 
      invitation_only = course.invitation_only 
      is_course_full = CourseEnrollment.objects.is_course_full(course) 
      active_reg_button = (not (registered or is_course_full or (not can_enroll))) 
      is_shib_course = uses_shib(course) 
      pre_requisite_courses = get_prerequisite_courses_display(course) 
      overview = CourseOverview.get_from_id(course.id) 
      context = {'course': course, 'course_details': course_details, 'staff_access': staff_access, 'studio_url': studio_url, 'registered': registered, 'course_target': course_target, 'is_cosmetic_price_enabled': settings.FEATURES.get('ENABLE_COSMETIC_DISPLAY_PRICE'), 'course_price': course_price, 'in_cart': in_cart, 'ecommerce_checkout': ecommerce_checkout, 'ecommerce_checkout_link': ecommerce_checkout_link, 'ecommerce_bulk_checkout_link': ecommerce_bulk_checkout_link, 'professional_mode': professional_mode, 'reg_then_add_to_cart_link': reg_then_add_to_cart_link, 'show_courseware_link': show_courseware_link, 'is_course_full': is_course_full, 'can_enroll': can_enroll, 'invitation_only': invitation_only, 'active_reg_button': active_reg_button, 'is_shib_course': is_shib_course, 'disable_courseware_header': True, 'can_add_course_to_cart': can_add_course_to_cart, 'cart_link': reverse('shoppingcart.views.show_cart'), 'pre_requisite_courses': pre_requisite_courses, 'course_image_urls': overview.image_urls} 
      inject_coursetalk_keys_into_context(context, course_key) 
      return render_to_response('courseware/course_about.html', context)"," 'Render a course about page. 
 The course about page is used to display information about a course. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not 
 enrolled in. 
 The course about page is only available for courses that are not","'Display the course\'s about page. 
 Assumes the course_id is in a valid format.'"
"def is_private(ip_addr): 
    return ipaddress.ip_address(ip_addr).is_private"," 'Checks if the given IP address is private. 
 Returns True if the address is private, False otherwise.'","'Check if the given IP address is a private address 
 .. versionadded:: 2014.7.0 
 .. versionchanged:: 2015.8.0 
 IPv6 support 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' network.is_private 10.0.0.3'"
"def hessenberg(a, calc_q=False, overwrite_a=False, check_finite=True): 
    a1 = _asarray_validated(a, check_finite=check_finite) 
   if ((len(a1.shape) != 2) or (a1.shape[0] != a1.shape[1])): 
      raise ValueError('expected   square   matrix') 
   overwrite_a = (overwrite_a or _datacopied(a1, a)) 
   if (a1.shape[0] <= 2): 
      if calc_q: 
         return (a1, numpy.eye(a1.shape[0])) 
      return a1 
   (gehrd, gebal, gehrd_lwork) = get_lapack_funcs(('gehrd', 'gebal', 'gehrd_lwork'), (a1,)) 
   (ba, lo, hi, pivscale, info) = gebal(a1, permute=0, overwrite_a=overwrite_a) 
   if (info < 0): 
      raise ValueError(('illegal   value   in   %d-th   argument   of   internal   gebal   (hessenberg)' % (- info))) 
   n = len(a1) 
   lwork = _compute_lwork(gehrd_lwork, ba.shape[0], lo=lo, hi=hi) 
   (hq, tau, info) = gehrd(ba, lo=lo, hi=hi, lwork=lwork, overwrite_a=1) 
   if (info < 0): 
      raise ValueError(('illegal   value   in   %d-th   argument   of   internal   gehrd   (hessenberg)' % (- info))) 
   h = numpy.triu(hq, (-1)) 
   if (not calc_q): 
      return h 
   (orghr, orghr_lwork) = get_lapack_funcs(('orghr', 'orghr_lwork'), (a1,)) 
   lwork = _compute_lwork(orghr_lwork, n, lo=lo, hi=hi) 
   (q, info) = orghr(a=hq, tau=tau, lo=lo, hi=hi, lwork=lwork, overwrite_a=1) 
   if (info < 0): 
      raise ValueError(('illegal   value   in   %d-th   argument   of   internal   orghr   (hessenberg)' % (- info))) 
   return (h, q)"," 'Returns the Hessenberg matrix of the input matrix. 
 The Hessenberg matrix is defined as the lower triangular matrix 
 obtained from the input matrix by removing the last row and 
 column. 
 Parameters 
 a : array_like 
 The input matrix. 
 calc_q : bool, optional 
 If True, return the Q matrix. 
 overwrite_a : bool, optional 
 If True, overwrite the input array. 
 check_finite : bool, optional 
 If True, check that the input array is finite. 
 Returns 
 h : ndarray 
 The Hessenberg matrix. 
 q : ndarray 
 The Q matrix. 
 Notes 
 The Hessenberg matrix is defined as the lower triangular matrix 
 obtained from the input matrix by removing the last row and 
 column. 
 Examples 
 >>> from scipy.linalg import hessenberg 
 >>> a = numpy.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 1","'Compute Hessenberg form of a matrix. 
 The Hessenberg decomposition is:: 
 A = Q H Q^H 
 where `Q` is unitary/orthogonal and `H` has only zero elements below 
 the first sub-diagonal. 
 Parameters 
 a : (M, M) array_like 
 Matrix to bring into Hessenberg form. 
 calc_q : bool, optional 
 Whether to compute the transformation matrix.  Default is False. 
 overwrite_a : bool, optional 
 Whether to overwrite `a`; may improve performance. 
 Default is False. 
 check_finite : bool, optional 
 Whether to check that the input matrix contains only finite numbers. 
 Disabling may give a performance gain, but may result in problems 
 (crashes, non-termination) if the inputs do contain infinities or NaNs. 
 Returns 
 H : (M, M) ndarray 
 Hessenberg form of `a`. 
 Q : (M, M) ndarray 
 Unitary/orthogonal similarity transformation matrix ``A = Q H Q^H``. 
 Only returned if ``calc_q=True``.'"
"def require_cuda_ndarray(obj): 
    if (not is_cuda_ndarray(obj)): 
      raise ValueError('require   an   cuda   ndarray   object')", 'Checks if obj is a cuda ndarray object.','Raises ValueError is is_cuda_ndarray(obj) evaluates False'
"def getRotationMatrix(arrayDictionary, derivation, path, point, pointIndex): 
    if ((len(path) < 2) or (not derivation.track)): 
      return matrix.Matrix() 
   point = point.dropAxis() 
   begin = path[(((pointIndex + len(path)) - 1) % len(path))].dropAxis() 
   end = path[((pointIndex + 1) % len(path))].dropAxis() 
   pointMinusBegin = (point - begin) 
   pointMinusBeginLength = abs(pointMinusBegin) 
   endMinusPoint = (end - point) 
   endMinusPointLength = abs(endMinusPoint) 
   if (not derivation.closed): 
      if ((pointIndex == 0) and (endMinusPointLength > 0.0)): 
         return getRotationMatrixByPolar(arrayDictionary, endMinusPoint, endMinusPointLength) 
      elif ((pointIndex == (len(path) - 1)) and (pointMinusBeginLength > 0.0)): 
         return getRotationMatrixByPolar(arrayDictionary, pointMinusBegin, pointMinusBeginLength) 
   if (pointMinusBeginLength <= 0.0): 
      print 'Warning,   point   equals   previous   point   in   getRotationMatrix   in   array   for:' 
      print path 
      print pointIndex 
      print derivation.elementNode 
      return matrix.Matrix() 
   pointMinusBegin /= pointMinusBeginLength 
   if (endMinusPointLength <= 0.0): 
      print 'Warning,   point   equals   next   point   in   getRotationMatrix   in   array   for:' 
      print path 
      print pointIndex 
      print derivation.elementNode 
      return matrix.Matrix() 
   endMinusPoint /= endMinusPointLength 
   averagePolar = (pointMinusBegin + endMinusPoint) 
   averagePolarLength = abs(averagePolar) 
   if (averagePolarLength <= 0.0): 
      print 'Warning,   averagePolarLength   is   zero   in   getRotationMatrix   in   array   for:' 
      print path 
      print pointIndex 
      print derivation.elementNode 
      return matrix.Matrix() 
   return getRotationMatrixByPolar(arrayDictionary, averagePolar, averagePolarLength)"," 'getRotationMatrix(arrayDictionary, derivation, path, point, pointIndex) 
 Generates the rotation matrix from the given path and point. 
 The derivation is the derivation of the path. 
 The point is the point on the path. 
 The pointIndex is the index of the point. 
 Returns the rotation matrix. 
 Parameters 
 arrayDictionary : dictionary 
 The dictionary of the array. 
 derivation : Derivation 
 The derivation of the path. 
 path : list 
 The path. 
 point : Point 
 The point. 
 pointIndex : int 
 The index of the point. 
 Returns 
 matrix.Matrix 
 The rotation matrix.'",'Get rotationMatrix.'
"def slicable(dim, pad=0): 
    dim0 = (np.prod(dim[:(-1)]) + pad) 
   return (dim0, dim[(-1)])"," 'Returns the size of the slice and the slice\'s last dimension. 
 Parameters 
 dim : tuple 
 The dimensions of the slice. 
 pad : int 
 The number of extra elements to add to the slice\'s last dimension. 
 Returns 
 tuple 
 The size of the slice and the slice\'s last dimension.'","'colapse outer dimensions into one and preserve inner dimension 
 this allows for easy cpu convolution in numpy 
 Arguments: 
 dim (tuple): dimensions list in a tuple 
 pad (int):  how many pixel paddings'"
"def _indent(text, prefix, predicate=None): 
    if (predicate is None): 
      predicate = (lambda line: line.strip()) 
   def prefixed_lines(): 
      for line in text.splitlines(True): 
         (yield ((prefix + line) if predicate(line) else line)) 
   return ''.join(prefixed_lines())"," 'Return a list of lines of text, each prefixed with \'prefix\'. 
 :param text: The text to be prefixed. 
 :param prefix: The prefix to be prepended to each line. 
 :param predicate: A function that takes a line of text and returns 
 True if it should be prefixed.'","'Adds \'prefix\' to the beginning of selected lines in \'text\'. 
 If \'predicate\' is provided, \'prefix\' will only be added to the lines 
 where \'predicate(line)\' is True. If \'predicate\' is not provided, 
 it will default to adding \'prefix\' to all non-empty lines that do not 
 consist solely of whitespace characters.'"
"def first_value(obj): 
    return six.next(six.itervalues(obj))"," 'Returns the first value of the object, or None if it is empty.'","'Return the first value 
 Parameters 
 obj: dict-like object'"
"def is_ssh_uri(url): 
    return (urllib_parse(url)[0] in ssh_uri_schemes)", 'Check if the given URL is an SSH URI.','Returns whether or not a URL represents an SSH connection.'
"def _Cobject(cls, ctype): 
    o = object.__new__(cls) 
   o._as_parameter_ = ctype 
   return o"," 'Return a new object of the given type, with the given _as_parameter_ 
 attribute. 
 This is a convenience function that can be used to create new objects 
 that are not objects of the given type. 
 Parameters 
 ctype : type 
 The type to create the object of. 
 Returns 
 o : object 
 The new object.'",'(INTERNAL) New instance from ctypes.'
"def plate_scale(platescale): 
    if platescale.unit.is_equivalent((si.arcsec / si.m)): 
      platescale_val = platescale.to((si.radian / si.m)).value 
   elif platescale.unit.is_equivalent((si.m / si.arcsec)): 
      platescale_val = (1 / platescale).to((si.radian / si.m)).value 
   else: 
      raise UnitsError(u'The   pixel   scale   must   be   in   angle/distance   or   distance/angle') 
   return [(si.m, si.radian, (lambda d: (d * platescale_val)), (lambda rad: (rad / platescale_val)))]"," 'Return a tuple of (pixels, radians, pixel_to_radian, radian_to_pixel) 
 for the given platescale. 
 If the platescale is in arcsec/m, then the resulting pixel_to_radian and radian_to_pixel 
 are in arcsec/m. 
 If the platescale is in m/arcsec, then the resulting pixel_to_radian and radian_to_pixel 
 are in m/arcsec.'","'Convert between lengths (to be interpreted as lengths in the focal plane) 
 and angular units with a specified ``platescale``. 
 Parameters 
 platescale : `~astropy.units.Quantity` 
 The pixel scale either in units of distance/pixel or distance/angle.'"
"@sensitive_post_parameters() 
 @never_cache 
 def password_reset_confirm(request, uidb36=None, token=None, template_name='registration/password_reset_confirm.html', token_generator=default_token_generator, set_password_form=SetPasswordForm, post_reset_redirect=None, current_app=None, extra_context=None): 
    assert ((uidb36 is not None) and (token is not None)) 
   if (post_reset_redirect is None): 
      post_reset_redirect = reverse('django.contrib.auth.views.password_reset_complete') 
   try: 
      uid_int = base36_to_int(uidb36) 
      user = User.objects.get(id=uid_int) 
   except (ValueError, User.DoesNotExist): 
      user = None 
   if ((user is not None) and token_generator.check_token(user, token)): 
      validlink = True 
      if (request.method == 'POST'): 
         form = set_password_form(user, request.POST) 
         if form.is_valid(): 
            form.save() 
            return HttpResponseRedirect(post_reset_redirect) 
      else: 
         form = set_password_form(None) 
   else: 
      validlink = False 
      form = None 
   context = {'form': form, 'validlink': validlink} 
   if (extra_context is not None): 
      context.update(extra_context) 
   return TemplateResponse(request, template_name, context, current_app=current_app)"," 'Confirm a password reset for a user. 
 The POST method is used to set a new password. 
 The GET method is used to display the password reset confirmation 
 page. 
 The form is the SetPasswordForm class. 
 The context is a dictionary that can be used to add additional 
 information to the page. 
 :param uidb36: The user id in base36. 
 :type uidb36: str 
 :param token: The token that was sent to the user. 
 :type token: str 
 :param template_name: The template name. 
 :type template_name: str 
 :param token_generator: A class that generates the token. 
 :type token_generator: class 
 :param set_password_form: The form class that is used to set a password. 
 :type set_password_form: class 
 :param post_reset_redirect: The url to redirect to after the password 
 is set. 
 :type post_reset_redirect: str 
 :param current_app: The current application. 
 :type current_app: django","'View that checks the hash in a password reset link and presents a 
 form for entering a new password.'"
"def randitems_from_structure(fmt, t): 
    (memlen, itemsize, _, _, _, _) = t 
   return gen_items((memlen // itemsize), ('#' + fmt), 'numpy')"," 'Return a list of random items from a numpy structure. 
 Parameters 
 fmt : str 
 A string describing the structure of the numpy array 
 t : tuple 
 A tuple describing the size of the numpy array'","'Return a list of random items for structure \'t\' with format 
 \'fmtchar\'.'"
"def track_to_mpd_format(track, position=None, stream_title=None): 
    if isinstance(track, TlTrack): 
      (tlid, track) = track 
   else: 
      (tlid, track) = (None, track) 
   if (not track.uri): 
      logger.warning(u'Ignoring   track   without   uri') 
      return [] 
   result = [(u'file', track.uri), (u'Time', ((track.length and (track.length // 1000)) or 0)), (u'Artist', concat_multi_values(track.artists, u'name')), (u'Album', ((track.album and track.album.name) or u''))] 
   if (stream_title is not None): 
      result.append((u'Title', stream_title)) 
      if track.name: 
         result.append((u'Name', track.name)) 
   else: 
      result.append((u'Title', (track.name or u''))) 
   if track.date: 
      result.append((u'Date', track.date)) 
   if ((track.album is not None) and (track.album.num_tracks is not None)): 
      result.append((u'Track', (u'%d/%d' % ((track.track_no or 0), track.album.num_tracks)))) 
   else: 
      result.append((u'Track', (track.track_no or 0))) 
   if ((position is not None) and (tlid is not None)): 
      result.append((u'Pos', position)) 
      result.append((u'Id', tlid)) 
   if ((track.album is not None) and (track.album.musicbrainz_id is not None)): 
      result.append((u'MUSICBRAINZ_ALBUMID', track.album.musicbrainz_id)) 
   if ((track.album is not None) and track.album.artists): 
      result.append((u'AlbumArtist', concat_multi_values(track.album.artists, u'name'))) 
      musicbrainz_ids = concat_multi_values(track.album.artists, u'musicbrainz_id') 
      if musicbrainz_ids: 
         result.append((u'MUSICBRAINZ_ALBUMARTISTID', musicbrainz_ids)) 
   if track.artists: 
      musicbrainz_ids = concat_multi_values(track.artists, u'musicbrainz_id') 
      if musicbrainz_ids: 
         result.append((u'MUSICBRAINZ_ARTISTID', musicbrainz_ids)) 
   if track.composers: 
      result.append((u'Composer', concat_multi_values(track.composers, u'name'))) 
   if track.performers: 
      result.append((u'Performer', concat_multi_values(track.performers, u'name'))) 
   if track.genre: 
      result.append((u'Genre', track.genre)) 
   if track.disc_no: 
      result.append((u'Disc', track.disc_no)) 
   if track.last_modified: 
      datestring = datetime.datetime.utcfromtimestamp((track.last_modified // 1000)).isoformat() 
      result.append((u'Last-Modified', (datestring + u'Z'))) 
   if (track.musicbrainz_id is not None): 
      result.append((u'MUSICBRAINZ_TRACKID', track.musicbrainz_id)) 
   if (track.album and track.album.uri): 
      result.append((u'X-AlbumUri', track.album.uri)) 
   if (track.album and track.album.images): 
      images = u';'.join((i for i in track.album.images if (i is not u''))) 
      result.append((u'X-AlbumImage', images)) 
   result = [element for element in result if _has_value(*element)] 
   return result", 'Convert track to mpd format.',"'Format track for output to MPD client. 
 :param track: the track 
 :type track: :class:`mopidy.models.Track` or :class:`mopidy.models.TlTrack` 
 :param position: track\'s position in playlist 
 :type position: integer 
 :param stream_title: the current streams title 
 :type position: string 
 :rtype: list of two-tuples'"
"def getReadRepository(repository): 
    text = archive.getFileText(archive.getProfilesPath(getProfileBaseName(repository)), False) 
   if (text == ''): 
      if (repository.baseNameSynonym != None): 
         text = archive.getFileText(archive.getProfilesPath(getProfileBaseNameSynonym(repository)), False) 
   if (text == ''): 
      print ('The   default   %s   will   be   written   in   the   .skeinforge   folder   in   the   home   directory.' % repository.title.lower()) 
      text = archive.getFileText(getProfilesDirectoryInAboveDirectory(getProfileBaseName(repository)), False) 
      if (text != ''): 
         readSettingsFromText(repository, text) 
      writeSettings(repository) 
      temporaryApplyOverrides(repository) 
      return repository 
   readSettingsFromText(repository, text) 
   temporaryApplyOverrides(repository) 
   return repository", 'Returns a read repository object.','Read and return settings from a file.'
"def findTypeParent(element, tag): 
    p = element 
   while True: 
      p = p.getparent() 
      if (p.tag == tag): 
         return p 
   return None"," 'Find the parent of an element with the given tag. 
 :param element: the element to find the parent of 
 :param tag: the tag of the parent to find 
 :return: the parent element, or None if not found'","'Finds fist parent of element of the given type 
 @param object element: etree element 
 @param string the tag parent to search for 
 @return object element: the found parent or None when not found'"
"def log(repo='.', paths=None, outstream=sys.stdout, max_entries=None, reverse=False, name_status=False): 
    with open_repo_closing(repo) as r: 
      walker = r.get_walker(max_entries=max_entries, paths=paths, reverse=reverse) 
      for entry in walker: 
         decode = (lambda x: commit_decode(entry.commit, x)) 
         print_commit(entry.commit, decode, outstream) 
         if name_status: 
            outstream.writelines([(l + '\n') for l in print_name_status(entry.changes())])"," 'Logs a commit to the given repo. 
 :param repo: The name of the repo to log to. 
 :param paths: A list of paths to log. 
 :param outstream: A file-like object to log to. 
 :param max_entries: The maximum number of entries to log. 
 :param reverse: If True, log entries in reverse order. 
 :param name_status: If True, log the status of the files changed by the commit.'","'Write commit logs. 
 :param repo: Path to repository 
 :param paths: Optional set of specific paths to print entries for 
 :param outstream: Stream to write log output to 
 :param reverse: Reverse order in which entries are printed 
 :param name_status: Print name status 
 :param max_entries: Optional maximum number of entries to display'"
"def get_rule_handle(table='filter', chain=None, rule=None, family='ipv4'): 
    if (not chain): 
      return 'Error:   Chain   needs   to   be   specified' 
   if (not rule): 
      return 'Error:   Rule   needs   to   be   specified' 
   if (not check_table(table, family=family)): 
      return 'Error:   table   {0}   in   family   {1}   does   not   exist'.format(table, family) 
   if (not check_chain(table, chain, family=family)): 
      return 'Error:   chain   {0}   in   table   {1}   in   family   {2}   does   not   exist'.format(chain, table, family) 
   if (not check(table, chain, rule, family=family)): 
      return 'Error:   rule   {0}   chain   {1}   in   table   {2}   in   family   {3}   does   not   exist'.format(rule, chain, table, family) 
   nft_family = _NFTABLES_FAMILIES[family] 
   cmd = '{0}   --numeric   --numeric   --numeric   --handle   list   chain   {1}   {2}   {3}'.format(_nftables_cmd(), nft_family, table, chain) 
   out = __salt__['cmd.run'](cmd, python_shell=False) 
   rules = re.split('\n+', out) 
   pat = re.compile('{0}   #   handle   (?P<handle>\\d+)'.format(rule)) 
   for r in rules: 
      match = pat.search(r) 
      if match: 
         return match.group('handle') 
   return 'Error:   could   not   find   rule   {0}'.format(rule)"," 'Returns a handle for a rule in the specified chain in the specified 
 table in the specified family. 
 :param table: The table to use 
 :param chain: The chain to use 
 :param rule: The rule to use 
 :param family: The family to use 
 :returns: The handle of the rule or an error message.'","'Get the handle for a particular rule 
 This function accepts a rule in a standard nftables command format, 
 starting with the chain. Trying to force users to adapt to a new 
 method of creating rules would be irritating at best, and we 
 already have a parser that can handle it. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' nftables.get_rule_handle filter input \ 
 rule=\'input tcp dport 22 log accept\' 
 IPv6: 
 salt \'*\' nftables.get_rule_handle filter input \ 
 rule=\'input tcp dport 22 log accept\' \ 
 family=ipv6'"
"def to_names(domain_obj_list): 
    objs = [] 
   for obj in domain_obj_list: 
      objs.append((obj.name if obj else None)) 
   return objs"," 'Returns a list of names for each domain object. 
 If the object does not have a name, then None is returned. 
 :param domain_obj_list: 
 :return: 
 :rtype: 
 :type domain_obj_list: 
 :param domain_obj_list: 
 :rtype: 
 :type domain_obj_list: 
 :return: 
 :rtype: 
 :type domain_obj_list: 
 :param domain_obj_list: 
 :rtype: 
 :type domain_obj_list: 
 :return: 
 :rtype: 
 :type domain_obj_list: 
 :return: 
 :rtype: 
 :type domain_obj_list: 
 :return: 
 :rtype: 
 :type domain_obj_list: 
 :return: 
 :rtype: 
 :type domain_obj_list: 
 :return: 
 :rtype: 
 :type domain_obj_list: 
 :return: 
 :rtype: 
 :type domain_obj_","'Takes a list of domain objects and returns a corresponding list 
 of their names.'"
"def load_json_dict(filename, *args): 
    data = {} 
   if os.path.exists(filename): 
      lock.acquire() 
      with open(filename, 'r') as f: 
         try: 
            data = _json.load(f) 
            if (not isinstance(data, dict)): 
               data = {} 
         except: 
            data = {} 
      lock.release() 
      if args: 
         return {key: data[key] for key in args if (key in data)} 
   return data"," 'Loads a json file and returns it as a dict. 
 :param filename: Path to the json file 
 :param args: A list of keys to return. If args is not specified, the 
 entire dict is returned.'",'Checks if file exists. Returns {} if something fails.'
"def _alarms_present(name, min_size_equals_max_size, alarms, alarms_from_pillar, region, key, keyid, profile): 
    tmp = copy.deepcopy(__salt__['config.option'](alarms_from_pillar, {})) 
   if alarms: 
      tmp = dictupdate.update(tmp, alarms) 
   merged_return_value = {'name': name, 'result': True, 'comment': '', 'changes': {}} 
   for (_, info) in six.iteritems(tmp): 
      info['name'] = ((name + '   ') + info['name']) 
      info['attributes']['description'] = ((name + '   ') + info['attributes']['description']) 
      if ('dimensions' not in info['attributes']): 
         info['attributes']['dimensions'] = {'AutoScalingGroupName': [name]} 
      scaling_policy_actions_only = True 
      for action_type in ['alarm_actions', 'insufficient_data_actions', 'ok_actions']: 
         if (action_type in info['attributes']): 
            new_actions = [] 
            for action in info['attributes'][action_type]: 
               if ('scaling_policy' not in action): 
                  scaling_policy_actions_only = False 
               if (':self:' in action): 
                  action = action.replace(':self:', ':{0}:'.format(name)) 
               new_actions.append(action) 
            info['attributes'][action_type] = new_actions 
      if (scaling_policy_actions_only and min_size_equals_max_size): 
         continue 
      kwargs = {'name': info['name'], 'attributes': info['attributes'], 'region': region, 'key': key, 'keyid': keyid, 'profile': profile} 
      results = __states__['boto_cloudwatch_alarm.present'](**kwargs) 
      if (not results['result']): 
         merged_return_value['result'] = False 
      if (results.get('changes', {}) != {}): 
         merged_return_value['changes'][info['name']] = results['changes'] 
      if ('comment' in results): 
         merged_return_value['comment'] += results['comment'] 
   return merged_return_value", 'Create alarms','helper method for present.  ensure that cloudwatch_alarms are set'
"@app.route('/stream/<int:n>') 
 def stream_n_messages(n): 
    response = get_dict('url', 'args', 'headers', 'origin') 
   n = min(n, 100) 
   def generate_stream(): 
      for i in range(n): 
         response['id'] = i 
         (yield (json.dumps(response) + '\n')) 
   return Response(generate_stream(), headers={'Content-Type': 'application/json'})", 'Return a stream of n messages.','Stream n JSON messages'
"def partial_velocity(vel_vecs, gen_speeds, frame): 
    if (not iterable(vel_vecs)): 
      raise TypeError('Velocity   vectors   must   be   contained   in   an   iterable.') 
   if (not iterable(gen_speeds)): 
      raise TypeError('Generalized   speeds   must   be   contained   in   an   iterable') 
   vec_partials = [] 
   for vec in vel_vecs: 
      partials = [] 
      for speed in gen_speeds: 
         partials.append(vec.diff(speed, frame, var_in_dcm=False)) 
      vec_partials.append(partials) 
   return vec_partials"," 'Returns a list of partial derivatives of velocity vectors. 
 Parameters 
 vel_vecs : iterable 
 An iterable of velocity vectors. 
 gen_speeds : iterable 
 An iterable of generalized speeds. 
 frame : string 
 The reference frame of the velocity vectors. 
 Examples 
 >>> from sympy.physics.vector import Velocity, Frame 
 >>> from sympy.physics.vector import partial_velocity 
 >>> vel_vecs = [Velocity([1, 2, 3], Frame(""xyz""))] 
 >>> gen_speeds = [Velocity([1, 0, 0], Frame(""xyz"")), 
 ...               Velocity([0, 1, 0], Frame(""xyz"")), 
 ...               Velocity([0, 0, 1], Frame(""xyz""))] 
 >>> partial_velocity(vel_vecs, gen_speeds, ""xyz"") 
 [[1, 2, 3], [1, 0, 0], [0, 1, 0], [0, 0, 1]]'","'Returns a list of partial velocities with respect to the provided 
 generalized speeds in the given reference frame for each of the supplied 
 velocity vectors. 
 The output is a list of lists. The outer list has a number of elements 
 equal to the number of supplied velocity vectors. The inner lists are, for 
 each velocity vector, the partial derivatives of that velocity vector with 
 respect to the generalized speeds supplied. 
 Parameters 
 vel_vecs : iterable 
 An iterable of velocity vectors (angular or linear). 
 gen_speeds : iterable 
 An iterable of generalized speeds. 
 frame : ReferenceFrame 
 The reference frame that the partial derivatives are going to be taken 
 in. 
 Examples 
 >>> from sympy.physics.vector import Point, ReferenceFrame 
 >>> from sympy.physics.vector import dynamicsymbols 
 >>> from sympy.physics.vector import partial_velocity 
 >>> u = dynamicsymbols(\'u\') 
 >>> N = ReferenceFrame(\'N\') 
 >>> P = Point(\'P\') 
 >>> P.set_vel(N, u * N.x) 
 >>> vel_vecs = [P.vel(N)] 
 >>> gen_speeds = [u] 
 >>> partial_velocity(vel_vecs, gen_speeds, N) 
 [[N.x]]'"
"def is_ascii(string): 
    return all(((ord(c) < 128) for c in string))", 'Return True if the string contains only ASCII characters.','Return whether a string is in ascii.'
"def docstring_errors(filename, global_dict=None): 
    if (global_dict is None): 
      global_dict = {} 
   if ('__file__' not in global_dict): 
      global_dict['__file__'] = filename 
   if ('__doc__' not in global_dict): 
      global_dict['__doc__'] = None 
   try: 
      with open(filename) as f: 
         code = compile(f.read(), filename, 'exec') 
         exec code in global_dict 
   except SystemExit: 
      pass 
   except SkipTest: 
      raise AssertionError(((""Couldn't   verify   format   of   "" + filename) + 'due   to   SkipTest')) 
   all_errors = [] 
   for (key, val) in six.iteritems(global_dict): 
      if (not key.startswith('_')): 
         module_name = '' 
         if hasattr(inspect.getmodule(val), '__name__'): 
            module_name = inspect.getmodule(val).__name__ 
         if ((inspect.isfunction(val) or inspect.isclass(val)) and ((inspect.getmodule(val) is None) or (module_name == '__builtin__'))): 
            if inspect.isfunction(val): 
               all_errors.extend(handle_function(val, key)) 
            elif inspect.isclass(val): 
               all_errors.extend(handle_class(val, key)) 
      elif (key == '__doc__'): 
         all_errors.extend(handle_module(val, key)) 
   if all_errors: 
      all_errors.insert(0, (('%s:' % filename),)) 
   return all_errors", 'Return a list of errors found in a module or function docstring.',"'Run a Python file, parse the docstrings of all the classes 
 and functions it declares, and return them. 
 Parameters 
 filename : str 
 Filename of the module to run. 
 global_dict : dict, optional 
 Globals dictionary to pass along to `execfile()`. 
 Returns 
 all_errors : list 
 Each entry of the list is a tuple, of length 2 or 3, with 
 format either 
 (func_or_class_name, docstring_error_description) 
 or 
 (class_name, method_name, docstring_error_description)'"
"def run_doctest(target_dir=None, *args, **kwargs): 
    import doctest 
   default_kwargs = {'optionflags': doctest.ELLIPSIS} 
   kwargs.update(default_kwargs) 
   cur_dir = os.path.abspath(os.curdir) 
   print('Running   doctests...') 
   try: 
      os.chdir(find_test_dir(target_dir)) 
      doctest.testmod(*args, **kwargs) 
   finally: 
      os.chdir(cur_dir) 
   print('Done')", 'Run the doctests in the current directory.','Runs doctest for the importing module.'
"@verbose 
 def _get_ico_tris(grade, verbose=None, return_surf=False): 
    ico = _get_ico_surface(grade) 
   if (not return_surf): 
      return ico['tris'] 
   else: 
      return ico"," 'Get the tris of the given ico. 
 Parameters 
 grade : int 
 The grade of the ico. 
 verbose : int, optional 
 The verbosity level. 
 return_surf : bool, optional 
 If True, return the ico surface, otherwise return the tris. 
 Returns 
 tris : list 
 The tris of the ico. 
 Examples 
 >>> from sympy.geometry.ico import _get_ico_tris 
 >>> _get_ico_tris(10) 
 [[0, 1, 2], [2, 3, 0], [3, 4, 2], [4, 5, 3], [5, 6, 4], [6, 7, 5], [7, 8, 6], [8, 9, 7], [9, 10, 8], [10, 11, 9]]'",'Get triangles for ico surface.'
"def get_last_modified(files): 
    files = list(files) 
   if files: 
      return max((datetime.datetime.fromtimestamp(os.path.getmtime(f)) for f in files)) 
   return datetime.datetime(1970, 1, 1)"," 'Returns the last modified time of the given files. 
 If there are no files, returns the epoch time (1970-01-01).'","'Returns the modification time of the most recently modified 
 file provided 
 :param list(str) files: names of files to check 
 :return: most recent modification time amongst the fileset 
 :rtype: datetime.datetime'"
"def discoverInfo(disp, jid, node=None): 
    '   According   to   JEP-0030:\n                        query   MAY   have   node   attribute\n                        identity:   MUST   HAVE   category   and   name   attributes   and   MAY   HAVE   type   attribute.\n                        feature:   MUST   HAVE   var   attribute' 
   (identities, features) = ([], []) 
   for i in _discover(disp, NS_DISCO_INFO, jid, node): 
      if (i.getName() == 'identity'): 
         identities.append(i.attrs) 
      elif (i.getName() == 'feature'): 
         features.append(i.getAttr('var')) 
      elif (i.getName() == 'agent'): 
         if i.getTag('name'): 
            i.setAttr('name', i.getTagData('name')) 
         if i.getTag('description'): 
            i.setAttr('name', i.getTagData('description')) 
         identities.append(i.attrs) 
         if i.getTag('groupchat'): 
            features.append(NS_GROUPCHAT) 
         if i.getTag('register'): 
            features.append(NS_REGISTER) 
         if i.getTag('search'): 
            features.append(NS_SEARCH) 
   return (identities, features)"," 'Discover Info about a jid. 
 :param disp: The XMPP connection 
 :param jid: The jid to discover info about 
 :param node: The node to discover info about. 
 :return: A tuple of (identities, features) where identities is a list of 
 dictionaries of the form: 
 { \'category\' : \'category\', 
 \'name\' : \'name\', 
 \'type\' : \'type\', 
 \'version\' : \'version\'' 
 and features is a list of strings.'",'Query remote object about info that it publishes. Returns identities and features lists.'
"def is_on(hass, entity_id=None): 
    entity_id = (entity_id or ENTITY_ID) 
   return hass.states.is_state(entity_id, STATE_ABOVE_HORIZON)"," 'Returns True if the entity is on, False otherwise.'",'Test if the sun is currently up based on the statemachine.'
"def p_postfix_expression_2(t): 
    pass", '2. Postfix expression','postfix_expression : postfix_expression LBRACKET expression RBRACKET'
"def available_oficial_plugins(): 
    return _availables_plugins(resources.PLUGINS_WEB)", 'Returns a list of available official plugins.','Returns a dict with OFICIAL availables plugins in NINJA-IDE web page'
"def can_introspect(field): 
    if (hasattr(field, '_south_introspects') and field._south_introspects): 
      return True 
   full_name = ('%s.%s' % (field.__class__.__module__, field.__class__.__name__)) 
   for regex in allowed_fields: 
      if re.match(regex, full_name): 
         return True 
   return False", 'Returns True if the field can be introspected',"'Returns True if we are allowed to introspect this field, False otherwise. 
 (\'allowed\' means \'in core\'. Custom fields can declare they are introspectable 
 by the default South rules by adding the attribute _south_introspects = True.)'"
"def user_passes_test(test_func, login_url=LOGIN_URL): 
    def _dec(view_func): 
      def _checklogin(request, *args, **kwargs): 
         if test_func(request.user): 
            return view_func(request, *args, **kwargs) 
         return HttpResponseRedirect(('%s?%s=%s' % (login_url, REDIRECT_FIELD_NAME, quote(request.get_full_path())))) 
      _checklogin.__doc__ = view_func.__doc__ 
      _checklogin.__dict__ = view_func.__dict__ 
      return _checklogin 
   return _dec"," 'Decorator for view functions. 
 If the user is not logged in, the view function is redirected to the 
 login page. 
 :param test_func: A callable that takes a request and returns True if the 
 user is logged in and False otherwise. 
 :param login_url: The URL to redirect to if the user is not logged in. 
 :return: A decorator that takes a view function and returns a view 
 function that checks if the user is logged in. 
 :rtype: callable'","'Decorator for views that checks that the user passes the given test, 
 redirecting to the log-in page if necessary. The test should be a callable 
 that takes the user object and returns True if the user passes.'"
"@handle_response_format 
 @treeio_login_required 
 @_process_mass_form 
 def task_view(request, task_id, response_format='html'): 
    task = get_object_or_404(Task, pk=task_id) 
   if (not request.user.profile.has_permission(task)): 
      return user_denied(request, message=""You   don't   have   access   to   this   Task"") 
   if request.user.profile.has_permission(task, mode='x'): 
      if request.POST: 
         if ('add-work' in request.POST): 
            return HttpResponseRedirect(reverse('projects_task_time_slot_add', args=[task.id])) 
         elif ('start-work' in request.POST): 
            return HttpResponseRedirect(reverse('projects_task_view', args=[task.id])) 
         record = UpdateRecord() 
         record.record_type = 'manual' 
         form = TaskRecordForm(request.user.profile, request.POST, instance=record) 
         if form.is_valid(): 
            record = form.save() 
            record.set_user_from_request(request) 
            record.save() 
            record.about.add(task) 
            task.set_last_updated() 
            return HttpResponseRedirect(reverse('projects_task_view', args=[task.id])) 
      else: 
         form = TaskRecordForm(request.user.profile) 
   else: 
      form = None 
   subtasks = Object.filter_by_request(request, Task.objects.filter(parent=task)) 
   time_slots = Object.filter_by_request(request, TaskTimeSlot.objects.filter(task=task)) 
   context = _get_default_context(request) 
   context.update({'task': task, 'subtasks': subtasks, 'record_form': form, 'time_slots': time_slots}) 
   if (('massform' in context) and ('project' in context['massform'].fields)): 
      del context['massform'].fields['project'] 
   return render_to_response('projects/task_view', context, context_instance=RequestContext(request), response_format=response_format)", 'View a task.','Single task view page'
"def decodeString(string): 
    decodedString = string 
   octalNumbers = re.findall('\\\\([0-7]{1-3})', decodedString, re.DOTALL) 
   for octal in octalNumbers: 
      try: 
         decodedString = decodedString.replace(('\\\\' + octal), chr(int(octal, 8))) 
      except: 
         return ((-1), 'Error   decoding   string') 
   return (0, decodedString)"," 'Decodes a string using the following rules: 
 \'\\\\\': Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Replaced by the character 
 \'\\\' : Repla","'Decode the given PDF string 
 @param string: A PDFString to decode 
 @return A tuple (status,statusContent), where statusContent is the decoded PDF string in case status = 0 or an error in case status = -1'"
"def checkRecursive(paths, reporter): 
    warnings = 0 
   for sourcePath in iterSourceCode(paths): 
      warnings += checkPath(sourcePath, reporter) 
   return warnings"," 'Check the source code in the given paths. 
 This function checks the source code for the following issues: 
 * Line breaks 
 * Lines longer than 80 characters 
 * Duplicate lines 
 * Empty lines 
 * Comments 
 * Unused variables 
 * Unused functions 
 * Unused classes 
 * Unused variables in classes 
 * Unused attributes in classes 
 * Unused variables in functions 
 * Unused attributes in functions 
 * Unused parameters in functions 
 * Unused return values in functions 
 * Unused arguments in functions 
 * Unused variables in loops 
 * Unused variables in conditionals 
 * Unused variables in conditional clauses 
 * Unused variables in while loops 
 * Unused variables in for loops 
 * Unused variables in for loops with range 
 * Unused variables in for loops with enumerate 
 * Unused variables in for loops with zip 
 * Unused variables in for loops with zip with repeat 
 * Unused variables in for loops with zip with repeat and repeat 
 * Unused variables in for loops with zip with repeat and repeat 
 * Un","'Recursively check all source files in C{paths}. 
 @param paths: A list of paths to Python source files and directories 
 containing Python source files. 
 @param reporter: A L{Reporter} where all of the warnings and errors 
 will be reported to. 
 @return: The number of warnings found.'"
"def setup_platform(hass, config, add_devices, discovery_info=None): 
    add_devices([VlcDevice(config.get(CONF_NAME), config.get(CONF_ARGUMENTS))])", 'Set up the VLC platform.','Setup the vlc platform.'
"def test_hsl_to_rgb_part_10(): 
    assert (hsl_to_rgb(180, 20, 50) == (102, 153, 153)) 
   assert (hsl_to_rgb(180, 60, 50) == (51, 204, 204)) 
   assert (hsl_to_rgb(180, 100, 50) == (0, 255, 255))", 'Testing the HSL to RGB function','Test hsl to rgb color function'
"def paths_from_event(self, event): 
    md = event.mimeData() 
   if (md.hasFormat(u'text/uri-list') and (not md.hasFormat(u'application/calibre+from_library'))): 
      urls = [unicode(u.toLocalFile()) for u in md.urls()] 
      return [u for u in urls if (os.path.splitext(u)[1] and os.path.exists(u))]", 'Return paths to files from an event.',"'Accept a drop event and return a list of paths that can be read from 
 and represent files with extensions.'"
"def f(t): 
    s1 = np.cos(((2 * np.pi) * t)) 
   e1 = np.exp((- t)) 
   return (s1 * e1)"," 'Returns a function with the following properties: 
 f(t) = cos(2*pi*t) * exp(-t) 
 f'(t) = -2*pi*sin(2*pi*t) * exp(-t) 
 f''(t) = -2*pi*cos(2*pi*t) * exp(-t)'",'a damped exponential'
"def func_np(a, b): 
    return np.exp(((2.1 * a) + (3.2 * b)))", 'Test for np.exp()','Control function using Numpy.'
"def _parse_file(document_file, validate, entry_class, entry_keyword='r', start_position=None, end_position=None, section_end_keywords=(), extra_args=()): 
    if start_position: 
      document_file.seek(start_position) 
   else: 
      start_position = document_file.tell() 
   if section_end_keywords: 
      first_keyword = None 
      line_match = KEYWORD_LINE.match(stem.util.str_tools._to_unicode(document_file.readline())) 
      if line_match: 
         first_keyword = line_match.groups()[0] 
      document_file.seek(start_position) 
      if (first_keyword in section_end_keywords): 
         return 
   while ((end_position is None) or (document_file.tell() < end_position)): 
      (desc_lines, ending_keyword) = _read_until_keywords(((entry_keyword,) + section_end_keywords), document_file, ignore_first=True, end_position=end_position, include_ending_keyword=True) 
      desc_content = bytes.join('', desc_lines) 
      if desc_content: 
         (yield entry_class(desc_content, validate, *extra_args)) 
         if (ending_keyword in section_end_keywords): 
            break 
      else: 
         break"," 'Parses a document file and yields entries. 
 :param document_file: The file to parse. 
 :param validate: Whether to validate the entries. 
 :param entry_class: The class to use to create entries. 
 :param entry_keyword: The keyword to look for in the file. 
 :param start_position: The position to start parsing from. 
 :param end_position: The position to stop parsing at. 
 :param section_end_keywords: The section end keywords. 
 :param extra_args: Any arguments to pass to the entry class. 
 :rtype: list of :class:`~.Entry` 
 :raises: :exc:`~.ParseError` if the file is malformed. 
 :raises: :exc:`~.ParseError` if the entry is malformed. 
 :raises: :exc:`~.ParseError` if the entry is not valid. 
 :raises: :exc:`~.ParseError` if the entry is invalid.'","'Reads a range of the document_file containing some number of entry_class 
 instances. We deliminate the entry_class entries by the keyword on their 
 first line (entry_keyword). When finished the document is left at the 
 end_position. 
 Either an end_position or section_end_keywords must be provided. 
 :param file document_file: file with network status document content 
 :param bool validate: checks the validity of the document\'s contents if 
 **True**, skips these checks otherwise 
 :param class entry_class: class to construct instance for 
 :param str entry_keyword: first keyword for the entry instances 
 :param int start_position: start of the section, default is the current position 
 :param int end_position: end of the section 
 :param tuple section_end_keywords: keyword(s) that deliminate the end of the 
 section if no end_position was provided 
 :param tuple extra_args: extra arguments for the entry_class (after the 
 content and validate flag) 
 :returns: iterator over entry_class instances 
 :raises: 
 * **ValueError** if the contents is malformed and validate is **True** 
 * **IOError** if the file can\'t be read'"
"@memoized 
 def flavor_list(request): 
    return novaclient(request).flavors.list()", 'Returns a list of flavors.','Get the list of available instance sizes (flavors).'
"def get_policy_string(base, policy_or_index): 
    if isinstance(policy_or_index, BaseStoragePolicy): 
      policy = policy_or_index 
   else: 
      policy = POLICIES.get_by_index(policy_or_index) 
      if (policy is None): 
         raise PolicyError('Unknown   policy', index=policy_or_index) 
   return _get_policy_string(base, int(policy))"," 'Returns the policy string for the policy with the given index or 
 the policy object itself.'","'Helper function to construct a string from a base and the policy. 
 Used to encode the policy index into either a file name or a 
 directory name by various modules. 
 :param base: the base string 
 :param policy_or_index: StoragePolicy instance, or an index 
 (string or int), if None the legacy 
 storage Policy-0 is assumed. 
 :returns: base name with policy index added 
 :raises: PolicyError if no policy exists with the given policy_index'"
"def upgrade(refresh=True): 
    ret = {'changes': {}, 'result': True, 'comment': ''} 
   old = list_pkgs() 
   if salt.utils.is_true(refresh): 
      refresh_db() 
   result = _call_brew('brew   upgrade', failhard=False) 
   __context__.pop('pkg.list_pkgs', None) 
   new = list_pkgs() 
   ret = salt.utils.compare_dicts(old, new) 
   if (result['retcode'] != 0): 
      raise CommandExecutionError('Problem   encountered   upgrading   packages', info={'changes': ret, 'result': result}) 
   return ret", 'Upgrade packages',"'Upgrade outdated, unpinned brews. 
 refresh 
 Fetch the newest version of Homebrew and all formulae from GitHub before installing. 
 Returns a dictionary containing the changes: 
 .. code-block:: python 
 {\'<package>\':  {\'old\': \'<old-version>\', 
 \'new\': \'<new-version>\'}} 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pkg.upgrade'"
"def resolve(): 
    filename = '/'.join(request.args) 
   path = apath(filename, r=request) 
   a = safe_read(path).split('\n') 
   try: 
      b = safe_read((path + '.1')).split('\n') 
   except IOError: 
      session.flash = 'Other   file,   no   longer   there' 
      redirect(URL('edit', args=request.args)) 
   d = difflib.ndiff(a, b) 
   def leading(line): 
      '      ' 
      z = '' 
      for (k, c) in enumerate(line): 
         if (c == '   '): 
            z += '&nbsp;' 
         elif (c == '    DCTB '): 
            z += '&nbsp;' 
         elif ((k == 0) and (c == '?')): 
            pass 
         else: 
            break 
      return XML(z) 
   def getclass(item): 
      '   Determine   item   class   ' 
      operators = {'   ': 'normal', '+': 'plus', '-': 'minus'} 
      return operators[item[0]] 
   if request.vars: 
      c = '\n'.join([item[2:].rstrip() for (i, item) in enumerate(d) if ((item[0] == '   ') or (('line%i' % i) in request.vars))]) 
      safe_write(path, c) 
      session.flash = 'files   merged' 
      redirect(URL('edit', args=request.args)) 
   else: 
      gen_data = (lambda index, item: (((not (item[:1] in ['+', '-'])) and '') or INPUT(_type='checkbox', _name=('line%i' % index), value=(item[0] == '+')))) 
      diff = TABLE(*[TR(TD(gen_data(i, item)), TD(item[0]), TD(leading(item[2:]), TT(item[2:].rstrip())), _class=getclass(item)) for (i, item) in enumerate(d) if (item[0] != '?')]) 
   return dict(diff=diff, filename=filename)"," 'This function is used to merge the two files together. 
 It will read the two files, diff them, and then write the 
 merged file back to the same location as the original file. 
 :param request: The current request object 
 :return: The merged file, or None if the files cannot be merged'",''
"def getmode(mode): 
    global _modes 
   if (not _modes): 
      from . import Image 
      modes = {} 
      for (m, (basemode, basetype, bands)) in Image._MODEINFO.items(): 
         modes[m] = ModeDescriptor(m, bands, basemode, basetype) 
      modes['RGBa'] = ModeDescriptor('RGBa', ('R', 'G', 'B', 'a'), 'RGB', 'L') 
      modes['LA'] = ModeDescriptor('LA', ('L', 'A'), 'L', 'L') 
      modes['La'] = ModeDescriptor('La', ('L', 'a'), 'L', 'L') 
      modes['PA'] = ModeDescriptor('PA', ('P', 'A'), 'RGB', 'L') 
      modes['I;16'] = ModeDescriptor('I;16', 'I', 'L', 'L') 
      modes['I;16L'] = ModeDescriptor('I;16L', 'I', 'L', 'L') 
      modes['I;16B'] = ModeDescriptor('I;16B', 'I', 'L', 'L') 
      _modes = modes 
   return _modes[mode]"," 'Return a ModeDescriptor object for a given mode. 
 Parameters 
 mode : str 
 Mode to look up. 
 Returns 
 ModeDescriptor 
 ModeDescriptor object for the given mode.'",'Gets a mode descriptor for the given mode.'
"def _save_and_restart(plugin, title=None): 
    try: 
      plugin.save(title) 
      plugin.restart() 
      return True 
   except le_errors.Error as error: 
      logger.error('Plugin   failed   to   save   and   restart   server:') 
      logger.exception(error) 
      return False"," 'Restart the server after saving the plugin settings. 
 :param plugin: The plugin to restart. 
 :param title: The title of the plugin. 
 :return: True if the server was restarted, False otherwise.'","'Saves and restart the plugin, returning True if no errors occurred'"
"def det_perm(M): 
    args = [] 
   s = True 
   n = M.rows 
   try: 
      list = M._mat 
   except AttributeError: 
      list = flatten(M.tolist()) 
   for perm in generate_bell(n): 
      fac = [] 
      idx = 0 
      for j in perm: 
         fac.append(list[(idx + j)]) 
         idx += n 
      term = Mul(*fac) 
      args.append((term if s else (- term))) 
      s = (not s) 
   return Add(*args)"," 'Determinant of a matrix. 
 Examples 
 >>> from sympy import Matrix 
 >>> from sympy.matrices.dense import det_perm 
 >>> M = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 >>> det_perm(M) 
 35 
 >>> det_perm(Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) 
 35 
 >>> det_perm(Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) 
 -35'","'Return the det(``M``) by using permutations to select factors. 
 For size larger than 8 the number of permutations becomes prohibitively 
 large, or if there are no symbols in the matrix, it is better to use the 
 standard determinant routines, e.g. `M.det()`. 
 See Also 
 det_minor 
 det_quick'"
"def config_validator(user): 
    from hadoop import job_tracker 
   from hadoop.fs import webhdfs 
   res = [] 
   submit_to = [] 
   has_default = False 
   for name in HDFS_CLUSTERS.keys(): 
      cluster = HDFS_CLUSTERS[name] 
      res.extend(webhdfs.test_fs_configuration(cluster)) 
      if (name == 'default'): 
         has_default = True 
   if (not has_default): 
      res.append(('hadoop.hdfs_clusters', ""You   should   have   an   HDFS   called   'default'."")) 
   mr_down = [] 
   for name in MR_CLUSTERS.keys(): 
      cluster = MR_CLUSTERS[name] 
      if cluster.SUBMIT_TO.get(): 
         mr_down.extend(job_tracker.test_jt_configuration(cluster)) 
         submit_to.append(('mapred_clusters.' + name)) 
   if (mr_down and (len(mr_down) == len(MR_CLUSTERS.keys()))): 
      res.extend(mr_down) 
   if YARN_CLUSTERS.keys(): 
      res.extend(test_yarn_configurations(user)) 
   for name in YARN_CLUSTERS.keys(): 
      cluster = YARN_CLUSTERS[name] 
      if cluster.SUBMIT_TO.get(): 
         submit_to.append(('yarn_clusters.' + name)) 
   if (not submit_to): 
      res.append(('hadoop', ""Please   designate   one   of   the   MapReduce   or   Yarn   clusters   with   `submit_to=true'   in   order   to   run   jobs."")) 
   return res"," 'Validate the configuration. 
 :param user: The user who is configuring the cluster. 
 :type user: User 
 :return: A list of configuration errors. 
 :rtype: list'","'config_validator() -> [ (config_variable, error_message) ] 
 Called by core check_config() view.'"
"def get_group(name, match_obj): 
    try: 
      obj = match_obj.group(name) 
   except: 
      return '' 
   else: 
      if (obj is not None): 
         return obj 
      else: 
         return ''"," 'Returns the value of the group name or an empty string if it is not 
 found.'",'return a blank string if the match group is None'
"def salted_hmac(key_salt, value, secret=None): 
    if (secret is None): 
      secret = settings.SECRET_KEY 
   key_salt = force_bytes(key_salt) 
   secret = force_bytes(secret) 
   key = hashlib.sha1((key_salt + secret)).digest() 
   return hmac.new(key, msg=force_bytes(value), digestmod=hashlib.sha1)"," 'Generate a salted HMAC. 
 :param key_salt: The salt for the HMAC 
 :param value: The value to be hashed 
 :param secret: The secret used to create the key_salt 
 :return: The HMAC digest as a string'","'Returns the HMAC-SHA1 of \'value\', using a key generated from key_salt and a 
 secret (which defaults to settings.SECRET_KEY). 
 A different key_salt should be passed in for every application of HMAC.'"
"def etc_hosts(attrs=None, where=None): 
    return _osquery_cmd(table='etc_hosts', attrs=attrs, where=where)", 'Returns the contents of the /etc/hosts file.',"'Return etc_hosts information from osquery 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' osquery.etc_hosts'"
"def skip_if_config(*args): 
    def decorator(f): 
      group = args[0] 
      name = args[1] 
      @functools.wraps(f) 
      def wrapper(self, *func_args, **func_kwargs): 
         if hasattr(CONF, group): 
            conf_group = getattr(CONF, group) 
            if hasattr(conf_group, name): 
               value = getattr(conf_group, name) 
               if value: 
                  if (len(args) == 3): 
                     msg = args[2] 
                  else: 
                     msg = ('Config   option   %s.%s   is   false' % (group, name)) 
                  raise testtools.TestCase.skipException(msg) 
         return f(self, *func_args, **func_kwargs) 
      return wrapper 
   return decorator"," 'Decorator to skip tests if config option is false. 
 :param args: (group, name) 
 :param args: (group, name, msg) 
 :param args: (group, name, msg, skip_if_config_exception) 
 :param args: (group, name, msg, skip_if_config_exception, skip_if_config_exception_class)'","'Raise a skipException if a config exists and is True 
 :param str group: The first arg, the option group to check 
 :param str name: The second arg, the option name to check 
 :param str msg: Optional third arg, the skip msg to use if a skip is raised 
 :raises testtools.TestCase.skipException: If the specified config option 
 exists and evaluates to True'"
"def lasso_path(X, y, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params): 
    return enet_path(X, y, l1_ratio=1.0, eps=eps, n_alphas=n_alphas, alphas=alphas, precompute=precompute, Xy=Xy, copy_X=copy_X, coef_init=coef_init, verbose=verbose, positive=positive, return_n_iter=return_n_iter, **params)"," 'Compute the Lasso path. 
 Parameters 
 X : array, shape (n_samples, n_features) 
 Design matrix. 
 y : array, shape (n_samples,) 
 Response vector. 
 eps : float, default 0.001 
 Regularization parameter. 
 n_alphas : int, default 100 
 Number of alphas to evaluate. 
 alphas : array, shape (n_alphas,) 
 Precomputed alphas. 
 precompute : {'auto', 'auto_alphas', 'auto_alphas_Xy', 'auto_alphas_Xy_Xy'}, default 'auto' 
 Determines how the alphas are computed. 
 - 'auto': Alphas are computed using the formula given in [1]_. 
 - 'auto_alphas': Alphas are computed using the formula given in [1]_, but the 
 alphas are precomputed and stored in `alphas`. 
 - 'auto_alphas_Xy': Alphas are computed using the formula given in [1]_, but 
 the","'Compute Lasso path with coordinate descent 
 The Lasso optimization function varies for mono and multi-outputs. 
 For mono-output tasks it is:: 
 (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1 
 For multi-output tasks it is:: 
 (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21 
 Where:: 
 ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2} 
 i.e. the sum of norm of each row. 
 Read more in the :ref:`User Guide <lasso>`. 
 Parameters 
 X : {array-like, sparse matrix}, shape (n_samples, n_features) 
 Training data. Pass directly as Fortran-contiguous data to avoid 
 unnecessary memory duplication. If ``y`` is mono-output then ``X`` 
 can be sparse. 
 y : ndarray, shape (n_samples,), or (n_samples, n_outputs) 
 Target values 
 eps : float, optional 
 Length of the path. ``eps=1e-3`` means that 
 ``alpha_min / alpha_max = 1e-3`` 
 n_alphas : int, optional 
 Number of alphas along the regularization path 
 alphas : ndarray, optional 
 List of alphas where to compute the models. 
 If ``None`` alphas are set automatically 
 precompute : True | False | \'auto\' | array-like 
 Whether to use a precomputed Gram matrix to speed up 
 calculations. If set to ``\'auto\'`` let us decide. The Gram 
 matrix can also be passed as argument. 
 Xy : array-like, optional 
 Xy = np.dot(X.T, y) that can be precomputed. It is useful 
 only when the Gram matrix is precomputed. 
 copy_X : boolean, optional, default True 
 If ``True``, X will be copied; else, it may be overwritten. 
 coef_init : array, shape (n_features, ) | None 
 The initial values of the coefficients. 
 verbose : bool or integer 
 Amount of verbosity. 
 params : kwargs 
 keyword arguments passed to the coordinate descent solver. 
 positive : bool, default False 
 If set to True, forces coefficients to be positive. 
 return_n_iter : bool 
 whether to return the number of iterations or not. 
 Returns 
 alphas : array, shape (n_alphas,) 
 The alphas along the path where models are computed. 
 coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas) 
 Coefficients along the path. 
 dual_gaps : array, shape (n_alphas,) 
 The dual gaps at the end of the optimization for each alpha. 
 n_iters : array-like, shape (n_alphas,) 
 The number of iterations taken by the coordinate descent optimizer to 
 reach the specified tolerance for each alpha. 
 Notes 
 See examples/linear_model/plot_lasso_coordinate_descent_path.py 
 for an example. 
 To avoid unnecessary memory duplication the X argument of the fit method 
 should be directly passed as a Fortran-contiguous numpy array. 
 Note that in certain cases, the Lars solver may be significantly 
 faster to implement this functionality. In particular, linear 
 interpolation can be used to retrieve model coefficients between the 
 values output by lars_path 
 Examples 
 Comparing lasso_path and lars_path with interpolation: 
 >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T 
 >>> y = np.array([1, 2, 3.1]) 
 >>> # Use lasso_path to compute a coefficient path 
 >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5]) 
 >>> print(coef_path) 
 [[ 0.          0.          0.46874778] 
 [ 0.2159048   0.4425765   0.23689075]] 
 >>> # Now use lars_path and 1D linear interpolation to compute the 
 >>> # same path 
 >>> from sklearn.linear_model import lars_path 
 >>> alphas, active, coef_path_lars = lars_path(X, y, method=\'lasso\') 
 >>> from scipy import interpolate 
 >>> coef_path_continuous = interpolate.interp1d(alphas[::-1], 
 ...                                             coef_path_lars[:, ::-1]) 
 >>> print(coef_path_continuous([5., 1., .5])) 
 [[ 0.          0.          0.46915237] 
 [ 0.2159048   0.4425765   0.23668876]] 
 See also 
 lars_path 
 Lasso 
 LassoLars 
 LassoCV 
 LassoLarsCV 
 sklearn.decomposition.sparse_encode'"
"def available_modules(): 
    extra_service_versions = set([]) 
   _tempest_modules = set(tempest_modules()) 
   plugin_services = ClientsRegistry().get_service_clients() 
   for plugin_name in plugin_services: 
      plug_service_versions = set([x['service_version'] for x in plugin_services[plugin_name]]) 
      if plug_service_versions: 
         if (not plug_service_versions.isdisjoint(extra_service_versions)): 
            detailed_error = ('Plugin   %s   is   trying   to   register   a   service   %s   already   claimed   by   another   one' % (plugin_name, (extra_service_versions & plug_service_versions))) 
            raise exceptions.PluginRegistrationException(name=plugin_name, detailed_error=detailed_error) 
         if (not plug_service_versions.isdisjoint(_tempest_internal_modules())): 
            detailed_error = ('Plugin   %s   is   trying   to   register   a   service   %s   already   claimed   by   a   Tempest   one' % (plugin_name, (_tempest_internal_modules() & plug_service_versions))) 
            raise exceptions.PluginRegistrationException(name=plugin_name, detailed_error=detailed_error) 
      extra_service_versions |= plug_service_versions 
   return (_tempest_modules | extra_service_versions)"," 'Returns a set of all the modules available to the user. 
 This is a combination of the set of modules available in Tempest and 
 any modules available in plugins. 
 :return: a set of modules available to the user. 
 :rtype: set'","'Set of service client modules available in Tempest and plugins 
 Set of stable service clients from Tempest and service clients exposed 
 by plugins. This set of available modules can be used for automatic 
 configuration. 
 :raise PluginRegistrationException: if a plugin exposes a service_version 
 already defined by Tempest or another plugin. 
 Examples: 
 >>> from tempest import config 
 >>> params = {} 
 >>> for service_version in available_modules(): 
 >>>     service = service_version.split(\'.\')[0] 
 >>>     params[service] = config.service_client_config(service) 
 >>> service_clients = ServiceClients(creds, identity_uri, 
 >>>                                  client_parameters=params)'"
"def print_exc(limit=None, file=None): 
    if (file is None): 
      file = sys.stderr 
   try: 
      (etype, value, tb) = sys.exc_info() 
      print_exception(etype, value, tb, limit, file) 
   finally: 
      etype = value = tb = None"," 'Print an exception. 
 :param limit: The maximum number of frames to print. 
 :param file: The file to print to. 
 :return: None 
 :raises: ValueError if limit is not an integer or less than 0.'","'Shorthand for \'print_exception(sys.exc_type, sys.exc_value, sys.exc_traceback, limit, file)\'. 
 (In fact, it uses sys.exc_info() to retrieve the same information 
 in a thread-safe way.)'"
"def lombscargle_fastchi2(t, y, dy, f0, df, Nf, normalization='standard', fit_mean=True, center_data=True, nterms=1, use_fft=True, trig_sum_kwds=None): 
    if ((nterms == 0) and (not fit_mean)): 
      raise ValueError('Cannot   have   nterms   =   0   without   fitting   bias') 
   if (dy is None): 
      dy = 1 
   (t, y, dy) = np.broadcast_arrays(t, y, dy) 
   if (t.ndim != 1): 
      raise ValueError('t,   y,   dy   should   be   one   dimensional') 
   if (f0 < 0): 
      raise ValueError('Frequencies   must   be   positive') 
   if (df <= 0): 
      raise ValueError('Frequency   steps   must   be   positive') 
   if (Nf <= 0): 
      raise ValueError('Number   of   frequencies   must   be   positive') 
   w = (dy ** (-2.0)) 
   ws = np.sum(w) 
   if (center_data or fit_mean): 
      y = (y - (np.dot(w, y) / ws)) 
   yw = (y / dy) 
   chi2_ref = np.dot(yw, yw) 
   kwargs = dict.copy((trig_sum_kwds or {})) 
   kwargs.update(f0=f0, df=df, use_fft=use_fft, N=Nf) 
   yws = np.sum((y * w)) 
   SCw = [(np.zeros(Nf), (ws * np.ones(Nf)))] 
   SCw.extend([trig_sum(t, w, freq_factor=i, **kwargs) for i in range(1, ((2 * nterms) + 1))]) 
   (Sw, Cw) = zip(*SCw) 
   SCyw = [(np.zeros(Nf), (yws * np.ones(Nf)))] 
   SCyw.extend([trig_sum(t, (w * y), freq_factor=i, **kwargs) for i in range(1, (nterms + 1))]) 
   (Syw, Cyw) = zip(*SCyw) 
   order = ([('C', 0)] if fit_mean else []) 
   order.extend(sum([[('S', i), ('C', i)] for i in range(1, (nterms + 1))], [])) 
   funcs = dict(S=(lambda m, i: Syw[m][i]), C=(lambda m, i: Cyw[m][i]), SS=(lambda m, n, i: (0.5 * (Cw[abs((m - n))][i] - Cw[(m + n)][i]))), CC=(lambda m, n, i: (0.5 * (Cw[abs((m - n))][i] + Cw[(m + n)][i]))), SC=(lambda m, n, i: (0.5 * ((np.sign((m - n)) * Sw[abs((m - n))][i]) + Sw[(m + n)][i]))), CS=(lambda m, n, i: (0.5 * ((np.sign((n - m)) * Sw[abs((n - m))][i]) + Sw[(n + m)][i])))) 
   def compute_power(i): 
      XTX = np.array([[funcs[(A[0] + B[0])](A[1], B[1], i) for A in order] for B in order]) 
      XTy = np.array([funcs[A[0]](A[1], i) for A in order]) 
      return np.dot(XTy.T, np.linalg.solve(XTX, XTy)) 
   p = np.array([compute_power(i) for i in range(Nf)]) 
   if (normalization == 'psd'): 
      p *= 0.5 
   elif (normalization == 'standard'): 
      p /= chi2_ref 
   elif (normalization == 'log'): 
      p = (- np.log((1 - (p / chi2_ref)))) 
   elif (normalization == 'model'): 
      p /= (chi2_ref - p) 
   else: 
      raise ValueError(""normalization='{0}'   not   recognized"".format(normalization)) 
   return p"," 'Computes Lomb-Scargle power spectrum using fast chi2. 
 Parameters 
 t : 1D array of time 
 y : 1D array of y 
 dy : 1D array of time step 
 f0 : frequency of the first harmonic 
 df : frequency step 
 Nf : number of frequencies 
 normalization : \'psd\' (power spectrum), \'standard\' (Lomb-Scargle), \'log\' (logarithmic), \'model\' (model-fitting) 
 fit_mean : boolean, if True, bias will be fitted 
 center_data : boolean, if True, data will be centered 
 nterms : number of terms in the expansion 
 use_fft : boolean, if True, use FFT 
 trig_sum_kwds : dictionary, keyword arguments for trig_sum 
 Returns 
 p : 1D array of power spectrum 
 Examples 
 >>> from astropy.time import Time 
 >>> from astropy.time.utils import TimeDelta 
 >>> from astropy.time import TimeDelta 
 >>> t = Time(245","'Lomb-Scargle Periodogram 
 This implements a fast chi-squared periodogram using the algorithm 
 outlined in [4]_. The result is identical to the standard Lomb-Scargle 
 periodogram. The advantage of this algorithm is the 
 ability to compute multiterm periodograms relatively quickly. 
 Parameters 
 t, y, dy : array_like  (NOT astropy.Quantities) 
 times, values, and errors of the data points. These should be 
 broadcastable to the same shape. 
 f0, df, Nf : (float, float, int) 
 parameters describing the frequency grid, f = f0 + df * arange(Nf). 
 normalization : string (optional, default=\'standard\') 
 Normalization to use for the periodogram. 
 Options are \'standard\', \'model\', \'log\', or \'psd\'. 
 fit_mean : bool (optional, default=True) 
 if True, include a constant offset as part of the model at each 
 frequency. This can lead to more accurate results, especially in the 
 case of incomplete phase coverage. 
 center_data : bool (optional, default=True) 
 if True, pre-center the data by subtracting the weighted mean 
 of the input data. This is especially important if ``fit_mean = False`` 
 nterms : int (optional, default=1) 
 Number of Fourier terms in the fit 
 Returns 
 power : array_like 
 Lomb-Scargle power associated with each frequency. 
 Units of the result depend on the normalization. 
 References 
 .. [1] M. Zechmeister and M. Kurster, A&A 496, 577-584 (2009) 
 .. [2] W. Press et al, Numerical Recipies in C (2002) 
 .. [3] Scargle, J.D. ApJ 263:835-853 (1982) 
 .. [4] Palmer, J. ApJ 695:496-502 (2009)'"
"def test_BoundaryNorm(): 
    boundaries = [0, 1.1, 2.2] 
   vals = [(-1), 0, 1, 2, 2.2, 4] 
   expected = [(-1), 0, 0, 1, 2, 2] 
   ncolors = (len(boundaries) - 1) 
   bn = mcolors.BoundaryNorm(boundaries, ncolors) 
   assert_array_equal(bn(vals), expected) 
   expected = [(-1), 0, 0, 2, 3, 3] 
   ncolors = len(boundaries) 
   bn = mcolors.BoundaryNorm(boundaries, ncolors) 
   assert_array_equal(bn(vals), expected) 
   boundaries = [0, 1, 2, 3] 
   vals = [(-1), 0.1, 1.1, 2.2, 4] 
   ncolors = 5 
   expected = [(-1), 0, 2, 4, 5] 
   bn = mcolors.BoundaryNorm(boundaries, ncolors) 
   assert_array_equal(bn(vals), expected) 
   boundaries = [0, 1, 2] 
   vals = [(-1), 0.1, 1.1, 2.2] 
   bn = mcolors.BoundaryNorm(boundaries, 2) 
   expected = [(-1), 0, 1, 2] 
   for (v, ex) in zip(vals, expected): 
      ret = bn(v) 
      assert isinstance(ret, six.integer_types) 
      assert_array_equal(ret, ex) 
      assert_array_equal(bn([v]), ex) 
   bn = mcolors.BoundaryNorm(boundaries, 3) 
   expected = [(-1), 0, 2, 3] 
   for (v, ex) in zip(vals, expected): 
      ret = bn(v) 
      assert isinstance(ret, six.integer_types) 
      assert_array_equal(ret, ex) 
      assert_array_equal(bn([v]), ex) 
   bn = mcolors.BoundaryNorm(boundaries, 3, clip=True) 
   expected = [0, 0, 2, 2] 
   for (v, ex) in zip(vals, expected): 
      ret = bn(v) 
      assert isinstance(ret, six.integer_types) 
      assert_array_equal(ret, ex) 
      assert_array_equal(bn([v]), ex) 
   boundaries = [0, 1.1, 2.2] 
   vals = np.ma.masked_invalid([(-1.0), np.NaN, 0, 1.4, 9]) 
   ncolors = (len(boundaries) - 1) 
   bn = mcolors.BoundaryNorm(boundaries, ncolors) 
   expected = np.ma.masked_array([(-1), (-99), 0, 1, 2], mask=[0, 1, 0, 0, 0]) 
   assert_array_equal(bn(vals), expected) 
   bn = mcolors.BoundaryNorm(boundaries, len(boundaries)) 
   expected = np.ma.masked_array([(-1), (-99), 0, 2, 3], mask=[0, 1, 0, 0, 0]) 
   assert_array_equal(bn(vals), expected) 
   vals = np.ma.masked_invalid([np.Inf, np.NaN]) 
   assert np.all(bn(vals).mask) 
   vals = np.ma.masked_invalid([np.Inf]) 
   assert np.all(bn(vals).mask)", 'Test BoundaryNorm.',"'Github issue #1258: interpolation was failing with numpy 
 1.7 pre-release.'"
"@require_context 
 def instance_create(context, values): 
    values = values.copy() 
   values['metadata'] = _metadata_refs(values.get('metadata'), models.InstanceMetadata) 
   values['system_metadata'] = _metadata_refs(values.get('system_metadata'), models.InstanceSystemMetadata) 
   instance_ref = models.Instance() 
   if (not values.get('uuid')): 
      values['uuid'] = str(uuid.uuid4()) 
   instance_ref['info_cache'] = models.InstanceInfoCache() 
   info_cache = values.pop('info_cache', None) 
   if (info_cache is not None): 
      instance_ref['info_cache'].update(info_cache) 
   security_groups = values.pop('security_groups', []) 
   instance_ref.update(values) 
   def _get_sec_group_models(session, security_groups): 
      models = [] 
      (_existed, default_group) = security_group_ensure_default(context, session=session) 
      if ('default' in security_groups): 
         models.append(default_group) 
         security_groups = [x for x in security_groups if (x != 'default')] 
      if security_groups: 
         models.extend(_security_group_get_by_names(context, session, context.project_id, security_groups)) 
      return models 
   session = get_session() 
   with session.begin(): 
      if ('hostname' in values): 
         _validate_unique_server_name(context, session, values['hostname']) 
      instance_ref.security_groups = _get_sec_group_models(session, security_groups) 
      instance_ref.save(session=session) 
   ec2_instance_create(context, instance_ref['uuid']) 
   return instance_ref"," 'Create a new instance. 
 :param values: The dictionary of values to use to create the instance. 
 :param session: A db session to use for the create. 
 :returns: The instance created. 
 :raises: :exc:`NotFound` if the instance could not be found.'","'Create a new Instance record in the database. 
 context - request context object 
 values - dict containing column values.'"
"def validate_ok_for_update(update): 
    validate_is_mapping('update', update) 
   if (not update): 
      raise ValueError('update   only   works   with   $   operators') 
   first = next(iter(update)) 
   if (not first.startswith('$')): 
      raise ValueError('update   only   works   with   $   operators')", 'Validate that the update is a valid mapping.','Validate an update document.'
"def find_prepositions(chunked): 
    for ch in chunked: 
      ch.append(u'O') 
   for (i, chunk) in enumerate(chunked): 
      if (chunk[2].endswith(u'PP') and (chunk[(-1)] == u'O')): 
         if ((i < (len(chunked) - 1)) and (chunked[(i + 1)][2].endswith((u'NP', u'PP')) or (chunked[(i + 1)][1] in (u'VBG', u'VBN')))): 
            chunk[(-1)] = u'B-PNP' 
            pp = True 
            for ch in chunked[(i + 1):]: 
               if (not (ch[2].endswith((u'NP', u'PP')) or (ch[1] in (u'VBG', u'VBN')))): 
                  break 
               if (ch[2].endswith(u'PP') and pp): 
                  ch[(-1)] = u'I-PNP' 
               if (not ch[2].endswith(u'PP')): 
                  ch[(-1)] = u'I-PNP' 
                  pp = False 
   return chunked"," 'Find prepositions in a chunked list of sentences. 
 :param chunked: List of sentences with punctuation 
 :return: List of sentences with punctuation and prepositions 
 :rtype: list'","'The input is a list of [token, tag, chunk]-items. 
 The output is a list of [token, tag, chunk, preposition]-items. 
 PP-chunks followed by NP-chunks make up a PNP-chunk.'"
"def eval(expression, _dict={}, **kw): 
    args = ops.copy() 
   args.update(_dict) 
   args.update(kw) 
   for (k, v) in list(args.items()): 
      if hasattr(v, 'im'): 
         args[k] = _Operand(v) 
   out = builtins.eval(expression, args) 
   try: 
      return out.im 
   except AttributeError: 
      return out"," 'Evaluate the given expression with the given arguments. 
 Examples 
 >>> from sympy.core.expr import eval 
 >>> from sympy.core.function import _eval 
 >>> from sympy.core.function import _eval 
 >>> from sympy.core.function import _eval 
 >>> x = Symbol(""x"") 
 >>> f = Function(""f"") 
 >>> g = Function(""g"") 
 >>> a = Symbol(""a"") 
 >>> b = Symbol(""b"") 
 >>> c = Symbol(""c"") 
 >>> d = Symbol(""d"") 
 >>> e = Symbol(""e"") 
 >>> f(x) 
 f(x) 
 >>> f(x, x) 
 f(x, x) 
 >>> f(x, x, x) 
 f(x, x, x) 
 >>> _eval(""f(x, x, x)"") 
 f(x, x, x) 
 >>> _eval(""f(x, x, x)"", x=a) 
 f(a, a, a) 
 >>> _eval(""f(x, x, x)"",","'Evaluates an image expression. 
 :param expression: A string containing a Python-style expression. 
 :param options: Values to add to the evaluation context.  You 
 can either use a dictionary, or one or more keyword 
 arguments. 
 :return: The evaluated expression. This is usually an image object, but can 
 also be an integer, a floating point value, or a pixel tuple, 
 depending on the expression.'"
"def evaluate(op, op_str, a, b, raise_on_error=False, use_numexpr=True, **eval_kwargs): 
    use_numexpr = (use_numexpr and _bool_arith_check(op_str, a, b)) 
   if use_numexpr: 
      return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error, **eval_kwargs) 
   return _evaluate_standard(op, op_str, a, b, raise_on_error=raise_on_error)"," 'Evaluate an operation with a and b. 
 Args: 
 op: a binary operation (e.g. `+`) 
 op_str: the string representation of the operation 
 a: the first argument 
 b: the second argument 
 raise_on_error: whether to raise an exception if an error occurs 
 use_numexpr: whether to use numexpr to evaluate the expression 
 eval_kwargs: keyword arguments to be passed to eval() 
 Returns: 
 The result of the operation'","'evaluate and return the expression of the op on a and b 
 Parameters 
 op :    the actual operand 
 op_str: the string version of the op 
 a :     left operand 
 b :     right operand 
 raise_on_error : pass the error to the higher level if indicated 
 (default is False), otherwise evaluate the op with and 
 return the results 
 use_numexpr : whether to try to use numexpr (default True)'"
"def gzip_file(source_path, archive_path): 
    with gzip_open(archive_path, 'wb') as archive: 
      if os.path.isfile(source_path): 
         with open(source_path, 'rb') as source: 
            copyfileobj(source, archive)"," 'Compress a file using gzip. 
 :param source_path: The path to the source file. 
 :param archive_path: The path to the archive file. 
 :type source_path: str 
 :type archive_path: str'","'Create a gzip compressed archive of ``source_path`` at ``archive_path``. 
 An empty archive file will be created if the source file does not exist. 
 This gives the diagnostic archive a consistent set of files which can 
 easily be tested.'"
"def _get_mask(X, value_to_mask): 
    if ((value_to_mask == 'NaN') or np.isnan(value_to_mask)): 
      return np.isnan(X) 
   else: 
      return (X == value_to_mask)", 'Returns a boolean mask of the values in X that equal value_to_mask.','Compute the boolean mask X == missing_values.'
"def benchmark_relu(): 
    x = T.ftensor4('inputs') 
   ops = [relu_(x).sum(), relu(x).sum(), relu__(x).sum(), T.grad(relu_(x).sum(), x), T.grad(relu(x).sum(), x), T.grad(relu__(x).sum(), x)] 
   names = ['fprop_old', 'fprop_alter', 'fprop_alter2', 'grad_old', 'grad_alter', 'grad_alter2'] 
   value = numpy.random.uniform(size=(512, 32, 32, 100)).astype(floatX) 
   times = [] 
   for (op, name) in zip(ops, names): 
      f = theano.function(inputs=[x], outputs=op, name=name) 
      n_loops = 10 
      t0 = time.time() 
      for i in range(n_loops): 
         f(value) 
      t1 = time.time() 
      benchmark = (t1 - t0) 
      times.append(benchmark) 
      print(name) 
      theano.printing.debugprint(f, print_type=True) 
   print(names) 
   print(times)", 'Benchmark relu function',"'Benchmark the speed of different relu implementations. 
 Both fprop and grad are tested.'"
"def load_parser_result_store(package_dirpath, open_for_write=False): 
    open_flag = ((open_for_write and 'c') or 'r') 
   sto_filepath = path.join(package_dirpath, PARSER_RESULT_STORE) 
   return shelve_open(sto_filepath, flag=open_flag)"," 'Load the parser result store. 
 :param package_dirpath: 
 :param open_for_write: 
 :return: 
 :rtype: 
 :class: 
 :type: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param: 
 :param","'Load parser result store from specified scenario package. 
 Args: 
 package_dirpath: str; Path to scenario package directory. 
 open_for_write: bool; Open store for writing. 
 Returns: 
 shelve.DbfilenameShelf; Looks and acts like a dict'"
"def colname(colx): 
    alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' 
   if (colx <= 25): 
      return alphabet[colx] 
   else: 
      (xdiv26, xmod26) = divmod(colx, 26) 
      return (alphabet[(xdiv26 - 1)] + alphabet[xmod26])", 'Returns a name for a column number',"'Utility function: ``7`` => ``\'H\'``, ``27`` => ``\'AB\'``'"
"def validate_bool_maybe_none(b): 
    if isinstance(b, six.string_types): 
      b = b.lower() 
   if ((b is None) or (b == u'none')): 
      return None 
   if (b in (u't', u'y', u'yes', u'on', u'true', u'1', 1, True)): 
      return True 
   elif (b in (u'f', u'n', u'no', u'off', u'false', u'0', 0, False)): 
      return False 
   else: 
      raise ValueError((u'Could   not   convert   ""%s""   to   boolean' % b))"," 'Validate a boolean, returning None if the value is None or ""none"". 
 If the value is not None and not a boolean, raise ValueError.'",'Convert b to a boolean or raise'
"def seek_wrapped_response(response): 
    if (not hasattr(response, 'seek')): 
      wrapper_class = get_seek_wrapper_class(response) 
      response = wrapper_class(response) 
   assert hasattr(response, 'get_data') 
   return response", 'Seek a wrapped response to the given offset.',"'Return a copy of response that supports seekable response interface. 
 Accepts responses from both mechanize and urllib2 handlers. 
 Copes with both oridinary response instances and HTTPError instances (which 
 can\'t be simply wrapped due to the requirement of preserving the exception 
 base class).'"
"def load_module(name, file, filename, details): 
    (suffix, mode, type_) = details 
   if (mode and ((not mode.startswith(('r', 'U'))) or ('+' in mode))): 
      raise ValueError('invalid   file   open   mode   {!r}'.format(mode)) 
   elif ((file is None) and (type_ in {PY_SOURCE, PY_COMPILED})): 
      msg = 'file   object   required   for   import   (type   code   {})'.format(type_) 
      raise ValueError(msg) 
   elif (type_ == PY_SOURCE): 
      return load_source(name, filename, file) 
   elif (type_ == PY_COMPILED): 
      return load_compiled(name, filename, file) 
   elif ((type_ == C_EXTENSION) and (load_dynamic is not None)): 
      if (file is None): 
         with open(filename, 'rb') as opened_file: 
            return load_dynamic(name, filename, opened_file) 
      else: 
         return load_dynamic(name, filename, file) 
   elif (type_ == PKG_DIRECTORY): 
      return load_package(name, filename) 
   elif (type_ == C_BUILTIN): 
      return init_builtin(name) 
   elif (type_ == PY_FROZEN): 
      return init_frozen(name) 
   else: 
      msg = ""Don't   know   how   to   import   {}   (type   code   {})"".format(name, type_) 
      raise ImportError(msg, name=name)"," 'Load a module. 
 :param name: the name of the module to load 
 :param file: a file-like object to read from, or None 
 :param filename: the path to the file to load, or None 
 :param details: a tuple of the type of the file to load and the mode in which 
 to open it. 
 :return: the module loaded or None'","'**DEPRECATED** 
 Load a module, given information returned by find_module(). 
 The module name must include the full package name, if any.'"
"def main(): 
    def _iterate(test_suite_or_case): 
      'Iterate   through   all   the   test   cases   in   `test_suite_or_case`.' 
      try: 
         suite = iter(test_suite_or_case) 
      except TypeError: 
         (yield test_suite_or_case) 
      else: 
         for test in suite: 
            for subtest in _iterate(test): 
               (yield subtest) 
   feconf.PLATFORM = 'gae' 
   for directory in DIRS_TO_ADD_TO_SYS_PATH: 
      if (not os.path.exists(os.path.dirname(directory))): 
         raise Exception(('Directory   %s   does   not   exist.' % directory)) 
      sys.path.insert(0, directory) 
   import dev_appserver 
   dev_appserver.fix_sys_path() 
   parsed_args = _PARSER.parse_args() 
   suites = create_test_suites(parsed_args.test_target) 
   results = [unittest.TextTestRunner(verbosity=2).run(suite) for suite in suites] 
   tests_run = 0 
   for result in results: 
      tests_run += result.testsRun 
      if (result.errors or result.failures): 
         raise Exception(('Test   suite   failed:   %s   tests   run,   %s   errors,   %s   failures.' % (result.testsRun, len(result.errors), len(result.failures)))) 
   if (tests_run == 0): 
      raise Exception('No   tests   were   run.')", 'Runs all tests in the specified test target.','Runs the tests.'
"@ensure_csrf_cookie 
 @cache_control(no_cache=True, no_store=True, must_revalidate=True) 
 @coach_dashboard 
 def ccx_invite(request, course, ccx=None): 
    if (not ccx): 
      raise Http404 
   action = request.POST.get('enrollment-button') 
   identifiers_raw = request.POST.get('student-ids') 
   identifiers = _split_input_list(identifiers_raw) 
   email_students = ('email-students' in request.POST) 
   course_key = CCXLocator.from_course_locator(course.id, unicode(ccx.id)) 
   email_params = get_email_params(course, auto_enroll=True, course_key=course_key, display_name=ccx.display_name) 
   ccx_students_enrolling_center(action, identifiers, email_students, course_key, email_params, ccx.coach) 
   url = reverse('ccx_coach_dashboard', kwargs={'course_id': course_key}) 
   return redirect(url)"," 'Invite students to enroll in a CCX. 
 :param request: The current request. 
 :param course: The course to which the CCX belongs. 
 :param ccx: The CCX to which students are being invited. 
 :return: A redirect to the coach dashboard. 
 :raises Http404: If the CCX is not in the current course.'",'Invite users to new ccx'
"def socktype_to_enum(num): 
    if (enum is None): 
      return num 
   else: 
      try: 
         return socket.AddressType(num) 
      except (ValueError, AttributeError): 
         return num", 'Convert a socket address type number to a socket.AddressType.',"'Convert a numeric socket type value to an IntEnum member. 
 If it\'s not a known member, return the numeric value itself.'"
"def is_asn1_token(token): 
    return (token[:3] == PKI_ASN1_PREFIX)"," 'Check if the token is a ASN.1 token. 
 :param token: the token to check. 
 :type token: str 
 :return: True if the token is a ASN.1 token, False otherwise.'","'Determine if a token appears to be PKI-based. 
 thx to ayoung for sorting this out. 
 base64 decoded hex representation of MII is 3082:: 
 In [3]: binascii.hexlify(base64.b64decode(\'MII=\')) 
 Out[3]: \'3082\' 
 re: http://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf 
 pg4:  For tags from 0 to 30 the first octet is the identfier 
 pg10: Hex 30 means sequence, followed by the length of that sequence. 
 pg5:  Second octet is the length octet 
 first bit indicates short or long form, next 7 bits encode the 
 number of subsequent octets that make up the content length octets 
 as an unsigned binary int 
 82 = 10000010 (first bit indicates long form) 
 0000010 = 2 octets of content length 
 so read the next 2 octets to get the length of the content. 
 In the case of a very large content length there could be a requirement to 
 have more than 2 octets to designate the content length, therefore 
 requiring us to check for MIM, MIQ, etc. 
 In [4]: base64.b64encode(binascii.a2b_hex(\'3083\')) 
 Out[4]: \'MIM=\' 
 In [5]: base64.b64encode(binascii.a2b_hex(\'3084\')) 
 Out[5]: \'MIQ=\' 
 Checking for MI would become invalid at 16 octets of content length 
 10010000 = 90 
 In [6]: base64.b64encode(binascii.a2b_hex(\'3090\')) 
 Out[6]: \'MJA=\' 
 Checking for just M is insufficient 
 But we will only check for MII: 
 Max length of the content using 2 octets is 3FFF or 16383. 
 It\'s not practical to support a token of this length or greater in http 
 therefore, we will check for MII only and ignore the case of larger tokens'"
"def readsav(file_name, idict=None, python_dict=False, uncompressed_file_name=None, verbose=False): 
    records = [] 
   if (python_dict or idict): 
      variables = {} 
   else: 
      variables = AttrDict() 
   f = open(file_name, 'rb') 
   signature = _read_bytes(f, 2) 
   if (signature != 'SR'): 
      raise Exception(('Invalid   SIGNATURE:   %s' % signature)) 
   recfmt = _read_bytes(f, 2) 
   if (recfmt == '\x00\x04'): 
      pass 
   elif (recfmt == '\x00\x06'): 
      if verbose: 
         print('IDL   Save   file   is   compressed') 
      if uncompressed_file_name: 
         fout = open(uncompressed_file_name, 'w+b') 
      else: 
         fout = tempfile.NamedTemporaryFile(suffix='.sav') 
      if verbose: 
         print(('   ->   expanding   to   %s' % fout.name)) 
      fout.write('SR\x00\x04') 
      while True: 
         rectype = _read_long(f) 
         fout.write(struct.pack('>l', int(rectype))) 
         nextrec = _read_uint32(f) 
         nextrec += (_read_uint32(f) * (2 ** 32)) 
         unknown = f.read(4) 
         if (RECTYPE_DICT[rectype] == 'END_MARKER'): 
            fout.write(struct.pack('>I', (int(nextrec) % (2 ** 32)))) 
            fout.write(struct.pack('>I', int(((nextrec - (nextrec % (2 ** 32))) / (2 ** 32))))) 
            fout.write(unknown) 
            break 
         pos = f.tell() 
         rec_string = zlib.decompress(f.read((nextrec - pos))) 
         nextrec = ((fout.tell() + len(rec_string)) + 12) 
         fout.write(struct.pack('>I', int((nextrec % (2 ** 32))))) 
         fout.write(struct.pack('>I', int(((nextrec - (nextrec % (2 ** 32))) / (2 ** 32))))) 
         fout.write(unknown) 
         fout.write(rec_string) 
      f.close() 
      f = fout 
      f.seek(4) 
   else: 
      raise Exception(('Invalid   RECFMT:   %s' % recfmt)) 
   while True: 
      r = _read_record(f) 
      records.append(r) 
      if ('end' in r): 
         if r['end']: 
            break 
   f.close() 
   heap = {} 
   for r in records: 
      if (r['rectype'] == 'HEAP_DATA'): 
         heap[r['heap_index']] = r['data'] 
   for r in records: 
      if (r['rectype'] == 'VARIABLE'): 
         (replace, new) = _replace_heap(r['data'], heap) 
         if replace: 
            r['data'] = new 
         variables[r['varname'].lower()] = r['data'] 
   if verbose: 
      for record in records: 
         if (record['rectype'] == 'TIMESTAMP'): 
            print(('-' * 50)) 
            print(('Date:   %s' % record['date'])) 
            print(('User:   %s' % record['user'])) 
            print(('Host:   %s' % record['host'])) 
            break 
      for record in records: 
         if (record['rectype'] == 'VERSION'): 
            print(('-' * 50)) 
            print(('Format:   %s' % record['format'])) 
            print(('Architecture:   %s' % record['arch'])) 
            print(('Operating   System:   %s' % record['os'])) 
            print(('IDL   Version:   %s' % record['release'])) 
            break 
      for record in records: 
         if (record['rectype'] == 'IDENTIFICATON'): 
            print(('-' * 50)) 
            print(('Author:   %s' % record['author'])) 
            print(('Title:   %s' % record['title'])) 
            print(('ID   Code:   %s' % record['idcode'])) 
            break 
      for record in records: 
         if (record['rectype'] == 'DESCRIPTION'): 
            print(('-' * 50)) 
            print(('Description:   %s' % record['description'])) 
            break 
      print(('-' * 50)) 
      print(('Successfully   read   %i   records   of   which:' % len(records))) 
      rectypes = [r['rectype'] for r in records] 
      for rt in set(rectypes): 
         if (rt != 'END_MARKER'): 
            print(('   -   %i   are   of   type   %s' % (rectypes.count(rt), rt))) 
      print(('-' * 50)) 
      if ('VARIABLE' in rectypes): 
         print('Available   variables:') 
         for var in variables: 
            print(('   -   %s   [%s]' % (var, type(variables[var])))) 
         print(('-' * 50)) 
   if idict: 
      for var in variables: 
         idict[var] = variables[var] 
      return idict 
   else: 
      return variables"," 'Reads an IDL save file and returns a dictionary with the variables. 
 If ``idict`` is given, the variables are returned as a dictionary. 
 If ``python_dict`` is given, the variables are returned as a Python 
 dictionary. 
 ``uncompressed_file_name`` is the name of the uncompressed file. 
 ``verbose`` is a boolean indicating whether to print out the version 
 information and the date. 
 ``idict`` is a dictionary where the key is the name of the variable and 
 the value is the value of the variable. 
 ``python_dict`` is a dictionary where the key is the name of the variable 
 and the value is the value of the variable.'","'Read an IDL .sav file. 
 Parameters 
 file_name : str 
 Name of the IDL save file. 
 idict : dict, optional 
 Dictionary in which to insert .sav file variables. 
 python_dict : bool, optional 
 By default, the object return is not a Python dictionary, but a 
 case-insensitive dictionary with item, attribute, and call access 
 to variables. To get a standard Python dictionary, set this option 
 to True. 
 uncompressed_file_name : str, optional 
 This option only has an effect for .sav files written with the 
 /compress option. If a file name is specified, compressed .sav 
 files are uncompressed to this file. Otherwise, readsav will use 
 the `tempfile` module to determine a temporary filename 
 automatically, and will remove the temporary file upon successfully 
 reading it in. 
 verbose : bool, optional 
 Whether to print out information about the save file, including 
 the records read, and available variables. 
 Returns 
 idl_dict : AttrDict or dict 
 If `python_dict` is set to False (default), this function returns a 
 case-insensitive dictionary with item, attribute, and call access 
 to variables. If `python_dict` is set to True, this function 
 returns a Python dictionary with all variable names in lowercase. 
 If `idict` was specified, then variables are written to the 
 dictionary specified, and the updated dictionary is returned.'"
"def surround_quotes(string): 
    if (in_bash() and string): 
      return '""{0}""'.format(string) 
   return string", 'Surround a string with double quotes if it is not already surrounded by double quotes.',"'Bash has problems dealing with certain paths so we\'re surrounding all 
 path outputs with quotes.'"
"def add_log_redaction_filter_to_logger(engine, logger): 
    if engine.policies: 
      redaction_filter = RedactionFilter(engine) 
      for handler in logger.handlers: 
         handler.addFilter(redaction_filter)"," 'Add redaction filter to all handlers of the logger. 
 :param engine: 
 :param logger: 
 :type logger: 
 :type engine: 
 :return: 
 :rtype: 
 :raises: 
 :param engine: 
 :param logger: 
 :type logger: 
 :type engine: 
 :return: 
 :rtype: 
 :raises: 
 :param engine: 
 :param logger: 
 :type logger: 
 :type engine: 
 :return: 
 :rtype: 
 :raises: 
 :param engine: 
 :param logger: 
 :type logger: 
 :type engine: 
 :return: 
 :rtype: 
 :raises: 
 :param engine: 
 :param logger: 
 :type logger: 
 :type engine: 
 :return: 
 :rtype: 
 :raises: 
 :param engine: 
 :param logger: 
 :type logger: 
 :type engine: 
 :return: 
 :rtype: 
 :","'`add_redaction_filter` injects the redaction filter into all of the `logger` 
 handlers. This must be called after all of the handlers have been added to 
 `logger`, otherwise those handlers may expose unredacted strings.'"
"def instance_key(model, instance_or_pk): 
    return ('%s.%s:%d' % (model._meta.app_label, model._meta.model_name, getattr(instance_or_pk, 'pk', instance_or_pk)))", 'Return the instance key for an instance or its primary key.',"'Returns the cache key for this (model, instance) pair.'"
"def stream_encode_multipart(values, use_tempfile=True, threshold=(1024 * 500), boundary=None, charset='utf-8'): 
    if (boundary is None): 
      boundary = ('---------------WerkzeugFormPart_%s%s' % (time(), random())) 
   _closure = [BytesIO(), 0, False] 
   if use_tempfile: 
      def write_binary(string): 
         (stream, total_length, on_disk) = _closure 
         if on_disk: 
            stream.write(string) 
         else: 
            length = len(string) 
            if ((length + _closure[1]) <= threshold): 
               stream.write(string) 
            else: 
               new_stream = TemporaryFile('wb+') 
               new_stream.write(stream.getvalue()) 
               new_stream.write(string) 
               _closure[0] = new_stream 
               _closure[2] = True 
            _closure[1] = (total_length + length) 
   else: 
      write_binary = _closure[0].write 
   def write(string): 
      write_binary(string.encode(charset)) 
   if (not isinstance(values, MultiDict)): 
      values = MultiDict(values) 
   for (key, values) in iterlists(values): 
      for value in values: 
         write(('--%s\r\nContent-Disposition:   form-data;   name=""%s""' % (boundary, key))) 
         reader = getattr(value, 'read', None) 
         if (reader is not None): 
            filename = getattr(value, 'filename', getattr(value, 'name', None)) 
            content_type = getattr(value, 'content_type', None) 
            if (content_type is None): 
               content_type = ((filename and mimetypes.guess_type(filename)[0]) or 'application/octet-stream') 
            if (filename is not None): 
               write((';   filename=""%s""\r\n' % filename)) 
            else: 
               write('\r\n') 
            write(('Content-Type:   %s\r\n\r\n' % content_type)) 
            while 1: 
               chunk = reader(16384) 
               if (not chunk): 
                  break 
               write_binary(chunk) 
         else: 
            if (not isinstance(value, string_types)): 
               value = str(value) 
            value = to_bytes(value, charset) 
            write('\r\n\r\n') 
            write_binary(value) 
         write('\r\n') 
   write(('--%s--\r\n' % boundary)) 
   length = int(_closure[0].tell()) 
   _closure[0].seek(0) 
   return (_closure[0], length, boundary)"," 'Returns a tuple of (file object, length, boundary) 
 multipart boundary string is generated. 
 :param values: values to be encoded 
 :type values: MultiDict 
 :param use_tempfile: whether to use temporary files 
 :type use_tempfile: bool 
 :param threshold: maximum size of a single file 
 :type threshold: int 
 :param boundary: multipart boundary string 
 :type boundary: str 
 :param charset: charset to use for encoding 
 :type charset: str 
 :return: tuple of (file object, length, boundary)'","'Encode a dict of values (either strings or file descriptors or 
 :class:`FileStorage` objects.) into a multipart encoded string stored 
 in a file descriptor.'"
"def setup_cuda_fft_resample(n_jobs, W, new_len): 
    cuda_dict = dict(use_cuda=False, fft_plan=None, ifft_plan=None, x_fft=None, x=None, y_fft=None, y=None) 
   (n_fft_x, n_fft_y) = (len(W), new_len) 
   cuda_fft_len_x = int((((n_fft_x - (n_fft_x % 2)) // 2) + 1)) 
   cuda_fft_len_y = int((((n_fft_y - (n_fft_y % 2)) // 2) + 1)) 
   if (n_jobs == 'cuda'): 
      n_jobs = 1 
      init_cuda() 
      if _cuda_capable: 
         from pycuda import gpuarray 
         cudafft = _get_cudafft() 
         try: 
            W = gpuarray.to_gpu((W[:cuda_fft_len_x].astype('complex_') / n_fft_y)) 
            cuda_dict.update(use_cuda=True, fft_plan=cudafft.Plan(n_fft_x, np.float64, np.complex128), ifft_plan=cudafft.Plan(n_fft_y, np.complex128, np.float64), x_fft=gpuarray.zeros(max(cuda_fft_len_x, cuda_fft_len_y), np.complex128), x=gpuarray.empty(max(int(n_fft_x), int(n_fft_y)), np.float64)) 
            logger.info('Using   CUDA   for   FFT   resampling') 
         except Exception: 
            logger.info('CUDA   not   used,   could   not   instantiate   memory   (arrays   may   be   too   large),   falling   back   to   n_jobs=1') 
      else: 
         logger.info('CUDA   not   used,   CUDA   could   not   be   initialized,   falling   back   to   n_jobs=1') 
   return (n_jobs, cuda_dict, W)"," 'Setup CUDA FFT resampling 
 This function is used to setup CUDA FFT resampling. It takes in 
 the number of jobs to be used, the window, the new length, and 
 returns a tuple of the number of jobs, the dictionary of CUDA 
 variables to be used, and the window. 
 Parameters 
 n_jobs : int 
 Number of jobs to be used for the FFT resampling. 
 W : array 
 Window. 
 new_len : int 
 New length of the window. 
 Returns 
 n_jobs : int 
 Number of jobs to be used for the FFT resampling. 
 cuda_dict : dict 
 Dictionary of CUDA variables to be used for the FFT resampling. 
 W : array 
 Window. 
 Examples 
 >>> from scipy.signal import resample 
 >>> from scipy.signal import fft, fftn, resample 
 >>> from scipy.signal import window_rank, window_len 
 >>> from scipy.signal import hann, blackman 
","'Set up CUDA FFT resampling. 
 Parameters 
 n_jobs : int | str 
 If n_jobs == \'cuda\', the function will attempt to set up for CUDA 
 FFT resampling. 
 W : array 
 The filtering function to be used during resampling. 
 If n_jobs=\'cuda\', this function will be shortened (since CUDA 
 assumes FFTs of real signals are half the length of the signal) 
 and turned into a gpuarray. 
 new_len : int 
 The size of the array following resampling. 
 Returns 
 n_jobs : int 
 Sets n_jobs = 1 if n_jobs == \'cuda\' was passed in, otherwise 
 original n_jobs is passed. 
 cuda_dict : dict 
 Dictionary with the following CUDA-related variables: 
 use_cuda : bool 
 Whether CUDA should be used. 
 fft_plan : instance of FFTPlan 
 FFT plan to use in calculating the FFT. 
 ifft_plan : instance of FFTPlan 
 FFT plan to use in calculating the IFFT. 
 x_fft : instance of gpuarray 
 Empty allocated GPU space for storing the result of the 
 frequency-domain multiplication. 
 x : instance of gpuarray 
 Empty allocated GPU space for the data to resample. 
 W : array | instance of gpuarray 
 This will either be a gpuarray (if CUDA enabled) or np.ndarray. 
 If CUDA is enabled, W will be modified appropriately for use 
 with filter.fft_multiply(). 
 Notes 
 This function is designed to be used with fft_resample().'"
"def distorted_inputs(dataset, batch_size=None, num_preprocess_threads=None): 
    if (not batch_size): 
      batch_size = FLAGS.batch_size 
   with tf.device('/cpu:0'): 
      (images, labels) = batch_inputs(dataset, batch_size, train=True, num_preprocess_threads=num_preprocess_threads, num_readers=FLAGS.num_readers) 
   return (images, labels)"," 'Preprocesses images and labels from a dataset. 
 Args: 
 dataset: Dataset to be preprocessed. 
 batch_size: Batch size. 
 num_preprocess_threads: Number of threads used for preprocessing. 
 Returns: 
 images: A tensor of shape [batch_size, height, width, 3]. 
 labels: A tensor of shape [batch_size]. 
 Raises: 
 ValueError: If `batch_size` is not specified.'","'Generate batches of distorted versions of ImageNet images. 
 Use this function as the inputs for training a network. 
 Distorting images provides a useful technique for augmenting the data 
 set during training in order to make the network invariant to aspects 
 of the image that do not effect the label. 
 Args: 
 dataset: instance of Dataset class specifying the dataset. 
 batch_size: integer, number of examples in batch 
 num_preprocess_threads: integer, total number of preprocessing threads but 
 None defaults to FLAGS.num_preprocess_threads. 
 Returns: 
 images: Images. 4D tensor of size [batch_size, FLAGS.image_size, 
 FLAGS.image_size, 3]. 
 labels: 1-D integer Tensor of [batch_size].'"
"def use_gl(target='gl2'): 
    target = (target or 'gl2') 
   target = target.replace('+', 'plus') 
   (target, _, options) = target.partition('   ') 
   debug = (config['gl_debug'] or ('debug' in options)) 
   try: 
      mod = __import__(target, globals(), level=1) 
   except ImportError as err: 
      msg = ('Could   not   import   gl   target   ""%s"":\n%s' % (target, str(err))) 
      raise RuntimeError(msg) 
   global current_backend 
   current_backend = mod 
   _clear_namespace() 
   if ('plus' in target): 
      _copy_gl_functions(mod._pyopengl2, globals()) 
      _copy_gl_functions(mod, globals(), True) 
   elif debug: 
      _copy_gl_functions(_debug_proxy, globals()) 
   else: 
      _copy_gl_functions(mod, globals())"," 'Load the appropriate GL backend. 
 If `target` is specified, the backend will be loaded using that target. 
 If `target` is not specified, the backend will be loaded using the default 
 target. 
 :param target: The target to load. 
 :type target: str 
 :returns: None 
 :rtype: None'","'Let Vispy use the target OpenGL ES 2.0 implementation 
 Also see ``vispy.use()``. 
 Parameters 
 target : str 
 The target GL backend to use. 
 Available backends: 
 * gl2 - Use ES 2.0 subset of desktop (i.e. normal) OpenGL 
 * gl+ - Use the desktop ES 2.0 subset plus all non-deprecated GL 
 functions on your system (requires PyOpenGL) 
 * es2 - Use the ES2 library (Angle/DirectX on Windows) 
 * pyopengl2 - Use ES 2.0 subset of pyopengl (for fallback and testing) 
 * dummy - Prevent usage of gloo.gl (for when rendering occurs elsewhere) 
 You can use vispy\'s config option ""gl_debug"" to check for errors 
 on each API call. Or, one can specify it as the target, e.g. ""gl2 
 debug"". (Debug does not apply to \'gl+\', since PyOpenGL has its own 
 debug mechanism)'"
"def gen_arg_base_type(fn): 
    mod = fn.module 
   fnty = fn.type.pointee 
   consts = [lc.MetaDataString.get(mod, str(a)) for a in fnty.args] 
   name = lc.MetaDataString.get(mod, 'kernel_arg_base_type') 
   return lc.MetaData.get(mod, ([name] + consts))"," 'Generates a base type for kernel arguments. 
 Args: 
 fn: the kernel function. 
 Returns: 
 A list of strings, each representing a type for a kernel argument.'",'Generate kernel_arg_base_type metadata'
"def _make_request(token, method_name, method='get', params=None, files=None, base_url=API_URL): 
    request_url = base_url.format(token, method_name) 
   logger.debug('Request:   method={0}   url={1}   params={2}   files={3}'.format(method, request_url, params, files)) 
   read_timeout = READ_TIMEOUT 
   connect_timeout = CONNECT_TIMEOUT 
   if params: 
      if ('timeout' in params): 
         read_timeout = (params['timeout'] + 10) 
      if ('connect-timeout' in params): 
         connect_timeout = (params['connect-timeout'] + 10) 
   result = req_session.request(method, request_url, params=params, files=files, timeout=(connect_timeout, read_timeout)) 
   logger.debug(""The   server   returned:   '{0}'"".format(result.text.encode('utf8'))) 
   return _check_result(method_name, result)['result']"," 'Make a request to the API. 
 :param token: API token 
 :param method_name: Name of the method to call 
 :param method: HTTP method to use. Defaults to \'get\'. 
 :param params: Dictionary of parameters to pass to the API method 
 :param files: Dictionary of files to upload to the API method 
 :param base_url: Base URL of the API 
 :return: Dictionary of the result of the API call'","'Makes a request to the Telegram API. 
 :param token: The bot\'s API token. (Created with @BotFather) 
 :param method_name: Name of the API method to be called. (E.g. \'getUpdates\') 
 :param method: HTTP method to be used. Defaults to \'get\'. 
 :param params: Optional parameters. Should be a dictionary with key-value pairs. 
 :param files: Optional files. 
 :return: The result parsed to a JSON dictionary.'"
"def loads(source): 
    return UnspacedList(RawNginxParser(source).as_list())"," 'Loads a Nginx configuration file and returns a list of Nginx 
 directives.'","'Parses from a string. 
 :param str souce: The string to parse 
 :returns: The parsed tree 
 :rtype: list'"
"def grey_dilation(input, size=None, footprint=None, structure=None, output=None, mode='reflect', cval=0.0, origin=0): 
    if ((size is None) and (footprint is None) and (structure is None)): 
      raise ValueError('size,   footprint   or   structure   must   be   specified') 
   if (structure is not None): 
      structure = numpy.asarray(structure) 
      structure = structure[tuple(([slice(None, None, (-1))] * structure.ndim))] 
   if (footprint is not None): 
      footprint = numpy.asarray(footprint) 
      footprint = footprint[tuple(([slice(None, None, (-1))] * footprint.ndim))] 
   input = numpy.asarray(input) 
   origin = _ni_support._normalize_sequence(origin, input.ndim) 
   for ii in range(len(origin)): 
      origin[ii] = (- origin[ii]) 
      if (footprint is not None): 
         sz = footprint.shape[ii] 
      elif (structure is not None): 
         sz = structure.shape[ii] 
      elif numpy.isscalar(size): 
         sz = size 
      else: 
         sz = size[ii] 
      if (not (sz & 1)): 
         origin[ii] -= 1 
   return filters._min_or_max_filter(input, size, footprint, structure, output, mode, cval, origin, 0)"," 'Return the image dilation of the input image with the specified 
 footprint, structure, and/or size. 
 Parameters 
 input : array_like 
 Input image. 
 size : int, optional 
 Size of the image. 
 footprint : int, optional 
 Footprint of the image. 
 structure : int, optional 
 Structure of the image. 
 output : array_like, optional 
 Output image. 
 mode : {'reflect', 'wrap'}, optional 
 Mode of the output image. 
 cval : scalar, optional 
 Value to fill outside the image. 
 origin : int, optional 
 Origin of the image. 
 Returns 
 output : ndarray 
 Output image. 
 Notes 
 The dilation is defined as the minimum or maximum of the input image 
 with the specified footprint, structure, and/or size. 
 Examples 
 >>> from skimage import data 
 >>> from skimage import gray_dilation 
 >>> img = data.camera() 
 >>> img 
 array([[0.12389569, 0.26","'Calculate a greyscale dilation, using either a structuring element, 
 or a footprint corresponding to a flat structuring element. 
 Grayscale dilation is a mathematical morphology operation. For the 
 simple case of a full and flat structuring element, it can be viewed 
 as a maximum filter over a sliding window. 
 Parameters 
 input : array_like 
 Array over which the grayscale dilation is to be computed. 
 size : tuple of ints 
 Shape of a flat and full structuring element used for the grayscale 
 dilation. Optional if `footprint` or `structure` is provided. 
 footprint : array of ints, optional 
 Positions of non-infinite elements of a flat structuring element 
 used for the grayscale dilation. Non-zero values give the set of 
 neighbors of the center over which the maximum is chosen. 
 structure : array of ints, optional 
 Structuring element used for the grayscale dilation. `structure` 
 may be a non-flat structuring element. 
 output : array, optional 
 An array used for storing the ouput of the dilation may be provided. 
 mode : {\'reflect\',\'constant\',\'nearest\',\'mirror\', \'wrap\'}, optional 
 The `mode` parameter determines how the array borders are 
 handled, where `cval` is the value when mode is equal to 
 \'constant\'. Default is \'reflect\' 
 cval : scalar, optional 
 Value to fill past edges of input if `mode` is \'constant\'. Default 
 is 0.0. 
 origin : scalar, optional 
 The `origin` parameter controls the placement of the filter. 
 Default 0 
 Returns 
 grey_dilation : ndarray 
 Grayscale dilation of `input`. 
 See also 
 binary_dilation, grey_erosion, grey_closing, grey_opening 
 generate_binary_structure, ndimage.maximum_filter 
 Notes 
 The grayscale dilation of an image input by a structuring element s defined 
 over a domain E is given by: 
 (input+s)(x) = max {input(y) + s(x-y), for y in E} 
 In particular, for structuring elements defined as 
 s(y) = 0 for y in E, the grayscale dilation computes the maximum of the 
 input image inside a sliding window defined by E. 
 Grayscale dilation [1]_ is a *mathematical morphology* operation [2]_. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Dilation_%28morphology%29 
 .. [2] http://en.wikipedia.org/wiki/Mathematical_morphology 
 Examples 
 >>> from scipy import ndimage 
 >>> a = np.zeros((7,7), dtype=int) 
 >>> a[2:5, 2:5] = 1 
 >>> a[4,4] = 2; a[2,3] = 3 
 >>> a 
 array([[0, 0, 0, 0, 0, 0, 0], 
 [0, 0, 0, 0, 0, 0, 0], 
 [0, 0, 1, 3, 1, 0, 0], 
 [0, 0, 1, 1, 1, 0, 0], 
 [0, 0, 1, 1, 2, 0, 0], 
 [0, 0, 0, 0, 0, 0, 0], 
 [0, 0, 0, 0, 0, 0, 0]]) 
 >>> ndimage.grey_dilation(a, size=(3,3)) 
 array([[0, 0, 0, 0, 0, 0, 0], 
 [0, 1, 3, 3, 3, 1, 0], 
 [0, 1, 3, 3, 3, 1, 0], 
 [0, 1, 3, 3, 3, 2, 0], 
 [0, 1, 1, 2, 2, 2, 0], 
 [0, 1, 1, 2, 2, 2, 0], 
 [0, 0, 0, 0, 0, 0, 0]]) 
 >>> ndimage.grey_dilation(a, footprint=np.ones((3,3))) 
 array([[0, 0, 0, 0, 0, 0, 0], 
 [0, 1, 3, 3, 3, 1, 0], 
 [0, 1, 3, 3, 3, 1, 0], 
 [0, 1, 3, 3, 3, 2, 0], 
 [0, 1, 1, 2, 2, 2, 0], 
 [0, 1, 1, 2, 2, 2, 0], 
 [0, 0, 0, 0, 0, 0, 0]]) 
 >>> s = ndimage.generate_binary_structure(2,1) 
 >>> s 
 array([[False,  True, False], 
 [ True,  True,  True], 
 [False,  True, False]], dtype=bool) 
 >>> ndimage.grey_dilation(a, footprint=s) 
 array([[0, 0, 0, 0, 0, 0, 0], 
 [0, 0, 1, 3, 1, 0, 0], 
 [0, 1, 3, 3, 3, 1, 0], 
 [0, 1, 1, 3, 2, 1, 0], 
 [0, 1, 1, 2, 2, 2, 0], 
 [0, 0, 1, 1, 2, 0, 0], 
 [0, 0, 0, 0, 0, 0, 0]]) 
 >>> ndimage.grey_dilation(a, size=(3,3), structure=np.ones((3,3))) 
 array([[1, 1, 1, 1, 1, 1, 1], 
 [1, 2, 4, 4, 4, 2, 1], 
 [1, 2, 4, 4, 4, 2, 1], 
 [1, 2, 4, 4, 4, 3, 1], 
 [1, 2, 2, 3, 3, 3, 1], 
 [1, 2, 2, 3, 3, 3, 1], 
 [1, 1, 1, 1, 1, 1, 1]])'"
"def _buildFakeFKTable(cls, fakeTableName): 
    countCols = 0 
   attrs = {} 
   for col in cls._imdbpySchema.cols: 
      countCols += 1 
      if (col.name == 'id'): 
         continue 
      if (not col.foreignKey): 
         attrs[col.name] = MAP_COLS[col.kind](**col.params) 
         continue 
      thisColName = col.name 
      if thisColName.endswith('ID'): 
         thisColName = thisColName[:(-2)] 
      fks = col.foreignKey.split('.', 1) 
      foreignTableName = fks[0] 
      if (len(fks) == 2): 
         foreignColName = fks[1] 
      else: 
         foreignColName = 'id' 
      fk = ForeignKey(foreignTableName, name=thisColName, default=None) 
      attrs[thisColName] = fk 
   newcls = type(fakeTableName, (SQLObject,), attrs) 
   return newcls"," 'Returns a new class that has the same schema as the original class 
 but with all FKs removed. 
 :param cls: the class to build a fake table for 
 :param fakeTableName: the name of the fake table to return 
 :returns: a new class with the same schema as cls but with all FKs removed'","'Return a ""fake"" table, with foreign keys where needed.'"
"def get_locked_port_and_binding(context, port_id): 
    try: 
      port = context.session.query(models_v2.Port).enable_eagerloads(False).filter_by(id=port_id).with_lockmode('update').one() 
      binding = context.session.query(models.PortBinding).enable_eagerloads(False).filter_by(port_id=port_id).with_lockmode('update').one() 
      return (port, binding) 
   except exc.NoResultFound: 
      return (None, None)"," 'Get the locked port and binding for the specified port_id. 
 :param context: nova.context.RequestContext 
 :param port_id: The id of the port 
 :returns: A tuple of (port, binding) 
 :raises: NoResultFound if the port or binding is not found.'",'Get port and port binding records for update within transaction.'
"def missing_whitespace_around_operator(logical_line, tokens): 
    parens = 0 
   need_space = False 
   prev_type = tokenize.OP 
   prev_text = prev_end = None 
   for (token_type, text, start, end, line) in tokens: 
      if (token_type in (tokenize.NL, tokenize.NEWLINE, tokenize.ERRORTOKEN)): 
         continue 
      if (text in ('(', 'lambda')): 
         parens += 1 
      elif (text == ')'): 
         parens -= 1 
      if need_space: 
         if (start != prev_end): 
            if ((need_space is not True) and (not need_space[1])): 
               (yield (need_space[0], 'E225   missing   whitespace   around   operator')) 
            need_space = False 
         elif ((text == '>') and (prev_text in ('<', '-'))): 
            pass 
         else: 
            if ((need_space is True) or need_space[1]): 
               (yield (prev_end, 'E225   missing   whitespace   around   operator')) 
            else: 
               (code, optype) = ('E226', 'arithmetic') 
               if (prev_text == '%'): 
                  (code, optype) = ('E228', 'modulo') 
               elif (prev_text not in ARITHMETIC_OP): 
                  (code, optype) = ('E227', 'bitwise   or   shift') 
               (yield (need_space[0], ('%s   missing   whitespace   around   %s   operator' % (code, optype)))) 
            need_space = False 
      elif ((token_type == tokenize.OP) and (prev_end is not None)): 
         if ((text == '=') and parens): 
            pass 
         elif (text in WS_NEEDED_OPERATORS): 
            need_space = True 
         elif (text in UNARY_OPERATORS): 
            if (prev_type == tokenize.OP): 
               binary_usage = (prev_text in '}])') 
            elif (prev_type == tokenize.NAME): 
               binary_usage = (prev_text not in KEYWORDS) 
            else: 
               binary_usage = (prev_type not in SKIP_TOKENS) 
            if binary_usage: 
               need_space = None 
         elif (text in WS_OPTIONAL_OPERATORS): 
            need_space = None 
         if (need_space is None): 
            need_space = (prev_end, (start != prev_end)) 
         elif (need_space and (start == prev_end)): 
            (yield (prev_end, 'E225   missing   whitespace   around   operator')) 
            need_space = False 
      prev_type = token_type 
      prev_text = text 
      prev_end = end"," 'Checks for missing whitespace around operators. 
 Arithmetic operators (and modulo, bitwise and shift) need whitespace 
 around them, and operators like ``==`` and ``<`` need whitespace 
 around them if they are followed by parentheses. 
 This is a fairly strict check, and will catch most cases where 
 whitespace is missing around operators. 
 Note that this check does not check for missing whitespace around 
 operators that are not part of an arithmetic expression. 
 Arithmetic operators are: 
 - ``+``, ``-``, ``*``, ``/`` 
 - ``%`` (modulo) 
 - ``<<`` (left shift), ``>>`` (right shift), ``>>>`` (signed right shift) 
 - ``&`` (bitwise AND), ``^`` (bitwise XOR), ``|`` (bitwise OR) 
 - ``~`` (bitwise NOT) 
 Modulo operators are: 
 - ``%`` 
 Bitwise operators are: 
 - ``&`` 
 - ``^`` 
 - ``|`` 
 - ``~`` 
 Shift operators are: 
 - ``","'- Always surround these binary operators with a single space on 
 either side: assignment (=), augmented assignment (+=, -= etc.), 
 comparisons (==, <, >, !=, <>, <=, >=, in, not in, is, is not), 
 Booleans (and, or, not). 
 - Use spaces around arithmetic operators. 
 Okay: i = i + 1 
 Okay: submitted += 1 
 Okay: x = x * 2 - 1 
 Okay: hypot2 = x * x + y * y 
 Okay: c = (a + b) * (a - b) 
 Okay: foo(bar, key=\'word\', *args, **kwargs) 
 Okay: alpha[:-i] 
 E225: i=i+1 
 E225: submitted +=1 
 E225: x = x /2 - 1 
 E225: z = x **y 
 E226: c = (a+b) * (a-b) 
 E226: hypot2 = x*x + y*y 
 E227: c = a|b 
 E228: msg = fmt%(errno, errmsg)'"
"def _fake_check_ldev_status(*args, **kwargs): 
    return None", 'fake check_ldev_status function','Assume LDEV status has changed as desired.'
"def test_diagonal_gaussian_sample_from_epsilon(): 
    mlp = MLP(layers=[Linear(layer_name='h', dim=5, irange=0.01, max_col_norm=0.01)]) 
   conditional = DiagonalGaussian(mlp=mlp, name='conditional') 
   vae = DummyVAE() 
   conditional.set_vae(vae) 
   input_space = VectorSpace(dim=5) 
   conditional.initialize_parameters(input_space=input_space, ndim=5) 
   conditional.sample_from_epsilon((2, 10, 5))", 'Test the diagonal Gaussian sampling from epsilon.','DiagonalGaussian.sample_from_epsilon doesn\'t crash'
"def edns_from_text(text): 
    return _from_text(text, _edns_by_text)"," 'Return a parsed EDNS message from text. 
 :param text: The text to parse. 
 :type text: str 
 :returns: A parsed EDNS message. 
 :rtype: :class:`~.edns.EDNS`'","'Convert a space-separated list of EDNS flag text values into a EDNS 
 flags value. 
 @rtype: int'"
"def _date_to_datetime(value): 
    if (not isinstance(value, datetime.date)): 
      raise TypeError(('Cannot   convert   to   datetime   expected   date   value;   received   %s' % value)) 
   return datetime.datetime(value.year, value.month, value.day)", 'Convert a date to a datetime object.',"'Convert a date to a datetime for datastore storage. 
 Args: 
 value: A datetime.date object. 
 Returns: 
 A datetime object with time set to 0:00.'"
"def find_bad_registrations(): 
    registrations = models.Node.find(Q('is_registration', 'eq', True)) 
   for registration in registrations: 
      meta = (registration.registered_meta or {}) 
      keys = meta.keys() 
      if (len(keys) != 1): 
         print 'Inconsistency:   Number   of   keys   on   project   {}   ({})   !=   1'.format(registration.title, registration._primary_key) 
         continue 
      if (keys[0] not in known_schemas): 
         print 'Inconsistency:   Registration   schema   {}   on   project   {}   ({})   not   in   known   schemas'.format(keys[0], registration.title, registration._primary_key)"," 'Finds bad registrations. 
 A bad registration is one that has a schema that is not known.'","'Find registrations with unexpected numbers of template keys or 
 outdated templates.'"
"def assert_has_n_elements_with_path(output, path, n): 
    xml = to_xml(output) 
   n = int(n) 
   num_elements = len(xml.findall(path)) 
   if (num_elements != n): 
      errmsg = ('Expected   to   find   %d   elements   with   path   %s,   but   %d   were   found.' % (n, path, num_elements)) 
      raise AssertionError(errmsg)", 'Check that a given path contains the expected number of elements.',"'Asserts the specified output has exactly n elements matching the 
 path specified.'"
"def test_warning_config_google_home_listen_port(): 
    with patch.object(_LOGGER, 'warning') as mock_warn: 
      Config(None, {'type': 'google_home', 'host_ip': '123.123.123.123', 'listen_port': 8300}) 
      assert mock_warn.called 
      assert (mock_warn.mock_calls[0][1][0] == 'When   targetting   Google   Home,   listening   port   has   to   be   port   80')", 'Test warning when listen port is not 80','Test we warn when non-default port is used for Google Home.'
"def auc(actual, posterior): 
    r = tied_rank(posterior) 
   num_positive = len([0 for x in actual if (x == 1)]) 
   num_negative = (len(actual) - num_positive) 
   sum_positive = sum([r[i] for i in range(len(r)) if (actual[i] == 1)]) 
   auc = ((sum_positive - ((num_positive * (num_positive + 1)) / 2.0)) / (num_negative * num_positive)) 
   return auc"," 'Computes the area under the ROC curve. 
 The area under the ROC curve (AUC) is a measure of the 
 performance of a binary classifier. 
 Parameters 
 actual : array 
 The actual values of the binary classification problem. 
 posterior : array 
 The posterior probabilities of the binary classification problem. 
 Returns 
 auc : float 
 The area under the ROC curve. 
 Notes 
 The AUC is defined as the area under the ROC curve. 
 The ROC curve is a plot of the true positive rate against the 
 false positive rate at various thresholds. 
 The AUC is a measure of the performance of a binary classifier. 
 It is the area under the ROC curve. 
 The AUC is a measure of the performance of a binary classifier. 
 It is the area under the ROC curve. 
 References 
 .. [1] http://en.wikipedia.org/wiki/ROC_curve 
 .. [2] http://en.wikipedia.org/wiki/Area_under_the_ROC_curve'","'Computes the area under the receiver-operater characteristic (AUC) 
 This function computes the AUC error metric for binary classification. 
 Parameters 
 actual : list of binary numbers, numpy array 
 The ground truth value 
 posterior : same type as actual 
 Defines a ranking on the binary numbers, from most likely to 
 be positive to least likely to be positive. 
 Returns 
 score : double 
 The mean squared error between actual and posterior'"
"def addYGroove(derivation, negatives, x): 
    if (derivation.topBevel <= 0.0): 
      return 
   bottom = (derivation.height - derivation.topBevel) 
   top = derivation.height 
   groove = [complex(x, bottom), complex((x - derivation.topBevel), top), complex((x + derivation.topBevel), top)] 
   triangle_mesh.addSymmetricYPath(negatives, groove, (1.0001 * derivation.topRight.imag))"," 'Add a Y-groove to the top of a derivation. 
 This is a helper function for the :func:`addBevel` function. 
 Parameters 
 derivation : Derivation 
 The derivation to which the groove will be added. 
 negatives : list of complex numbers 
 The negative of the top bevel of the derivation. 
 x : complex number 
 The x-coordinate of the top of the derivation. 
 Returns 
 None'",'Add y groove'
"def _get_channel_stub(): 
    return apiproxy_stub_map.apiproxy.GetStub('channel')", 'Returns a Channel stub.',"'Gets the ChannelServiceStub instance from the API proxy stub map. 
 Returns: 
 The ChannelServiceStub instance as registered in the API stub map.'"
"@given('the   model   elements   with   name   and   tags') 
 def step_given_named_model_elements_with_tags(context): 
    assert context.table, 'REQUIRE:   context.table' 
   context.table.require_columns(['name', 'tags']) 
   model_element_names = set() 
   model_elements = [] 
   for row in context.table.rows: 
      name = row['name'].strip() 
      tags = convert_model_element_tags(row['tags']) 
      assert (name not in model_element_names), ('DUPLICATED:   name=%s' % name) 
      model_elements.append(ModelElement(name, tags=tags)) 
      model_element_names.add(name) 
   context.model_elements = model_elements", 'Given the model elements with name and tags',"'.. code-block:: gherkin 
 Given the model elements with name and tags: 
 | name | tags   | 
 | S1   | @foo   | 
 Then the tag expression select model elements with: 
 | tag expression | selected?    | 
 |  @foo          | S1, S3       | 
 | -@foo          | S0, S2, S3   |'"
"def check_map(infile, disable_primer_check, barcode_type='golay_12', added_demultiplex_field=None, has_barcodes=True): 
    if (barcode_type == 'variable_length'): 
      var_len_barcodes = True 
   else: 
      var_len_barcodes = False 
   if (barcode_type == '0'): 
      has_barcodes = False 
   (hds, mapping_data, run_description, errors, warnings) = process_id_map(infile, has_barcodes=has_barcodes, disable_primer_check=disable_primer_check, added_demultiplex_field=added_demultiplex_field, variable_len_barcodes=var_len_barcodes) 
   if errors: 
      raise ValueError((('Errors   were   found   with   mapping   file,   ' + 'please   run   validate_mapping_file.py   to   ') + 'identify   problems.')) 
   id_map = {} 
   for curr_data in mapping_data: 
      id_map[curr_data[0]] = {} 
   for header in range(len(hds)): 
      for curr_data in mapping_data: 
         id_map[curr_data[0]][hds[header]] = curr_data[header] 
   barcode_to_sample_id = {} 
   primer_seqs_lens = {} 
   all_primers = {} 
   for (sample_id, sample) in id_map.items(): 
      if added_demultiplex_field: 
         barcode_to_sample_id[((sample['BarcodeSequence'].upper() + ',') + sample[added_demultiplex_field])] = sample_id 
      else: 
         barcode_to_sample_id[sample['BarcodeSequence'].upper()] = sample_id 
      if (not disable_primer_check): 
         raw_primers = sample['LinkerPrimerSequence'].upper().split(',') 
         if (len(raw_primers[0].strip()) == 0): 
            raise ValueError(('No   primers   detected,   please   use   the   ' + '-p   parameter   to   disable   primer   detection.')) 
         expanded_primers = expand_degeneracies(raw_primers) 
         curr_bc_primers = {} 
         for primer in expanded_primers: 
            curr_bc_primers[primer] = len(primer) 
            all_primers[primer] = len(primer) 
         primer_seqs_lens[sample['BarcodeSequence']] = curr_bc_primers 
   return (hds, id_map, barcode_to_sample_id, warnings, errors, primer_seqs_lens, all_primers)"," 'Checks mapping file. 
 :param infile: The input file to check. 
 :param disable_primer_check: If True, primer checking is disabled. 
 :param barcode_type: The type of barcode to check. 
 :param added_demultiplex_field: The field to add to the demultiplex file. 
 :param has_barcodes: If True, barcodes are expected. 
 :return: A tuple of header names, a dictionary of sample id to mapping data, a dictionary of barcode to sample id, a list of warnings, a list of errors, a dictionary of primer sequence length, and a dictionary of all primer sequences.'","'Check mapping file and extract list of valid barcodes, primers'"
"def upload_template_and_reload(name): 
    template = get_templates()[name] 
   local_path = template[u'local_path'] 
   if (not os.path.exists(local_path)): 
      project_root = os.path.dirname(os.path.abspath(__file__)) 
      local_path = os.path.join(project_root, local_path) 
   remote_path = template[u'remote_path'] 
   reload_command = template.get(u'reload_command') 
   owner = template.get(u'owner') 
   mode = template.get(u'mode') 
   remote_data = u'' 
   if exists(remote_path): 
      with hide(u'stdout'): 
         remote_data = sudo((u'cat   %s' % remote_path), show=False) 
   with open(local_path, u'r') as f: 
      local_data = f.read() 
      local_data = re.sub(u'%(?!\\(\\w+\\)s)', u'%%', local_data) 
      if (u'%(db_pass)s' in local_data): 
         env.db_pass = db_pass() 
      local_data %= env 
   clean = (lambda s: s.replace(u'\n', u'').replace(u'\r', u'').strip()) 
   if (clean(remote_data) == clean(local_data)): 
      return 
   upload_template(local_path, remote_path, env, use_sudo=True, backup=False) 
   if owner: 
      sudo((u'chown   %s   %s' % (owner, remote_path))) 
   if mode: 
      sudo((u'chmod   %s   %s' % (mode, remote_path))) 
   if reload_command: 
      sudo(reload_command)", 'Upload a template and reload it',"'Uploads a template only if it has changed, and if so, reload the 
 related service.'"
"@pytest.mark.parametrize('confcutdir,passed,error', [('.', 2, 0), ('src', 1, 1), (None, 1, 1)]) 
 def test_search_conftest_up_to_inifile(testdir, confcutdir, passed, error): 
    root = testdir.tmpdir 
   src = root.join('src').ensure(dir=1) 
   src.join('pytest.ini').write('[pytest]') 
   src.join('conftest.py').write(_pytest._code.Source('\n                        import   pytest\n                        @pytest.fixture\n                        def   fix1():   pass\n            ')) 
   src.join('test_foo.py').write(_pytest._code.Source('\n                        def   test_1(fix1):\n                                    pass\n                        def   test_2(out_of_reach):\n                                    pass\n            ')) 
   root.join('conftest.py').write(_pytest._code.Source('\n                        import   pytest\n                        @pytest.fixture\n                        def   out_of_reach():   pass\n            ')) 
   args = [str(src)] 
   if confcutdir: 
      args = [('--confcutdir=%s' % root.join(confcutdir))] 
   result = testdir.runpytest(*args) 
   match = '' 
   if passed: 
      match += ('*%d   passed*' % passed) 
   if error: 
      match += ('*%d   error*' % error) 
   result.stdout.fnmatch_lines(match)", 'Test that search for conftest fixtures only looks in the current dir',"'Test that conftest files are detected only up to a ini file, unless 
 an explicit --confcutdir option is given.'"
"def is_valid_number_for_region(numobj, region_code): 
    country_code = numobj.country_code 
   if (region_code is None): 
      return False 
   metadata = PhoneMetadata.metadata_for_region_or_calling_code(country_code, region_code.upper()) 
   if ((metadata is None) or ((region_code != REGION_CODE_FOR_NON_GEO_ENTITY) and (country_code != country_code_for_valid_region(region_code)))): 
      return False 
   nsn = national_significant_number(numobj) 
   return (_number_type_helper(nsn, metadata) != PhoneNumberType.UNKNOWN)", 'Returns True if the number is valid for the given region.',"'Tests whether a phone number is valid for a certain region. 
 Note this doesn\'t verify the number is actually in use, which is 
 impossible to tell by just looking at a number itself. If the country 
 calling code is not the same as the country calling code for the region, 
 this immediately exits with false. After this, the specific number pattern 
 rules for the region are examined. This is useful for determining for 
 example whether a particular number is valid for Canada, rather than just 
 a valid NANPA number. 
 Warning: In most cases, you want to use is_valid_number instead. For 
 example, this method will mark numbers from British Crown dependencies 
 such as the Isle of Man as invalid for the region ""GB"" (United Kingdom), 
 since it has its own region code, ""IM"", which may be undesirable. 
 Arguments: 
 numobj -- The phone number object that we want to validate. 
 region_code -- The region that we want to validate the phone number for. 
 Returns a boolean that indicates whether the number is of a valid pattern.'"
"def test_elemwise_collapse2(): 
    shape = (4, 5, 9) 
   a = cuda_ndarray.CudaNdarray(theano._asarray(numpy.random.rand(*shape), dtype='float32')) 
   a = theano._asarray(numpy.random.rand(*shape), dtype='float32') 
   a2 = tcn.shared_constructor(a, 'a') 
   a3 = a2.dimshuffle(0, 'x', 1, 2) 
   b = tcn.CudaNdarrayType((False, False, False, False))() 
   c = (a3 + b) 
   f = pfunc([b], [c], mode=mode_with_gpu) 
   v = theano._asarray(numpy.random.rand(shape[0], 5, *shape[1:]), dtype='float32') 
   v = cuda_ndarray.CudaNdarray(v) 
   out = f(v)[0] 
   assert numpy.allclose(out, (a.reshape(shape[0], 1, *shape[1:]) + v))", 'Test elemwise collapse','Test when only one inputs have one broadcastable dimension'
"def is_lyrics(text, artist=None): 
    if (not text): 
      return False 
   badTriggersOcc = [] 
   nbLines = text.count('\n') 
   if (nbLines <= 1): 
      log.debug(u""Ignoring   too   short   lyrics   '{0}'"".format(text)) 
      return False 
   elif (nbLines < 5): 
      badTriggersOcc.append('too_short') 
   else: 
      text = remove_credits(text) 
   badTriggers = ['lyrics', 'copyright', 'property', 'links'] 
   if artist: 
      badTriggersOcc += [artist] 
   for item in badTriggers: 
      badTriggersOcc += ([item] * len(re.findall(('\\W%s\\W' % item), text, re.I))) 
   if badTriggersOcc: 
      log.debug(u'Bad   triggers   detected:   {0}'.format(badTriggersOcc)) 
   return (len(badTriggersOcc) < 2)"," 'Returns True if the text is a lyrics, False otherwise.'",'Determine whether the text seems to be valid lyrics.'
"def _as_meg_type_evoked(evoked, ch_type='grad', mode='fast'): 
    evoked = evoked.copy() 
   if (ch_type not in ['mag', 'grad']): 
      raise ValueError(('to_type   must   be   ""mag""   or   ""grad"",   not   ""%s""' % ch_type)) 
   pick_from = pick_types(evoked.info, meg=True, eeg=False, ref_meg=False) 
   pick_to = pick_types(evoked.info, meg=ch_type, eeg=False, ref_meg=False) 
   if (len(pick_to) == 0): 
      raise ValueError('No   channels   matching   the   destination   channel   type   found   in   info.   Please   pass   an   evoked   containingboth   the   original   and   destination   channels.   Only   the   locations   of   the   destination   channels   will   be   used   for   interpolation.') 
   info_from = pick_info(evoked.info, pick_from) 
   info_to = pick_info(evoked.info, pick_to) 
   mapping = _map_meg_channels(info_from, info_to, mode=mode) 
   data = np.dot(mapping, evoked.data[pick_from]) 
   evoked.pick_types(meg=ch_type, eeg=False, ref_meg=False) 
   evoked.data = data 
   for ch in evoked.info['chs']: 
      ch['ch_name'] += '_virtual' 
   evoked.info._update_redundant() 
   evoked.info._check_consistency() 
   return evoked"," 'Convert an evoked to the specified channel type. 
 Parameters 
 evoked : Evoked 
 An evoked object. 
 ch_type : str 
 The channel type to convert to. 
 mode : str 
 The interpolation mode to use. 
 Returns 
 An Evoked object with the specified channel type. 
 See Also 
 to_meg 
 to_eeg 
 to_ref_meg 
 to_ref_eeg 
 to_eeg_raw 
 to_ref_eeg_raw 
 to_meg_raw 
 to_ref_meg_raw 
 to_meg_raw_nlin 
 to_ref_meg_raw_nlin 
 to_meg_raw_bem 
 to_ref_meg_raw_bem 
 to_meg_raw_bem_nlin 
 to_ref_meg_raw_bem_nlin 
 Notes 
 This function is similar to to_meg, but it does not use the 
 interpolation of the original data. 
 It is assumed that the channel","'Compute virtual evoked using interpolated fields in mag/grad channels. 
 Parameters 
 evoked : instance of mne.Evoked 
 The evoked object. 
 ch_type : str 
 The destination channel type. It can be \'mag\' or \'grad\'. 
 mode : str 
 Either `\'accurate\'` or `\'fast\'`, determines the quality of the 
 Legendre polynomial expansion used. `\'fast\'` should be sufficient 
 for most applications. 
 Returns 
 evoked : instance of mne.Evoked 
 The transformed evoked object containing only virtual channels.'"
"def getFunctionsWithStringByFileNames(fileNames, searchString): 
    functions = [] 
   for fileName in fileNames: 
      functions += getFunctionsWithStringByFileName(fileName, searchString) 
   functions.sort() 
   return functions"," 'Returns a list of functions with the searchString in their name. 
 :param fileNames: a list of file names 
 :param searchString: the search string 
 :return: a list of functions with the searchString in their name'",'Get the functions with the search string in the files.'
"def libvlc_video_get_title_description(p_mi): 
    f = (_Cfunctions.get('libvlc_video_get_title_description', None) or _Cfunction('libvlc_video_get_title_description', ((1,),), None, ctypes.POINTER(TrackDescription), MediaPlayer)) 
   return f(p_mi)"," 'Get the description of the current title. 
 @param p_mi: the media player. 
 @return: the description of the current title, or None if there is no title.'","'Get the description of available titles. 
 @param p_mi: the media player. 
 @return: list containing description of available titles.'"
"@validator 
 def domain(value): 
    return pattern.match(value)", 'Validate the domain.',"'Return whether or not given value is a valid domain. 
 If the value is valid domain name this function returns ``True``, otherwise 
 :class:`~validators.utils.ValidationFailure`. 
 Examples:: 
 >>> domain(\'example.com\') 
 True 
 >>> domain(\'example.com/\') 
 ValidationFailure(func=domain, ...) 
 Supports IDN domains as well:: 
 >>> domain(\'xn----gtbspbbmkef.xn--p1ai\') 
 True 
 .. versionadded:: 0.9 
 .. versionchanged:: 0.10 
 Added support for internationalized domain name (IDN) validation. 
 :param value: domain string to validate'"
"@login_required 
 @ensure_csrf_cookie 
 @require_http_methods(('GET', 'POST', 'PUT')) 
 @expect_json 
 def advanced_settings_handler(request, course_key_string): 
    course_key = CourseKey.from_string(course_key_string) 
   with modulestore().bulk_operations(course_key): 
      course_module = get_course_and_check_access(course_key, request.user) 
      if (('text/html' in request.META.get('HTTP_ACCEPT', '')) and (request.method == 'GET')): 
         return render_to_response('settings_advanced.html', {'context_course': course_module, 'advanced_dict': CourseMetadata.fetch(course_module), 'advanced_settings_url': reverse_course_url('advanced_settings_handler', course_key)}) 
      elif ('application/json' in request.META.get('HTTP_ACCEPT', '')): 
         if (request.method == 'GET'): 
            return JsonResponse(CourseMetadata.fetch(course_module)) 
         else: 
            try: 
               (is_valid, errors, updated_data) = CourseMetadata.validate_and_update_from_json(course_module, request.json, user=request.user) 
               if is_valid: 
                  try: 
                     _refresh_course_tabs(request, course_module) 
                  except InvalidTabsException as err: 
                     log.exception(err.message) 
                     response_message = [{'message': _('An   error   occurred   while   trying   to   save   your   tabs'), 'model': {'display_name': _('Tabs   Exception')}}] 
                     return JsonResponseBadRequest(response_message) 
                  modulestore().update_item(course_module, request.user.id) 
                  return JsonResponse(updated_data) 
               else: 
                  return JsonResponseBadRequest(errors) 
            except (TypeError, ValueError, InvalidTabsException) as err: 
               return HttpResponseBadRequest(django.utils.html.escape(err.message), content_type='text/plain')"," 'Advanced settings for a course. 
 The course_key_string is the course_id of the course. 
 The request will be a GET or POST depending on the Accept header. 
 If the Accept header is text/html, then the response will be an HTML 
 page with the advanced settings for the course. 
 If the Accept header is application/json, then the response will be a 
 JSON object containing the course metadata.'","'Course settings configuration 
 GET 
 html: get the page 
 json: get the model 
 PUT, POST 
 json: update the Course\'s settings. The payload is a json rep of the 
 metadata dicts.'"
"def NeedsMacOSXProxyFakes(): 
    return ((sys.platform == 'darwin') and ((2, 6, 0) <= sys.version_info < (2, 6, 4)))", 'Returns True if Mac OS X 10.6.0 or later is required.','Returns True if the MacOS X urllib fakes should be installed.'
"def generateUUID(): 
    return str(uuid.uuid4())", 'Generate a unique identifier','Utility function; generates UUIDs'
"def event(name, priority=128): 
    def decorator(func): 
      add_event_handler(name, func, priority) 
      return func 
   return decorator", 'Decorator to add an event to the event queue.','Register event to function with a decorator'
"def show_frontends(socket='/var/run/haproxy.sock'): 
    ha_conn = _get_conn(socket) 
   ha_cmd = haproxy.cmds.showFrontends() 
   return ha_conn.sendCmd(ha_cmd)", 'Returns the list of frontends.',"'Show HaProxy frontends 
 socket 
 haproxy stats socket 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' haproxy.show_frontends'"
"def copy(src, dst, createpath=0, copydates=1, forcetype=None): 
    src = File.pathname(src) 
   dst = File.pathname(dst) 
   if createpath: 
      mkdirs(os.path.split(dst)[0]) 
   ifp = open(src, 'rb') 
   ofp = open(dst, 'wb') 
   d = ifp.read(BUFSIZ) 
   while d: 
      ofp.write(d) 
      d = ifp.read(BUFSIZ) 
   ifp.close() 
   ofp.close() 
   ifp = openrf(src, '*rb') 
   ofp = openrf(dst, '*wb') 
   d = ifp.read(BUFSIZ) 
   while d: 
      ofp.write(d) 
      d = ifp.read(BUFSIZ) 
   ifp.close() 
   ofp.close() 
   srcfss = File.FSSpec(src) 
   dstfss = File.FSSpec(dst) 
   sf = srcfss.FSpGetFInfo() 
   df = dstfss.FSpGetFInfo() 
   (df.Creator, df.Type) = (sf.Creator, sf.Type) 
   if (forcetype is not None): 
      df.Type = forcetype 
   df.Flags = (sf.Flags & COPY_FLAGS) 
   dstfss.FSpSetFInfo(df) 
   if copydates: 
      srcfsr = File.FSRef(src) 
      dstfsr = File.FSRef(dst) 
      (catinfo, _, _, _) = srcfsr.FSGetCatalogInfo(Files.kFSCatInfoAllDates) 
      dstfsr.FSSetCatalogInfo(Files.kFSCatInfoAllDates, catinfo)"," 'Copy a file or folder. 
 :param src: Source path 
 :param dst: Destination path 
 :param createpath: If True, create the destination folder 
 :param copydates: If True, copy the file modification date from the source to the destination 
 :param forcetype: If specified, force the destination file to be of the specified type 
 :returns: None'","'Copy a file, including finder info, resource fork, etc'"
"def assert_snr(actual, desired, tol): 
    from nose.tools import assert_true 
   snr = (linalg.norm(desired, ord='fro') / linalg.norm((desired - actual), ord='fro')) 
   assert_true((snr >= tol), msg=('%f   <   %f' % (snr, tol)))", 'Check if the SNR is within tol of desired.','Assert actual and desired arrays are within some SNR tolerance'
"def get_widgets(request): 
    widgets = {} 
   widgets.update(WIDGETS) 
   try: 
      agent = request.user.profile.serviceagent_set.all()[0] 
   except Exception: 
      agent = None 
   if (not agent): 
      del widgets['widget_index_assigned'] 
   return widgets", 'Return a dictionary of widgets.','Returns a set of all available widgets'
"def prepare_wmt_data(data_dir, en_vocabulary_size, fr_vocabulary_size, tokenizer=None): 
    train_path = get_wmt_enfr_train_set(data_dir) 
   dev_path = get_wmt_enfr_dev_set(data_dir) 
   from_train_path = (train_path + '.en') 
   to_train_path = (train_path + '.fr') 
   from_dev_path = (dev_path + '.en') 
   to_dev_path = (dev_path + '.fr') 
   return prepare_data(data_dir, from_train_path, to_train_path, from_dev_path, to_dev_path, en_vocabulary_size, fr_vocabulary_size, tokenizer)"," 'Prepare the data for the WMT dataset. 
 :param data_dir: Path to the data directory 
 :param en_vocabulary_size: Vocabulary size of the English side 
 :param fr_vocabulary_size: Vocabulary size of the French side 
 :param tokenizer: The tokenizer to use. 
 :return: The dictionary of data, with the keys: 
 \'from_train_path\', \'to_train_path\', \'from_dev_path\', \'to_dev_path\''","'Get WMT data into data_dir, create vocabularies and tokenize data. 
 Args: 
 data_dir: directory in which the data sets will be stored. 
 en_vocabulary_size: size of the English vocabulary to create and use. 
 fr_vocabulary_size: size of the French vocabulary to create and use. 
 tokenizer: a function to use to tokenize each data sentence; 
 if None, basic_tokenizer will be used. 
 Returns: 
 A tuple of 6 elements: 
 (1) path to the token-ids for English training data-set, 
 (2) path to the token-ids for French training data-set, 
 (3) path to the token-ids for English development data-set, 
 (4) path to the token-ids for French development data-set, 
 (5) path to the English vocabulary file, 
 (6) path to the French vocabulary file.'"
"def vacuum(verbose=False): 
    ret = {} 
   imgadm = _check_imgadm() 
   cmd = '{0}   vacuum   -f'.format(imgadm) 
   res = __salt__['cmd.run_all'](cmd) 
   retcode = res['retcode'] 
   if (retcode != 0): 
      ret['Error'] = _exit_status(retcode) 
      return ret 
   result = {} 
   for image in res['stdout'].splitlines(): 
      image = [var for var in image.split('   ') if var] 
      result[image[2]] = {'name': image[3][1:image[3].index('@')], 'version': image[3][(image[3].index('@') + 1):(-1)]} 
   if verbose: 
      return result 
   else: 
      return list(result.keys())"," 'Run the vacuum command on the images. 
 The vacuum command removes unused images from the image repository. 
 This command is only applicable to images that have been tagged. 
 :param verbose: If True, the output is returned, otherwise only the 
 list of image names is returned. 
 :returns: The list of image names that were removed, or the list of 
 image names if verbose is False.'","'Remove unused images 
 verbose : boolean (False) 
 toggle verbose output 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' imgadm.vacuum [verbose=True]'"
"def diff_tree(repo, old_tree, new_tree, outstream=sys.stdout): 
    with open_repo_closing(repo) as r: 
      write_tree_diff(outstream, r.object_store, old_tree, new_tree)"," 'Generate a diff of the trees in the repository. 
 :param repo: The repository to diff. 
 :param old_tree: The old tree. 
 :param new_tree: The new tree. 
 :param outstream: The stream to write to.'","'Compares the content and mode of blobs found via two tree objects. 
 :param repo: Path to repository 
 :param old_tree: Id of old tree 
 :param new_tree: Id of new tree 
 :param outstream: Stream to write to'"
"def test_markovchain_pmatrices(): 
    Q = np.zeros((6, 6)) 
   (Q[(0, 1)], Q[(1, 0)]) = (1, 1) 
   Q[(2, [0, 3])] = (1 / 2) 
   (Q[(3, 4)], Q[(4, 5)], Q[(5, 3)]) = (1, 1, 1) 
   Q_stationary_dists = np.array([[(1 / 2), (1 / 2), 0, 0, 0, 0], [0, 0, 0, (1 / 3), (1 / 3), (1 / 3)]]) 
   testset = [{'P': np.array([[0.4, 0.6], [0.2, 0.8]]), 'stationary_dists': np.array([[0.25, 0.75]]), 'comm_classes': [np.arange(2)], 'rec_classes': [np.arange(2)], 'is_irreducible': True, 'period': 1, 'is_aperiodic': True, 'cyclic_classes': [np.arange(2)]}, {'P': sparse.csr_matrix([[0.4, 0.6], [0.2, 0.8]]), 'stationary_dists': np.array([[0.25, 0.75]]), 'comm_classes': [np.arange(2)], 'rec_classes': [np.arange(2)], 'is_irreducible': True, 'period': 1, 'is_aperiodic': True, 'cyclic_classes': [np.arange(2)]}, {'P': np.array([[0, 1], [1, 0]]), 'stationary_dists': np.array([[0.5, 0.5]]), 'comm_classes': [np.arange(2)], 'rec_classes': [np.arange(2)], 'is_irreducible': True, 'period': 2, 'is_aperiodic': False, 'cyclic_classes': [np.array([0]), np.array([1])]}, {'P': np.eye(2), 'stationary_dists': np.array([[1, 0], [0, 1]]), 'comm_classes': [np.array([0]), np.array([1])], 'rec_classes': [np.array([0]), np.array([1])], 'is_irreducible': False, 'period': 1, 'is_aperiodic': True}, {'P': np.array([[1, 0], [1, 0]]), 'stationary_dists': np.array([[1, 0]]), 'comm_classes': [np.array([0]), np.array([1])], 'rec_classes': [np.array([0])], 'is_irreducible': False, 'period': 1, 'is_aperiodic': True}, {'P': Q, 'stationary_dists': Q_stationary_dists, 'comm_classes': [np.array([0, 1]), np.array([2]), np.array([3, 4, 5])], 'rec_classes': [np.array([0, 1]), np.array([3, 4, 5])], 'is_irreducible': False, 'period': 6, 'is_aperiodic': False}, {'P': sparse.csr_matrix(Q), 'stationary_dists': Q_stationary_dists, 'comm_classes': [np.array([0, 1]), np.array([2]), np.array([3, 4, 5])], 'rec_classes': [np.array([0, 1]), np.array([3, 4, 5])], 'is_irreducible': False, 'period': 6, 'is_aperiodic': False}] 
   for test_dict in testset: 
      mc = MarkovChain(test_dict['P']) 
      computed = mc.stationary_distributions 
      assert_allclose(computed, test_dict['stationary_dists']) 
      assert (mc.num_communication_classes == len(test_dict['comm_classes'])) 
      assert (mc.is_irreducible == test_dict['is_irreducible']) 
      assert (mc.num_recurrent_classes == len(test_dict['rec_classes'])) 
      list_of_array_equal(sorted(mc.communication_classes, key=(lambda x: x[0])), sorted(test_dict['comm_classes'], key=(lambda x: x[0]))) 
      list_of_array_equal(sorted(mc.recurrent_classes, key=(lambda x: x[0])), sorted(test_dict['rec_classes'], key=(lambda x: x[0]))) 
      assert (mc.period == test_dict['period']) 
      assert (mc.is_aperiodic == test_dict['is_aperiodic']) 
      try: 
         list_of_array_equal(sorted(mc.cyclic_classes, key=(lambda x: x[0])), sorted(test_dict['cyclic_classes'], key=(lambda x: x[0]))) 
      except NotImplementedError: 
         assert (mc.is_irreducible is False) 
      computed = mc_compute_stationary(test_dict['P']) 
      assert_allclose(computed, test_dict['stationary_dists'])", 'Test that the stationary distributions of Markov Chains are computed correctly.',"'Test the methods of MarkovChain, as well as mc_compute_stationary, 
 with P matrix and known solutions'"
"def words(string, filter=(lambda w: w.strip(""'"").isalnum()), punctuation=PUNCTUATION, **kwargs): 
    string = decode_utf8(string) 
   string = re.sub(""([a-z|A-Z])'(m|s|ve|re|ll|d)"", u'\\1   <QUOTE/>\\2', string) 
   string = re.sub(""(c|d|gl|j|l|m|n|s|t|un)'([a-z|A-Z])"", u'\\1<QUOTE/>   \\2', string) 
   words = (w.strip(punctuation).replace(u'<QUOTE/>', ""'"", 1) for w in string.split()) 
   words = (w for w in words if ((filter is None) or (filter(w) is not False))) 
   words = [w for w in words if w] 
   return words"," 'Returns a list of words from a string. 
 :param string: The string to break into words. 
 :param filter: A callable that takes a word and returns True if the word should 
 be included in the list, False otherwise. 
 :param punctuation: A list of punctuation characters to remove from the string. 
 :param kwargs: Additional keyword arguments are passed to the :func:`re.sub` 
 function. 
 :return: A list of words from the string.'","'Returns a list of words (alphanumeric character sequences) from the given string. 
 Common punctuation marks are stripped from words.'"
"@login_required 
 def invitation_error(request, error_message='You   do   not   have   any   invitations   at   this   time.', template_name='invitations/invitation_error.html'): 
    return render(request, template_name, {'error_message': error_message})", 'Renders an error page with the given error message.',"'Returns an error template. 
 Template: ``invitations/invitation_error.html`` 
 Context: 
 error_message 
 String containing the error message.'"
"def getTypeFromProgID(prog_id): 
    return Type.GetTypeFromProgID(prog_id)"," 'Returns the type corresponding to the given ProgID. 
 @param prog_id: The ProgID of the type to get. 
 @return: The type corresponding to the given ProgID. 
 @rtype: Type'",'Returns the Type object for prog_id.'
"def resolve_hostname(hostname): 
    res = socket.getaddrinfo(hostname, None)[0] 
   (family, socktype, proto, canonname, sockaddr) = res 
   return sockaddr[0]"," 'Resolve hostname to IP address. 
 :param hostname: Hostname to resolve 
 :type hostname: str 
 :return: IP address 
 :rtype: str'",'Resolves host name to IP address.'
"def get_d3_section_grade_distrib(course_id, section): 
    course = modulestore().get_course(course_id, depth=4) 
   problem_set = [] 
   problem_info = {} 
   c_subsection = 0 
   for subsection in course.get_children()[section].get_children(): 
      c_subsection += 1 
      c_unit = 0 
      for unit in subsection.get_children(): 
         c_unit += 1 
         c_problem = 0 
         for child in unit.get_children(): 
            if (child.location.category == 'problem'): 
               c_problem += 1 
               problem_set.append(child.location) 
               problem_info[child.location] = {'id': child.location.to_deprecated_string(), 'x_value': 'P{0}.{1}.{2}'.format(c_subsection, c_unit, c_problem), 'display_name': own_metadata(child).get('display_name', '')} 
   grade_distrib = get_problem_set_grade_distrib(course_id, problem_set) 
   d3_data = [] 
   for problem in problem_set: 
      stack_data = [] 
      if (problem in grade_distrib): 
         max_grade = float(grade_distrib[problem]['max_grade']) 
         for (grade, count_grade) in grade_distrib[problem]['grade_distrib']: 
            percent = 0.0 
            if (max_grade > 0): 
               percent = round(((grade * 100.0) / max_grade), 1) 
            tooltip = {'type': 'problem', 'problem_info_x': problem_info[problem]['x_value'], 'count_grade': count_grade, 'percent': percent, 'problem_info_n': problem_info[problem]['display_name'], 'grade': grade, 'max_grade': max_grade} 
            stack_data.append({'color': percent, 'value': count_grade, 'tooltip': tooltip}) 
      d3_data.append({'xValue': problem_info[problem]['x_value'], 'stackData': stack_data}) 
   return d3_data"," 'Returns a list of D3 data points for the given course and section. 
 This data is used to generate a D3 bar chart of the problem set for the 
 given course and section. 
 Args: 
 course_id: the ID of the course. 
 section: the ID of the section. 
 Returns: 
 A list of D3 data points.'","'Returns the grade distribution for the problems in the `section` section in a format for the d3 code. 
 `course_id` a string that is the course\'s ID. 
 `section` an int that is a zero-based index into the course\'s list of sections. 
 Navigates to the section specified to find all the problems associated with that section and then finds the grade 
 distribution for those problems. Finally returns an object formated the way the d3_stacked_bar_graph.js expects its 
 data object to be in. 
 If this is requested multiple times quickly for the same course, it is better to call 
 get_d3_problem_grade_distrib and pick out the sections of interest. 
 Returns an array of dicts with the following keys (taken from d3_stacked_bar_graph.js\'s documentation) 
 \'xValue\' - Corresponding value for the x-axis 
 \'stackData\' - Array of objects with key, value pairs that represent a bar: 
 \'color\' - Defines what ""color"" the bar will map to 
 \'value\' - Maps to the height of the bar, along the y-axis 
 \'tooltip\' - (Optional) Text to display on mouse hover'"
"def _minimize_scalar_golden(func, brack=None, args=(), xtol=_epsilon, maxiter=5000, **unknown_options): 
    _check_unknown_options(unknown_options) 
   tol = xtol 
   if (brack is None): 
      (xa, xb, xc, fa, fb, fc, funcalls) = bracket(func, args=args) 
   elif (len(brack) == 2): 
      (xa, xb, xc, fa, fb, fc, funcalls) = bracket(func, xa=brack[0], xb=brack[1], args=args) 
   elif (len(brack) == 3): 
      (xa, xb, xc) = brack 
      if (xa > xc): 
         (xc, xa) = (xa, xc) 
      if (not ((xa < xb) and (xb < xc))): 
         raise ValueError('Not   a   bracketing   interval.') 
      fa = func(*((xa,) + args)) 
      fb = func(*((xb,) + args)) 
      fc = func(*((xc,) + args)) 
      if (not ((fb < fa) and (fb < fc))): 
         raise ValueError('Not   a   bracketing   interval.') 
      funcalls = 3 
   else: 
      raise ValueError('Bracketing   interval   must   be   length   2   or   3   sequence.') 
   _gR = 0.61803399 
   _gC = (1.0 - _gR) 
   x3 = xc 
   x0 = xa 
   if (numpy.abs((xc - xb)) > numpy.abs((xb - xa))): 
      x1 = xb 
      x2 = (xb + (_gC * (xc - xb))) 
   else: 
      x2 = xb 
      x1 = (xb - (_gC * (xb - xa))) 
   f1 = func(*((x1,) + args)) 
   f2 = func(*((x2,) + args)) 
   funcalls += 2 
   nit = 0 
   for i in xrange(maxiter): 
      if (numpy.abs((x3 - x0)) <= (tol * (numpy.abs(x1) + numpy.abs(x2)))): 
         break 
      if (f2 < f1): 
         x0 = x1 
         x1 = x2 
         x2 = ((_gR * x1) + (_gC * x3)) 
         f1 = f2 
         f2 = func(*((x2,) + args)) 
      else: 
         x3 = x2 
         x2 = x1 
         x1 = ((_gR * x2) + (_gC * x0)) 
         f2 = f1 
         f1 = func(*((x1,) + args)) 
      funcalls += 1 
      nit += 1 
   if (f1 < f2): 
      xmin = x1 
      fval = f1 
   else: 
      xmin = x2 
      fval = f2 
   return OptimizeResult(fun=fval, nfev=funcalls, x=xmin, nit=nit, success=(nit < maxiter))"," 'Minimize scalar function, using golden section search. 
 Parameters 
 func : function 
 The scalar function to minimize. 
 args : tuple 
 The arguments to the function. 
 brack : sequence 
 The bracketing interval for the search. 
 xtol : float 
 The tolerance for the function value. 
 maxiter : int 
 The maximum number of iterations. 
 unknown_options : dict 
 Keyword arguments for the function. 
 Returns 
 OptimizeResult 
 An OptimizeResult object, containing the minimum value found and the 
 number of function evaluations made. 
 Notes 
 The golden section search is a simplex search.  It works well for 
 minimizing functions with a single variable. 
 Examples 
 >>> from sympy.optimize import minimize_scalar_golden 
 >>> from sympy import symbols 
 >>> x, y = symbols(\'x, y\') 
 >>> f = lambda x, y: (x**2 + y**2) 
 >>> res = minimize_scalar_golden(f, x, y","'Options 
 maxiter : int 
 Maximum number of iterations to perform. 
 xtol : float 
 Relative error in solution `xopt` acceptable for convergence.'"
"@task() 
 @timeit 
 def maybe_award_badge(badge_template, year, user): 
    badge = get_or_create_badge(badge_template, year) 
   if badge.is_awarded_to(user): 
      return 
   qs = Reply.objects.filter(user=user, created__gte=date(year, 1, 1), created__lt=date((year + 1), 1, 1)) 
   if (qs.count() >= 50): 
      badge.award_to(user) 
      return True"," 'Award the badge to the user if they have made 50 replies in the 
 current year.'",'Award the specific badge to the user if they\'ve earned it.'
"def purge(name=None, pkgs=None, **kwargs): 
    return remove(name=name, pkgs=pkgs)", 'Purge a package from the repository.',"'.. versionchanged:: 2015.8.12,2016.3.3,2016.11.0 
 On minions running systemd>=205, `systemd-run(1)`_ is now used to 
 isolate commands which modify installed packages from the 
 ``salt-minion`` daemon\'s control group. This is done to keep systemd 
 from killing any yum/dnf commands spawned by Salt when the 
 ``salt-minion`` service is restarted. (see ``KillMode`` in the 
 `systemd.kill(5)`_ manpage for more information). If desired, usage of 
 `systemd-run(1)`_ can be suppressed by setting a :mod:`config option 
 <salt.modules.config.get>` called ``systemd.scope``, with a value of 
 ``False`` (no quotes). 
 .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html 
 .. _`systemd.kill(5)`: https://www.freedesktop.org/software/systemd/man/systemd.kill.html 
 Package purges are not supported by yum, this function is identical to 
 :mod:`pkg.remove <salt.modules.yumpkg.remove>`. 
 name 
 The name of the package to be purged 
 Multiple Package Options: 
 pkgs 
 A list of packages to delete. Must be passed as a python list. The 
 ``name`` parameter will be ignored if this option is passed. 
 .. versionadded:: 0.16.0 
 Returns a dict containing the changes. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pkg.purge <package name> 
 salt \'*\' pkg.purge <package1>,<package2>,<package3> 
 salt \'*\' pkg.purge pkgs=\'[""foo"", ""bar""]\''"
"def buildRequestBytes(headers, data, frameFactory=None, streamID=1): 
    frames = buildRequestFrames(headers, data, frameFactory, streamID) 
   return ''.join((f.serialize() for f in frames))"," 'Builds a request body for the given headers and data. 
 :param headers: A dict of headers to add to the request. 
 :param data: A dict of data to add to the request. 
 :param frameFactory: A factory function for building frames. 
 :param streamID: The stream ID to use for the request. 
 :return: A bytes object containing the request body. 
 :rtype: bytes'","'Provides the byte sequence for a collection of HTTP/2 frames representing 
 the provided request. 
 @param headers: The HTTP/2 headers to send. 
 @type headers: L{list} of L{tuple} of L{bytes} 
 @param data: The HTTP data to send. Each list entry will be sent in its own 
 frame. 
 @type data: L{list} of L{bytes} 
 @param frameFactory: The L{FrameFactory} that will be used to construct the 
 frames. 
 @type frameFactory: L{FrameFactory} 
 @param streamID: The ID of the stream on which to send the request. 
 @type streamID: L{int}'"
"def request_fingerprint(request, include_headers=None): 
    if include_headers: 
      include_headers = tuple([h.lower() for h in sorted(include_headers)]) 
   cache = _fingerprint_cache.setdefault(request, {}) 
   if (include_headers not in cache): 
      fp = hashlib.sha1() 
      fp.update(request.method) 
      fp.update(canonicalize_url(request.url)) 
      fp.update((request.body or '')) 
      if include_headers: 
         for hdr in include_headers: 
            if (hdr in request.headers): 
               fp.update(hdr) 
               for v in request.headers.getlist(hdr): 
                  fp.update(v) 
      cache[include_headers] = fp.hexdigest() 
   return cache[include_headers]"," 'Return the fingerprint for the given request. 
 The fingerprint is a hash of the request\'s method, url, and headers. 
 The fingerprint is used to determine whether a request is a duplicate 
 of another request. 
 :param include_headers: A list of headers to include in the fingerprint. 
 :type include_headers: list 
 :return: The fingerprint for the given request. 
 :rtype: str'","'Return the request fingerprint. 
 The request fingerprint is a hash that uniquely identifies the resource the 
 request points to. For example, take the following two urls: 
 http://www.example.com/query?id=111&cat=222 
 http://www.example.com/query?cat=222&id=111 
 Even though those are two different URLs both point to the same resource 
 and are equivalent (ie. they should return the same response). 
 Another example are cookies used to store session ids. Suppose the 
 following page is only accesible to authenticated users: 
 http://www.example.com/members/offers.html 
 Lot of sites use a cookie to store the session id, which adds a random 
 component to the HTTP Request and thus should be ignored when calculating 
 the fingerprint. 
 For this reason, request headers are ignored by default when calculating 
 the fingeprint. If you want to include specific headers use the 
 include_headers argument, which is a list of Request headers to include.'"
"@mock_streams('stderr') 
 @with_patched_object(output, 'warnings', True) 
 def test_warn(): 
    warn('Test') 
   eq_('\nWarning:   Test\n\n', sys.stderr.getvalue())", 'Test that the warnings module is patched.','warn() should print \'Warning\' plus given text'
"def setraw(fd, when=TCSAFLUSH): 
    mode = tcgetattr(fd) 
   mode[IFLAG] = (mode[IFLAG] & (~ ((((BRKINT | ICRNL) | INPCK) | ISTRIP) | IXON))) 
   mode[OFLAG] = (mode[OFLAG] & (~ OPOST)) 
   mode[CFLAG] = (mode[CFLAG] & (~ (CSIZE | PARENB))) 
   mode[CFLAG] = (mode[CFLAG] | CS8) 
   mode[LFLAG] = (mode[LFLAG] & (~ (((ECHO | ICANON) | IEXTEN) | ISIG))) 
   mode[CC][VMIN] = 1 
   mode[CC][VTIME] = 0 
   tcsetattr(fd, when, mode)"," 'Sets raw mode on a file descriptor. 
 :param fd: file descriptor 
 :param when: TCSAFLUSH or TCIFLUSH 
 :return: None'",'Put terminal into a raw mode.'
"def validate(cls, model): 
    models.get_apps() 
   opts = model._meta 
   validate_base(cls, model) 
   if hasattr(cls, 'list_display'): 
      check_isseq(cls, 'list_display', cls.list_display) 
      for (idx, field) in enumerate(cls.list_display): 
         if (not callable(field)): 
            if (not hasattr(cls, field)): 
               if (not hasattr(model, field)): 
                  try: 
                     opts.get_field(field) 
                  except models.FieldDoesNotExist: 
                     raise ImproperlyConfigured(('%s.list_display[%d],   %r   is   not   a   callable   or   an   attribute   of   %r   or   found   in   the   model   %r.' % (cls.__name__, idx, field, cls.__name__, model._meta.object_name))) 
               else: 
                  f = fetch_attr(cls, model, opts, ('list_display[%d]' % idx), field) 
                  if isinstance(f, models.ManyToManyField): 
                     raise ImproperlyConfigured((""'%s.list_display[%d]',   '%s'   is   a   ManyToManyField   which   is   not   supported."" % (cls.__name__, idx, field))) 
   if hasattr(cls, 'list_display_links'): 
      check_isseq(cls, 'list_display_links', cls.list_display_links) 
      for (idx, field) in enumerate(cls.list_display_links): 
         if (field not in cls.list_display): 
            raise ImproperlyConfigured((""'%s.list_display_links[%d]'   refers   to   '%s'   which   is   not   defined   in   'list_display'."" % (cls.__name__, idx, field))) 
   if hasattr(cls, 'list_filter'): 
      check_isseq(cls, 'list_filter', cls.list_filter) 
      for (idx, fpath) in enumerate(cls.list_filter): 
         try: 
            get_fields_from_path(model, fpath) 
         except (NotRelationField, FieldDoesNotExist) as e: 
            raise ImproperlyConfigured((""'%s.list_filter[%d]'   refers   to   '%s'   which   does   not   refer   to   a   Field."" % (cls.__name__, idx, fpath))) 
   if (hasattr(cls, 'list_per_page') and (not isinstance(cls.list_per_page, int))): 
      raise ImproperlyConfigured((""'%s.list_per_page'   should   be   a   integer."" % cls.__name__)) 
   if (hasattr(cls, 'list_editable') and cls.list_editable): 
      check_isseq(cls, 'list_editable', cls.list_editable) 
      for (idx, field_name) in enumerate(cls.list_editable): 
         try: 
            field = opts.get_field_by_name(field_name)[0] 
         except models.FieldDoesNotExist: 
            raise ImproperlyConfigured((""'%s.list_editable[%d]'   refers   to   a   field,   '%s',   not   defined   on   %s."" % (cls.__name__, idx, field_name, model.__name__))) 
         if (field_name not in cls.list_display): 
            raise ImproperlyConfigured((""'%s.list_editable[%d]'   refers   to   '%s'   which   is   not   defined   in   'list_display'."" % (cls.__name__, idx, field_name))) 
         if (field_name in cls.list_display_links): 
            raise ImproperlyConfigured((""'%s'   cannot   be   in   both   '%s.list_editable'   and   '%s.list_display_links'"" % (field_name, cls.__name__, cls.__name__))) 
         if ((not cls.list_display_links) and (cls.list_display[0] in cls.list_editable)): 
            raise ImproperlyConfigured((""'%s.list_editable[%d]'   refers   to   the   first   field   in   list_display,   '%s',   which   can't   be   used   unless   list_display_links   is   set."" % (cls.__name__, idx, cls.list_display[0]))) 
         if (not field.editable): 
            raise ImproperlyConfigured((""'%s.list_editable[%d]'   refers   to   a   field,   '%s',   which   isn't   editable   through   the   admin."" % (cls.__name__, idx, field_name))) 
   if hasattr(cls, 'search_fields'): 
      check_isseq(cls, 'search_fields', cls.search_fields) 
   if cls.date_hierarchy: 
      f = get_field(cls, model, opts, 'date_hierarchy', cls.date_hierarchy) 
      if (not isinstance(f, (models.DateField, models.DateTimeField))): 
         raise ImproperlyConfigured((""'%s.date_hierarchy   is   neither   an   instance   of   DateField   nor   DateTimeField."" % cls.__name__)) 
   if cls.ordering: 
      check_isseq(cls, 'ordering', cls.ordering) 
      for (idx, field) in enumerate(cls.ordering): 
         if ((field == '?') and (len(cls.ordering) != 1)): 
            raise ImproperlyConfigured((""'%s.ordering'   has   the   random   ordering   marker   '?',   but   contains   other   fields   as   well.   Please   either   remove   '?'   or   the   other   fields."" % cls.__name__)) 
         if (field == '?'): 
            continue 
         if field.startswith('-'): 
            field = field[1:] 
         if ('__' in field): 
            continue 
         get_field(cls, model, opts, ('ordering[%d]' % idx), field) 
   if hasattr(cls, 'readonly_fields'): 
      check_readonly_fields(cls, model, opts) 
   for attr in ('list_select_related', 'save_as', 'save_on_top'): 
      if (not isinstance(getattr(cls, attr), bool)): 
         raise ImproperlyConfigured((""'%s.%s'   should   be   a   boolean."" % (cls.__name__, attr))) 
   if hasattr(cls, 'inlines'): 
      check_isseq(cls, 'inlines', cls.inlines) 
      for (idx, inline) in enumerate(cls.inlines): 
         if (not issubclass(inline, BaseModelAdmin)): 
            raise ImproperlyConfigured((""'%s.inlines[%d]'   does   not   inherit   from   BaseModelAdmin."" % (cls.__name__, idx))) 
         if (not inline.model): 
            raise ImproperlyConfigured((""'model'   is   a   required   attribute   of   '%s.inlines[%d]'."" % (cls.__name__, idx))) 
         if (not issubclass(inline.model, models.Model)): 
            raise ImproperlyConfigured((""'%s.inlines[%d].model'   does   not   inherit   from   models.Model."" % (cls.__name__, idx))) 
         validate_base(inline, inline.model) 
         validate_inline(inline, cls, model)"," 'Validate a ModelAdmin class. 
 This checks the various fields defined on the class, and ensures they are 
 valid. 
 The fields to validate are: 
 list_display 
 list_display_links 
 list_editable 
 list_filter 
 list_per_page 
 search_fields 
 date_hierarchy 
 ordering 
 readonly_fields 
 save_as 
 save_on_top 
 inlines 
 The validation checks are done in the following order: 
 1. list_display 
 2. list_display_links 
 3. list_editable 
 4. list_filter 
 5. list_per_page 
 6. search_fields 
 7. date_hierarchy 
 8. ordering 
 9. readonly_fields 
 10. save_as 
 11. save_on_top 
 12. inlines'","'Does basic ModelAdmin option validation. Calls custom validation 
 classmethod in the end if it is provided in cls. The signature of the 
 custom validation classmethod should be: def validate(cls, model).'"
"@task 
 def foo(): 
    pass", 'This is a task.','Foos!'
"def posixToNtSlashes(filepath): 
    return filepath.replace('/', '\\')"," 'Converts a posix-style path to a windows-style path. 
 :param filepath: The filepath to convert. 
 :return: The converted filepath.'","'Replaces all occurances of Posix slashes (/) in provided 
 filepath with NT ones (\) 
 >>> posixToNtSlashes(\'C:/Windows\') 
 \'C:\\Windows\''"
"def hexdump(data): 
    values = [] 
   ascii = [] 
   offset = 0 
   for (h, a) in sixteen(data): 
      if (h is None): 
         (yield (offset, '   '.join([''.join(values), ''.join(ascii)]))) 
         del values[:] 
         del ascii[:] 
         offset += 16 
      else: 
         values.append(h) 
         ascii.append(a)", 'Convert a byte string to a hex dump.','yield lines with hexdump of data'
"def _make_dssp_dict(handle): 
    dssp = {} 
   start = 0 
   keys = [] 
   for l in handle.readlines(): 
      sl = l.split() 
      if (len(sl) < 2): 
         continue 
      if (sl[1] == 'RESIDUE'): 
         start = 1 
         continue 
      if (not start): 
         continue 
      if (l[9] == '   '): 
         continue 
      dssp_index = int(l[:5]) 
      resseq = int(l[5:10]) 
      icode = l[10] 
      chainid = l[11] 
      aa = l[13] 
      ss = l[16] 
      if (ss == '   '): 
         ss = '-' 
      try: 
         NH_O_1_relidx = int(l[38:45]) 
         NH_O_1_energy = float(l[46:50]) 
         O_NH_1_relidx = int(l[50:56]) 
         O_NH_1_energy = float(l[57:61]) 
         NH_O_2_relidx = int(l[61:67]) 
         NH_O_2_energy = float(l[68:72]) 
         O_NH_2_relidx = int(l[72:78]) 
         O_NH_2_energy = float(l[79:83]) 
         acc = int(l[34:38]) 
         phi = float(l[103:109]) 
         psi = float(l[109:115]) 
      except ValueError as exc: 
         if (l[34] != '   '): 
            shift = l[34:].find('   ') 
            NH_O_1_relidx = int(l[(38 + shift):(45 + shift)]) 
            NH_O_1_energy = float(l[(46 + shift):(50 + shift)]) 
            O_NH_1_relidx = int(l[(50 + shift):(56 + shift)]) 
            O_NH_1_energy = float(l[(57 + shift):(61 + shift)]) 
            NH_O_2_relidx = int(l[(61 + shift):(67 + shift)]) 
            NH_O_2_energy = float(l[(68 + shift):(72 + shift)]) 
            O_NH_2_relidx = int(l[(72 + shift):(78 + shift)]) 
            O_NH_2_energy = float(l[(79 + shift):(83 + shift)]) 
            acc = int(l[(34 + shift):(38 + shift)]) 
            phi = float(l[(103 + shift):(109 + shift)]) 
            psi = float(l[(109 + shift):(115 + shift)]) 
         else: 
            raise ValueError(exc) 
      res_id = ('   ', resseq, icode) 
      dssp[(chainid, res_id)] = (aa, ss, acc, phi, psi, dssp_index, NH_O_1_relidx, NH_O_1_energy, O_NH_1_relidx, O_NH_1_energy, NH_O_2_relidx, NH_O_2_energy, O_NH_2_relidx, O_NH_2_energy) 
      keys.append((chainid, res_id)) 
   return (dssp, keys)", 'Read DSSP file and return dictionary of residue-atom pairs.',"'Internal function used by mask_dssp_dict (PRIVATE). 
 Return a DSSP dictionary that maps (chainid, resid) to an amino acid, 
 secondary structure symbol, solvent accessibility value, and hydrogen bond 
 information (relative dssp indices and hydrogen bond energies) from an open 
 DSSP file object. :: 
 Parameters 
 handle : file 
 the open DSSP output file handle'"
"def null_safe(rule): 
    def null_safe_rl(expr): 
      result = rule(expr) 
      if (result is None): 
         return expr 
      else: 
         return result 
   return null_safe_rl", 'Return a rule that returns None if the rule fails to match.','Return original expr if rule returns None'
"def lucene_search(trans, cntrller, search_term, search_url, **kwd): 
    message = escape(kwd.get('message', '')) 
   status = kwd.get('status', 'done') 
   full_url = ('%s/find?%s' % (search_url, urllib.urlencode({'kwd': search_term}))) 
   response = urllib2.urlopen(full_url) 
   ldda_ids = loads(response.read())['ids'] 
   response.close() 
   lddas = [trans.sa_session.query(trans.app.model.LibraryDatasetDatasetAssociation).get(ldda_id) for ldda_id in ldda_ids] 
   return (status, message, get_sorted_accessible_library_items(trans, cntrller, lddas, 'name'))"," 'Search for library items matching the search term. 
 :param trans: The current transaction. 
 :param cntrller: The current controller. 
 :param search_term: The search term to search for. 
 :param search_url: The URL to search. 
 :param kwd: Keyword arguments to pass to the search. 
 :return: A tuple with the status, message and accessible library items.'",'Return display of results from a full-text lucene search of data libraries.'
"def _generate_graphs(): 
    with gzip.open(ATLAS_FILE, 'rb') as f: 
      line = f.readline() 
      while (line and line.startswith('GRAPH')): 
         graph_index = int(line[6:].rstrip()) 
         line = f.readline() 
         num_nodes = int(line[6:].rstrip()) 
         edgelist = [] 
         line = f.readline() 
         while (line and (not line.startswith('GRAPH'))): 
            edgelist.append(line.rstrip()) 
            line = f.readline() 
         G = nx.Graph() 
         G.name = 'G{}'.format(graph_index) 
         G.add_nodes_from(range(num_nodes)) 
         G.add_edges_from((tuple(map(int, e.split())) for e in edgelist)) 
         (yield G)", 'Reads the ATLAS file and generates a graph for each graph in the file.',"'Sequentially read the file containing the edge list data for the 
 graphs in the atlas and generate the graphs one at a time. 
 This function reads the file given in :data:`.ATLAS_FILE`.'"
"@jinja2.contextfunction 
 @library.global_function 
 def number(context, n): 
    if (n is None): 
      return '' 
   return format_decimal(n, locale=_babel_locale(_contextual_locale(context)))", 'Returns a formatted number.',"'Return the localized representation of an integer or decimal. 
 For None, print nothing.'"
"def split_qexpr_parts(e): 
    expr_part = [] 
   qexpr_part = [] 
   for arg in e.args: 
      if (not isinstance(arg, QExpr)): 
         expr_part.append(arg) 
      else: 
         qexpr_part.append(arg) 
   return (expr_part, qexpr_part)"," 'Split a QExpr into two parts: the arguments and the QExpr itself. 
 This is needed for the case of a QExpr with a nested QExpr, where the 
 nested QExpr is not a part of the arguments.'",'Split an expression into Expr and noncommutative QExpr parts.'
"def test_column_width(): 
    assert_equals(strings.column_width(u'\u3042\u3044\u3046\u3048\u304a'), 10)", 'Test column width','strings.column_width'
"def timefunc(num_tries=1, verbose=True): 
    def real_decorator(function): 
      @wraps(function) 
      def wrapper(*args, **kwargs): 
         ts = time.time() 
         for i in range(num_tries): 
            result = function(*args, **kwargs) 
         te = time.time() 
         tt = ((te - ts) / num_tries) 
         if verbose: 
            log.info(u'{0}   took   {1}   s   on   AVERAGE   for   {2}   call(s).'.format(function.__name__, tt, num_tries)) 
         return (tt, result) 
      return wrapper 
   return real_decorator"," 'Decorator that times a function call. 
 This decorator is used to measure the time taken by a function call 
 on a given set of inputs. 
 Parameters 
 num_tries : int, optional 
 The number of times the function will be called. 
 verbose : bool, optional 
 If True, print the results to the log. 
 Returns 
 A decorator that times a function call. 
 Examples 
 >>> @timefunc 
 ... def test_func(): 
 ...     pass 
 >>> test_func() 
 (0.000000000000000000, None) 
 >>> test_func(num_tries=10) 
 (0.000000000000000000, None)'","'Decorator to time a function or method. 
 Parameters 
 num_tries : int, optional 
 Number of calls to make. Timer will take the 
 average run time. 
 verbose : bool, optional 
 Extra log information. 
 Returns 
 tt : float 
 Average run time in seconds. 
 result 
 Output(s) from the function. 
 Examples 
 To add timer to time `numpy.log` for 100 times with 
 verbose output:: 
 import numpy as np 
 from astropy.utils.timer import timefunc 
 @timefunc(100) 
 def timed_log(x): 
 return np.log(x) 
 To run the decorated function above: 
 >>> t, y = timed_log(100) 
 INFO: timed_log took 9.29832458496e-06 s on AVERAGE for 100 call(s). [...] 
 >>> t 
 9.298324584960938e-06 
 >>> y 
 4.6051701859880918'"
"def pmap_field(key_type, value_type, optional=False, invariant=_valid, initial=_UNDEFINED, factory=None): 
    input_factory = factory 
   class TheMap(CheckedPMap, ): 
      __key_type__ = key_type 
      __value_type__ = value_type 
   TheMap.__name__ = ((key_type.__name__.capitalize() + value_type.__name__.capitalize()) + 'PMap') 
   if optional: 
      def mapping_factory(argument): 
         if (argument is None): 
            return None 
         else: 
            return TheMap(argument) 
   else: 
      mapping_factory = TheMap 
   if input_factory: 
      factory = (lambda x: mapping_factory(input_factory(x))) 
   else: 
      factory = mapping_factory 
   if (initial is _UNDEFINED): 
      initial = TheMap() 
   else: 
      initial = factory(initial) 
   return field(mandatory=True, initial=initial, type=(optional_type(TheMap) if optional else TheMap), factory=factory, invariant=invariant)"," 'A map of a given type. 
 :param key_type: 
 The type of the key. 
 :param value_type: 
 The type of the value. 
 :param optional: 
 If True, the map is optional. 
 :param invariant: 
 A callable that takes a value and returns True if the value is 
 valid, False otherwise. 
 :param initial: 
 The initial value of the map. 
 :param factory: 
 A callable that takes a value and returns a value of the given type. 
 :return: 
 A field of type :py:class:`~.field.Field` that represents a map of the 
 given type. 
 :rtype: 
 :py:class:`~.field.Field`'","'Create a checked ``PMap`` field. 
 :param key: The required type for the keys of the map. 
 :param value: The required type for the values of the map. 
 :param bool optional: If true, ``None`` can be used as a value for this 
 field. 
 :param invariant: Pass-through to ``field``. 
 :param initial: An initial value for the field.  This will first be coerced 
 using the field\'s factory.  If not given, the initial value is an empty 
 map. 
 :param factory: A factory used to convert input arguments to the stored 
 value whenever it is set. Note that this will be composed with the 
 constructor for the ``CheckedPMap`` class constructed for this field. 
 :return: A ``field`` containing a ``CheckedPMap``.'"
"def is_safe_path_component(path): 
    return (path and ('/' not in path) and (path not in ('.', '..')))", 'Returns True if the path component is safe to use.',"'Check if path is a single component of a path. 
 Check that the path is safe to join too.'"
"def upload(): 
    os.system('cd   build/html;   rsync   -avz   .   pandas@pandas.pydata.org:/usr/share/nginx/pandas/pandas-docs/vbench/   -essh')", 'Upload the docs to the web server','push a copy to the site'
"def send_mail(to_addr, mail, mimetype='plain', from_addr=None, mailer=None, username=None, password=None, callback=None, **context): 
    from_addr = (from_addr or settings.FROM_EMAIL) 
   mailer = (mailer or tasks.send_email) 
   subject = mail.subject(**context) 
   message = (mail.text(**context) if (mimetype in ('plain', 'txt')) else mail.html(**context)) 
   ttls = login = (not settings.DEBUG_MODE) 
   logger.debug('Sending   email...') 
   logger.debug(u'To:   {to_addr}\nFrom:   {from_addr}\nSubject:   {subject}\nMessage:   {message}'.format(**locals())) 
   kwargs = dict(from_addr=from_addr, to_addr=to_addr, subject=subject, message=message, mimetype=mimetype, ttls=ttls, login=login, username=username, password=password, categories=mail.categories) 
   if settings.USE_EMAIL: 
      if settings.USE_CELERY: 
         return mailer.apply_async(kwargs=kwargs, link=callback) 
      else: 
         ret = mailer(**kwargs) 
         if callback: 
            callback() 
         return ret"," 'Send an email. 
 :param to_addr: The recipient\'s email address. 
 :param mail: The email to send. 
 :param mimetype: The mimetype of the email (either \'plain\' or \'html\'). 
 :param from_addr: The sender\'s email address. 
 :param mailer: The mailer to use. 
 :param username: The username to use to send the email. 
 :param password: The password to use to send the email. 
 :param callback: A function to call after the email has been sent. 
 :param context: A dictionary of context values to use in the email. 
 :returns: A deferred which is resolved when the email is sent.'","'Send an email from the OSF. 
 Example: :: 
 from website import mails 
 mails.send_email(\'foo@bar.com\', mails.TEST, name=""Foo"") 
 :param str to_addr: The recipient\'s email address 
 :param Mail mail: The mail object 
 :param str mimetype: Either \'plain\' or \'html\' 
 :param function callback: celery task to execute after send_mail completes 
 :param **context: Context vars for the message template 
 .. note: 
 Uses celery if available'"
"def templatize(src): 
    from django.template import Lexer, TOKEN_TEXT, TOKEN_VAR, TOKEN_BLOCK 
   out = StringIO() 
   intrans = False 
   inplural = False 
   singular = [] 
   plural = [] 
   for t in Lexer(src, None).tokenize(): 
      if intrans: 
         if (t.token_type == TOKEN_BLOCK): 
            endbmatch = endblock_re.match(t.contents) 
            pluralmatch = plural_re.match(t.contents) 
            if endbmatch: 
               if inplural: 
                  out.write(('   ngettext(%r,%r,count)   ' % (''.join(singular), ''.join(plural)))) 
                  for part in singular: 
                     out.write(blankout(part, 'S')) 
                  for part in plural: 
                     out.write(blankout(part, 'P')) 
               else: 
                  out.write(('   gettext(%r)   ' % ''.join(singular))) 
                  for part in singular: 
                     out.write(blankout(part, 'S')) 
               intrans = False 
               inplural = False 
               singular = [] 
               plural = [] 
            elif pluralmatch: 
               inplural = True 
            else: 
               raise SyntaxError, ('Translation   blocks   must   not   include   other   block   tags:   %s' % t.contents) 
         elif (t.token_type == TOKEN_VAR): 
            if inplural: 
               plural.append(('%%(%s)s' % t.contents)) 
            else: 
               singular.append(('%%(%s)s' % t.contents)) 
         elif (t.token_type == TOKEN_TEXT): 
            if inplural: 
               plural.append(t.contents) 
            else: 
               singular.append(t.contents) 
      elif (t.token_type == TOKEN_BLOCK): 
         imatch = inline_re.match(t.contents) 
         bmatch = block_re.match(t.contents) 
         cmatches = constant_re.findall(t.contents) 
         if imatch: 
            g = imatch.group(1) 
            if (g[0] == '""'): 
               g = g.strip('""') 
            elif (g[0] == ""'""): 
               g = g.strip(""'"") 
            out.write(('   gettext(%r)   ' % g)) 
         elif bmatch: 
            intrans = True 
            inplural = False 
            singular = [] 
            plural = [] 
         elif cmatches: 
            for cmatch in cmatches: 
               out.write(('   _(%s)   ' % cmatch)) 
         else: 
            out.write(blankout(t.contents, 'B')) 
      elif (t.token_type == TOKEN_VAR): 
         parts = t.contents.split('|') 
         cmatch = constant_re.match(parts[0]) 
         if cmatch: 
            out.write(('   _(%s)   ' % cmatch.group(1))) 
         for p in parts[1:]: 
            if (p.find(':_(') >= 0): 
               out.write(('   %s   ' % p.split(':', 1)[1])) 
            else: 
               out.write(blankout(p, 'F')) 
      else: 
         out.write(blankout(t.contents, 'X')) 
   return out.getvalue()"," 'Templatize a string of translation blocks. 
 This is a convenience function that takes a string of translation 
 blocks and returns a string with all the translation blocks templatized 
 into calls to gettext(). 
 The string of translation blocks can contain any of the following: 
 - Translation blocks (i.e. ``gettext(...)``) 
 - Variable names (i.e. ``_foo``) 
 - Constant names (i.e. ``FOO``) 
 - Text (i.e. ``""foo""``) 
 - Blocks (i.e. ``{% block ... %}``) 
 - Inline tags (i.e. ``{% if ... %}``) 
 - Inline translations (i.e. ``{% trans \""foo\"" %}``) 
 - Inline blocks (i.e. ``{% block ... %}``) 
 - Inline text (i.e. ``{% trans \""foo\"" %}`) 
 - Inline translations (i.e. ``{% trans \""foo\"" %}`) 
 - Inline blocks (i.e. ``{% block ... %}``) 
","'Turns a Django template into something that is understood by xgettext. It 
 does so by translating the Django translation tags into standard gettext 
 function invocations.'"
"def processSVGElementpath(svgReader, xmlElement): 
    if ('d' not in xmlElement.attributeDictionary): 
      print 'Warning,   in   processSVGElementpath   in   svgReader   can   not   get   a   value   for   d   in:' 
      print xmlElement.attributeDictionary 
      return 
   rotatedLoopLayer = svgReader.getRotatedLoopLayer() 
   PathReader(rotatedLoopLayer.loops, xmlElement, svgReader.yAxisPointingUpward)"," 'Process the SVG \'elementpath\' element. 
 This function processes the \'elementpath\' element, which is used to 
 define a path using a set of element references. 
 :param svgReader: the SVG reader. 
 :param xmlElement: the element to process. 
 :returns: None.'",'Process xmlElement by svgReader.'
"def record_usage_multi(prefix_slices): 
    keys = [_make_ratelimit_cache_key(k, t) for (k, t) in prefix_slices] 
   try: 
      now = int(time.time()) 
      for (key, (_, time_slice)) in zip(keys, prefix_slices): 
         g.ratelimitcache.add(key, 0, time=((time_slice.end - now) + 1)) 
      try: 
         recent_usage = g.ratelimitcache.incr_multi(keys) 
      except pylibmc.NotFound: 
         now = int(time.time()) 
         if (now < time_slice.end): 
            recent_usage = [] 
            for (key, (_, time_slice)) in zip(keys, prefix_slices): 
               if g.ratelimitcache.add(key, 1, time=((time_slice.end - now) + 1)): 
                  recent_usage.append(1) 
                  g.stats.simple_event('ratelimit.eviction') 
               else: 
                  recent_usage.append(g.ratelimitcache.get(key)) 
      return recent_usage 
   except pylibmc.Error as e: 
      raise RatelimitError(e)"," 'Record usage for prefix_slices. 
 Returns a list of recent usage for each prefix_slice. 
 If the prefix_slices are not already in the cache, they will be added. 
 If a prefix_slice is already in the cache, its usage will be incremented. 
 If a prefix_slice is not in the cache, it will be added, and its usage 
 will be incremented.'","'Record usage of multiple rate limits. 
 If any of the of the rate limits expire during the processing of the 
 function, the usage counts may be inaccurate and it is not defined 
 which, if any, of the keys have been updated in the underlying cache. 
 Arguments: 
 prefix_slices: A list of (prefix, timeslice) 
 Returns: 
 A list of the usage counts in the same order as prefix_slices. 
 Raises: 
 RateLimitError if anything goes wrong. 
 It is not defined which, if any, of the keys have been updated if this 
 happens.'"
"@command(name='hash', usage='compute   hashes') 
 def print_hash(args): 
    import lixian_hash 
   import lixian_cli_parser 
   lixian_hash.main(lixian_cli_parser.expand_command_line(args))", 'Compute hashes',"'lx hash --sha1 file... 
 lx hash --md5 file... 
 lx hash --md4 file... 
 lx hash --dcid file... 
 lx hash --ed2k file... 
 lx hash --info-hash xxx.torrent... 
 lx hash --verify-sha1 file hash 
 lx hash --verify-md5 file hash 
 lx hash --verify-md4 file hash 
 lx hash --verify-dcid file hash 
 lx hash --verify-ed2k file ed2k://... 
 lx hash --verify-bt file xxx.torrent'"
"def print_environ(environ=os.environ): 
    keys = environ.keys() 
   keys.sort() 
   print 
   print '<H3>Shell   Environment:</H3>' 
   print '<DL>' 
   for key in keys: 
      print '<DT>', escape(key), '<DD>', escape(environ[key]) 
   print '</DL>' 
   print", 'Print the environment as a table.','Dump the shell environment as HTML.'
"def for_signed_dtypes_combination(names=('dtype',), full=None): 
    return for_dtypes_combination(_signed_dtypes, names=names, full=full)", 'Returns a tuple of all signed integer and float dtypes.',"'Decorator for parameterized test w.r.t. the product set of signed dtypes. 
 Args: 
 names(list of str): Argument names to which dtypes are passed. 
 full(bool): If ``True``, then all combinations of dtypes 
 will be tested. 
 Otherwise, the subset of combinations will be tested 
 (see description in :func:`cupy.testing.for_dtypes_combination`). 
 .. seealso:: :func:`cupy.testing.for_dtypes_combination`'"
"def urldecode(query): 
    if (query and (not (set(query) <= urlencoded))): 
      raise ValueError('Invalid   characters   in   query   string.') 
   invalid_hex = u'%[^0-9A-Fa-f]|%[0-9A-Fa-f][^0-9A-Fa-f]' 
   if len(re.findall(invalid_hex, query)): 
      raise ValueError('Invalid   hex   encoding   in   query   string.') 
   query = (query.decode('utf-8') if isinstance(query, str) else query) 
   params = urlparse.parse_qsl(query, keep_blank_values=True) 
   return decode_params_utf8(params)"," 'Decode a query string into a dictionary of key/value pairs. 
 :param query: A string containing the query string. 
 :return: A dictionary of key/value pairs.'","'Decode a query string in x-www-form-urlencoded format into a sequence 
 of two-element tuples. 
 Unlike urlparse.parse_qsl(..., strict_parsing=True) urldecode will enforce 
 correct formatting of the query string by validation. If validation fails 
 a ValueError will be raised. urllib.parse_qsl will only raise errors if 
 any of name-value pairs omits the equals sign.'"
"def test_emacs_cursor_movements(): 
    (result, cli) = _feed_cli_with_input(u'hello\x01X\n') 
   assert (result.text == u'Xhello') 
   (result, cli) = _feed_cli_with_input(u'hello\x01X\x05Y\n') 
   assert (result.text == u'XhelloY') 
   (result, cli) = _feed_cli_with_input(u'hello\x08X\n') 
   assert (result.text == u'hellX') 
   (result, cli) = _feed_cli_with_input(u'hello\x1b[D\x1b[D\x1b[3~\n') 
   assert (result.text == u'helo') 
   (result, cli) = _feed_cli_with_input(u'hello\x1b[DX\n') 
   assert (result.text == u'hellXo') 
   (result, cli) = _feed_cli_with_input(u'hello\x01\x1b[CX\n') 
   assert (result.text == u'hXello') 
   (result, cli) = _feed_cli_with_input(u'hello\x01\x1b[CX\n') 
   assert (result.text == u'hXello') 
   (result, cli) = _feed_cli_with_input(u'hello\x02X\n') 
   assert (result.text == u'hellXo') 
   (result, cli) = _feed_cli_with_input(u'hello\x01\x06X\n') 
   assert (result.text == u'hXello') 
   with pytest.raises(KeyboardInterrupt): 
      (result, cli) = _feed_cli_with_input(u'hello\x03\n') 
      assert (result.text == u'hello') 
   with pytest.raises(EOFError): 
      (result, cli) = _feed_cli_with_input(u'\x04\n') 
      assert (result.text == u'hello') 
   (result, cli) = _feed_cli_with_input(u'hello\x01\x04\n') 
   assert (result.text == u'ello') 
   (result, cli) = _feed_cli_with_input(u'hello\x04\n') 
   assert (result.text == u'hello') 
   (result, cli) = _feed_cli_with_input(u'hello\x1b[D\x1b[D\x0b\n') 
   assert (result.text == u'hel') 
   (result, cli) = _feed_cli_with_input(u'hello\x1b[D\x1b[D\x1b-\x0b\n') 
   assert (result.text == u'lo') 
   (result, cli) = _feed_cli_with_input(u'hello\x0c\n') 
   assert (result.text == u'hello') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x01X\x1b[1;5CY\n') 
   assert (result.text == u'XhelloY   world') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x1b[1;5DY\n') 
   assert (result.text == u'hello   Yworld') 
   (result, cli) = _feed_cli_with_input(u'hello   world   abc   def\x01\x1b3\x1bfX\n') 
   assert (result.text == u'hello   world   abcX   def') 
   (result, cli) = _feed_cli_with_input(u'hello   world   abc   def\x1b-\x1b3\x1bfX\n') 
   assert (result.text == u'hello   Xworld   abc   def') 
   (result, cli) = _feed_cli_with_input(u'hello   world   abc   def\x1b3\x1bbX\n') 
   assert (result.text == u'hello   Xworld   abc   def') 
   (result, cli) = _feed_cli_with_input(u'hello   world   abc   def\x01\x1b-\x1b3\x1bbX\n') 
   assert (result.text == u'hello   world   abc   Xdef') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x17\n') 
   assert (result.text == u'hello   ') 
   assert (cli.clipboard.get_data().text == u'world') 
   (result, cli) = _feed_cli_with_input(u'test   hello   world\x1b2\x17\n') 
   assert (result.text == u'test   ') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x1b\x7f\n') 
   assert (result.text == u'hello   ') 
   assert (cli.clipboard.get_data().text == u'world') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x1b\x08\n') 
   assert (result.text == u'hello   ') 
   assert (cli.clipboard.get_data().text == u'world') 
   (result, cli) = _feed_cli_with_input(u'hello   world\x7f\n') 
   assert (result.text == u'hello   worl') 
   assert (result.cursor_position == len(u'hello   worl')) 
   (result, cli) = _feed_cli_with_input(u'hello   world\x08\n') 
   assert (result.text == u'hello   worl') 
   assert (result.cursor_position == len(u'hello   worl')) 
   (result, cli) = _feed_cli_with_input(u'hello   world\x01\x1b[3~\n') 
   assert (result.text == u'ello   world') 
   assert (result.cursor_position == 0) 
   (result, cli) = _feed_cli_with_input(u'hello               world\x1b8\x02\x1b\\\n') 
   assert (result.text == u'helloworld') 
   assert (result.cursor_position == len(u'hello'))", 'Test cursor movements in emacs mode.','Test cursor movements with Emacs key bindings.'
"def _RecurseOverObject(obj, factory, parent=None): 
    if _IsSudsIterable(obj): 
      copy_of_obj = tuple(obj) 
      for item in copy_of_obj: 
         if _IsSudsIterable(item): 
            if ('xsi_type' in item): 
               if isinstance(obj, tuple): 
                  parent[obj[0]] = _PackForSuds(obj[1], factory) 
               else: 
                  obj.remove(item) 
                  obj.append(_PackForSuds(item, factory)) 
            _RecurseOverObject(item, factory, obj)"," 'Recursively process the given object. 
 If the object is an iterable, recursively process each element of the 
 iterable. 
 :param obj: The object to process. 
 :param factory: The factory to use for packing and unpacking. 
 :param parent: The parent object to use when recursing over an iterable.'","'Recurses over a nested structure to look for changes in Suds objects. 
 Args: 
 obj: A parameter for a SOAP request field which is to be inspected and 
 will be packed for Suds if an xsi_type is specified, otherwise will be 
 left unaltered. 
 factory: The suds.client.Factory object which can create instances of the 
 classes generated from the WSDL. 
 parent: The parent object that contains the obj parameter to be inspected.'"
"def s_byte(value, endian='<', format='binary', signed=False, full_range=False, fuzzable=True, name=None): 
    byte = primitives.byte(value, endian, format, signed, full_range, fuzzable, name) 
   blocks.CURRENT.push(byte)", 'Return a byte from a string.',"'Push a byte onto the current block stack. 
 @see: Aliases: s_char() 
 @type  value:      Integer 
 @param value:      Default integer value 
 @type  endian:     Character 
 @param endian:     (Optional, def=LITTLE_ENDIAN) Endianess of the bit field (LITTLE_ENDIAN: <, BIG_ENDIAN: >) 
 @type  format:     String 
 @param format:     (Optional, def=binary) Output format, ""binary"" or ""ascii"" 
 @type  signed:     Boolean 
 @param signed:     (Optional, def=False) Make size signed vs. unsigned (applicable only with format=""ascii"") 
 @type  full_range: Boolean 
 @param full_range: (Optional, def=False) If enabled the field mutates through *all* possible values. 
 @type  fuzzable:   Boolean 
 @param fuzzable:   (Optional, def=True) Enable/disable fuzzing of this primitive 
 @type  name:       String 
 @param name:       (Optional, def=None) Specifying a name gives you direct access to a primitive'"
"def publish_collection(committer_id, collection_id): 
    _publish_activity(committer_id, collection_id, feconf.ACTIVITY_TYPE_COLLECTION)", 'Publishes the collection with the given id.',"'This is called by the publish_collection_and_update_user_profiles 
 function in collection_services.py. It publishes an collection and commits 
 changes. 
 It is the responsibility of the caller to check that the collection is 
 valid prior to publication.'"
"def _ipconfig_getnode(): 
    import os, re 
   dirs = ['', 'c:\\windows\\system32', 'c:\\winnt\\system32'] 
   try: 
      import ctypes 
      buffer = ctypes.create_string_buffer(300) 
      ctypes.windll.kernel32.GetSystemDirectoryA(buffer, 300) 
      dirs.insert(0, buffer.value.decode('mbcs')) 
   except: 
      pass 
   for dir in dirs: 
      try: 
         pipe = os.popen((os.path.join(dir, 'ipconfig') + '   /all')) 
      except IOError: 
         continue 
      for line in pipe: 
         value = line.split(':')[(-1)].strip().lower() 
         if re.match('([0-9a-f][0-9a-f]-){5}[0-9a-f][0-9a-f]', value): 
            return int(value.replace('-', ''), 16)", 'Returns the IP address of the machine.','Get the hardware address on Windows by running ipconfig.exe.'
"def findTwistedProjects(baseDirectory): 
    projects = [] 
   for filePath in baseDirectory.walk(): 
      if (filePath.basename() == 'topfiles'): 
         projectDirectory = filePath.parent() 
         projects.append(Project(projectDirectory)) 
   return projects", 'Find twisted projects in a directory.',"'Find all Twisted-style projects beneath a base directory. 
 @param baseDirectory: A L{twisted.python.filepath.FilePath} to look inside. 
 @return: A list of L{Project}.'"
"def aes_encrypt(data, expanded_key): 
    rounds = ((len(expanded_key) // BLOCK_SIZE_BYTES) - 1) 
   data = xor(data, expanded_key[:BLOCK_SIZE_BYTES]) 
   for i in range(1, (rounds + 1)): 
      data = sub_bytes(data) 
      data = shift_rows(data) 
      if (i != rounds): 
         data = mix_columns(data) 
      data = xor(data, expanded_key[(i * BLOCK_SIZE_BYTES):((i + 1) * BLOCK_SIZE_BYTES)]) 
   return data"," 'Encrypts a string using AES. 
 :param data: The string to encrypt. 
 :param expanded_key: The expanded key. 
 :return: The encrypted string.'","'Encrypt one block with aes 
 @param {int[]} data          16-Byte state 
 @param {int[]} expanded_key  176/208/240-Byte expanded key 
 @returns {int[]}             16-Byte cipher'"
"def group(seq, size): 
    if (not hasattr(seq, 'next')): 
      seq = iter(seq) 
   while True: 
      (yield [seq.next() for i in xrange(size)])", 'Return a generator that yields groups of size size.','Breaks \'seq\' into a generator of lists with length \'size\'.'
"def test_wraps(): 
    def foo(a, b, c=1, d=2, e=3, **kwargs): 
      u'A   test   function.' 
      return (a, b, c, d, e, kwargs) 
   @wraps(foo) 
   def bar(*args, **kwargs): 
      return ((u'test',) + foo(*args, **kwargs)) 
   expected = (u'test', 1, 2, 3, 4, 5, {u'f': 6, u'g': 7}) 
   assert (bar(1, 2, 3, 4, 5, f=6, g=7) == expected) 
   assert (bar.__name__ == u'foo') 
   if (foo.__doc__ is not None): 
      assert (bar.__doc__ == u'A   test   function.') 
   if hasattr(foo, u'__qualname__'): 
      assert (bar.__qualname__ == foo.__qualname__) 
   if six.PY2: 
      argspec = inspect.getargspec(bar) 
      assert (argspec.keywords == u'kwargs') 
   else: 
      argspec = inspect.getfullargspec(bar) 
      assert (argspec.varkw == u'kwargs') 
   assert (argspec.args == [u'a', u'b', u'c', u'd', u'e']) 
   assert (argspec.defaults == (1, 2, 3))", 'Test that @wraps() works.',"'Tests the compatibility replacement for functools.wraps which supports 
 argument preservation across all supported Python versions.'"
"def agent_settings(name, contact, location, services=None): 
    ret = {'name': name, 'changes': {}, 'comment': str(), 'result': None} 
   ret_settings = {'changes': dict(), 'failures': dict()} 
   if (not services): 
      services = ['None'] 
   services = sorted(set(services)) 
   settings = {'contact': contact, 'location': location, 'services': services} 
   current_settings = __salt__['win_snmp.get_agent_settings']() 
   for setting in settings: 
      if (str(settings[setting]) != str(current_settings[setting])): 
         ret_settings['changes'][setting] = {'old': current_settings[setting], 'new': settings[setting]} 
   if (not ret_settings['changes']): 
      ret['comment'] = 'Agent   settings   already   contain   the   provided   values.' 
      ret['result'] = True 
      return ret 
   elif __opts__['test']: 
      ret['comment'] = 'Agent   settings   will   be   changed.' 
      ret['changes'] = ret_settings 
      return ret 
   __salt__['win_snmp.set_agent_settings'](**settings) 
   new_settings = __salt__['win_snmp.get_agent_settings']() 
   for setting in settings: 
      if (settings[setting] != new_settings[setting]): 
         ret_settings['failures'][setting] = {'old': current_settings[setting], 'new': new_settings[setting]} 
         ret_settings['changes'].pop(setting, None) 
   if ret_settings['failures']: 
      ret['comment'] = 'Some   agent   settings   failed   to   change.' 
      ret['changes'] = ret_settings 
      ret['result'] = False 
   else: 
      ret['comment'] = 'Set   agent   settings   to   contain   the   provided   values.' 
      ret['changes'] = ret_settings['changes'] 
      ret['result'] = True 
   return ret"," 'Set the SNMP agent settings to the provided values. 
 Returns: 
 {name: \'name\', changes: {contact: {old: \'old\', new: \'new\'}, location: {old: \'old\', new: \\'new\'}, services: {old: \'old\', new: \'new\'}, comment: \'comment\', result: \'result\'} } 
 Example: 
 agent_settings(""my_agent"", ""my_contact"", ""my_location"", [""my_service""]) 
 \'my_agent\' will be set to the provided values for the agent settings contact, location, and services. 
 Note: 
 The SNMP agent settings are: 
 - contact 
 - location 
 - services 
 - timeout 
 - version 
 - privacy 
 - privpass 
 - privkey 
 - privkey_pass 
 - privkey_pass_md5 
 - privkey_pass_sha 
 - privkey_pass_sha256 
 - privkey_pass_sha512 
 - privkey_pass_","'Manage the SNMP sysContact, sysLocation, and sysServices settings. 
 :param str contact: The SNMP contact. 
 :param str location: The SNMP location. 
 :param str services: A list of selected services. 
 Example of usage: 
 .. code-block:: yaml 
 snmp-agent-settings: 
 win_snmp.agent_settings: 
 - contact: Test Contact 
 - location: Test Location 
 - services: 
 - Physical 
 - Internet'"
"def displayFile(filename): 
    print (('File   ' + filename) + '   is   being   displayed.') 
   fileText = archive.getFileText(filename) 
   gcodec.writeFileMessageSuffix(filename, displayText(fileText), 'The   gcode   log   file   is   saved   as   ', '_log')", 'Display the file named filename','Parse a gcode file and display the commands.'
"def test_extract_Array_float(): 
    for a in np.arange(2.51, 3.49, 0.1): 
      assert np.all((extract_array(np.arange(5), 3, a) == np.array([2, 3, 4])))", 'Test that we can extract a 1D array from a 2D array.','integer is at bin center'
"def _heapify_max(x): 
    n = len(x) 
   for i in reversed(range((n // 2))): 
      _siftup_max(x, i)"," 'heapify the max-heap, i.e. the max-heap becomes a max-heap'","'Transform list into a maxheap, in-place, in O(len(x)) time.'"
"def curve4_bezier(p1, p2, p3, p4): 
    (x1, y1) = p1 
   (x2, y2) = p2 
   (x3, y3) = p3 
   (x4, y4) = p4 
   points = [] 
   _curve4_recursive_bezier(points, x1, y1, x2, y2, x3, y3, x4, y4) 
   (dx, dy) = ((points[0][0] - x1), (points[0][1] - y1)) 
   if (((dx * dx) + (dy * dy)) > 1e-10): 
      points.insert(0, (x1, y1)) 
   (dx, dy) = ((points[(-1)][0] - x4), (points[(-1)][1] - y4)) 
   if (((dx * dx) + (dy * dy)) > 1e-10): 
      points.append((x4, y4)) 
   return np.array(points).reshape(len(points), 2)"," 'Compute a Bezier curve with four control points. 
 Parameters 
 p1, p2, p3, p4 : 4-tuples of floats 
 Control points of the curve. 
 Returns 
 points : 2-tuple of arrays 
 Control points of the curve. 
 Notes 
 This function is based on the algorithm described in 
 ""A New Algorithm for Computing Bezier Curves"", by 
 A. M. Cohen, Computer Graphics (Proc. SIGGRAPH), 1981.'","'Generate the vertices for a third order Bezier curve. 
 The vertices returned by this function can be passed to a LineVisual or 
 ArrowVisual. 
 Parameters 
 p1 : array 
 2D coordinates of the start point 
 p2 : array 
 2D coordinates of the first curve point 
 p3 : array 
 2D coordinates of the second curve point 
 p4 : array 
 2D coordinates of the end point 
 Returns 
 coords : list 
 Vertices for the Bezier curve. 
 See Also 
 curve3_bezier 
 Notes 
 For more information about Bezier curves please refer to the `Wikipedia`_ 
 page. 
 .. _Wikipedia: https://en.wikipedia.org/wiki/B%C3%A9zier_curve'"
"def num_windows_of_length_M_on_buffers_of_length_N(M, N): 
    return ((N - M) + 1)", 'Return the number of windows of length M on buffers of length N.',"'For a window of length M rolling over a buffer of length N, 
 there are (N - M) + 1 legal windows. 
 Example: 
 If my array has N=4 rows, and I want windows of length M=2, there are 
 3 legal windows: data[0:2], data[1:3], and data[2:4].'"
"def instance_type_access_remove(context, flavor_id, project_id): 
    return IMPL.instance_type_access_remove(context, flavor_id, project_id)", 'Remove flavor access for a project.','Remove flavor access for project.'
"def cram(text, maxlen): 
    if (len(text) > maxlen): 
      pre = max(0, ((maxlen - 3) // 2)) 
      post = max(0, ((maxlen - 3) - pre)) 
      return ((text[:pre] + '...') + text[(len(text) - post):]) 
   return text", 'Return the text with the first and last 3 chars removed.','Omit part of a string if needed to make it fit in a maximum length.'
"def quota_usage_update(context, project_id, resource, **kwargs): 
    return IMPL.quota_usage_update(context, project_id, resource, **kwargs)", 'Update project quota usage.','Update a quota usage or raise if it does not exist.'
"def build_provider_location(system, lun_type, lun_id, base_lun_name, version): 
    location_dict = {'system': system, 'type': lun_type, 'id': six.text_type(lun_id), 'base_lun_name': six.text_type(base_lun_name), 'version': version} 
   return dump_provider_location(location_dict)"," 'Builds a provider location dictionary. 
 :param system: The name of the system. 
 :param lun_type: The type of the lun. 
 :param lun_id: The id of the lun. 
 :param base_lun_name: The name of the base lun. 
 :param version: The version of the lun. 
 :returns: A dictionary containing the information about the lun.'","'Builds provider_location for volume or snapshot. 
 :param system: VNX serial number 
 :param lun_id: LUN ID in VNX 
 :param lun_type: \'lun\' or \'smp\' 
 :param base_lun_name: primary LUN name, 
 it will be used when creating snap lun 
 :param version: driver version'"
"def cr_notification_dispatcher(r, **attr): 
    if ((r.representation == 'html') and (r.name == 'shelter') and r.id and (not r.component)): 
      T = current.T 
      msg = current.msg 
      s3db = current.s3db 
      record = r.record 
      ctable = s3db.pr_contact 
      stable = s3db.cr_shelter 
      message = '' 
      text = '' 
      s_id = record.id 
      s_name = record.name 
      s_phone = record.phone 
      s_email = record.email 
      s_status = record.status 
      if (s_phone in ('', None)): 
         s_phone = T('Not   Defined') 
      if (s_email in ('', None)): 
         s_phone = T('Not   Defined') 
      if (s_status in ('', None)): 
         s_status = T('Not   Defined') 
      elif (s_status == 1): 
         s_status = 'Open' 
      elif (s_status == 2): 
         s_status = 'Close' 
      else: 
         s_status = 'Unassigned   Shelter   Status' 
      text += '************************************************' 
      text += ('\n%s   ' % T('Automatic   Message')) 
      text += ('\n%s:   %s   ' % (T('Shelter   ID'), s_id)) 
      text += ('   %s:   %s' % (T('Shelter   name'), s_name)) 
      text += ('\n%s:   %s   ' % (T('Email'), s_email)) 
      text += ('   %s:   %s' % (T('Phone'), s_phone)) 
      text += ('\n%s:   %s   ' % (T('Working   Status'), s_status)) 
      text += '\n************************************************\n' 
      url = URL(c='cr', f='shelter', args=r.id) 
      opts = dict(type='SMS', subject=T('Deployment   Request'), message=(message + text), url=url) 
      output = msg.compose(**opts) 
      if attr.get('rheader'): 
         rheader = attr['rheader'](r) 
         if rheader: 
            output['rheader'] = rheader 
      output['title'] = T('Send   Notification') 
      current.response.view = 'msg/compose.html' 
      return output 
   else: 
      raise HTTP(501, current.messages.BADMETHOD)"," 'Dispatch a notification to the specified contact. 
 This is for use in the Shelter component. 
 :param r: the current request 
 :param attr: any additional attributes to add to the response 
 :returns: the response to send back to the client'",'Send a notification.'
"def _image_member_format(member_ref): 
    return {'id': member_ref['id'], 'image_id': member_ref['image_id'], 'member': member_ref['member'], 'can_share': member_ref['can_share'], 'status': member_ref['status'], 'created_at': member_ref['created_at'], 'updated_at': member_ref['updated_at'], 'deleted': member_ref['deleted']}", 'Returns a dictionary of member details.','Format a member ref for consumption outside of this module.'
"def emitter(p=0.03): 
    while True: 
      v = np.random.rand(1) 
      if (v > p): 
         (yield 0.0) 
      else: 
         (yield np.random.rand(1))"," 'Emitter emits 0.0 with probability p. 
 The emitter is a special kind of generator that only emits 0.0 with 
 probability p. 
 Parameters 
 p : float 
 The probability of emitting 0.0. 
 Returns 
 generator 
 A generator that emits 0.0 with probability p.'","'return a random value with probability p, else 0'"
"def _policyFileReplaceOrAppend(this_string, policy_data, append_only=False): 
    if (not policy_data): 
      policy_data = '' 
   specialValueRegex = '(\\*\\*Del\\.|\\*\\*DelVals\\.){0,1}' 
   item_key = None 
   item_value_name = None 
   data_to_replace = None 
   if (not append_only): 
      item_key = this_string.split('{0};'.format(chr(0)))[0].lstrip('[') 
      item_value_name = re.sub(specialValueRegex, '', this_string.split('{0};'.format(chr(0)))[1], flags=re.IGNORECASE) 
      log.debug('item   value   name   is   {0}'.format(item_value_name)) 
      data_to_replace = _regexSearchKeyValueCombo(policy_data, item_key, item_value_name) 
   if data_to_replace: 
      log.debug('replacing   {0}   with   {1}'.format([data_to_replace], [this_string])) 
      policy_data = policy_data.replace(data_to_replace, this_string) 
   else: 
      log.debug('appending   {0}'.format([this_string])) 
      policy_data = ''.join([policy_data, this_string]) 
   return policy_data"," 'Replace or append a value from policy data. 
 Args: 
 this_string: The string to be replaced or appended. 
 policy_data: The policy data to be searched. 
 append_only: Whether to append or replace. 
 Returns: 
 The new policy data. 
 Raises: 
 None 
 Notes: 
 If policy_data is empty, this_string is replaced. 
 If policy_data is not empty, this_string is appended.'","'helper function to take a ADMX policy string for registry.pol file data and 
 update existing string or append the string to the data'"
"def emits_warning_on(db, *warnings): 
    spec = db_spec(db) 
   @decorator 
   def decorate(fn, *args, **kw): 
      if isinstance(db, util.string_types): 
         if (not spec(config._current)): 
            return fn(*args, **kw) 
         else: 
            wrapped = emits_warning(*warnings)(fn) 
            return wrapped(*args, **kw) 
      elif (not _is_excluded(*db)): 
         return fn(*args, **kw) 
      else: 
         wrapped = emits_warning(*warnings)(fn) 
         return wrapped(*args, **kw) 
   return decorate"," 'Decorator that emits a warning if the database is not 
 configured. 
 This decorator is used to wrap any function that requires a 
 database connection.  The decorator will emit a warning if the 
 database is not configured.  This is useful when you want to 
 warn the user that they need to configure the database before 
 using your library. 
 :param db: The database connection. 
 :param warnings: The warnings to emit. 
 :return: A decorator that emits a warning if the database is not 
 configured.'","'Mark a test as emitting a warning on a specific dialect. 
 With no arguments, squelches all SAWarning failures.  Or pass one or more 
 strings; these will be matched to the root of the warning description by 
 warnings.filterwarnings().'"
"def uldap(): 
    def construct(): 
      try: 
         secret_file = open('/etc/ldap.secret', 'r') 
         bind_dn = 'cn=admin,{}'.format(base_dn()) 
      except IOError: 
         secret_file = open('/etc/machine.secret', 'r') 
         bind_dn = config_registry()['ldap/hostdn'] 
      pwd_line = secret_file.readline() 
      pwd = re.sub('\n', '', pwd_line) 
      import univention.admin.uldap 
      return univention.admin.uldap.access(host=config_registry()['ldap/master'], base=base_dn(), binddn=bind_dn, bindpw=pwd, start_tls=1) 
   return _singleton('uldap', construct)", 'Returns a singleton of an LDAP connection.','Return a configured univention uldap object'
"def _getTagsWith(s, cont, toClosure=False, maxRes=None): 
    lres = [] 
   bi = s.find(cont) 
   if (bi != (-1)): 
      btag = s[:bi].rfind('<') 
      if (btag != (-1)): 
         if (not toClosure): 
            etag = s[(bi + 1):].find('>') 
            if (etag != (-1)): 
               endidx = ((bi + 2) + etag) 
               lres.append(s[btag:endidx]) 
               if ((maxRes is not None) and (len(lres) >= maxRes)): 
                  return lres 
               lres += _getTagsWith(s[endidx:], cont, toClosure=toClosure) 
         else: 
            spaceidx = s[btag:].find('   ') 
            if (spaceidx != (-1)): 
               ctag = ('</%s>' % s[(btag + 1):(btag + spaceidx)]) 
               closeidx = s[bi:].find(ctag) 
               if (closeidx != (-1)): 
                  endidx = ((bi + closeidx) + len(ctag)) 
                  lres.append(s[btag:endidx]) 
                  if ((maxRes is not None) and (len(lres) >= maxRes)): 
                     return lres 
                  lres += _getTagsWith(s[endidx:], cont, toClosure=toClosure) 
   return lres"," 'Get tags with the given content. 
 :param s: The string to search. 
 :param cont: The content to search for. 
 :param toClosure: If True, the search will be done in the 
 context of the tag. 
 :param maxRes: The maximum number of results to return. 
 :return: The list of tags found. 
 :rtype: list'","'Return the html tags in the \'s\' string containing the \'cont\' 
 string; if toClosure is True, everything between the opening 
 tag and the closing tag is returned.'"
"def digest_auth(realm, get_ha1, key, debug=False): 
    request = cherrypy.serving.request 
   auth_header = request.headers.get('authorization') 
   nonce_is_stale = False 
   if (auth_header is not None): 
      with cherrypy.HTTPError.handle(ValueError, 400, 'The   Authorization   header   could   not   be   parsed.'): 
         auth = HttpDigestAuthorization(auth_header, request.method, debug=debug) 
      if debug: 
         TRACE(str(auth)) 
      if auth.validate_nonce(realm, key): 
         ha1 = get_ha1(realm, auth.username) 
         if (ha1 is not None): 
            digest = auth.request_digest(ha1, entity_body=request.body) 
            if (digest == auth.response): 
               if debug: 
                  TRACE('digest   matches   auth.response') 
               nonce_is_stale = auth.is_nonce_stale(max_age_seconds=600) 
               if (not nonce_is_stale): 
                  request.login = auth.username 
                  if debug: 
                     TRACE(('authentication   of   %s   successful' % auth.username)) 
                  return 
   header = www_authenticate(realm, key, stale=nonce_is_stale) 
   if debug: 
      TRACE(header) 
   cherrypy.serving.response.headers['WWW-Authenticate'] = header 
   raise cherrypy.HTTPError(401, 'You   are   not   authorized   to   access   that   resource')"," 'Digest authentication handler. 
 :param realm: the realm to use. 
 :param get_ha1: a function that takes a username and returns a hash. 
 :param key: the key to use for signing. 
 :param debug: whether to enable debug logging. 
 :returns: None. 
 :raises: HTTPError.'","'A CherryPy tool which hooks at before_handler to perform 
 HTTP Digest Access Authentication, as specified in :rfc:`2617`. 
 If the request has an \'authorization\' header with a \'Digest\' scheme, 
 this tool authenticates the credentials supplied in that header. 
 If the request has no \'authorization\' header, or if it does but the 
 scheme is not ""Digest"", or if authentication fails, the tool sends 
 a 401 response with a \'WWW-Authenticate\' Digest header. 
 realm 
 A string containing the authentication realm. 
 get_ha1 
 A callable which looks up a username in a credentials store 
 and returns the HA1 string, which is defined in the RFC to be 
 MD5(username : realm : password).  The function\'s signature is: 
 ``get_ha1(realm, username)`` 
 where username is obtained from the request\'s \'authorization\' header. 
 If username is not found in the credentials store, get_ha1() returns 
 None. 
 key 
 A secret string known only to the server, used in the synthesis 
 of nonces.'"
"def make_tarball(base_name, base_dir, compress='gzip', verbose=0, dry_run=0): 
    compress_ext = {'gzip': '.gz', 'bzip2': '.bz2', 'compress': '.Z'} 
   compress_flags = {'gzip': ['-f9'], 'compress': ['-f'], 'bzip2': ['-f9']} 
   if ((compress is not None) and (compress not in compress_ext.keys())): 
      raise ValueError, ""bad   value   for   'compress':   must   be   None,   'gzip',   or   'compress'"" 
   archive_name = (base_name + '.tar') 
   mkpath(os.path.dirname(archive_name), dry_run=dry_run) 
   cmd = ['tar', '-cf', archive_name, base_dir] 
   spawn(cmd, dry_run=dry_run) 
   if compress: 
      spawn((([compress] + compress_flags[compress]) + [archive_name]), dry_run=dry_run) 
      return (archive_name + compress_ext[compress]) 
   else: 
      return archive_name"," 'Create a tarball of the given directory. 
 :param base_name: The name of the tarball. 
 :param base_dir: The directory to tar. 
 :param compress: The compression type to use. 
 :param verbose: The verbosity level. 
 :param dry_run: If True, don\'t actually create the tarball, just 
 print the command to create it. 
 :return: The name of the tarball.'","'Create a (possibly compressed) tar file from all the files under 
 \'base_dir\'.  \'compress\' must be ""gzip"" (the default), ""compress"", 
 ""bzip2"", or None.  Both ""tar"" and the compression utility named by 
 \'compress\' must be on the default program search path, so this is 
 probably Unix-specific.  The output tar file will be named \'base_dir\' + 
 "".tar"", possibly plus the appropriate compression extension ("".gz"", 
 "".bz2"" or "".Z"").  Return the output filename.'"
"def TRIX(ds, count, timeperiod=(- (2 ** 31))): 
    return call_talib_with_ds(ds, count, talib.TRIX, timeperiod)"," 'Returns the TRIX indicator for a given data series. 
 The TRIX indicator is a short-term momentum oscillator that is designed to 
 identify overbought and oversold conditions in a market. It is calculated as 
 the 3-day exponential moving average of the 13-day exponential moving average 
 of the price series. 
 Parameters 
 ds : DataSeries 
 The input data series. 
 count : int 
 The number of periods for the exponential moving average. 
 timeperiod : int 
 The number of periods for the TRIX indicator. 
 Returns 
 A DataSeries containing the TRIX indicator. 
 Examples 
 >>> from pylab import plot, show 
 >>> from investingpy.indicators import TRIX 
 >>> data = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20] 
 >>> TRIX(data, 10).plot() 
 >>> plot(data) 
 >>> show() 
 >>> data = [2, 4,",'1-day Rate-Of-Change (ROC) of a Triple Smooth EMA'
"def _acquire_download_cache_lock(): 
    lockdir = os.path.join(_get_download_cache_locs()[0], u'lock') 
   for i in range(conf.download_cache_lock_attempts): 
      try: 
         os.mkdir(lockdir) 
         with open(os.path.join(lockdir, u'pid'), u'w') as f: 
            f.write(str(os.getpid())) 
      except OSError: 
         time.sleep(1) 
      else: 
         return 
   msg = u""Unable   to   acquire   lock   for   cache   directory   ({0}   exists).   You   may   need   to   delete   the   lock   if   the   python   interpreter   wasn't   shut   down   properly."" 
   raise RuntimeError(msg.format(lockdir))", 'Acquire a lock on the download cache directory.',"'Uses the lock directory method.  This is good because `mkdir` is 
 atomic at the system call level, so it\'s thread-safe.'"
"def get_options(select_browser_query): 
    return Select(select_browser_query.first.results[0]).options", 'Returns the options from the select element.','Returns all the options for the given select.'
"def wrap_paragraphs(text, ncols=80): 
    paragraph_re = re.compile('\\n(\\s*\\n)+', re.MULTILINE) 
   text = dedent(text).strip() 
   paragraphs = paragraph_re.split(text)[::2] 
   out_ps = [] 
   indent_re = re.compile('\\n\\s+', re.MULTILINE) 
   for p in paragraphs: 
      if (indent_re.search(p) is None): 
         p = textwrap.fill(p, ncols) 
      out_ps.append(p) 
   return out_ps"," 'Wrap paragraphs into lines of `ncols` characters. 
 :param text: 
 :param ncols: 
 :return: 
 :rtype: 
 :type text: 
 :type ncols: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: 
 :rtype: 
 :type out_ps: ","'Wrap multiple paragraphs to fit a specified width. 
 This is equivalent to textwrap.wrap, but with support for multiple 
 paragraphs, as separated by empty lines. 
 Returns 
 list of complete paragraphs, wrapped to fill `ncols` columns.'"
"def remove_file(path): 
    if os.path.isfile(path): 
      os.remove(path)"," 'Remove the file at the given path. 
 :param path: The path to the file to remove. 
 :return: None'",':type path: str'
"def parse_content(documentation_type, content): 
    try: 
      to_index = PyQuery(content).text() 
   except ValueError: 
      return '' 
   return to_index"," 'Parses the content of a documentation page. 
 :param documentation_type: The documentation type. 
 :param content: The content of the documentation page. 
 :return: The parsed content.'","'Prepare the text of the html file. 
 Returns the body text of a document'"
"@register.tag('blocktrans') 
 def do_block_translate(parser, token): 
    bits = token.split_contents() 
   options = {} 
   remaining_bits = bits[1:] 
   while remaining_bits: 
      option = remaining_bits.pop(0) 
      if (option in options): 
         raise TemplateSyntaxError(('The   %r   option   was   specified   more   than   once.' % option)) 
      if (option == 'with'): 
         value = token_kwargs(remaining_bits, parser, support_legacy=True) 
         if (not value): 
            raise TemplateSyntaxError(('""with""   in   %r   tag   needs   at   least   one   keyword   argument.' % bits[0])) 
      elif (option == 'count'): 
         value = token_kwargs(remaining_bits, parser, support_legacy=True) 
         if (len(value) != 1): 
            raise TemplateSyntaxError(('""count""   in   %r   tag   expected   exactly   one   keyword   argument.' % bits[0])) 
      elif (option == 'context'): 
         try: 
            value = remaining_bits.pop(0) 
            value = parser.compile_filter(value) 
         except Exception: 
            raise TemplateSyntaxError(('""context""   in   %r   tag   expected   exactly   one   argument.' % bits[0])) 
      else: 
         raise TemplateSyntaxError(('Unknown   argument   for   %r   tag:   %r.' % (bits[0], option))) 
      options[option] = value 
   if ('count' in options): 
      (countervar, counter) = options['count'].items()[0] 
   else: 
      (countervar, counter) = (None, None) 
   if ('context' in options): 
      message_context = options['context'] 
   else: 
      message_context = None 
   extra_context = options.get('with', {}) 
   singular = [] 
   plural = [] 
   while parser.tokens: 
      token = parser.next_token() 
      if (token.token_type in (TOKEN_VAR, TOKEN_TEXT)): 
         singular.append(token) 
      else: 
         break 
   if (countervar and counter): 
      if (token.contents.strip() != 'plural'): 
         raise TemplateSyntaxError(""'blocktrans'   doesn't   allow   other   block   tags   inside   it"") 
      while parser.tokens: 
         token = parser.next_token() 
         if (token.token_type in (TOKEN_VAR, TOKEN_TEXT)): 
            plural.append(token) 
         else: 
            break 
   if (token.contents.strip() != 'endblocktrans'): 
      raise TemplateSyntaxError((""'blocktrans'   doesn't   allow   other   block   tags   (seen   %r)   inside   it"" % token.contents)) 
   return BlockTranslateNode(extra_context, singular, plural, countervar, counter, message_context)"," 'Translate a block of text using a given translation function. 
 .. versionadded:: 0.9 
 .. seealso:: 
 :func:`blocktrans` 
 :func:`blocktrans_context` 
 :func:`blocktrans_with` 
 :func:`blocktrans_count` 
 :func:`blocktrans_with_count` 
 :func:`blocktrans_with_context` 
 :func:`blocktrans_with_count_context`'","'This will translate a block of text with parameters. 
 Usage:: 
 {% blocktrans with bar=foo|filter boo=baz|filter %} 
 This is {{ bar }} and {{ boo }}. 
 {% endblocktrans %} 
 Additionally, this supports pluralization:: 
 {% blocktrans count count=var|length %} 
 There is {{ count }} object. 
 {% plural %} 
 There are {{ count }} objects. 
 {% endblocktrans %} 
 This is much like ngettext, only in template syntax. 
 The ""var as value"" legacy format is still supported:: 
 {% blocktrans with foo|filter as bar and baz|filter as boo %} 
 {% blocktrans count var|length as count %} 
 Contextual translations are supported:: 
 {% blocktrans with bar=foo|filter context ""greeting"" %} 
 This is {{ bar }}. 
 {% endblocktrans %} 
 This is equivalent to calling pgettext/npgettext instead of 
 (u)gettext/(u)ngettext.'"
"def setEntryText(entry, value): 
    if (entry == None): 
      return 
   entry.delete(0, Tkinter.END) 
   entry.insert(0, str(value))", 'Set the text of the given entry widget.','Set the entry text.'
"def levene(*args, **kwds): 
    center = 'median' 
   proportiontocut = 0.05 
   for (kw, value) in kwds.items(): 
      if (kw not in ['center', 'proportiontocut']): 
         raise TypeError((""levene()   got   an   unexpected   keyword   argument   '%s'"" % kw)) 
      if (kw == 'center'): 
         center = value 
      else: 
         proportiontocut = value 
   k = len(args) 
   if (k < 2): 
      raise ValueError('Must   enter   at   least   two   input   sample   vectors.') 
   Ni = zeros(k) 
   Yci = zeros(k, 'd') 
   if (center not in ['mean', 'median', 'trimmed']): 
      raise ValueError(""Keyword   argument   <center>   must   be   'mean',   'median'   or   'trimmed'."") 
   if (center == 'median'): 
      func = (lambda x: np.median(x, axis=0)) 
   elif (center == 'mean'): 
      func = (lambda x: np.mean(x, axis=0)) 
   else: 
      args = tuple((stats.trimboth(np.sort(arg), proportiontocut) for arg in args)) 
      func = (lambda x: np.mean(x, axis=0)) 
   for j in range(k): 
      Ni[j] = len(args[j]) 
      Yci[j] = func(args[j]) 
   Ntot = np.sum(Ni, axis=0) 
   Zij = ([None] * k) 
   for i in range(k): 
      Zij[i] = abs((asarray(args[i]) - Yci[i])) 
   Zbari = zeros(k, 'd') 
   Zbar = 0.0 
   for i in range(k): 
      Zbari[i] = np.mean(Zij[i], axis=0) 
      Zbar += (Zbari[i] * Ni[i]) 
   Zbar /= Ntot 
   numer = ((Ntot - k) * np.sum((Ni * ((Zbari - Zbar) ** 2)), axis=0)) 
   dvar = 0.0 
   for i in range(k): 
      dvar += np.sum(((Zij[i] - Zbari[i]) ** 2), axis=0) 
   denom = ((k - 1.0) * dvar) 
   W = (numer / denom) 
   pval = distributions.f.sf(W, (k - 1), (Ntot - k)) 
   return LeveneResult(W, pval)"," 'Levene\'s test of homogeneity of variances. 
 Parameters 
 args : array_like 
 Array of shape (n,) or (n, k). 
 kwds : dict 
 Keyword arguments. 
 center : \'mean\', \'median\', \'trimmed\' 
 The method used to center the input data. 
 proportiontocut : float 
 The proportion of the data to cut off. 
 Returns 
 LeveneResult 
 A LeveneResult object. 
 Notes 
 The Levene test of homogeneity of variances is a test of the null hypothesis 
 that the variances of two or more groups are equal. 
 The test statistic is defined as: 
 .. math:: W = \frac{N_{tot} - k}{k - 1} \frac{\sum_{i=1}^k \sum_{j=1}^k (Z_{ij} - Z_{bar,i})^2}{\sum_{i=1}^k \sum_{j=1}^k (Z_{ij} - Z_{bar,i})^2} 
","'Perform Levene test for equal variances. 
 The Levene test tests the null hypothesis that all input samples 
 are from populations with equal variances.  Levene\'s test is an 
 alternative to Bartlett\'s test `bartlett` in the case where 
 there are significant deviations from normality. 
 Parameters 
 sample1, sample2, ... : array_like 
 The sample data, possibly with different lengths 
 center : {\'mean\', \'median\', \'trimmed\'}, optional 
 Which function of the data to use in the test.  The default 
 is \'median\'. 
 proportiontocut : float, optional 
 When `center` is \'trimmed\', this gives the proportion of data points 
 to cut from each end. (See `scipy.stats.trim_mean`.) 
 Default is 0.05. 
 Returns 
 statistic : float 
 The test statistic. 
 pvalue : float 
 The p-value for the test. 
 Notes 
 Three variations of Levene\'s test are possible.  The possibilities 
 and their recommended usages are: 
 * \'median\' : Recommended for skewed (non-normal) distributions> 
 * \'mean\' : Recommended for symmetric, moderate-tailed distributions. 
 * \'trimmed\' : Recommended for heavy-tailed distributions. 
 References 
 .. [1]  http://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm 
 .. [2]   Levene, H. (1960). In Contributions to Probability and Statistics: 
 Essays in Honor of Harold Hotelling, I. Olkin et al. eds., 
 Stanford University Press, pp. 278-292. 
 .. [3]  Brown, M. B. and Forsythe, A. B. (1974), Journal of the American 
 Statistical Association, 69, 364-367'"
"def createExtraFillLoops(nestedRing, radius, radiusAround, shouldExtraLoopsBeAdded): 
    for innerNestedRing in nestedRing.innerNestedRings: 
      createFillForSurroundings(innerNestedRing.innerNestedRings, radius, radiusAround, shouldExtraLoopsBeAdded) 
   allFillLoops = intercircle.getInsetSeparateLoopsFromAroundLoops(nestedRing.getLoopsToBeFilled(), radius, max((1.4 * radius), radiusAround)) 
   if (len(allFillLoops) < 1): 
      return 
   if shouldExtraLoopsBeAdded: 
      nestedRing.extraLoops += allFillLoops 
      nestedRing.penultimateFillLoops = nestedRing.lastFillLoops 
   nestedRing.lastFillLoops = allFillLoops"," 'Creates extra fill loops for the inner rings of a nested ring. 
 :param nestedRing: The nested ring to add the fill loops to. 
 :type nestedRing: NestedRing 
 :param radius: The radius of the outer ring. 
 :type radius: float 
 :param radiusAround: The radius of the inner rings. 
 :type radiusAround: float 
 :param shouldExtraLoopsBeAdded: Whether to add extra fill loops. 
 :type shouldExtraLoopsBeAdded: bool 
 :return: The extra fill loops added to the nested ring. 
 :rtype: list of NestedRing'",'Create extra fill loops.'
"def phone2numeric(phone): 
    letters = re.compile('[A-PR-Y]', re.I) 
   char2number = (lambda m: {'a': '2', 'c': '2', 'b': '2', 'e': '3', 'd': '3', 'g': '4', 'f': '3', 'i': '4', 'h': '4', 'k': '5', 'j': '5', 'm': '6', 'l': '5', 'o': '6', 'n': '6', 'p': '7', 's': '7', 'r': '7', 'u': '8', 't': '8', 'w': '9', 'v': '8', 'y': '9', 'x': '9'}.get(m.group(0).lower())) 
   return letters.sub(char2number, phone)"," 'Convert a phone number to its numeric representation. 
 :param phone: phone number 
 :type phone: str 
 :return: phone number in numeric representation 
 :rtype: str'",'Converts a phone number with letters into its numeric equivalent.'
"def p_unary_expression_4(t): 
    pass", '4. unary expression','unary_expression : unary_operator cast_expression'
"def ensure_dirs(filename): 
    (dirname, _) = os.path.split(filename) 
   if (dirname and (not os.path.exists(dirname))): 
      os.makedirs(dirname)", 'Ensure that the directory exists for the given filename.','Make sure the directories exist for `filename`.'
"def evaluateRegression(features, labels, nExp, MethodName, Params): 
    (featuresNorm, MEAN, STD) = normalizeFeatures([features]) 
   featuresNorm = featuresNorm[0] 
   nSamples = labels.shape[0] 
   partTrain = 0.9 
   ErrorsAll = [] 
   ErrorsTrainAll = [] 
   ErrorsBaselineAll = [] 
   for (Ci, C) in enumerate(Params): 
      Errors = [] 
      ErrorsTrain = [] 
      ErrorsBaseline = [] 
      for e in range(nExp): 
         randperm = numpy.random.permutation(range(nSamples)) 
         nTrain = int(round((partTrain * nSamples))) 
         featuresTrain = [featuresNorm[randperm[i]] for i in range(nTrain)] 
         featuresTest = [featuresNorm[randperm[(i + nTrain)]] for i in range((nSamples - nTrain))] 
         labelsTrain = [labels[randperm[i]] for i in range(nTrain)] 
         labelsTest = [labels[randperm[(i + nTrain)]] for i in range((nSamples - nTrain))] 
         featuresTrain = numpy.matrix(featuresTrain) 
         if (MethodName == 'svm'): 
            [Classifier, trainError] = trainSVMregression(featuresTrain, labelsTrain, C) 
         ErrorTest = [] 
         ErrorTestBaseline = [] 
         for (itest, fTest) in enumerate(featuresTest): 
            R = regressionWrapper(Classifier, MethodName, fTest) 
            Rbaseline = numpy.mean(labelsTrain) 
            ErrorTest.append(((R - labelsTest[itest]) * (R - labelsTest[itest]))) 
            ErrorTestBaseline.append(((Rbaseline - labelsTest[itest]) * (Rbaseline - labelsTest[itest]))) 
         Error = numpy.array(ErrorTest).mean() 
         ErrorBaseline = numpy.array(ErrorTestBaseline).mean() 
         Errors.append(Error) 
         ErrorsTrain.append(trainError) 
         ErrorsBaseline.append(ErrorBaseline) 
      ErrorsAll.append(numpy.array(Errors).mean()) 
      ErrorsTrainAll.append(numpy.array(ErrorsTrain).mean()) 
      ErrorsBaselineAll.append(numpy.array(ErrorsBaseline).mean()) 
   bestInd = numpy.argmin(ErrorsAll) 
   print '{0:s} DCTB  DCTB {1:s} DCTB  DCTB {2:s} DCTB  DCTB {3:s}'.format('Param', 'MSE', 'T-MSE', 'R-MSE') 
   for i in range(len(ErrorsAll)): 
      print '{0:.4f} DCTB  DCTB {1:.2f} DCTB  DCTB {2:.2f} DCTB  DCTB {3:.2f}'.format(Params[i], ErrorsAll[i], ErrorsTrainAll[i], ErrorsBaselineAll[i]), 
      if (i == bestInd): 
         print ' DCTB  DCTB    best', 
      print 
   return Params[bestInd]"," 'Evaluates the performance of the regression models trained with the DCTB method. 
 The performance is evaluated with the mean squared error (MSE), the mean 
 absolute error (T-MSE) and the root-mean-square error (R-MSE). 
 Parameters 
 features : numpy.ndarray 
 Feature matrix. 
 labels : numpy.ndarray 
 Label matrix. 
 nExp : int 
 Number of experiments. 
 MethodName : str 
 Method name. 
 Params : list 
 List of parameters. 
 Returns 
 Params : list 
 Best parameter. 
 Examples 
 >>> from sklearn.svm import SVC 
 >>> from sklearn.linear_model import LinearRegression 
 >>> from sklearn.datasets import make_regression 
 >>> X, y = make_regression(n_samples=1000, n_features=100, n_informative=50) 
 >>> params = [0.1, 0.5, 1.0, 2.0, 5.0, 10.","'ARGUMENTS: 
 features:     numpy matrices of features [numOfSamples x numOfDimensions] 
 labels:       list of sample labels 
 nExp:         number of cross-validation experiments 
 MethodName:   ""svm"" or ""randomforest"" 
 Params:       list of classifier params to be evaluated 
 RETURNS: 
 bestParam:   the value of the input parameter that optimizes the selected performance measure'"
"def get_record(zone_id, record_id, profile): 
    conn = _get_driver(profile=profile) 
   return conn.get_record(zone_id, record_id)"," 'Returns the record for the specified zone and record ID. 
 :param zone_id: The zone ID. 
 :param record_id: The record ID. 
 :param profile: The profile to use. 
 :returns: The record object.'","'Get record information for the given zone_id on the given profile 
 :param zone_id: Zone to export. 
 :type  zone_id: ``str`` 
 :param record_id: Record to delete. 
 :type  record_id: ``str`` 
 :param profile: The profile key 
 :type  profile: ``str`` 
 CLI Example: 
 .. code-block:: bash 
 salt myminion libcloud_dns.get_record google.com www profile1'"
"def get_restart_power_failure(): 
    ret = salt.utils.mac_utils.execute_return_result('systemsetup   -getrestartpowerfailure') 
   return (salt.utils.mac_utils.validate_enabled(salt.utils.mac_utils.parse_return(ret)) == 'on')", 'Returns whether or not the power failure restart option is enabled.',"'Displays whether \'restart on power failure\' is on or off if supported 
 :return: A string value representing the ""restart on power failure"" settings 
 :rtype: string 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' power.get_restart_power_failure'"
"def _plot_ci_bars(ax, x, central_data, ci, color, err_kws, **kwargs): 
    for (x_i, y_i, (low, high)) in zip(x, central_data, ci.T): 
      ax.plot([x_i, x_i], [low, high], color=color, solid_capstyle='round', **err_kws)"," 'Plot confidence intervals for a set of central data points. 
 Parameters 
 ax : Axes 
 The axes to plot on. 
 x : array_like 
 The x-coordinates of the central data points. 
 central_data : array_like 
 The central data points. 
 ci : array_like 
 The confidence intervals. 
 color : color or sequence of colors 
 The colors to use for the confidence intervals. 
 err_kws : dict 
 Keyword arguments for :func:`matplotlib.lines.Line2D`. 
 kwargs : dict 
 Keyword arguments for :func:`matplotlib.lines.Line2D`.'",'Plot error bars at each data point.'
"def get_hqe_percentage_complete(**filter_data): 
    query = models.HostQueueEntry.query_objects(filter_data) 
   complete_count = query.filter(complete=True).count() 
   total_count = query.count() 
   if (total_count == 0): 
      return 1 
   return (float(complete_count) / total_count)", 'Returns the percentage of HostQueueEntries that are complete.',"'Computes the fraction of host queue entries matching the given filter data 
 that are complete.'"
"def encrypt_stream(mode, in_stream, out_stream, block_size=BLOCK_SIZE, padding=PADDING_DEFAULT): 
    encrypter = Encrypter(mode, padding=padding) 
   _feed_stream(encrypter, in_stream, out_stream, block_size)"," 'Encrypts a stream of bytes using the given mode. 
 :param mode: The encryption mode. 
 :param in_stream: The input stream to be encrypted. 
 :param out_stream: The output stream to be filled with the encrypted 
 data. 
 :param block_size: The size of the blocks in the stream. 
 :param padding: The padding to use. 
 :rtype: None'",'Encrypts a stream of bytes from in_stream to out_stream using mode.'
"def build_query_rep(query, divider=u'   -   '): 
    return divider.join([el[0] for el in query])"," 'Build a representation of a query. 
 :param query: The query. 
 :type query: list of tuples 
 :param divider: The divider to use. 
 :type divider: str 
 :return: The representation of the query. 
 :rtype: str'","'Build a string representation of a query, without metadata types'"
"def parent_dir(path): 
    return os.path.abspath(os.path.join(path, os.pardir))"," 'Returns the parent directory of the given path. 
 :param path: path to the directory. 
 :returns: the parent directory of the given path.'",'Return the parent of a directory.'
"def load_meta_sources(): 
    meta_sources = {} 
   for (module_path, class_name) in SOURCES.items(): 
      module = import_module(((METASYNC_MODULE + '.') + module_path)) 
      meta_sources[class_name.lower()] = getattr(module, class_name) 
   return meta_sources", 'Loads the meta sources.',"'Returns a dictionary of all the MetaSources 
 E.g., {\'itunes\': Itunes} with isinstance(Itunes, MetaSource) true'"
"def _get_resources(context, alias): 
    alias_sql = sqlalchemy.text(u'SELECT   alias_of   FROM   ""_table_metadata""\n                        WHERE   name   =   :alias   AND   alias_of   IS   NOT   NULL') 
   results = context['connection'].execute(alias_sql, alias=alias).fetchall() 
   return [x[0] for x in results]", 'Get all the resources that are associated with the given alias.',"'Get a list of resources for an alias. There could be more than one alias 
 in a resource_dict.'"
"def shell_init_func(): 
    from simplewiki import database 
   wiki = make_wiki() 
   wiki.bind_to_context() 
   return {'wiki': wiki, 'db': database}"," 'This is a function that is executed when the shell is initialized. 
 It is responsible for setting up the wiki and the database.'",'Called on shell initialization.  Adds useful stuff to the namespace.'
"def maybe_download(filename, work_directory): 
    if (not os.path.exists(work_directory)): 
      os.mkdir(work_directory) 
   filepath = os.path.join(work_directory, filename) 
   if (not os.path.exists(filepath)): 
      print('Downloading   MNIST...') 
      (filepath, _) = urllib.request.urlretrieve((SOURCE_URL + filename), filepath) 
      statinfo = os.stat(filepath) 
      print('Succesfully   downloaded', filename, statinfo.st_size, 'bytes.') 
   return filepath"," 'Download a file if it does not exist locally. 
 :param filename: The filename of the file to download 
 :param work_directory: The directory to download the file to 
 :return: The path to the downloaded file'","'Download the data from Yann\'s website, unless it\'s already here.'"
"def set_range_metadata(builder, load, lower_bound, upper_bound): 
    range_operands = [Constant.int(load.type, lower_bound), Constant.int(load.type, upper_bound)] 
   md = builder.module.add_metadata(range_operands) 
   load.set_metadata('range', md)", 'Set the metadata for a range load.',"'Set the ""range"" metadata on a load instruction. 
 Note the interval is in the form [lower_bound, upper_bound).'"
"def get_log_for_pid(pid): 
    found_pid = False 
   pid_str = ('   PID:   %s   ' % pid) 
   for line in fileinput.input(glob.glob((static.DEBUG_FILE + '*'))): 
      if (pid_str in line): 
         (yield line) 
         found_pid = True 
      elif (found_pid and ('   PID:   ' not in line)): 
         (yield line) 
      else: 
         found_pid = False", 'Return a list of lines from the log file that contain the given PID.','Fetches the logs from the debug log file for a given StarCluster run by PID'
"def _layout_figure(params): 
    size = (params['fig'].get_size_inches() * params['fig'].dpi) 
   scroll_width = 25 
   hscroll_dist = 25 
   vscroll_dist = 10 
   l_border = 100 
   r_border = 10 
   t_border = 35 
   b_border = 40 
   if ((size[0] < (2 * scroll_width)) or (size[1] < ((2 * scroll_width) + hscroll_dist))): 
      return 
   scroll_width_x = (scroll_width / size[0]) 
   scroll_width_y = (scroll_width / size[1]) 
   vscroll_dist /= size[0] 
   hscroll_dist /= size[1] 
   l_border /= size[0] 
   r_border /= size[0] 
   t_border /= size[1] 
   b_border /= size[1] 
   ax_width = ((((1.0 - scroll_width_x) - l_border) - r_border) - vscroll_dist) 
   ax_y = ((hscroll_dist + scroll_width_y) + b_border) 
   ax_height = ((1.0 - ax_y) - t_border) 
   pos = [l_border, ax_y, ax_width, ax_height] 
   params['ax'].set_position(pos) 
   if ('ax2' in params): 
      params['ax2'].set_position(pos) 
   params['ax'].set_position(pos) 
   pos = [((ax_width + l_border) + vscroll_dist), ax_y, scroll_width_x, ax_height] 
   params['ax_vscroll'].set_position(pos) 
   pos = [l_border, b_border, ax_width, scroll_width_y] 
   params['ax_hscroll'].set_position(pos) 
   if ('ax_button' in params): 
      pos = [((l_border + ax_width) + vscroll_dist), b_border, scroll_width_x, scroll_width_y] 
      params['ax_button'].set_position(pos) 
   if ('ax_help_button' in params): 
      pos = [((l_border - vscroll_dist) - (scroll_width_x * 2)), b_border, (scroll_width_x * 2), scroll_width_y] 
      params['ax_help_button'].set_position(pos) 
   params['fig'].canvas.draw()"," 'Adjust the figure layout to fit the figure into the available space. 
 Parameters 
 params : dict 
 The figure parameters. 
 Returns 
 None'",'Function for setting figure layout. Shared with raw and epoch plots.'
"@requires_good_network 
 def test_fetch_file_html(): 
    _test_fetch('http://google.com')", 'Test fetching of an HTML file','Test file downloading over http.'
"def hadoop_fs_ls(stdout, stderr, environ, *args): 
    if (mock_hadoop_uses_yarn(environ) and args and (args[0] == '-R')): 
      path_args = args[1:] 
      recursive = True 
   else: 
      path_args = args 
      recursive = False 
   return _hadoop_fs_ls('ls', stdout, stderr, environ, path_args=path_args, recursive=recursive)"," 'A wrapper for the hadoop_fs_ls function that will only return the 
 results of a recursive ls if the first argument is ""-R"". 
 :param stdout: the output stream to write to 
 :param stderr: the error stream to write to 
 :param environ: the environment dictionary 
 :param args: the arguments to pass to the hadoop_fs_ls function'",'Implements hadoop fs -ls.'
"def _indent(s, indent=4): 
    return re.sub('(?m)^(?!$)', (indent * '   '), s)", 'Indent a string',"'Add the given number of space characters to the beginning every 
 non-blank line in `s`, and return the result.'"
"def calibrate_2d_polynomial(cal_pt_cloud, screen_size=(1, 1), threshold=35, binocular=False): 
    model_n = 7 
   if binocular: 
      model_n = 13 
   cal_pt_cloud = np.array(cal_pt_cloud) 
   (cx, cy, err_x, err_y) = fit_poly_surface(cal_pt_cloud, model_n) 
   (err_dist, err_mean, err_rms) = fit_error_screen(err_x, err_y, screen_size) 
   if cal_pt_cloud[(err_dist <= threshold)].shape[0]: 
      (cx, cy, new_err_x, new_err_y) = fit_poly_surface(cal_pt_cloud[(err_dist <= threshold)], model_n) 
      map_fn = make_map_function(cx, cy, model_n) 
      (new_err_dist, new_err_mean, new_err_rms) = fit_error_screen(new_err_x, new_err_y, screen_size) 
      logger.info('first   iteration.   root-mean-square   residuals:   {},   in   pixel'.format(err_rms)) 
      logger.info('second   iteration:   ignoring   outliers.   root-mean-square   residuals:   {}   in   pixel'.format(new_err_rms)) 
      logger.info('used   {}   data   points   out   of   the   full   dataset   {}:   subset   is   {}   percent'.format((cal_pt_cloud[(err_dist <= threshold)].shape[0], cal_pt_cloud.shape[0], ((100 * float(cal_pt_cloud[(err_dist <= threshold)].shape[0])) / cal_pt_cloud.shape[0])))) 
      return (map_fn, (err_dist <= threshold), (cx, cy, model_n)) 
   else: 
      map_fn = make_map_function(cx, cy, model_n) 
      logger.error('First   iteration.   root-mean-square   residuals:   {}   in   pixel,   this   is   bad!'.format(err_rms)) 
      logger.error('The   data   cannot   be   represented   by   the   model   in   a   meaningfull   way.') 
      return (map_fn, (err_dist <= threshold), (cx, cy, model_n))"," 'Fit a polynomial surface to the calibration points, and return the 
 surface map and the residuals. 
 Parameters 
 cal_pt_cloud : (n, 3) array 
 The calibration points. 
 screen_size : (2,) array 
 The screen size. 
 threshold : int 
 The threshold for the residuals. 
 binocular : bool 
 Whether the calibration is done for binocular vision. 
 Returns 
 map_fn : (n, 3) array 
 The surface map. 
 err_dist : (n,) array 
 The residuals. 
 err_mean : (n,) array 
 The mean of the residuals. 
 err_rms : (n,) array 
 The root-mean-square of the residuals. 
 Examples 
 >>> from pycam import calibrate_2d_polynomial, make_map_function 
 >>> from pycam.calibration import fit_error_screen 
 >>> cal_pt_cloud = np.array([[0, 0, 0], [0, 0","'we do a simple two pass fitting to a pair of bi-variate polynomials 
 return the function to map vector'"
"@requires_version('scipy', '0.16') 
 def test_iir_stability(): 
    sig = np.empty(1000) 
   sfreq = 1000 
   assert_raises(RuntimeError, filter_data, sig, sfreq, 0.6, None, method='iir', iir_params=dict(ftype='butter', order=8, output='ba')) 
   filter_data(sig, sfreq, 0.6, None, method='iir', iir_params=dict(ftype='butter', order=8, output='sos')) 
   assert_raises(ValueError, filter_data, sig, sfreq, 0.6, None, method='iir', iir_params=dict(ftype='butter', order=8, output='foo')) 
   assert_raises(RuntimeError, filter_data, sig, sfreq, 0.6, None, method='iir', iir_params=dict(order=8, output='sos')) 
   assert_raises(RuntimeError, filter_data, sig, sfreq, 0.6, None, method='iir', iir_params=dict(order=8, ftype='foo', output='sos')) 
   assert_raises(RuntimeError, filter_data, sig, sfreq, 0.6, None, method='iir', iir_params=dict(gpass=0.5, output='sos')) 
   assert_raises(ValueError, filter_data, sig, sfreq, 0.1, None, method='fft', iir_params=dict(ftype='butter', order=2, output='sos')) 
   assert_raises(TypeError, filter_data, sig, sfreq, 0.1, None, method=1) 
   assert_raises(ValueError, filter_data, sig, sfreq, 0.1, None, method='blah') 
   assert_raises(TypeError, filter_data, sig, sfreq, 0.1, None, method='iir', iir_params='blah') 
   assert_raises(ValueError, filter_data, sig, sfreq, 0.1, None, method='fft', iir_params=dict()) 
   iir_params = dict(ftype='butter', order=2, output='sos') 
   x_sos = filter_data(sig, 250, 0.5, None, method='iir', iir_params=iir_params) 
   iir_params_sos = construct_iir_filter(iir_params, f_pass=0.5, sfreq=250, btype='highpass') 
   x_sos_2 = filter_data(sig, 250, 0.5, None, method='iir', iir_params=iir_params_sos) 
   assert_allclose(x_sos[100:(-100)], x_sos_2[100:(-100)]) 
   x_ba = filter_data(sig, 250, 0.5, None, method='iir', iir_params=dict(ftype='butter', order=2, output='ba')) 
   assert_allclose(x_sos[100:(-100)], x_ba[100:(-100)])", 'Test that iir filter construction works correctly.','Test IIR filter stability check.'
"def safe_extra(extra): 
    return re.sub('[^A-Za-z0-9.-]+', '_', extra).lower()"," 'Returns a safe version of the extra string, by removing invalid characters.'","'Convert an arbitrary string to a standard \'extra\' name 
 Any runs of non-alphanumeric characters are replaced with a single \'_\', 
 and the result is always lowercased.'"
"def readSettingsFromText(repository, text): 
    lines = archive.getTextLines(text) 
   shortDictionary = {} 
   for setting in repository.preferences: 
      shortDictionary[getShortestUniqueSettingName(setting.name, repository.preferences)] = setting 
   for lineIndex in xrange(len(lines)): 
      setRepositoryToLine(lineIndex, lines, shortDictionary)"," 'Reads settings from text file. 
 :param repository: The repository to read the settings for. 
 :param text: The text file containing the settings. 
 :return: The settings in the repository.'",'Read settings from a text.'
"def versions_report(): 
    return '\n'.join(salt.version.versions_report())", 'Return a list of the versions of the installed Salt packages',"'Returns versions of components used by salt 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' test.versions_report'"
"def runCalibration(eyegaze_control): 
    import subprocess, time 
   result = pEyeGaze.EgExit(byref(eyegaze_control)) 
   eyegaze_control = None 
   p = subprocess.Popen(('calibrate.exe', '')) 
   while (p.poll() is None): 
      time.sleep(0.05) 
   return initEyeGaze()", 'Run the calibration program and return the EyeGaze instance.',"'Function to run the external calibrate.exe program. 
 Returns a new instance of _stEgControl (probably not \'necessary\').'"
"def run_mantel_correlogram(fps, distmats, num_perms, comment, alpha, sample_id_map=None, variable_size_distance_classes=False): 
    if (len(fps) != len(distmats)): 
      raise ValueError('Must   provide   the   same   number   of   filepaths   as   there   are   distance   matrices.') 
   if (comment is None): 
      comment = '' 
   result = ((((comment + 'DM1 DCTB DM2 DCTB Number   of   entries DCTB ') + 'Number   of   permutations DCTB Class   index DCTB ') + 'Number   of   distances DCTB Mantel   r   statistic DCTB ') + 'p-value DCTB p-value   (Bonferroni   corrected) DCTB Tail   type\n') 
   correlogram_fps = [] 
   correlograms = [] 
   for (i, (fp1, (dm1_labels, dm1_data))) in enumerate(zip(fps, distmats)): 
      for (fp2, (dm2_labels, dm2_data)) in zip(fps, distmats)[(i + 1):]: 
         ((dm1_labels, dm1_data), (dm2_labels, dm2_data)) = make_compatible_distance_matrices((dm1_labels, dm1_data), (dm2_labels, dm2_data), lookup=sample_id_map) 
         if (len(dm1_labels) < 3): 
            result += ('%s DCTB %s DCTB %d DCTB Too   few   samples\n' % (fp1, fp2, len(dm1_labels))) 
            continue 
         dm1 = DistanceMatrix(dm1_data, dm1_labels) 
         dm2 = DistanceMatrix(dm2_data, dm2_labels) 
         mc = MantelCorrelogram(dm1, dm2, alpha=alpha, variable_size_distance_classes=variable_size_distance_classes) 
         results = mc(num_perms) 
         dm1_name = path.basename(fp1) 
         dm2_name = path.basename(fp2) 
         correlogram_fps.append(('_'.join((dm1_name, 'AND', dm2_name, 'mantel_correlogram')) + '.')) 
         correlograms.append(results['correlogram_plot']) 
         first_time = True 
         for (class_idx, num_dist, r, p, p_corr) in zip(results['class_index'], results['num_dist'], results['mantel_r'], results['mantel_p'], results['mantel_p_corr']): 
            p_str = None 
            if (p is not None): 
               p_str = p_value_to_str(p, num_perms) 
            p_corr_str = None 
            if (p_corr is not None): 
               p_corr_str = p_value_to_str(p_corr, num_perms) 
            if (r is None): 
               tail_type = None 
            elif (r < 0): 
               tail_type = 'less' 
            else: 
               tail_type = 'greater' 
            if first_time: 
               result += ('%s DCTB %s DCTB %d DCTB %d DCTB %s DCTB %d DCTB %s DCTB %s DCTB %s DCTB %s\n' % (fp1, fp2, len(dm1_labels), num_perms, class_idx, num_dist, r, p_str, p_corr_str, tail_type)) 
               first_time = False 
            else: 
               result += (' DCTB  DCTB  DCTB  DCTB %s DCTB %d DCTB %s DCTB %s DCTB %s DCTB %s\n' % (class_idx, num_dist, r, p_str, p_corr_str, tail_type)) 
   return (result, correlogram_fps, correlograms)"," 'Runs the Mantel Correlogram analysis on a list of distance matrices 
 and returns a list of the results. 
 Parameters 
 fps : list of str 
 A list of filepaths to distance matrices. 
 distmats : list of DistanceMatrix 
 A list of distance matrices. 
 num_perms : int 
 The number of permutations to run the Mantel Correlogram on. 
 comment : str 
 A string to append to the output. 
 alpha : float 
 The alpha value for the Mantel Correlogram. 
 sample_id_map : dict 
 A dictionary of sample ids to filepaths. 
 variable_size_distance_classes : bool 
 Whether or not to use variable size distance classes. 
 Returns 
 result : str 
 A string containing the results of the Mantel Correlogram analysis. 
 correlogram_fps : list of str 
 A list of filepaths to the correlogram plots. 
 correlograms : list of str 
 A list of strings containing the correlogram plots. 
 Notes 
 The Mantel Corre","'Runs a Mantel correlogram analysis on all pairs of distance matrices. 
 Returns a string suitable for writing out to a file containing the results 
 of the test, a list of correlogram filepath names, and a list of matplotlib 
 Figure objects representing each correlogram. 
 The correlogram filepaths can have an extension string appended to the end 
 of them and then be used to save each of the correlogram Figures to a file. 
 Each correlogram filepath will be a combination of the two distance matrix 
 filepaths that were used to create it. 
 WARNING: Only symmetric, hollow distance matrices may be used as input. 
 Asymmetric distance matrices, such as those obtained by the UniFrac Gain 
 metric (i.e. beta_diversity.py -m unifrac_g), should not be used as input. 
 Arguments: 
 fps - list of filepaths of the distance matrices 
 distmats - list of tuples containing dm labels and dm data (i.e. the 
 output of parse_distmat) 
 num_perms - the number of permutations to use to calculate the 
 p-value(s) 
 comment - comment string to add to the beginning of the results string 
 alpha - the alpha value to use to determine significance in the 
 correlogram plots 
 sample_id_map - dict mapping sample IDs (i.e. what is expected by 
 make_compatible_distance_matrices) 
 variable_size_distance_classes - create distance classes that vary in 
 size (i.e. width) but have the same number of distances in each 
 class'"
"def launch(dpid, port, port_eth=None, name=None, __INSTANCE__=None): 
    if (port_eth in (True, None)): 
      pass 
   else: 
      port_eth = EthAddr(port_eth) 
   dpid = str_to_dpid(dpid) 
   try: 
      port = int(port) 
   except: 
      pass 
   def dhcpclient_init(): 
      n = name 
      if (n is None): 
         s = '' 
         while True: 
            if (not core.hasComponent(('DHCPClient' + s))): 
               n = ('DHCPClient' + s) 
               break 
            s = str((int(('0' + s)) + 1)) 
      elif core.hasComponent(n): 
         self.log.error('Already   have   component   %s', n) 
         return 
      client = DHCPClient(port=port, dpid=dpid, name=n, port_eth=port_eth) 
      core.register(n, client) 
   core.call_when_ready(dhcpclient_init, ['openflow'])"," 'Launch a DHCP client on the specified port. 
 :param dpid: DPID of the port 
 :param port: Port number to launch the DHCP client on 
 :param port_eth: The ethernet address of the port 
 :param name: Name of the component 
 :param __INSTANCE__: The instance name (defaults to \'openflow\')'","'Launch 
 port_eth unspecified: ""DPID MAC"" 
 port_eth enabled: Port MAC 
 port_eth specified: Use that'"
"def __validate__(config): 
    if (not isinstance(config, dict)): 
      return (False, 'Configuration   for   btmp   beacon   must   be   a   list   of   dictionaries.') 
   return (True, 'Valid   beacon   configuration')", 'Validate the configuration for the btmp beacon.','Validate the beacon configuration'
"def heatmap(data, vmin=None, vmax=None, cmap=None, center=None, robust=False, annot=None, fmt='.2g', annot_kws=None, linewidths=0, linecolor='white', cbar=True, cbar_kws=None, cbar_ax=None, square=False, ax=None, xticklabels=True, yticklabels=True, mask=None, **kwargs): 
    plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask) 
   kwargs['linewidths'] = linewidths 
   kwargs['edgecolor'] = linecolor 
   if (ax is None): 
      ax = plt.gca() 
   if square: 
      ax.set_aspect('equal') 
   plotter.plot(ax, cbar_ax, kwargs) 
   return ax"," 'Plot a heatmap. 
 Parameters 
 data : array-like 
 Data to be plotted. 
 vmin : float, optional 
 Minimum value of the color scale. 
 vmax : float, optional 
 Maximum value of the color scale. 
 cmap : str or function, optional 
 Color map to use. If a function, it should take the data as input and 
 return a map from data values to colors. If a string, it should be the 
 name of a matplotlib colormap. 
 center : float, optional 
 Center of the color map. 
 robust : bool, optional 
 If True, use a robust estimation of the density. 
 annot : dict, optional 
 Annotations to be displayed. 
 fmt : str, optional 
 Format string for the x-axis. 
 annot_kws : dict, optional 
 Keyword arguments to be passed to matplotlib.text. 
 cbar : bool, optional 
 If True, a colorbar will be displayed. 
 cbar_kws : dict, optional 
 Keyword arguments to be passed to matplotlib.colorbar. ","'Plot rectangular data as a color-encoded matrix. 
 This function tries to infer a good colormap to use from the data, but 
 this is not guaranteed to work, so take care to make sure the kind of 
 colormap (sequential or diverging) and its limits are appropriate. 
 This is an Axes-level function and will draw the heatmap into the 
 currently-active Axes if none is provided to the ``ax`` argument.  Part of 
 this Axes space will be taken and used to plot a colormap, unless ``cbar`` 
 is False or a separate Axes is provided to ``cbar_ax``. 
 Parameters 
 data : rectangular dataset 
 2D dataset that can be coerced into an ndarray. If a Pandas DataFrame 
 is provided, the index/column information will be used to label the 
 columns and rows. 
 vmin, vmax : floats, optional 
 Values to anchor the colormap, otherwise they are inferred from the 
 data and other keyword arguments. When a diverging dataset is inferred, 
 one of these values may be ignored. 
 cmap : matplotlib colormap name or object, optional 
 The mapping from data values to color space. If not provided, this 
 will be either a cubehelix map (if the function infers a sequential 
 dataset) or ``RdBu_r`` (if the function infers a diverging dataset). 
 center : float, optional 
 The value at which to center the colormap. Passing this value implies 
 use of a diverging colormap. 
 robust : bool, optional 
 If True and ``vmin`` or ``vmax`` are absent, the colormap range is 
 computed with robust quantiles instead of the extreme values. 
 annot : bool or rectangular dataset, optional 
 If True, write the data value in each cell. If an array-like with the 
 same shape as ``data``, then use this to annotate the heatmap instead 
 of the raw data. 
 fmt : string, optional 
 String formatting code to use when adding annotations. 
 annot_kws : dict of key, value mappings, optional 
 Keyword arguments for ``ax.text`` when ``annot`` is True. 
 linewidths : float, optional 
 Width of the lines that will divide each cell. 
 linecolor : color, optional 
 Color of the lines that will divide each cell. 
 cbar : boolean, optional 
 Whether to draw a colorbar. 
 cbar_kws : dict of key, value mappings, optional 
 Keyword arguments for `fig.colorbar`. 
 cbar_ax : matplotlib Axes, optional 
 Axes in which to draw the colorbar, otherwise take space from the 
 main Axes. 
 square : boolean, optional 
 If True, set the Axes aspect to ""equal"" so each cell will be 
 square-shaped. 
 ax : matplotlib Axes, optional 
 Axes in which to draw the plot, otherwise use the currently-active 
 Axes. 
 xticklabels : list-like, int, or bool, optional 
 If True, plot the column names of the dataframe. If False, don\'t plot 
 the column names. If list-like, plot these alternate labels as the 
 xticklabels. If an integer, use the column names but plot only every 
 n label. 
 yticklabels : list-like, int, or bool, optional 
 If True, plot the row names of the dataframe. If False, don\'t plot 
 the row names. If list-like, plot these alternate labels as the 
 yticklabels. If an integer, use the index names but plot only every 
 n label. 
 mask : boolean array or DataFrame, optional 
 If passed, data will not be shown in cells where ``mask`` is True. 
 Cells with missing values are automatically masked. 
 kwargs : other keyword arguments 
 All other keyword arguments are passed to ``ax.pcolormesh``. 
 Returns 
 ax : matplotlib Axes 
 Axes object with the heatmap. 
 Examples 
 Plot a heatmap for a numpy array: 
 .. plot:: 
 :context: close-figs 
 >>> import numpy as np; np.random.seed(0) 
 >>> import seaborn as sns; sns.set() 
 >>> uniform_data = np.random.rand(10, 12) 
 >>> ax = sns.heatmap(uniform_data) 
 Change the limits of the colormap: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(uniform_data, vmin=0, vmax=1) 
 Plot a heatmap for data centered on 0: 
 .. plot:: 
 :context: close-figs 
 >>> normal_data = np.random.randn(10, 12) 
 >>> ax = sns.heatmap(normal_data) 
 Plot a dataframe with meaningful row and column labels: 
 .. plot:: 
 :context: close-figs 
 >>> flights = sns.load_dataset(""flights"") 
 >>> flights = flights.pivot(""month"", ""year"", ""passengers"") 
 >>> ax = sns.heatmap(flights) 
 Annotate each cell with the numeric value using integer formatting: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(flights, annot=True, fmt=""d"") 
 Add lines between each cell: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(flights, linewidths=.5) 
 Use a different colormap: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(flights, cmap=""YlGnBu"") 
 Center the colormap at a specific value: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(flights, center=flights.loc[""January"", 1955]) 
 Plot every other column label and don\'t plot row labels: 
 .. plot:: 
 :context: close-figs 
 >>> data = np.random.randn(50, 20) 
 >>> ax = sns.heatmap(data, xticklabels=2, yticklabels=False) 
 Don\'t draw a colorbar: 
 .. plot:: 
 :context: close-figs 
 >>> ax = sns.heatmap(flights, cbar=False) 
 Use different axes for the colorbar: 
 .. plot:: 
 :context: close-figs 
 >>> grid_kws = {""height_ratios"": (.9, .05), ""hspace"": .3} 
 >>> f, (ax, cbar_ax) = plt.subplots(2, gridspec_kw=grid_kws) 
 >>> ax = sns.heatmap(flights, ax=ax, 
 ...                  cbar_ax=cbar_ax, 
 ...                  cbar_kws={""orientation"": ""horizontal""}) 
 Use a mask to plot only part of a matrix 
 .. plot:: 
 :context: close-figs 
 >>> corr = np.corrcoef(np.random.randn(10, 200)) 
 >>> mask = np.zeros_like(corr) 
 >>> mask[np.triu_indices_from(mask)] = True 
 >>> with sns.axes_style(""white""): 
 ...     ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True)'"
"def cached_property(fun): 
    @wraps(fun) 
   def get(self): 
      try: 
         return self._cache[fun] 
      except AttributeError: 
         self._cache = {} 
      except KeyError: 
         pass 
      ret = self._cache[fun] = fun(self) 
      return ret 
   return property(get)"," 'Decorator for caching a property. 
 This decorator makes the property lazy, so that the property is only 
 calculated once, and then the result is cached. 
 This is useful for properties that are expensive to calculate, but 
 rarely change. 
 Note that the property is only calculated once per instance. 
 If you want to calculate the property on every access, use the 
 :meth:`~cachedproperty.property_cached` decorator instead. 
 :param fun: The function to wrap. 
 :type fun: function 
 :param name: The name of the property. 
 :type name: str 
 :param doc: The documentation string for the property. 
 :type doc: str 
 :param doc: The name of the property. 
 :type doc: str 
 :param doc: The docstring for the property. 
 :type doc: str 
 :param doc: The docstring for the property. 
 :type doc: str 
 :param doc: The docstring for the property. 
 :type doc: str 
 :param doc: The docstring for the property. 
 :",'A memoize decorator for class properties.'
"def buggy_mkl_svd(function): 
    @wraps(function) 
   def dec(*args, **kwargs): 
      try: 
         return function(*args, **kwargs) 
      except np.linalg.LinAlgError as exp: 
         if ('SVD   did   not   converge' in str(exp)): 
            from nose.plugins.skip import SkipTest 
            msg = 'Intel   MKL   SVD   convergence   error   detected,   skipping   test' 
            warn(msg) 
            raise SkipTest(msg) 
         raise 
   return dec", 'Decorator that skips test if MKL SVD fails','Decorator for tests that make calls to SVD and intermittently fail.'
"def _check_all_tasks(tasks): 
    running_tasks_data = [] 
   for task in tasks: 
      if task.isAlive(): 
         running_tasks_data.append(('      %s   (started   %s)' % (task.name, time.strftime('%H:%M:%S', time.localtime(task.start_time))))) 
      if task.exception: 
         ALL_ERRORS.append(task.exception) 
   if running_tasks_data: 
      log('----------------------------------------') 
      log('Tasks   still   running:') 
      for task_details in running_tasks_data: 
         log(task_details)", 'Checks if all tasks are finished.','Checks the results of all tasks.'
"def _option(value): 
    if (value in __opts__): 
      return __opts__[value] 
   master_opts = __pillar__.get('master', {}) 
   if (value in master_opts): 
      return master_opts[value] 
   if (value in __pillar__): 
      return __pillar__[value]"," 'Return the value of a pillar option. 
 :param value: 
 :return: 
 :rtype: 
 :raises: 
 :type: 
 :param value: 
 :return: 
 :rtype: 
 :raises: 
 :type: 
 :param value: 
 :return: 
 :rtype: 
 :raises: 
 :type: 
 :param value: 
 :return: 
 :rtype: 
 :raises: 
 :type: 
 :param value: 
 :return: 
 :rtype: 
 :raises: 
 :type: 
 :param value: 
 :return: 
 :rtype: 
 :raises: 
 :type: 
 :param value: 
 :return: 
 :rtype: 
 :raises: 
 :type: 
 :param value: 
 :return: 
 :rtype: 
 :raises: 
 :type: 
 :param value: 
 :return: 
 :rtype: 
 :raises",'Look up the value for an option.'
"def socket_pair(): 
    port = socket() 
   port.bind(('', 0)) 
   port.listen(1) 
   client = socket() 
   client.setblocking(False) 
   client.connect_ex(('127.0.0.1', port.getsockname()[1])) 
   client.setblocking(True) 
   server = port.accept()[0] 
   server.send(b('x')) 
   assert (client.recv(1024) == b('x')) 
   client.send(b('y')) 
   assert (server.recv(1024) == b('y')) 
   server.setblocking(False) 
   client.setblocking(False) 
   return (server, client)"," 'Create a socket pair for testing. 
 The server and client sockets will be bound to the same port, 
 and the server socket will be listening for connections. 
 The server socket will be blocking, while the client socket will 
 be non-blocking. 
 The server will send a message to the client, and the client will 
 receive it. 
 The server will then send a message to the client, and the client 
 will receive it. 
 The client will then send a message to the server, and the server 
 will receive it. 
 The server will then send a message to the client, and the client 
 will receive it. 
 The server will then close the connection, and the client will 
 receive the close event.'",'Establish and return a pair of network sockets connected to each other.'
"@manager.command 
 def deploy(): 
    from flask_migrate import upgrade 
   from app.models import Role, User 
   upgrade() 
   Role.insert_roles() 
   User.add_self_follows()", 'Deploy the app.','Run deployment tasks.'
"def split_statements(hql): 
    statements = [] 
   current = '' 
   prev = '' 
   between_quotes = None 
   is_comment = None 
   start_row = 0 
   start_col = 0 
   end_row = 0 
   end_col = (len(hql) - 1) 
   if (hql.find(';') in ((-1), (len(hql) - 1))): 
      return [((start_row, start_col), (end_row, end_col), hql)] 
   lines = hql.splitlines() 
   for (row, line) in enumerate(lines): 
      end_col = 0 
      end_row = row 
      if ((start_row == row) and (line.strip() == '')): 
         start_row += 1 
      elif (current.strip() == ''): 
         start_row = row 
         start_col = 0 
      for (col, c) in enumerate(line): 
         current += c 
         if ((c in ('""', ""'"")) and (prev != '\\') and (is_comment is None)): 
            if (between_quotes == c): 
               between_quotes = None 
            elif (between_quotes is None): 
               between_quotes = c 
         elif ((c == '-') and (prev == '-') and (between_quotes is None) and (is_comment is None)): 
            is_comment = True 
         elif (c == ';'): 
            if ((between_quotes is None) and (is_comment is None)): 
               current = current.strip() 
               current = current[:(-1)] 
               if (len(current) > 1): 
                  statements.append(((start_row, start_col), (row, (col + 1)), current)) 
                  start_col = (col + 1) 
               current = '' 
         if ((prev == '\\') and (between_quotes is not None)): 
            c = '' 
         prev = c 
         end_col = col 
      is_comment = None 
      prev = os.linesep 
      if (current != ''): 
         current += os.linesep 
   if (current and (current != ';')): 
      current = current.strip() 
      statements.append(((start_row, start_col), (end_row, (end_col + 1)), current)) 
   return statements"," 'Split a HQL statement into a list of Statement objects. 
 :param hql: The HQL statement to split. 
 :return: A list of Statement objects. 
 :rtype: list(Statement)'","'Split statements at semicolons ignoring the ones inside quotes and comments. 
 The comment symbols that come inside quotes should be ignored.'"
"def _toUTF8(data, encoding): 
    if ((len(data) >= 4) and (data[:2] == _l2bytes([254, 255])) and (data[2:4] != _l2bytes([0, 0]))): 
      encoding = 'utf-16be' 
      data = data[2:] 
   elif ((len(data) >= 4) and (data[:2] == _l2bytes([255, 254])) and (data[2:4] != _l2bytes([0, 0]))): 
      encoding = 'utf-16le' 
      data = data[2:] 
   elif (data[:3] == _l2bytes([239, 187, 191])): 
      encoding = 'utf-8' 
      data = data[3:] 
   elif (data[:4] == _l2bytes([0, 0, 254, 255])): 
      encoding = 'utf-32be' 
      data = data[4:] 
   elif (data[:4] == _l2bytes([255, 254, 0, 0])): 
      encoding = 'utf-32le' 
      data = data[4:] 
   newdata = unicode(data, encoding) 
   declmatch = re.compile('^<\\?xml[^>]*?>') 
   newdecl = ""<?xml   version='1.0'   encoding='utf-8'?>"" 
   if declmatch.search(newdata): 
      newdata = declmatch.sub(newdecl, newdata) 
   else: 
      newdata = ((newdecl + u'\n') + newdata) 
   return newdata.encode('utf-8')"," 'Convert a string to utf-8. 
 If the string is already utf-8, it is returned unchanged. 
 If the string is a byte string, it is converted to utf-8. 
 If the string is a unicode string, it is converted to utf-8. 
 If the string is a byte string containing a byte sequence that is 
 not valid utf-8, it is converted to utf-8. 
 If the string is a byte string containing a byte sequence that is 
 valid utf-8, but the encoding is not utf-8, it is converted to utf-8. 
 If the string is a unicode string containing a byte sequence that is 
 not valid utf-8, it is converted to utf-8. 
 If the string is a unicode string containing a byte sequence that is 
 valid utf-8, but the encoding is not utf-8, it is converted to utf-8. 
 If the string is a byte string containing a byte sequence that is 
 utf-16, it is converted to utf-8. 
 If the string is a byte string containing a byte sequence that is 
 utf-16, but the","'Changes an XML data stream on the fly to specify a new encoding 
 data is a raw sequence of bytes (not Unicode) that is presumed to be in %encoding already 
 encoding is a string recognized by encodings.aliases'"
"def cbServerGreeting(proto, username, password): 
    tp = TrivialPrompter() 
   stdio.StandardIO(tp) 
   proto.prompt = tp.prompt 
   proto.display = tp.display 
   return proto.authenticate(password).addCallback(cbAuthentication, proto).addErrback(ebAuthentication, proto, username, password)"," 'Provide a greeting to the client. 
 :param proto: The protocol to greet. 
 :param username: The username to greet. 
 :param password: The password to greet. 
 :rtype: defer.Deferred'",'Initial callback - invoked after the server sends us its greet message.'
"def Ql(filter_, thing): 
    res = Q(filter_, thing) 
   if isinstance(filter_, type({})): 
      for k in res: 
         res[k] = list(res[k]) 
      return res 
   else: 
      return list(res)"," 'Returns a list of all the things that satisfy the given filter. 
 :param filter_: a function that takes a thing and returns a boolean 
 :type filter_: callable 
 :param thing: a thing to filter 
 :type thing: object 
 :return: a list of things that satisfy the filter 
 :rtype: list 
 :raise ValueError: if the filter is not callable'","'same as Q, but returns a list, not a generator'"
"def getLevel(level): 
    return _levelNames.get(level, ('Level   %s' % level))", 'Returns the name of the given level.',"'Return the textual representation of logging level \'level\'. 
 If the level is one of the predefined levels (CRITICAL, ERROR, WARNING, 
 INFO, DEBUG) then you get the corresponding string. If you have 
 associated levels with names using addLevelName then the name you have 
 associated with \'level\' is returned. 
 If a numeric value corresponding to one of the defined levels is passed 
 in, the corresponding string representation is returned. 
 Otherwise, the string ""Level %s"" % level is returned.'"
"def _delete_rpm_probes(probes): 
    return __salt__['probes.delete_probes'](_ordered_dict_to_dict(probes), commit=False)", 'Delete RPM probes',"'Calls the Salt module ""probes"" to delete probes from the device.'"
"@login_required 
 def dissociate(request, template_name='authopenid/dissociate.html', dissociate_form=OpenidDissociateForm, redirect_field_name=REDIRECT_FIELD_NAME, default_redirect=settings.LOGIN_REDIRECT_URL, extra_context=None): 
    redirect_to = request.REQUEST.get(redirect_field_name, '') 
   if ((not redirect_to) or ('//' in redirect_to) or ('   ' in redirect_to)): 
      redirect_to = default_redirect 
   rels = UserAssociation.objects.filter(user__id=request.user.id) 
   associated_openids = [rel.openid_url for rel in rels] 
   if ((len(associated_openids) == 1) and (not request.user.has_usable_password())): 
      msg = _(""You   can't   remove   this   openid.   You   should   set   a   password   first."") 
      return HttpResponseRedirect(('%s?%s' % (redirect_to, urllib.urlencode({'msg': msg})))) 
   if request.POST: 
      form = dissociate_form(request.POST) 
      if form.is_valid(): 
         openid_url = form.cleaned_data['openid_url'] 
         msg = '' 
         if (openid_url not in associated_openids): 
            msg = (_('%s   is   not   associated   to   your   account') % openid_url) 
         if (not msg): 
            UserAssociation.objects.get(openid_url__exact=openid_url).delete() 
            if (openid_url == request.session.get('openid_url')): 
               del request.session['openid_url'] 
            msg = _('openid   removed.') 
         return HttpResponseRedirect(('%s?%s' % (redirect_to, urllib.urlencode({'msg': msg})))) 
   else: 
      openid_url = request.GET.get('openid_url', '') 
      if (not openid_url): 
         msg = _('Invalid   OpenID   url.') 
         return HttpResponseRedirect(('%s?%s' % (redirect_to, urllib.urlencode({'msg': msg})))) 
      form = dissociate_form(initial={'openid_url': openid_url}) 
   return render(template_name, {'form': form, 'openid_url': openid_url}, context_instance=_build_context(request, extra_context=extra_context))"," 'Dissociate an openid from the user. 
 :param request: The current request. 
 :param template_name: The template name to render. 
 :param dissociate_form: The form to use. 
 :param redirect_field_name: The name of the field to use for the redirect. 
 :param default_redirect: The default redirect URL. 
 :param extra_context: The extra context to add to the template. 
 :returns: The rendered template. 
 :rtype: str'",'view used to dissociate an openid from an account'
"def check_complete(task, out_queue): 
    logger.debug('Checking   if   %s   is   complete', task) 
   try: 
      is_complete = task.complete() 
   except Exception: 
      is_complete = TracebackWrapper(traceback.format_exc()) 
   out_queue.put((task, is_complete))", 'Check if the task is complete',"'Checks if task is complete, puts the result to out_queue.'"
"def remove(name=None, pkgs=None, **kwargs): 
    pkg2rm = '' 
   if pkgs: 
      for pkg in pkgs: 
         pkg2rm += '{0}   '.format(pkg) 
      log.debug('Installing   these   packages   instead   of   {0}:   {1}'.format(name, pkg2rm)) 
   else: 
      pkg2rm = '{0}'.format(name) 
   old = list_pkgs() 
   cmd = '/bin/pkg   uninstall   -v   {0}'.format(pkg2rm) 
   out = __salt__['cmd.run_all'](cmd, output_loglevel='trace') 
   __context__.pop('pkg.list_pkgs', None) 
   new = list_pkgs() 
   ret = salt.utils.compare_dicts(old, new) 
   if (out['retcode'] != 0): 
      raise CommandExecutionError('Error   occurred   removing   package(s)', info={'changes': ret, 'retcode': ips_pkg_return_values[out['retcode']], 'errors': [out['stderr']]}) 
   return ret"," 'Remove package(s). 
 :param name: Name of package to remove. 
 :param pkgs: List of packages to remove. 
 :param kwargs: 
 .. note:: 
 This function is not idempotent. 
 :returns: dict of changes made.'","'Remove specified package. Accepts full or partial FMRI. 
 In case of multiple match, the command fails and won\'t modify the OS. 
 name 
 The name of the package to be deleted. 
 Multiple Package Options: 
 pkgs 
 A list of packages to delete. Must be passed as a python list. The 
 ``name`` parameter will be ignored if this option is passed. 
 Returns a list containing the removed packages. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pkg.remove <package name> 
 salt \'*\' pkg.remove tcsh 
 salt \'*\' pkg.remove pkg://solaris/shell/tcsh 
 salt \'*\' pkg.remove pkgs=\'[""foo"", ""bar""]\''"
"def create_test_db(): 
    from inbox.config import config 
   database_hosts = config.get_required('DATABASE_HOSTS') 
   schemas = [(shard['SCHEMA_NAME'], host['HOSTNAME']) for host in database_hosts for shard in host['SHARDS']] 
   assert all([('test' in s) for (s, h) in schemas]) 
   for (name, host) in schemas: 
      cmd = 'DROP   DATABASE   IF   EXISTS   {name};   CREATE   DATABASE   IF   NOT   EXISTS   {name}   DEFAULT   CHARACTER   SET   utf8mb4   DEFAULT   COLLATE   utf8mb4_general_ci'.format(name=name) 
      subprocess.check_call('mysql   -h   {}   -uinboxtest   -pinboxtest   -e   ""{}""'.format(host, cmd), shell=True)", 'Create a test database.',"'Creates new, empty test databases.'"
"def _get_encryption_headers(key, source=False): 
    if (key is None): 
      return {} 
   key = _to_bytes(key) 
   key_hash = hashlib.sha256(key).digest() 
   key_hash = base64.b64encode(key_hash).rstrip() 
   key = base64.b64encode(key).rstrip() 
   if source: 
      prefix = 'X-Goog-Copy-Source-Encryption-' 
   else: 
      prefix = 'X-Goog-Encryption-' 
   return {(prefix + 'Algorithm'): 'AES256', (prefix + 'Key'): _bytes_to_unicode(key), (prefix + 'Key-Sha256'): _bytes_to_unicode(key_hash)}"," 'Returns a dictionary of headers to be added to the request 
 for encryption. 
 :param key: The encryption key. 
 :type key: bytes 
 :param source: True if the request is being sent by the source bucket, 
 false if it is being sent by the destination bucket. 
 :type source: bool 
 :return: A dictionary of headers to be added to the request for encryption.'","'Builds customer encryption key headers 
 :type key: bytes 
 :param key: 32 byte key to build request key and hash. 
 :type source: bool 
 :param source: If true, return headers for the ""source"" blob; otherwise, 
 return headers for the ""destination"" blob. 
 :rtype: dict 
 :returns: dict of HTTP headers being sent in request.'"
"def _unpickle_appattr(reverse_name, args): 
    return get_current_app()._rgetattr(reverse_name)(*args)", 'Unpickle an application attribute.','Unpickle app.'
"def is_mobile_available_for_user(user, descriptor): 
    return (auth.user_has_role(user, CourseBetaTesterRole(descriptor.id)) or _has_staff_access_to_descriptor(user, descriptor, descriptor.id) or _is_descriptor_mobile_available(descriptor))", 'Returns True if the user has access to the descriptor or if the descriptor is available on mobile.',"'Returns whether the given course is mobile_available for the given user. 
 Checks: 
 mobile_available flag on the course 
 Beta User and staff access overrides the mobile_available flag 
 Arguments: 
 descriptor (CourseDescriptor|CourseOverview): course or overview of course in question'"
"@conf.commands.register 
 def sr1(x, promisc=None, filter=None, iface=None, nofilter=0, *args, **kargs): 
    if (not kargs.has_key('timeout')): 
      kargs['timeout'] = (-1) 
   s = conf.L3socket(promisc=promisc, filter=filter, nofilter=nofilter, iface=iface) 
   (a, b) = sndrcv(s, x, *args, **kargs) 
   s.close() 
   if (len(a) > 0): 
      return a[0][1] 
   else: 
      return None"," 'Send one packet to the specified interface 
 @param x: The packet to send 
 @param promisc: Promiscuous mode 
 @param filter: The filter to apply 
 @param iface: The interface to send the packet to 
 @param nofilter: Disable filtering 
 @param timeout: The timeout in seconds to wait for a response'","'Send packets at layer 3 and return only the first answer 
 nofilter: put 1 to avoid use of BPF filters 
 retry:    if positive, how many times to resend unanswered packets 
 if negative, how many times to retry when no more packets are answered 
 timeout:  how much time to wait after the last packet has been sent 
 verbose:  set verbosity level 
 multi:    whether to accept multiple answers for the same stimulus 
 filter:   provide a BPF filter 
 iface:    listen answers only on the given interface'"
"def clear_caches(): 
    global FS_CACHE, MR_CACHE 
   old = (FS_CACHE, MR_CACHE) 
   (FS_CACHE, MR_CACHE) = (None, None) 
   return old", 'Clear all caches.',"'Clears cluster\'s internal caches.  Returns 
 something that can be given back to restore_caches.'"
"def length_is(value, arg): 
    return (len(value) == int(arg))"," 'Check if the length of the value is equal to the argument. 
 >>> length_is(1, 1) 
 True 
 >>> length_is(1, 2) 
 False 
 >>> length_is(1, 1.0) 
 False'",'Returns a boolean of whether the value\'s length is the argument'
"def search(opts, returners, whitelist=None): 
    return LazyLoader(_module_dirs(opts, 'search', 'search'), opts, tag='search', whitelist=whitelist, pack={'__ret__': returners})"," 'Return a function that searches for a given string in a file. 
 :param opts: Options to use for the search. 
 :param returners: A list of returners to use for the search. 
 :param whitelist: A list of patterns to whitelist. 
 :return: A function that searches for a given string in a file. 
 :rtype: function'","'Returns the search modules 
 :param dict opts: The Salt options dictionary 
 :param returners: Undocumented 
 :param whitelist: Undocumented'"
"def base64_b64encode(instr): 
    if six.PY3: 
      b = salt.utils.to_bytes(instr) 
      b64 = base64.b64encode(b) 
      return salt.utils.to_str(b64) 
   return base64.b64encode(instr)"," 'Base64 encode a string. 
 :param instr: String to encode. 
 :return: Base64 encoded string.'","'Encode a string as base64 using the ""modern"" Python interface. 
 Among other possible differences, the ""modern"" encoder does not include 
 newline (\'\n\') characters in the encoded output. 
 .. versionadded:: 2016.3.0 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' hashutil.base64_b64encode \'get salted\''"
"def iteritems(d): 
    return getattr(d, _iteritems)()"," 'Iterate over the keys and values of a dictionary. 
 :param d: Dictionary to iterate over. 
 :type d: dict 
 :return: A generator of (key, value) pairs. 
 :rtype: (str, Any)'","'Return an iterator over the (key, value) pairs of a dictionary.'"
"def safe_encode(text, incoming=None, encoding='utf-8', errors='strict'): 
    if (not isinstance(text, basestring)): 
      raise TypeError((""%s   can't   be   encoded"" % type(text))) 
   if (not incoming): 
      incoming = (sys.stdin.encoding or sys.getdefaultencoding()) 
   if isinstance(text, unicode): 
      return text.encode(encoding, errors) 
   elif (text and (encoding != incoming)): 
      text = safe_decode(text, incoming, errors) 
      return text.encode(encoding, errors) 
   return text"," 'Safe encode text. 
 :param text: text to encode 
 :param incoming: encoding of the input (if not None, will be used to 
 decode the text) 
 :param encoding: encoding to use 
 :param errors: errors to use when decoding 
 :return: encoded text'","'Encodes incoming str/unicode using `encoding`. If 
 incoming is not specified, text is expected to 
 be encoded with current python\'s default encoding. 
 (`sys.getdefaultencoding`) 
 :param incoming: Text\'s current encoding 
 :param encoding: Expected encoding for text (Default UTF-8) 
 :param errors: Errors handling policy. See here for valid 
 values http://docs.python.org/2/library/codecs.html 
 :returns: text or a bytestring `encoding` encoded 
 representation of it. 
 :raises TypeError: If text is not an isntance of basestring'"
"def get_storage_hash(storage): 
    if isinstance(storage, LazyObject): 
      if (storage._wrapped is None): 
         storage._setup() 
      storage = storage._wrapped 
   if (not isinstance(storage, six.string_types)): 
      storage_cls = storage.__class__ 
      storage = ('%s.%s' % (storage_cls.__module__, storage_cls.__name__)) 
   return hashlib.md5(storage.encode('utf8')).hexdigest()"," 'Returns a hash of the storage object. 
 :param storage: Storage object 
 :type storage: :class:`Storage` or :class:`LazyObject` 
 :return: Storage hash 
 :rtype: str'","'Return a hex string hash for a storage object (or string containing 
 \'full.path.ClassName\' referring to a storage object).'"
"def test_get_debug_values_no_debugger(): 
    prev_value = config.compute_test_value 
   try: 
      config.compute_test_value = 'off' 
      x = T.vector() 
      for x_val in op.get_debug_values(x): 
         assert False 
   finally: 
      config.compute_test_value = prev_value", 'Test get_debug_values with no debugger.','get_debug_values should return [] when debugger is off'
"def has_default_value(option, value): 
    return ((option in helpful_parser.defaults) and (helpful_parser.defaults[option] == value))", 'Check if option has a default value and if it is equal to the provided value',"'Does option have the default value? 
 If the default value of option is not known, False is returned. 
 :param str option: configuration variable being considered 
 :param value: value of the configuration variable named option 
 :returns: True if option has the default value, otherwise, False 
 :rtype: bool'"
"def rate_id(context, id_, val=3): 
    def rate_element(driver): 
      try: 
         inner_wrapper = find_id_with_wait(context, id_, wait_time=2) 
         els = inner_wrapper.find_elements_by_class_name(STAR_RATING_OPTION_CLASS) 
         rating_el = [el for el in filter((lambda x: (int(x.get_attribute('data-val')) == val)), els)][0] 
         rating_el.click() 
         return True 
      except (NoSuchElementException, StaleElementReferenceException, TimeoutException, IndexError): 
         return False 
   try: 
      WebDriverWait(context.browser, 30).until(rate_element) 
   except TimeoutException: 
      raise Exception(""Unable   to   enter   rating   for   container   with   id   '{id:s}'"".format(id=id_))"," 'Rate an element with a specific rating value. 
 This function is intended to be used to rate elements that have a 
 ""star rating"" feature. 
 :param context: The context to use. 
 :param id_: The id of the element to rate. 
 :param val: The rating value to rate the element with. 
 :rtype: bool'","'Enter a star rating given the id of the container 
 :param context: behave context 
 :param id: id of the container element'"
"def _get_client(timeout=None): 
    if ('docker.client' not in __context__): 
      client_kwargs = {} 
      for (key, val) in (('base_url', 'docker.url'), ('version', 'docker.version')): 
         param = __salt__['config.get'](val, NOTSET) 
         if (param is not NOTSET): 
            client_kwargs[key] = param 
      if (('base_url' not in client_kwargs) and ('DOCKER_HOST' in os.environ)): 
         client_kwargs['base_url'] = os.environ.get('DOCKER_HOST') 
      if ('version' not in client_kwargs): 
         client_kwargs['version'] = 'auto' 
      docker_machine = __salt__['config.get']('docker.machine', NOTSET) 
      if (docker_machine is not NOTSET): 
         docker_machine_json = __salt__['cmd.run'](('docker-machine   inspect   ' + docker_machine)) 
         try: 
            docker_machine_json = json.loads(docker_machine_json) 
            docker_machine_tls = docker_machine_json['HostOptions']['AuthOptions'] 
            docker_machine_ip = docker_machine_json['Driver']['IPAddress'] 
            client_kwargs['base_url'] = (('https://' + docker_machine_ip) + ':2376') 
            client_kwargs['tls'] = docker.tls.TLSConfig(client_cert=(docker_machine_tls['ClientCertPath'], docker_machine_tls['ClientKeyPath']), ca_cert=docker_machine_tls['CaCertPath'], assert_hostname=False, verify=True) 
         except Exception as exc: 
            raise CommandExecutionError('Docker   machine   {0}   failed:   {1}'.format(docker_machine, exc)) 
      try: 
         __context__['docker.client'] = docker.Client(**client_kwargs) 
      except docker.errors.DockerException: 
         log.error('Could   not   initialize   Docker   client') 
         return False 
   if ((timeout is not None) and (__context__['docker.client'].timeout != timeout)): 
      __context__['docker.client'].timeout = timeout", 'Get a Docker client.',"'Obtains a connection to a docker API (socket or URL) based on config.get 
 mechanism (pillar -> grains) 
 By default it will use the base docker-py defaults which 
 at the time of writing are using the local socket and 
 the 1.4 API 
 Set those keys in your configuration tree somehow: 
 - docker.url: URL to the docker service 
 - docker.version: API version to use (default: ""auto"")'"
"def convert_to_RGB_255(colors): 
    rgb_components = [] 
   for component in colors: 
      rounded_num = decimal.Decimal(str((component * 255.0))).quantize(decimal.Decimal('1'), rounding=decimal.ROUND_HALF_EVEN) 
      rounded_num = int(rounded_num) 
      rgb_components.append(rounded_num) 
   return (rgb_components[0], rgb_components[1], rgb_components[2])", 'Convert a list of colors to a list of RGB components in the range 0-255.',"'Multiplies each element of a triplet by 255 
 Each coordinate of the color tuple is rounded to the nearest float and 
 then is turned into an integer. If a number is of the form x.5, then 
 if x is odd, the number rounds up to (x+1). Otherwise, it rounds down 
 to just x. This is the way rounding works in Python 3 and in current 
 statistical analysis to avoid rounding bias 
 :param (list) rgb_components: grabs the three R, G and B values to be 
 returned as computed in the function'"
"def p_expr_list_2(t): 
    pass", '2-arg list','exprlist : expression'
"def p_relational_expression_5(t): 
    pass", '5. relational expression','relational_expression : relational_expression GE shift_expression'
"def tryall(context, prefix=None): 
    context = context.copy() 
   results = {} 
   for (k, v) in context.iteritems(): 
      if (not hasattr(v, '__call__')): 
         continue 
      if (prefix and (not k.startswith(prefix))): 
         continue 
      print (k + ':'), 
      try: 
         r = v() 
         dictincr(results, r) 
         print r 
      except: 
         print 'ERROR' 
         dictincr(results, 'ERROR') 
         print ('         ' + '\n         '.join(traceback.format_exc().split('\n'))) 
   print ('-' * 40) 
   print 'results:' 
   for (k, v) in results.iteritems(): 
      print ('   ' * 2), (str(k) + ':'), v"," 'Runs a function with the given context, and prints the results. 
 The context is a dictionary of keyword arguments to pass to the function. 
 If the function raises an exception, the traceback is printed. 
 If the function returns a dictionary, it is printed, with the keys and values 
 separated by a colon. 
 If the function returns a string, it is printed. 
 If the function returns None, the string \'None\''","'Tries a series of functions and prints their results. 
 `context` is a dictionary mapping names to values; 
 the value will only be tried if it\'s callable. 
 For example, you might have a file `test/stuff.py` 
 with a series of functions testing various things in it. 
 At the bottom, have a line: 
 if __name__ == ""__main__"": tryall(globals()) 
 Then you can run `python test/stuff.py` and get the results of 
 all the tests.'"
"def getProfileDirectory(): 
    craftTypeName = getCraftTypeName() 
   return os.path.join(craftTypeName, getProfileName(craftTypeName))"," 'Returns the profile directory for the current craft type. 
 :returns: path to the profile directory'",'Get the profile directory.'
"def berp_zeros(nt): 
    if ((not isscalar(nt)) or (floor(nt) != nt) or (nt <= 0)): 
      raise ValueError('nt   must   be   positive   integer   scalar.') 
   return specfun.klvnzo(nt, 5)"," 'Returns the zeros of the Bessel function of the first kind of order 
 `nt`. 
 Examples 
 >>> from sympy.integrals.special.bessel import berp_zeros 
 >>> berp_zeres(1) 
 -1.000000000000000000000000000000000000000000000000000000000000000 
 >>> berp_zeres(2) 
 0.000000000000000000000000000000000000000000000000000000000000000 
 >>> berp_zeres(3) 
 0.0000000000000000000000000000000000000","'Compute nt zeros of the Kelvin function ber\'(x). 
 References 
 .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special 
 Functions"", John Wiley and Sons, 1996. 
 http://jin.ece.illinois.edu/specfunc.html'"
"def referenced(word, article=INDEFINITE, gender=MALE, role=SUBJECT): 
    return ('%s   %s' % (_article(word, article, gender, role), word))", 'Returns the article and the word.','Returns a string with the article + the word.'
"def nagios_from_file(results_file): 
    data = open(results_file).read().strip() 
   pieces = data.split('|') 
   if (not (len(pieces) == 4)): 
      state = 'UNKNOWN' 
      ret = 3 
      data = 'Results   file   malformed' 
   else: 
      timestamp = int(pieces[0]) 
      time_diff = (time.time() - timestamp) 
      if (time_diff > (60 * 2)): 
         ret = 3 
         state = 'UNKNOWN' 
         data = 'Results   file   is   stale' 
      else: 
         ret = int(pieces[1]) 
         state = pieces[2] 
         data = pieces[3] 
   return (ret, ('%s:   %s' % (state, data)))"," 'Read a nagios results file and return the retcode and message. 
 The results file is expected to be in the format: 
 |timestamp|retcode|state|message 
 For example: 
 1234567890|1|CRITICAL|Host   is   down'","'Returns a nagios-appropriate string and return code obtained by 
 parsing the desired file on disk. The file on disk should be of format 
 %s|%s % (timestamp, nagios_string) 
 This file is created by various nagios checking cron jobs such as 
 check-rabbitmq-queues and check-rabbitmq-consumers'"
"@pytest.mark.django_db 
 def test_max_revision(revision, project0_nongnu, store0): 
    store0.sync() 
   store0.update(store0.file.store) 
   initial_max_revision = Unit.max_revision() 
   initial_revision = Revision.get() 
   assert (initial_max_revision == initial_revision) 
   for i in range(10): 
      _update_translation(store0, 0, {'target': str(i)}, sync=False) 
   end_max_revision = Unit.max_revision() 
   end_revision = Revision.get() 
   assert (end_max_revision == end_revision) 
   assert (end_max_revision != initial_max_revision) 
   assert (end_revision != initial_revision) 
   assert (end_revision == (10 + initial_revision))", 'Test that we can update the max_revision and the revision.','Tests `max_revision()` gets the latest revision.'
"def json_http_response(data): 
    return JsonResponse(data)", 'Return a JSON response.',"'Return an HttpResponse with the data json-serialized and the right content 
 type header.'"
"def expect_mc(dist, func=(lambda x: 1), size=50000): 
    def fun(x): 
      return func(x) 
   rvs = dist.rvs(size=size) 
   return fun(rvs).mean(0)"," 'Expect the mean of the distribution of a function of random variables. 
 Parameters 
 dist : distribution 
 The distribution to sample from. 
 func : callable 
 The function to evaluate on the random variables. 
 Returns 
 mean : float 
 The mean of the distribution of the function of the random variables.'","'calculate expected value of function by Monte Carlo integration 
 Parameters 
 dist : distribution instance 
 needs to have rvs defined as a method for drawing random numbers 
 func : callable 
 function for which expectation is calculated, this function needs to 
 be vectorized, integration is over axis=0 
 size : int 
 number of random samples to use in the Monte Carlo integration, 
 Notes 
 this doesn\'t batch 
 Returns 
 expected value : ndarray 
 return of function func integrated over axis=0 by MonteCarlo, this will 
 have the same shape as the return of func without axis=0 
 Examples 
 integrate probability that both observations are negative 
 >>> mvn = mve.MVNormal([0,0],2.) 
 >>> mve.expect_mc(mvn, lambda x: (x<np.array([0,0])).all(-1), size=100000) 
 0.25306000000000001 
 get tail probabilities of marginal distribution (should be 0.1) 
 >>> c = stats.norm.isf(0.05, scale=np.sqrt(2.)) 
 >>> expect_mc(mvn, lambda x: (np.abs(x)>np.array([c, c])), size=100000) 
 array([ 0.09969,  0.0986 ]) 
 or calling the method 
 >>> mvn.expect_mc(lambda x: (np.abs(x)>np.array([c, c])), size=100000) 
 array([ 0.09937,  0.10075])'"
"def overrideRootMenu(root, flist): 
    from Tkinter import Menu, Text, Text 
   from idlelib.EditorWindow import prepstr, get_accelerator 
   from idlelib import Bindings 
   from idlelib import WindowList 
   from idlelib.MultiCall import MultiCallCreator 
   closeItem = Bindings.menudefs[0][1][(-2)] 
   del Bindings.menudefs[0][1][(-3):] 
   Bindings.menudefs[0][1].insert(6, closeItem) 
   del Bindings.menudefs[(-1)][1][0:2] 
   del Bindings.menudefs[(-2)][1][0:2] 
   menubar = Menu(root) 
   root.configure(menu=menubar) 
   menudict = {} 
   menudict['windows'] = menu = Menu(menubar, name='windows') 
   menubar.add_cascade(label='Window', menu=menu, underline=0) 
   def postwindowsmenu(menu=menu): 
      end = menu.index('end') 
      if (end is None): 
         end = (-1) 
      if (end > 0): 
         menu.delete(0, end) 
      WindowList.add_windows_to_menu(menu) 
   WindowList.register_callback(postwindowsmenu) 
   def about_dialog(event=None): 
      from idlelib import aboutDialog 
      aboutDialog.AboutDialog(root, 'About   IDLE') 
   def config_dialog(event=None): 
      from idlelib import configDialog 
      root.instance_dict = flist.inversedict 
      configDialog.ConfigDialog(root, 'Settings') 
   def help_dialog(event=None): 
      from idlelib import textView 
      fn = path.join(path.abspath(path.dirname(__file__)), 'help.txt') 
      textView.view_file(root, 'Help', fn) 
   root.bind('<<about-idle>>', about_dialog) 
   root.bind('<<open-config-dialog>>', config_dialog) 
   root.createcommand('::tk::mac::ShowPreferences', config_dialog) 
   if flist: 
      root.bind('<<close-all-windows>>', flist.close_all_callback) 
      root.createcommand('exit', flist.close_all_callback) 
   if isCarbonTk(): 
      menudict['application'] = menu = Menu(menubar, name='apple') 
      menubar.add_cascade(label='IDLE', menu=menu) 
      Bindings.menudefs.insert(0, ('application', [('About   IDLE', '<<about-idle>>'), None])) 
      tkversion = root.tk.eval('info   patchlevel') 
      if (tuple(map(int, tkversion.split('.'))) < (8, 4, 14)): 
         Bindings.menudefs[0][1].append(('_Preferences....', '<<open-config-dialog>>')) 
   if isCocoaTk(): 
      root.createcommand('tkAboutDialog', about_dialog) 
      root.createcommand('::tk::mac::ShowHelp', help_dialog) 
      del Bindings.menudefs[(-1)][1][0]", 'Set the root menu for the given flist.',"'Replace the Tk root menu by something that is more appropriate for 
 IDLE with an Aqua Tk.'"
"def limitedTime(second, func, *args, **kw): 
    return func(*args, **kw)"," 'A decorator that limits the time a function is executed. 
 :param second: The number of seconds the function should run. 
 :param func: The function to decorate. 
 :param args: The arguments to pass to the function. 
 :param kw: The keyword arguments to pass to the function. 
 :returns: The function to call.'","'Call func(*args, **kw) with a timeout of second seconds.'"
"def rnn_helper(inp, length, cell_type=None, direction='forward', name=None, *args, **kwargs): 
    assert (cell_type is not None) 
   rnn_func = None 
   if (cell_type == 'lstm'): 
      rnn_func = lstm_layer 
   assert (rnn_func is not None) 
   assert (direction in ['forward', 'backward', 'bidirectional']) 
   with tf.variable_scope(name): 
      if (direction in ['forward', 'bidirectional']): 
         forward = rnn_func(inp=inp, length=length, backward=False, name='forward', *args, **kwargs) 
         if isinstance(forward, tuple): 
            forward = forward[0] 
      if (direction in ['backward', 'bidirectional']): 
         backward = rnn_func(inp=inp, length=length, backward=True, name='backward', *args, **kwargs) 
         if isinstance(backward, tuple): 
            backward = backward[0] 
      if (direction == 'forward'): 
         out = forward 
      elif (direction == 'backward'): 
         out = backward 
      else: 
         out = tf.concat(2, [forward, backward]) 
   return out"," 'Helper function to create a recurrent neural network. 
 Parameters 
 inp : tensor 
 Input tensor. 
 length : int 
 Length of input sequence. 
 cell_type : str, optional 
 Type of recurrent cell to use. 
 direction : str, optional 
 Direction of recurrent computation. 
 name : str, optional 
 Name of recurrent layer. 
 *args, **kwargs : 
 Args and kwargs to pass to the recurrent layer. 
 Returns 
 tensor 
 Output tensor. 
 Examples 
 >>> x = tf.constant([[1, 2, 3], [4, 5, 6]]) 
 >>> rnn_helper(x, 3) 
 tensor([[ 1.,  2.,  3.], [ 4.,  5.,  6.]])'","'Adds ops for a recurrent neural network layer. 
 This function calls an actual implementation of a recurrent neural network 
 based on `cell_type`. 
 There are three modes depending on the value of `direction`: 
 forward: Adds a forward RNN. 
 backward: Adds a backward RNN. 
 bidirectional: Adds both forward and backward RNNs and creates a 
 bidirectional RNN. 
 Args: 
 inp: A 3-D tensor of shape [`batch_size`, `max_length`, `feature_dim`]. 
 length: A 1-D tensor of shape [`batch_size`] and type int64. Each element 
 represents the length of the corresponding sequence in `inp`. 
 cell_type: Cell type of RNN. Currently can only be ""lstm"". 
 direction: One of ""forward"", ""backward"", ""bidirectional"". 
 name: Name of the op. 
 *args: Other arguments to the layer. 
 **kwargs: Keyword arugments to the layer. 
 Returns: 
 A 3-D tensor of shape [`batch_size`, `max_length`, `num_nodes`].'"
"def generate_random_alphanumeric(length): 
    return ''.join((random.choice((string.ascii_uppercase + string.digits)) for _x in range(length)))"," 'Generate a random alphanumeric string of length `length`. 
 Parameters 
 length : int 
 The length of the string to generate. 
 Returns 
 str 
 The generated string.'",'Creates a random alphanumeric string of specified length.'
"def scheme_node_from_element(node_el, registry): 
    try: 
      widget_desc = registry.widget(node_el.get('qualified_name')) 
   except KeyError as ex: 
      raise UnknownWidgetDefinition(*ex.args) 
   title = node_el.get('title') 
   pos = node_el.get('position') 
   if (pos is not None): 
      pos = tuple_eval(pos) 
   return SchemeNode(widget_desc, title=title, position=pos)", 'Returns a SchemeNode object from the given Element.','Create a SchemeNode from an `Element` instance.'
"def canonicalize_emails(changelog, mapping): 
    for (alias, email_address) in mapping.iteritems(): 
      changelog = changelog.replace(alias, email_address) 
   return changelog"," 'Replace email aliases in the changelog with the corresponding email 
 addresses.'","'Takes in a string and an email alias mapping and replaces all 
 instances of the aliases in the string with their real email.'"
"def _ordered_count(iterable): 
    c = OrderedDict() 
   for elem in iterable: 
      c[elem] = (c.get(elem, 0) + 1) 
   return c"," 'Returns a dictionary of the number of times each element of an 
 iterable occurs. 
 :param iterable: 
 :type iterable: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
 :return: 
 :rtype: 
","'Return dict of element counts, in the order they were first seen'"
"def pull_dkr(url, name, index): 
    return _pull_image('dkr', url, name, index=index)", 'Pull an image from Docker Hub.',"'Execute a ``machinectl pull-dkr`` to download a docker image and add it to 
 /var/lib/machines as a new container. 
 .. note:: 
 **Requires systemd >= 219** 
 url 
 URL from which to download the container 
 name 
 Name for the new container 
 index 
 URL of the Docker index server from which to pull (must be an 
 ``http://`` or ``https://`` URL). 
 CLI Examples: 
 .. code-block:: bash 
 salt myminion nspawn.pull_dkr centos/centos6 cent6 index=https://get.docker.com 
 salt myminion nspawn.pull_docker centos/centos6 cent6 index=https://get.docker.com'"
"def managedcloud(vm_): 
    return config.get_cloud_config_value('managedcloud', vm_, __opts__, default='False', search_global=False)", 'Returns managed cloud flag for the given vm.',"'Determine if we should wait for the managed cloud automation before 
 running. Either \'False\' (default) or \'True\'.'"
"def assert_raises_regex(exception_class, expected_regexp, callable_obj=None, *args, **kwargs): 
    __tracebackhide__ = True 
   nose = import_nose() 
   if (sys.version_info.major >= 3): 
      funcname = nose.tools.assert_raises_regex 
   else: 
      funcname = nose.tools.assert_raises_regexp 
   return funcname(exception_class, expected_regexp, callable_obj, *args, **kwargs)"," 'This function is a Nose plugin that tests that a callable raises a 
 specific exception. 
 :param exception_class: the class of the exception to raise. 
 :param expected_regexp: the expected exception message. 
 :param callable_obj: the callable to test. 
 :param args: the arguments to pass to the callable. 
 :param kwargs: the keyword arguments to pass to the callable. 
 :return: the result of the callable.'","'Fail unless an exception of class exception_class and with message that 
 matches expected_regexp is thrown by callable when invoked with arguments 
 args and keyword arguments kwargs. 
 Name of this function adheres to Python 3.2+ reference, but should work in 
 all versions down to 2.6. 
 Notes 
 .. versionadded:: 1.8.0'"
"def delete_files(processPath, notwantedFiles, result, force=False): 
    if ((not result.result) and force): 
      result.output += logHelper(u'Forcing   deletion   of   files,   even   though   last   result   was   not   success', sickrage.srCore.srLogger.DEBUG) 
   elif (not result.result): 
      return 
   for cur_file in notwantedFiles: 
      cur_file_path = os.path.join(processPath, cur_file) 
      if (not os.path.isfile(cur_file_path)): 
         continue 
      result.output += logHelper((u'Deleting   file   %s' % cur_file), sickrage.srCore.srLogger.DEBUG) 
      file_attribute = os.stat(cur_file_path)[0] 
      if (not (file_attribute & stat.S_IWRITE)): 
         result.output += logHelper((u'Changing   ReadOnly   Flag   for   file   %s' % cur_file), sickrage.srCore.srLogger.DEBUG) 
         try: 
            os.chmod(cur_file_path, stat.S_IWRITE) 
         except OSError as e: 
            result.output += logHelper((u'Cannot   change   permissions   of   %s:   %s' % (cur_file, str(e.strerror).decode(sickrage.SYS_ENCODING))), sickrage.srCore.srLogger.DEBUG) 
      try: 
         os.remove(cur_file_path) 
      except OSError as e: 
         result.output += logHelper((u'Unable   to   delete   file   %s:   %s' % (cur_file, str(e.strerror).decode(sickrage.SYS_ENCODING))), sickrage.srCore.srLogger.DEBUG)"," 'Delete files from the specified path. 
 :param processPath: Path to the process 
 :param notwantedFiles: List of files to delete 
 :param result: Result object to update'","'Remove files from filesystem 
 :param processPath: path to process 
 :param notwantedFiles: files we do not want 
 :param result: Processor results 
 :param force: Boolean, force deletion, defaults to false'"
"def confirmation_option(*param_decls, **attrs): 
    def decorator(f): 
      def callback(ctx, param, value): 
         if (not value): 
            ctx.abort() 
      attrs.setdefault('is_flag', True) 
      attrs.setdefault('callback', callback) 
      attrs.setdefault('expose_value', False) 
      attrs.setdefault('prompt', 'Do   you   want   to   continue?') 
      attrs.setdefault('help', 'Confirm   the   action   without   prompting.') 
      return option(*(param_decls or ('--yes',)), **attrs)(f) 
   return decorator"," 'Decorator to add confirmation options. 
 This decorator adds a confirmation option to a command. 
 This option can be used to confirm a command without prompting the 
 user. 
 For example:: 
 @confirmation_option 
 def add(ctx, param, value): 
 if (not value): 
 ctx.abort() 
 This will abort the command if the user does not enter a value. 
 The confirmation option can also be used to show a prompt:: 
 @confirmation_option 
 def add(ctx, param, value): 
 if (not value): 
 prompt = \'Do   you   want   to   add   this   file?\' 
 confirm(prompt) 
 This will show a prompt and abort the command if the user does not 
 enter a value. 
 The confirmation option can also be used to show a prompt and 
 expose the value:: 
 @confirmation_option 
 def add(ctx, param, value): 
 if (not value): 
 prompt = \'Do   you   want   to   add   this   file?\' ","'Shortcut for confirmation prompts that can be ignored by passing 
 ``--yes`` as parameter. 
 This is equivalent to decorating a function with :func:`option` with 
 the following parameters:: 
 def callback(ctx, param, value): 
 if not value: 
 ctx.abort() 
 @click.command() 
 @click.option(\'--yes\', is_flag=True, callback=callback, 
 expose_value=False, prompt=\'Do you want to continue?\') 
 def dropdb(): 
 pass'"
"@inspect_command(alias=u'dump_schedule') 
 def scheduled(state, **kwargs): 
    return list(_iter_schedule_requests(state.consumer.timer))"," 'List the scheduled requests. 
 :param state: The state. 
 :param kwargs: The keyword arguments. 
 :returns: The scheduled requests. 
 :rtype: list(schedule.Request)'",'List of currently scheduled ETA/countdown tasks.'
"def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None, input_data=None, expected_output=None, expected_output_dtype=None, fixed_batch_size=False): 
    if (input_data is None): 
      assert input_shape 
      if (not input_dtype): 
         input_dtype = K.floatx() 
      input_data_shape = list(input_shape) 
      for (i, e) in enumerate(input_data_shape): 
         if (e is None): 
            input_data_shape[i] = np.random.randint(1, 4) 
      input_data = (10 * np.random.random(input_data_shape)) 
      input_data = input_data.astype(input_dtype) 
   elif (input_shape is None): 
      input_shape = input_data.shape 
   if (expected_output_dtype is None): 
      expected_output_dtype = input_dtype 
   layer = layer_cls(**kwargs) 
   weights = layer.get_weights() 
   layer.set_weights(weights) 
   if ('weights' in inspect.getargspec(layer_cls.__init__)): 
      kwargs['weights'] = weights 
      layer = layer_cls(**kwargs) 
   if fixed_batch_size: 
      x = Input(batch_shape=input_shape, dtype=input_dtype) 
   else: 
      x = Input(shape=input_shape[1:], dtype=input_dtype) 
   y = layer(x) 
   assert (K.dtype(y) == expected_output_dtype) 
   model = Model(input=x, output=y) 
   model.compile('rmsprop', 'mse') 
   expected_output_shape = layer.get_output_shape_for(input_shape) 
   actual_output = model.predict(input_data) 
   actual_output_shape = actual_output.shape 
   for (expected_dim, actual_dim) in zip(expected_output_shape, actual_output_shape): 
      if (expected_dim is not None): 
         assert (expected_dim == actual_dim) 
   if (expected_output is not None): 
      assert_allclose(actual_output, expected_output, rtol=0.001) 
   model_config = model.get_config() 
   model = Model.from_config(model_config) 
   model.compile('rmsprop', 'mse') 
   layer_config = layer.get_config() 
   layer_config['batch_input_shape'] = input_shape 
   layer = layer.__class__.from_config(layer_config) 
   model = Sequential() 
   model.add(layer) 
   model.compile('rmsprop', 'mse') 
   actual_output = model.predict(input_data) 
   actual_output_shape = actual_output.shape 
   for (expected_dim, actual_dim) in zip(expected_output_shape, actual_output_shape): 
      if (expected_dim is not None): 
         assert (expected_dim == actual_dim) 
   if (expected_output is not None): 
      assert_allclose(actual_output, expected_output, rtol=0.001) 
   json_model = model.to_json() 
   model = model_from_json(json_model) 
   return actual_output", 'Tests a layer.',"'Test routine for a layer with a single input tensor 
 and single output tensor.'"
"@instrumented_task(name='sentry.tasks.post_process.plugin_post_process_group', stat_suffix=(lambda plugin_slug, *a, **k: plugin_slug)) 
 def plugin_post_process_group(plugin_slug, event, **kwargs): 
    Raven.tags_context({'project': event.project_id}) 
   plugin = plugins.get(plugin_slug) 
   safe_execute(plugin.post_process, event=event, group=event.group, **kwargs)", 'Run post process for a group of events.','Fires post processing hooks for a group.'
"def is_valid_asn(asn): 
    return (isinstance(asn, numbers.Integral) and (0 <= asn <= 4294967295))", 'Checks if asn is a valid ASN.','Returns True if the given AS number is Two or Four Octet.'
"def bin(number): 
    tmp = [BIN_HEX_DICT[hstr] for hstr in hex(number)[2:]] 
   return BIN_ZSTRIP.sub('0b', ''.join(tmp))"," 'Converts a number to binary. 
 The number can be any number type (int, float, etc.) 
 Examples 
 >>> bin(25) 
 \'0b11111111\' 
 >>> bin(25.5) 
 \'0b1111111101000000\' 
 >>> bin(25.5, 16) 
 \'0b111111110100000000000000000000\' 
 >>> bin(25.5, 16, 2) 
 \'0b1111111101000000000000000000000000000000000000000000000000000\' 
 >>> bin(25.5, 16, 3) 
 \'0b111111110100","'Adapted from http://code.activestate.com/recipes/576847/ 
 :codeauthor: Vishal Sapre 
 :license: MIT 
 A foolishly simple look-up method of getting binary string from an integer 
 This happens to be faster than all other ways!!!'"
"def parseTargetDirect(): 
    if (not conf.direct): 
      return 
   details = None 
   remote = False 
   for dbms in SUPPORTED_DBMS: 
      details = re.search(('^(?P<dbms>%s)://(?P<credentials>(?P<user>.+?)\\:(?P<pass>.*)\\@)?(?P<remote>(?P<hostname>.+?)\\:(?P<port>[\\d]+)\\/)?(?P<db>[\\w\\d\\   \\:\\.\\_\\-\\/\\\\]+?)$' % dbms), conf.direct, re.I) 
      if details: 
         conf.dbms = details.group('dbms') 
         if details.group('credentials'): 
            conf.dbmsUser = details.group('user') 
            conf.dbmsPass = details.group('pass') 
         elif conf.dbmsCred: 
            (conf.dbmsUser, conf.dbmsPass) = conf.dbmsCred.split(':') 
         else: 
            conf.dbmsUser = unicode() 
            conf.dbmsPass = unicode() 
         if (not conf.dbmsPass): 
            conf.dbmsPass = None 
         if details.group('remote'): 
            remote = True 
            conf.hostname = details.group('hostname').strip() 
            conf.port = int(details.group('port')) 
         else: 
            conf.hostname = 'localhost' 
            conf.port = 0 
         conf.dbmsDb = details.group('db') 
         conf.parameters[None] = 'direct   connection' 
         break 
   if (not details): 
      errMsg = 'invalid   target   details,   valid   syntax   is   for   instance   ' 
      errMsg += ""'mysql://USER:PASSWORD@DBMS_IP:DBMS_PORT/DATABASE_NAME'   "" 
      errMsg += ""or   'access://DATABASE_FILEPATH'"" 
      raise SqlmapSyntaxException(errMsg) 
   for (dbmsName, data) in DBMS_DICT.items(): 
      if ((dbmsName == conf.dbms) or (conf.dbms.lower() in data[0])): 
         try: 
            if (dbmsName in (DBMS.ACCESS, DBMS.SQLITE, DBMS.FIREBIRD)): 
               if remote: 
                  warnMsg = 'direct   connection   over   the   network   for   ' 
                  warnMsg += ('%s   DBMS   is   not   supported' % dbmsName) 
                  logger.warn(warnMsg) 
                  conf.hostname = 'localhost' 
                  conf.port = 0 
            elif (not remote): 
               errMsg = 'missing   remote   connection   details   (e.g.   ' 
               errMsg += ""'mysql://USER:PASSWORD@DBMS_IP:DBMS_PORT/DATABASE_NAME'   "" 
               errMsg += ""or   'access://DATABASE_FILEPATH')"" 
               raise SqlmapSyntaxException(errMsg) 
            if (dbmsName in (DBMS.MSSQL, DBMS.SYBASE)): 
               import _mssql 
               import pymssql 
               if ((not hasattr(pymssql, '__version__')) or (pymssql.__version__ < '1.0.2')): 
                  errMsg = (""'%s'   third-party   library   must   be   "" % data[1]) 
                  errMsg += 'version   >=   1.0.2   to   work   properly.   ' 
                  errMsg += (""Download   from   '%s'"" % data[2]) 
                  raise SqlmapMissingDependence(errMsg) 
            elif (dbmsName == DBMS.MYSQL): 
               import pymysql 
            elif (dbmsName == DBMS.PGSQL): 
               import psycopg2 
            elif (dbmsName == DBMS.ORACLE): 
               import cx_Oracle 
            elif (dbmsName == DBMS.SQLITE): 
               import sqlite3 
            elif (dbmsName == DBMS.ACCESS): 
               import pyodbc 
            elif (dbmsName == DBMS.FIREBIRD): 
               import kinterbasdb 
         except ImportError: 
            if (_sqlalchemy and (data[3] in _sqlalchemy.dialects.__all__)): 
               pass 
            else: 
               errMsg = (""sqlmap   requires   '%s'   third-party   library   "" % data[1]) 
               errMsg += 'in   order   to   directly   connect   to   the   DBMS   ' 
               errMsg += (""%s.   You   can   download   it   from   '%s'"" % (dbmsName, data[2])) 
               errMsg += "".   Alternative   is   to   use   a   package   'python-sqlalchemy'   "" 
               errMsg += (""with   support   for   dialect   '%s'   installed"" % data[3]) 
               raise SqlmapMissingDependence(errMsg)"," 'Parse direct target details. 
 :param conf: Configuration object 
 :type conf: SqlmapConf 
 :return: None'",'Parse target dbms and set some attributes into the configuration singleton.'
"def create_access_key(user_name, region=None, key=None, keyid=None, profile=None): 
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
   try: 
      return conn.create_access_key(user_name) 
   except boto.exception.BotoServerError as e: 
      log.debug(e) 
      log.error('Failed   to   create   access   key.') 
      return str(e)"," 'Create a new access key for the given user. 
 :param user_name: The name of the user. 
 :param region: The region to create the access key for. 
 :param key: The key to use for authentication. 
 :param keyid: The key ID to use for authentication. 
 :param profile: The profile to use for authentication. 
 :returns: The access key or an error message. 
 :rtype: str or boto.s3.connection.Key'","'Create access key id for a user. 
 .. versionadded:: 2015.8.0 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_iam.create_access_key myuser'"
"def _eintr_retry(func, *args): 
    while True: 
      try: 
         return func(*args) 
      except (OSError, select.error) as e: 
         if (e.args[0] != errno.EINTR): 
            raise", 'Retry on EINTR errors','restart a system call interrupted by EINTR'
"def validate_color(s): 
    if (s.lower() == 'none'): 
      return 'None' 
   if is_color_like(s): 
      return s 
   stmp = ('#' + s) 
   if is_color_like(stmp): 
      return stmp 
   colorarg = s 
   msg = '' 
   if (s.find(',') >= 0): 
      stmp = ''.join([c for c in s if (c.isdigit() or (c == '.') or (c == ','))]) 
      vals = stmp.split(',') 
      if (len(vals) != 3): 
         msg = '\nColor   tuples   must   be   length   3' 
      else: 
         try: 
            colorarg = [float(val) for val in vals] 
         except ValueError: 
            msg = '\nCould   not   convert   all   entries   to   floats' 
   if ((not msg) and is_color_like(colorarg)): 
      return colorarg 
   raise ValueError(('%s   does   not   look   like   a   color   arg%s' % (s, msg)))"," 'Validates a color argument. 
 If the argument is a color name, returns it. 
 If the argument is a color string, returns it. 
 If the argument is a color tuple, returns it. 
 Otherwise, raises a ValueError. 
 :param s: color argument 
 :returns: color argument'",'return a valid color arg'
"def find_tests(testdir, prefixes=DEFAULT_PREFIXES, suffix='.py', excludes=(), remove_suffix=True): 
    tests = [] 
   for name in os.listdir(testdir): 
      if ((not suffix) or name.endswith(suffix)): 
         for prefix in prefixes: 
            if name.startswith(prefix): 
               if (remove_suffix and name.endswith(suffix)): 
                  name = name[:(- len(suffix))] 
               if (name not in excludes): 
                  tests.append(name) 
   tests.sort() 
   return tests"," 'Return a list of test files in the test directory. 
 If ``remove_suffix`` is True, the ``suffix`` will be removed from the 
 test name. 
 ``excludes`` is a list of test names to exclude. 
 ``prefixes`` is a list of prefixes to use for searching. 
 ``suffix`` is the suffix to look for at the end of the test name.'",'Return a list of all applicable test modules.'
"@lockutils.synchronized('storage-registry-lock', 'nova-', external=True) 
 def get_storage_users(storage_path): 
    d = {} 
   id_path = os.path.join(storage_path, 'compute_nodes') 
   if os.path.exists(id_path): 
      with open(id_path) as f: 
         d = json.loads(f.read()) 
   recent_users = [] 
   for node in d: 
      if ((time.time() - d[node]) < TWENTY_FOUR_HOURS): 
         recent_users.append(node) 
   return recent_users"," 'Returns a list of storage users that are active 
 within the last 24 hours. 
 :param storage_path: The path to the storage 
 :returns: A list of storage users that are active 
 :rtype: list'",'Get a list of all the users of this storage path.'
"def tob(data, enc='utf8'): 
    return (data.encode(enc) if isinstance(data, unicode) else bytes(data))", 'Return bytes object of data.','Convert anything to bytes'
"@raise_if_offline 
 def schema_has_table(table_name): 
    bind = op.get_bind() 
   insp = sa.engine.reflection.Inspector.from_engine(bind) 
   return (table_name in insp.get_table_names())"," 'Returns True if the given table exists in the database. 
 :param table_name: the name of the table to check for. 
 :returns: True if the table exists, False otherwise.'","'Check whether the specified table exists in the current schema. 
 This method cannot be executed in offline mode.'"
"@event(u'manager.startup') 
 def init_parsers(manager): 
    for parser_type in PARSER_TYPES: 
      parsers[parser_type] = {} 
      for p in plugin.get_plugins(interface=(parser_type + u'_parser')): 
         parsers[parser_type][p.name.replace(u'parser_', u'')] = p.instance 
      func_name = (u'parse_' + parser_type) 
      default_parsers[parser_type] = max(iter(parsers[parser_type].items()), key=(lambda p: getattr(getattr(p[1], func_name), u'priority', 0)))[0] 
      log.debug((u'setting   default   %s   parser   to   %s.   (options:   %s)' % (parser_type, default_parsers[parser_type], parsers[parser_type])))"," 'Initialize the parsers. 
 :param manager: The manager instance. 
 :type manager: :class:`~.manager.Manager` 
 :return: None 
 :rtype: None'",'Prepare our list of parsing plugins and default parsers.'
"def parseline(line): 
    fields = [] 
   (i, n) = (0, len(line)) 
   while (i < n): 
      (field, i) = parsefield(line, i, n) 
      fields.append(field) 
      i = (i + 1) 
   if (len(fields) < 2): 
      return (None, None) 
   (key, view, rest) = (fields[0], fields[1], fields[2:]) 
   fields = {'view': view} 
   for field in rest: 
      i = field.find('=') 
      if (i < 0): 
         fkey = field 
         fvalue = '' 
      else: 
         fkey = field[:i].strip() 
         fvalue = field[(i + 1):].strip() 
      if (fkey in fields): 
         pass 
      else: 
         fields[fkey] = fvalue 
   return (key, fields)"," 'Parse a line of the form: 
 <key> = <view> <field>=<value> [<field>=<value> ...] 
 Returns (key, fields) where fields is a dict of the form: 
 {""view"": view, ""field"": value, ...} 
 If the line does not parse properly, returns None, None.'","'Parse one entry in a mailcap file and return a dictionary. 
 The viewing command is stored as the value with the key ""view"", 
 and the rest of the fields produce key-value pairs in the dict.'"
"def _qsturng(p, r, v): 
    global A, p_keys, v_keys 
   if ((p < 0.1) or (p > 0.999)): 
      raise ValueError('p   must   be   between   .1   and   .999') 
   if (p < 0.9): 
      if (v < 2): 
         raise ValueError('v   must   be   >   2   when   p   <   .9') 
   elif (v < 1): 
      raise ValueError('v   must   be   >   1   when   p   >=   .9') 
   p = float(p) 
   if isinstance(v, np.ndarray): 
      v = v.item() 
   if ((p, v) in A): 
      y = (_func(A[(p, v)], p, r, v) + 1.0) 
   elif ((p not in p_keys) and (v not in (v_keys + ([], [1])[(p >= 0.9)]))): 
      (v0, v1, v2) = _select_vs(v, p) 
      (p0, p1, p2) = _select_ps(p) 
      r0_sq = (_interpolate_p(p, r, v0) ** 2) 
      r1_sq = (_interpolate_p(p, r, v1) ** 2) 
      r2_sq = (_interpolate_p(p, r, v2) ** 2) 
      (v_, v0_, v1_, v2_) = ((1.0 / v), (1.0 / v0), (1.0 / v1), (1.0 / v2)) 
      d2 = ((2.0 * (((r2_sq - r1_sq) / (v2_ - v1_)) - ((r0_sq - r1_sq) / (v0_ - v1_)))) / (v2_ - v0_)) 
      if ((v2_ + v0_) >= (v1_ + v1_)): 
         d1 = (((r2_sq - r1_sq) / (v2_ - v1_)) - ((0.5 * d2) * (v2_ - v1_))) 
      else: 
         d1 = (((r1_sq - r0_sq) / (v1_ - v0_)) + ((0.5 * d2) * (v1_ - v0_))) 
      d0 = r1_sq 
      y = math.sqrt(((((d2 / 2.0) * ((v_ - v1_) ** 2.0)) + (d1 * (v_ - v1_))) + d0)) 
   elif (v not in (v_keys + ([], [1])[(p >= 0.9)])): 
      y = _interpolate_v(p, r, v) 
   elif (p not in p_keys): 
      y = _interpolate_p(p, r, v) 
   return ((math.sqrt(2) * (- y)) * scipy.stats.t.isf(((1.0 + p) / 2.0), (v, 1e+38)[(v > 1e+38)]))"," 'Returns the value of the t-distribution with degrees of freedom 
 `df` and location parameter `loc` given a probability `p` and 
 location parameter `v`. 
 Parameters 
 p : float, in [0, 1] 
 The probability of the t-distribution. 
 r : float, in [0, 1] 
 The location parameter of the t-distribution. 
 v : float, in [0, 1] 
 The shape parameter of the t-distribution. 
 Returns 
 y : float 
 The value of the t-distribution. 
 Notes 
 The t-distribution is defined as: 
 .. math:: 
 \begin{align} 
 f(x|p,r,v) &= \frac{1}{\sqrt{2\pi v}} \exp \left(-\frac{(x-r)^2}{2v} \right) 
 \frac{1}{\Gamma(v)} \left(\frac{x-r}{v}\right)^{v-1} 
 \end{align} 
 where `x` is the t-distribution's random variable and `r`",'scalar version of qsturng'
"def delete(blob_key, **options): 
    fut = delete_async(blob_key, **options) 
   return fut.get_result()"," 'Delete a blob. 
 :param blob_key: Blob key. 
 :param options: Keyword arguments to be passed to the underlying 
 :py:meth:`DeleteBlob` call. 
 :returns: The result of the delete operation. 
 :rtype: :py:class:`azure.storage.blob.models.DeleteBlobResult`` 
 :raises :py:exc:`~azure.storage.common.StorageError`: If the request 
 failed. 
 :raises :py:exc:`~azure.storage.common.ServiceError`: If the service 
 returned an error response.'","'Delete a blob from Blobstore. 
 Args: 
 blob_key: A blob key.  This may be a str, unicode or BlobKey instance. 
 **options: Options for create_rpc().'"
"def TRANGE(barDs, count): 
    return call_talib_with_hlc(barDs, count, talib.TRANGE)"," 'Returns the TRANGE indicator for the given barDs and count. 
 The TRANGE indicator is a moving average of the range. 
 Parameters 
 barDs : bar_frame 
 The bar_frame object. 
 count : int 
 The number of bars to calculate the TRANGE indicator for. 
 Returns 
 The TRANGE indicator for the given barDs and count. 
 Examples 
 >>> from fxcm.talib import TRANGE 
 >>> from fxcm.barframe import bar_frame 
 >>> barDs = bar_frame(start_date=datetime(2010, 1, 1), end_date=datetime(2010, 1, 10), timeframe=barframe.Daily) 
 >>> TRANGE(barDs, 10) 
 [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] 
 >>> TRANGE(barDs, 20) 
",'True Range'
"def valid_year(year): 
    return (1920 <= year < 2030)", 'Check if the year is valid.','Check if number is a valid year'
"def orchestrate_high(data, test=None, queue=False, pillar=None, **kwargs): 
    if ((pillar is not None) and (not isinstance(pillar, dict))): 
      raise SaltInvocationError('Pillar   data   must   be   formatted   as   a   dictionary') 
   __opts__['file_client'] = 'local' 
   minion = salt.minion.MasterMinion(__opts__) 
   running = minion.functions['state.high'](data, test=None, queue=False, pillar=pillar, **kwargs) 
   ret = {minion.opts['id']: running} 
   __jid_event__.fire_event({'data': ret, 'outputter': 'highstate'}, 'progress') 
   return ret"," 'Orchestrate high state on a minion. 
 :param data: 
 :param test: 
 :param queue: 
 :param pillar: 
 :return: 
 :raises: 
 :rtype: dict'","'Execute a single state orchestration routine 
 .. versionadded:: 2015.5.0 
 CLI Example: 
 .. code-block:: bash 
 salt-run state.orchestrate_high \'{ 
 stage_one: 
 {salt.state: [{tgt: ""db*""}, {sls: postgres_setup}]}, 
 stage_two: 
 {salt.state: [{tgt: ""web*""}, {sls: apache_setup}, { 
 require: [{salt: stage_one}],'"
"def flatten(a): 
    if isinstance(a, (tuple, list, set)): 
      l = [] 
      for item in a: 
         l.extend(flatten(item)) 
      return l 
   else: 
      return [a]", 'Flatten a nested list of lists',"'Recursively flatten tuple, list and set in a list.'"
"def run(): 
    all_cats = Category.objects.filter(type=amo.ADDON_WEBAPP) 
   try: 
      entertainment = all_cats.filter(slug='entertainment-sports')[0] 
   except IndexError: 
      print 'Could   not   find   Category   with   slug=""entertainment-sports""' 
   else: 
      entertainment.name = 'Entertainment' 
      entertainment.slug = 'entertainment' 
      entertainment.save() 
      print 'Renamed   ""Entertainment   &   Sports""   to   ""Entertainment""' 
   Category.objects.create(type=amo.ADDON_WEBAPP, slug='sports', name='Sports') 
   print 'Created   ""Sports""' 
   try: 
      music = all_cats.filter(slug='music')[0] 
   except IndexError: 
      print 'Could   not   find   Category   with   slug=""music""' 
   else: 
      music.name = 'Music' 
      music.save() 
      print 'Renamed   ""Music   &   Audio""   to   ""Music""' 
   try: 
      social = all_cats.filter(slug='social')[0] 
   except IndexError: 
      print 'Could   not   find   Category   with   slug=""social""' 
   else: 
      social.name = 'Social' 
      social.save() 
      print 'Renamed   ""Social   &   Communication""   to   ""Social""' 
   try: 
      books = all_cats.filter(slug='books-reference')[0] 
   except IndexError: 
      print 'Could   not   find   Category   with   slug=""books-reference""' 
   else: 
      books.name = 'Books' 
      books.slug = 'books' 
      books.save() 
      print 'Renamed   ""Books   &   Reference""   to   ""Books""' 
   Category.objects.create(type=amo.ADDON_WEBAPP, slug='reference', name='Reference') 
   print 'Created   ""Reference""' 
   try: 
      photos = all_cats.filter(slug='photos-media')[0] 
   except IndexError: 
      print 'Could   not   find   Category   with   slug=""photos-media""' 
   else: 
      photos.name = 'Photo   &   Video' 
      photos.slug = 'photo-video' 
      photos.save() 
      print 'Renamed   ""Photos   &   Media""   to   ""Photo   &   Video""' 
   Category.objects.create(type=amo.ADDON_WEBAPP, slug='maps-navigation', name='Maps   &   Navigation') 
   print 'Created   ""Maps   &   Navigation""'"," 'Rename all categories with ""&"" in the slug to ""&"" instead.'","'We reorganized our categories: 
 https://bugzilla.mozilla.org/show_bug.cgi?id=854499 
 Usage:: 
 python -B manage.py runscript migrations.575-reorganize-cats'"
"def load_class(dotted_path): 
    dotted_path_split = dotted_path.split('.') 
   if (len(dotted_path_split) > 1): 
      klass_name = dotted_path_split[(-1)] 
      module_name = '.'.join(dotted_path_split[:(-1)]) 
      module = load_module(module_name) 
      if has_attribute(module, klass_name): 
         klass = getattr(module, klass_name) 
         return klass 
      else: 
         raise AttributeError(('Module   %s   does   not   have   class   attribute   %s' % (module_name, klass_name))) 
   else: 
      raise ValueError(('Dotted   module   path   %s   must   contain   a   module   name   and   a   classname' % dotted_path))"," 'Loads a class from a dotted path. 
 The dotted path is a string in the format: 
 ``module.submodule.ClassName`` 
 Returns the class object or raises an exception if it can\'t be found. 
 The class object is returned as is, so it may be a subclass of 
 :class:`~sage.structure.freezer.Freezer`. 
 Examples 
 >>> from sage.structure.freezer import load_class 
 >>> load_class(""sage.structure.freezer.Freezer"") 
 <class \'sage.structure.freezer.Freezer\'> 
 >>> load_class(""sage.structure.freezer.Freezer.Subclass"") 
 <class \'sage.structure.freezer.Freezer.Subclass\'>'","'Loads and returns a class definition provided a dotted path 
 specification the last part of the dotted path is the class name 
 and there is at least one module name preceding the class name. 
 Notes: 
 You will need to ensure that the module you are trying to load 
 exists in the Python path. 
 Examples: 
 - module.name.ClassName    # Provided module.name is in the Python path. 
 - module.ClassName         # Provided module is in the Python path. 
 What won\'t work: 
 - ClassName 
 - modle.name.ClassName     # Typo in module name. 
 - module.name.ClasNam      # Typo in classname.'"
"def resolve_link(stats): 
    return stats", 'Resolve a link.','Passes through the projected Files/Dirs for link resolution.'
"def deconv(X, w, subsample=(1, 1), border_mode=(0, 0), conv_mode='conv'): 
    img = gpu_contiguous(X) 
   kerns = gpu_contiguous(w) 
   desc = GpuDnnConvDesc(border_mode=border_mode, subsample=subsample, conv_mode=conv_mode)(gpu_alloc_empty(img.shape[0], kerns.shape[1], (img.shape[2] * subsample[0]), (img.shape[3] * subsample[1])).shape, kerns.shape) 
   out = gpu_alloc_empty(img.shape[0], kerns.shape[1], (img.shape[2] * subsample[0]), (img.shape[3] * subsample[1])) 
   d_img = GpuDnnConvGradI()(kerns, img, out, desc) 
   return d_img"," 'Deconvolution of a 4D tensor with a 4D tensor. 
 Parameters 
 X : 4D tensor 
 Input tensor. 
 w : 4D tensor 
 Weights tensor. 
 subsample : tuple 
 Subsampling factors. 
 border_mode : tuple 
 Border mode. 
 conv_mode : str 
 Convolution mode. 
 Returns 
 d_img : 4D tensor 
 Deconvolution output. 
 Notes 
 The output is a 4D tensor with the same shape as the input. 
 The weights are normalized with respect to the input tensor. 
 The output is normalized with respect to the input tensor. 
 Examples 
 >>> import torch 
 >>> from torch.nn.functional import conv2d, deconv 
 >>> img = torch.randn(3, 10, 10, 1) 
 >>> w = torch.randn(3, 3, 3, 3) 
 >>> out = deconv(img, w) 
 >>> out.shape 
 torch.Size([3,","'sets up dummy convolutional forward pass and uses its grad as deconv 
 currently only tested/working with same padding'"
"def raw_command(cmd, capture=False, env=None, data=None, cwd=None, explain=False, stdin=None, stdout=None): 
    if (not cwd): 
      cwd = os.getcwd() 
   if (not env): 
      env = common_environment() 
   cmd = list(cmd) 
   escaped_cmd = '   '.join((pipes.quote(c) for c in cmd)) 
   display.info(('Run   command:   %s' % escaped_cmd), verbosity=1) 
   display.info(('Working   directory:   %s' % cwd), verbosity=2) 
   program = find_executable(cmd[0], cwd=cwd, path=env['PATH'], required='warning') 
   if program: 
      display.info(('Program   found:   %s' % program), verbosity=2) 
   for key in sorted(env.keys()): 
      display.info(('%s=%s' % (key, env[key])), verbosity=2) 
   if explain: 
      return (None, None) 
   communicate = False 
   if (stdin is not None): 
      data = None 
      communicate = True 
   elif (data is not None): 
      stdin = subprocess.PIPE 
      communicate = True 
   if stdout: 
      communicate = True 
   if capture: 
      stdout = (stdout or subprocess.PIPE) 
      stderr = subprocess.PIPE 
      communicate = True 
   else: 
      stderr = None 
   start = time.time() 
   try: 
      process = subprocess.Popen(cmd, env=env, stdin=stdin, stdout=stdout, stderr=stderr, cwd=cwd) 
   except OSError as ex: 
      if (ex.errno == errno.ENOENT): 
         raise ApplicationError(('Required   program   ""%s""   not   found.' % cmd[0])) 
      raise 
   if communicate: 
      (stdout, stderr) = process.communicate(data) 
   else: 
      process.wait() 
      (stdout, stderr) = (None, None) 
   status = process.returncode 
   runtime = (time.time() - start) 
   display.info(('Command   exited   with   status   %s   after   %s   seconds.' % (status, runtime)), verbosity=4) 
   if (status == 0): 
      return (stdout, stderr) 
   raise SubprocessError(cmd, status, stdout, stderr, runtime)"," 'Run a command. 
 :param cmd: The command to run. 
 :param capture: If True, capture the standard output and standard error. 
 :param env: The environment to run the command in. 
 :param data: If provided, the data to pass to the command. 
 :param cwd: The working directory to run the command in. 
 :param explain: If True, return None and None instead of the standard output and standard error. 
 :param stdin: The input to the command. 
 :param stdout: The output to capture. 
 :param stderr: The error output to capture. 
 :return: A tuple of the standard output and standard error. 
 :raises SubprocessError: If the command exits with a non-zero status. 
 :raises ApplicationError: If the required program is not found.'","':type cmd: collections.Iterable[str] 
 :type capture: bool 
 :type env: dict[str, str] | None 
 :type data: str | None 
 :type cwd: str | None 
 :type explain: bool 
 :type stdin: file | None 
 :type stdout: file | None 
 :rtype: str | None, str | None'"
"def getAlterationLines(fileName): 
    return archive.getTextLines(getAlterationFile(fileName))"," 'Return the lines in the alteration file for the given file name. 
 :param fileName: The name of the file to get alterations for. 
 :return: A list of lines in the alteration file.'",'Get the text lines from the fileName in the alterations directories.'
"def isFileLocked(checkfile, writeLockCheck=False): 
    checkfile = ek(os.path.abspath, checkfile) 
   if (not ek(os.path.exists, checkfile)): 
      return True 
   try: 
      f = ek(io.open, checkfile, u'rb') 
      f.close() 
   except IOError: 
      return True 
   if writeLockCheck: 
      lockFile = (checkfile + u'.lckchk') 
      if ek(os.path.exists, lockFile): 
         ek(os.remove, lockFile) 
      try: 
         ek(os.rename, checkfile, lockFile) 
         time.sleep(1) 
         ek(os.rename, lockFile, checkfile) 
      except (OSError, IOError): 
         return True 
   return False"," 'Checks if the file is locked by another process. 
 If writeLockCheck is True, the check will be done with a write lock. 
 If writeLockCheck is False, the check will be done with a read lock. 
 :param checkfile: the file to check 
 :type checkfile: str 
 :param writeLockCheck: if True, the write lock will be used 
 :type writeLockCheck: bool 
 :return: True if the file is locked, False otherwise'","'Checks to see if a file is locked. Performs three checks 
 1. Checks if the file even exists 
 2. Attempts to open the file for reading. This will determine if the file has a write lock. 
 Write locks occur when the file is being edited or copied to, e.g. a file copy destination 
 3. If the readLockCheck parameter is True, attempts to rename the file. If this fails the 
 file is open by some other process for reading. The file can be read, but not written to 
 or deleted. 
 :param file: the file being checked 
 :param writeLockCheck: when true will check if the file is locked for writing (prevents move operations)'"
"def get_era(): 
    return UUID(hex=_BOOT_ID.getContent().strip())", 'Return the current era.',':return UUID: A node- and boot-specific globally unique id.'
"def build_pdf(branch): 
    os.chdir(os.path.join(gitdname, 'statsmodels', 'docs')) 
   sphinx_dir = os.path.join(virtual_dir, 'bin') 
   retcode = subprocess.call('   '.join(['make', 'latexpdf', (('SPHINXBUILD=' + sphinx_dir) + '/sphinx-build')]), shell=True) 
   if (retcode != 0): 
      msg = ('Could   not   build   the   pdf   docs   for   branch   %s' % branch) 
      raise Exception(msg) 
   os.chdir(dname)", 'Build the pdf docs for a branch',"'Changes into new_branch_dir and builds the docs using sphinx in the 
 BUILDENV virtualenv'"
"def attach_ordered_steps(workflow, steps): 
    ordered_steps = order_workflow_steps(steps) 
   workflow.has_cycles = (not bool(ordered_steps)) 
   for (i, step) in enumerate((ordered_steps or steps)): 
      step.order_index = i 
      workflow.steps.append(step)"," 'Attach a sequence of steps to the end of a workflow. 
 :param workflow: The workflow to attach the steps to. 
 :param steps: The sequence of steps to attach to the workflow. 
 :return: The workflow with the steps attached.'","'Attempt to topologically order steps and attach to workflow. If this 
 fails - the workflow contains cycles so it mark it as such.'"
"def check_message(keywords, message): 
    (exc_type, exc_value, exc_traceback) = sys.exc_info() 
   if set(str(exc_value).split('   ')).issuperset(set(keywords)): 
      exc_value._safe_message = message 
      raise", 'Raise a new exception with a custom message.',"'Checks an exception for given keywords and raises a new ``ActionError`` 
 with the desired message if the keywords are found. This allows selective 
 control over API error messages.'"
"def _fix_global_ids(html): 
    html = re.sub('id=""\\d+""', 'id=""###""', html) 
   global_id = 1 
   while (len(re.findall('id=""###""', html)) > 0): 
      html = re.sub('id=""###""', ('id=""%s""' % global_id), html, count=1) 
      global_id += 1 
   return html"," 'Fixes global ids in HTML, so that they are unique.'",'Fix the global_ids after reordering in _render_toc().'
"def test_scenario_has_name(): 
    scenario = Scenario.from_string(SCENARIO1) 
   assert isinstance(scenario, Scenario) 
   assert_equals(scenario.name, 'Adding   some   students   to   my   university   database')", 'Test scenario name','It should extract the name of the scenario'
"def volumedriver(cls): 
    _volume_register.append(cls) 
   return cls", 'Registers a volume driver to be used by the volume driver manager.','Decorator for concrete volume driver implementations.'
"def _matchingString(constantString, inputString): 
    if isinstance(constantString, bytes): 
      otherType = constantString.decode('ascii') 
   else: 
      otherType = constantString.encode('ascii') 
   if (type(constantString) == type(inputString)): 
      return constantString 
   else: 
      return otherType"," 'Returns a string that matches the input string, or the input string itself 
 if it is already a string.'","'Some functions, such as C{os.path.join}, operate on string arguments which 
 may be bytes or text, and wish to return a value of the same type.  In 
 those cases you may wish to have a string constant (in the case of 
 C{os.path.join}, that constant would be C{os.path.sep}) involved in the 
 parsing or processing, that must be of a matching type in order to use 
 string operations on it.  L{_matchingString} will take a constant string 
 (either L{bytes} or L{unicode}) and convert it to the same type as the 
 input string.  C{constantString} should contain only characters from ASCII; 
 to ensure this, it will be encoded or decoded regardless. 
 @param constantString: A string literal used in processing. 
 @type constantString: L{unicode} or L{bytes} 
 @param inputString: A byte string or text string provided by the user. 
 @type inputString: L{unicode} or L{bytes} 
 @return: C{constantString} converted into the same type as C{inputString} 
 @rtype: the type of C{inputString}'"
"def make_tarball(base_name, base_dir, compress='gzip', verbose=0, dry_run=0, owner=None, group=None): 
    tar_compression = {'gzip': 'gz', 'bzip2': 'bz2', None: '', 'compress': ''} 
   compress_ext = {'gzip': '.gz', 'bzip2': '.bz2', 'compress': '.Z'} 
   if ((compress is not None) and (compress not in compress_ext.keys())): 
      raise ValueError, ""bad   value   for   'compress':   must   be   None,   'gzip',   'bzip2'   or   'compress'"" 
   archive_name = (base_name + '.tar') 
   if (compress != 'compress'): 
      archive_name += compress_ext.get(compress, '') 
   mkpath(os.path.dirname(archive_name), dry_run=dry_run) 
   import tarfile 
   log.info('Creating   tar   archive') 
   uid = _get_uid(owner) 
   gid = _get_gid(group) 
   def _set_uid_gid(tarinfo): 
      if (gid is not None): 
         tarinfo.gid = gid 
         tarinfo.gname = group 
      if (uid is not None): 
         tarinfo.uid = uid 
         tarinfo.uname = owner 
      return tarinfo 
   if (not dry_run): 
      tar = tarfile.open(archive_name, ('w|%s' % tar_compression[compress])) 
      try: 
         tar.add(base_dir, filter=_set_uid_gid) 
      finally: 
         tar.close() 
   if (compress == 'compress'): 
      warn(""'compress'   will   be   deprecated."", PendingDeprecationWarning) 
      compressed_name = (archive_name + compress_ext[compress]) 
      if (sys.platform == 'win32'): 
         cmd = [compress, archive_name, compressed_name] 
      else: 
         cmd = [compress, '-f', archive_name] 
      spawn(cmd, dry_run=dry_run) 
      return compressed_name 
   return archive_name"," 'Make a tarball of a directory. 
 :param base_name: base name of tarball 
 :param base_dir: directory to tarball 
 :param compress: compression type, one of: 
 :param dry_run: if True, do not actually create the tarball 
 :param owner: user to set owner of tarball 
 :param group: group to set group of tarball 
 :return: path to tarball'","'Create a (possibly compressed) tar file from all the files under 
 \'base_dir\'. 
 \'compress\' must be ""gzip"" (the default), ""compress"", ""bzip2"", or None. 
 (compress will be deprecated in Python 3.2) 
 \'owner\' and \'group\' can be used to define an owner and a group for the 
 archive that is being built. If not provided, the current owner and group 
 will be used. 
 The output tar file will be named \'base_dir\' +  "".tar"", possibly plus 
 the appropriate compression extension ("".gz"", "".bz2"" or "".Z""). 
 Returns the output filename.'"
"def select(rlist, wlist, xlist, timeout=None): 
    allevents = [] 
   timeout = Timeout.start_new(timeout) 
   result = SelectResult() 
   try: 
      try: 
         for readfd in rlist: 
            allevents.append(core.read_event(get_fileno(readfd), result.update, arg=readfd)) 
         for writefd in wlist: 
            allevents.append(core.write_event(get_fileno(writefd), result.update, arg=writefd)) 
      except IOError as ex: 
         raise error(*ex.args) 
      result.event.wait(timeout=timeout) 
      return (result.read, result.write, []) 
   finally: 
      for evt in allevents: 
         evt.cancel() 
      timeout.cancel()"," 'Select on a set of file descriptors. 
 :param rlist: list of file descriptors to read from 
 :param wlist: list of file descriptors to write to 
 :param xlist: list of file descriptors to be polled 
 :param timeout: optional timeout for the select operation 
 :return: (read, write, x) where read, write are the number of bytes read 
 or written and x is the number of events that were polled but not read 
 or written. 
 :raises IOError: if there is an error while reading or writing.'","'An implementation of :meth:`select.select` that blocks only the current greenlet. 
 Note: *xlist* is ignored.'"
"def quote_chinese(url, encodeing='utf-8'): 
    if isinstance(url, six.text_type): 
      return quote_chinese(url.encode(encodeing)) 
   if six.PY3: 
      res = [(six.int2byte(b).decode('latin-1') if (b < 128) else ('%%%02X' % b)) for b in url] 
   else: 
      res = [(b if (ord(b) < 128) else ('%%%02X' % ord(b))) for b in url] 
   return ''.join(res)"," 'Quote a URL in Chinese. 
 :param url: 
 :type url: 
 :param encodeing: 
 :type encodeing: 
 :return: 
 :rtype: 
 :example: 
 >>> quote_chinese(""http://www.google.com"") 
 ""http://www.google.com"" 
 >>> quote_chinese(""http://www.google.com/"") 
 ""http://www.google.com/%E7%A7%91%E7%94%B1"" 
 >>> quote_chinese(""http://www.google.com/中文"") 
 ""http://www.google.com/%E5%8A%A0%E5%85%B7"" 
 >>> quote_chinese(""http://www.google.com/中文/"") 
 ""http://www.google.com/%E5%8A%A0%E5%85%B7/%E5%85%B7"" 
 >>> quote_chinese(""http://www.google.com/中文/中文"") 
 ""http://",'Quote non-ascii characters'
"@task(base=BaseInstructorTask, routing_key=settings.GRADES_DOWNLOAD_ROUTING_KEY) 
 def generate_certificates(entry_id, xmodule_instance_args): 
    action_name = ugettext_noop('certificates   generated') 
   TASK_LOG.info(u'Task:   %s,   InstructorTask   ID:   %s,   Task   type:   %s,   Preparing   for   task   execution', xmodule_instance_args.get('task_id'), entry_id, action_name) 
   task_fn = partial(generate_students_certificates, xmodule_instance_args) 
   return run_main_task(entry_id, task_fn, action_name)"," 'Generate certificates for students. 
 This task is invoked by the Gradebook API when a student completes a 
 course. 
 Args: 
 entry_id (str): The entry ID of the course. 
 xmodule_instance_args (dict): A dictionary containing the following keys: 
 task_id (str): The task ID of the task. 
 course_id (str): The course ID of the course. 
 course_run (CourseRun): The course run. 
 course_mode (str): The course mode. 
 course_mode_slug (str): The course mode slug. 
 course_mode_display_name (str): The course mode display name. 
 course_display_name (str): The course display name. 
 course_display_id (str): The course display ID. 
 course_display_slug (str): The course display slug. 
 course_display_url (str): The course display URL. 
 course_url (str): The course URL. 
 course_id_string (str): The course ID string. 
 course_url_string (str):",'Grade students and generate certificates.'
"@gzip_page 
 @cache_control(max_age=settings.CACHE_MIDDLEWARE_SECONDS) 
 def commonplace(request, repo, **kwargs): 
    if (repo not in settings.FRONTEND_REPOS): 
      raise Http404 
   BUILD_ID = get_build_id(repo) 
   ua = request.META.get('HTTP_USER_AGENT', '').lower() 
   include_splash = False 
   detect_region_with_geoip = False 
   if (repo == 'fireplace'): 
      include_splash = True 
      has_sim_info_in_query = (('mccs' in request.GET) or (('mcc' in request.GET) and ('mnc' in request.GET))) 
      if (not has_sim_info_in_query): 
         detect_region_with_geoip = True 
   (fxa_auth_state, fxa_auth_url) = fxa_auth_info() 
   site_settings = {'dev_pay_providers': settings.DEV_PAY_PROVIDERS, 'fxa_auth_state': fxa_auth_state, 'fxa_auth_url': fxa_auth_url} 
   ctx = {'BUILD_ID': BUILD_ID, 'LANG': request.LANG, 'langdir': lang_dir(request.LANG), 'include_splash': include_splash, 'repo': repo, 'robots': ('googlebot' in ua), 'site_settings': site_settings, 'newrelic_header': newrelic.agent.get_browser_timing_header, 'newrelic_footer': newrelic.agent.get_browser_timing_footer} 
   if (repo == 'fireplace'): 
      resolved_url = resolve(request.path) 
      if (resolved_url.url_name == 'detail'): 
         ctx = add_app_ctx(ctx, resolved_url.kwargs['app_slug']) 
   ctx['waffle_switches'] = list(waffle.models.Switch.objects.filter(active=True).values_list('name', flat=True)) 
   media_url = urlparse(settings.MEDIA_URL) 
   if media_url.netloc: 
      ctx['media_origin'] = ((media_url.scheme + '://') + media_url.netloc) 
   if detect_region_with_geoip: 
      region_middleware = RegionMiddleware() 
      ctx['geoip_region'] = region_middleware.region_from_request(request) 
   if (repo == 'marketplace-tv-front-end'): 
      return render(request, 'commonplace/index_tv.html', ctx) 
   elif (repo in settings.REACT_REPOS): 
      return render(request, 'commonplace/index_react.html', ctx) 
   elif (repo in settings.COMMONPLACE_REPOS): 
      return render(request, 'commonplace/index.html', ctx)"," 'Commonplace view for the frontend. 
 This view is used by the frontend to render the main page for 
 a given repository. 
 :param request: The current request. 
 :param repo: The repository to render the page for. 
 :param kwargs: Additional arguments to pass to the template. 
 :return: A response. 
 :rtype: django.http.HttpResponse'",'Serves the frontend single-page apps.'
"def _find_image_files(data_dir, labels_file): 
    print(('Determining   list   of   input   files   and   labels   from   %s.' % data_dir)) 
   challenge_synsets = [l.strip() for l in tf.gfile.FastGFile(labels_file, 'r').readlines()] 
   labels = [] 
   filenames = [] 
   synsets = [] 
   label_index = 1 
   for synset in challenge_synsets: 
      jpeg_file_path = ('%s/%s/*.JPEG' % (data_dir, synset)) 
      matching_files = tf.gfile.Glob(jpeg_file_path) 
      labels.extend(([label_index] * len(matching_files))) 
      synsets.extend(([synset] * len(matching_files))) 
      filenames.extend(matching_files) 
      if (not (label_index % 100)): 
         print(('Finished   finding   files   in   %d   of   %d   classes.' % (label_index, len(challenge_synsets)))) 
      label_index += 1 
   shuffled_index = list(range(len(filenames))) 
   random.seed(12345) 
   random.shuffle(shuffled_index) 
   filenames = [filenames[i] for i in shuffled_index] 
   synsets = [synsets[i] for i in shuffled_index] 
   labels = [labels[i] for i in shuffled_index] 
   print(('Found   %d   JPEG   files   across   %d   labels   inside   %s.' % (len(filenames), len(challenge_synsets), data_dir))) 
   return (filenames, synsets, labels)"," 'Returns a tuple of filenames, synsets, and labels. 
 The filenames are the names of the JPEG files. 
 The synsets are the labels, which are the synsets that were provided 
 in the labels file. 
 The labels are the indices of the labels in the labels file.'","'Build a list of all images files and labels in the data set. 
 Args: 
 data_dir: string, path to the root directory of images. 
 Assumes that the ImageNet data set resides in JPEG files located in 
 the following directory structure. 
 data_dir/n01440764/ILSVRC2012_val_00000293.JPEG 
 data_dir/n01440764/ILSVRC2012_val_00000543.JPEG 
 where \'n01440764\' is the unique synset label associated with these images. 
 labels_file: string, path to the labels file. 
 The list of valid labels are held in this file. Assumes that the file 
 contains entries as such: 
 n01440764 
 n01443537 
 n01484850 
 where each line corresponds to a label expressed as a synset. We map 
 each synset contained in the file to an integer (based on the alphabetical 
 ordering) starting with the integer 1 corresponding to the synset 
 contained in the first line. 
 The reason we start the integer labels at 1 is to reserve label 0 as an 
 unused background class. 
 Returns: 
 filenames: list of strings; each string is a path to an image file. 
 synsets: list of strings; each string is a unique WordNet ID. 
 labels: list of integer; each integer identifies the ground truth.'"
"def order_by_precedence(media_type_lst): 
    ret = [set(), set(), set(), set()] 
   for media_type in media_type_lst: 
      precedence = _MediaType(media_type).precedence 
      ret[(3 - precedence)].add(media_type) 
   return [media_types for media_types in ret if media_types]", 'Returns a list of media types ordered by precedence.',"'Returns a list of sets of media type strings, ordered by precedence. 
 Precedence is determined by how specific a media type is: 
 3. \'type/subtype; param=val\' 
 2. \'type/subtype\' 
 1. \'type/*\' 
 0. \'*/*\''"
"def filter_label_1(context, label): 
    return False", 'Filter label 1','Test Filter Label 1'
"def run(*arg, **kw): 
    kw['exit'] = False 
   return TestProgram(*arg, **kw).success"," 'Runs a test program. 
 :param arg: The arguments to pass to the test program. 
 :param kw: Additional keyword arguments to pass to the test program. 
 :return: The return value of the test program. 
 :rtype: int'","'Collect and run tests, returning success or failure. 
 The arguments to `run()` are the same as to `main()`: 
 * module: All tests are in this module (default: None) 
 * defaultTest: Tests to load (default: \'.\') 
 * argv: Command line arguments (default: None; sys.argv is read) 
 * testRunner: Test runner instance (default: None) 
 * testLoader: Test loader instance (default: None) 
 * env: Environment; ignored if config is provided (default: None; 
 os.environ is read) 
 * config: :class:`nose.config.Config` instance (default: None) 
 * suite: Suite or list of tests to run (default: None). Passing a 
 suite or lists of tests will bypass all test discovery and 
 loading. *ALSO NOTE* that if you pass a unittest.TestSuite 
 instance as the suite, context fixtures at the class, module and 
 package level will not be used, and many plugin hooks will not 
 be called. If you want normal nose behavior, either pass a list 
 of tests, or a fully-configured :class:`nose.suite.ContextSuite`. 
 * plugins: List of plugins to use; ignored if config is provided 
 (default: load plugins with DefaultPluginManager) 
 * addplugins: List of **extra** plugins to use. Pass a list of plugin 
 instances in this argument to make custom plugins available while 
 still using the DefaultPluginManager. 
 With the exception that the ``exit`` argument is always set 
 to False.'"
"def assert_samelines(testcase, text1, text2, msg=None): 
    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)", 'Test that two strings are equal line-by-line.',"'Asserts text1 and text2 have the same lines, ignoring differences in 
 line endings between platforms'"
"def main(*args): 
    if (not args): 
      args = sys.argv[1:] 
   logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(message)s') 
   for gt in args: 
      load_grammar(gt, save=True, force=True) 
   return True"," 'Load the grammar from a file. 
 :param args: 
 :return: 
 :rtype: 
 :raises: 
 :usage: 
 :example: 
 >>> from grammer import * 
 >>> main()'","'Main program, when run as a script: produce grammar pickle files. 
 Calls load_grammar for each argument, a path to a grammar text file.'"
"def getProcessOutput(executable, args=(), env={}, path=None, reactor=None, errortoo=0): 
    return _callProtocolWithDeferred((lambda d: _BackRelay(d, errortoo=errortoo)), executable, args, env, path, reactor)"," 'Returns a Deferred that will be fired with the output of the given 
 executable. 
 The executable is run with the given arguments in the given environment 
 and the given path. 
 If the process is killed by the operating system, the returned Deferred 
 will fire with an error. 
 If the process is killed by the user, the returned Deferred will fire with 
 an error. 
 If the process is killed by the user because it was too slow, the returned 
 Deferred will fire with the output of the process. 
 If the process is killed by the user because it failed, the returned 
 Deferred will fire with an error.'","'Spawn a process and return its output as a deferred returning a string. 
 @param executable: The file name to run and get the output of - the 
 full path should be used. 
 @param args: the command line arguments to pass to the process; a 
 sequence of strings. The first string should *NOT* be the 
 executable\'s name. 
 @param env: the environment variables to pass to the processs; a 
 dictionary of strings. 
 @param path: the path to run the subprocess in - defaults to the 
 current directory. 
 @param reactor: the reactor to use - defaults to the default reactor 
 @param errortoo: If true, include stderr in the result.  If false, if 
 stderr is received the returned L{Deferred} will errback with an 
 L{IOError} instance with a C{processEnded} attribute.  The 
 C{processEnded} attribute refers to a L{Deferred} which fires when the 
 executed process ends.'"
"def is_sorted(exp): 
    return _contains(exp, Sorted)", 'Returns True if the expression is sorted.','Does `exp` contain a `Reduced` node.'
"def extra_job_filters(not_yet_run=False, running=False, finished=False): 
    assert (not ((not_yet_run and running) or (not_yet_run and finished) or (running and finished))), 'Cannot   specify   more   than   one   filter   to   this   function' 
   not_queued = ('(SELECT   job_id   FROM   afe_host_queue_entries   WHERE   status   !=   ""%s"")' % models.HostQueueEntry.Status.QUEUED) 
   not_finished = '(SELECT   job_id   FROM   afe_host_queue_entries   WHERE   not   complete)' 
   if not_yet_run: 
      where = [('id   NOT   IN   ' + not_queued)] 
   elif running: 
      where = [('(id   IN   %s)   AND   (id   IN   %s)' % (not_queued, not_finished))] 
   elif finished: 
      where = [('id   NOT   IN   ' + not_finished)] 
   else: 
      return {} 
   return {'where': where}"," 'Generate a where clause for the extra_job_filters function. 
 :param not_yet_run: 
 :param running: 
 :param finished: 
 :return: 
 :rtype: 
 :return: 
 :rtype: dict'","'Generate a SQL WHERE clause for job status filtering, and return it in 
 a dict of keyword args to pass to query.extra().  No more than one of 
 the parameters should be passed as True. 
 * not_yet_run: all HQEs are Queued 
 * finished: all HQEs are complete 
 * running: everything else'"
"@contextmanager 
 def _latextools_module_hack(): 
    plugins_whitelist = get_setting('plugins_whitelist', ['external', 'getTeXRoot', 'latextools_utils']) 
   plugins_whitelist.append('latextools_plugin') 
   overwritten_modules = {} 
   whitelist = [(name, None) for name in plugins_whitelist] 
   whitelist.extend(internal._WHITELIST_ADDED) 
   __dir__ = os.path.dirname(__file__) 
   if (__dir__ == '.'): 
      __dir__ = os.path.join(sublime.packages_path(), 'LaTeXTools') 
   sys.path.insert(0, __dir__) 
   for (name, module) in whitelist: 
      if callable(module): 
         module = module() 
      if (name in sys.modules): 
         overwritten_modules[name] = sys.modules[name] 
      if (module is None): 
         latextools_module_name = _get_sublime_module_name(__dir__, name) 
         if (latextools_module_name in sys.modules): 
            sys.modules[name] = sys.modules[latextools_module_name] 
         else: 
            try: 
               sys.modules[name] = _load_module(name, name, __dir__) 
            except ImportError: 
               print('An   error   occurred   while   trying   to   load   white-listed   module   {0}'.format(name)) 
               traceback.print_exc() 
      else: 
         sys.modules[name] = module 
   sys.path.pop(0) 
   (yield) 
   for module in plugins_whitelist: 
      if (_get_sublime_module_name(__dir__, module) != module): 
         del sys.modules[module] 
      if (module in overwritten_modules): 
         sys.modules[module] = overwritten_modules[module]", 'Hack to make sure that LaTeXTools is loaded before any plugins.',"'Context manager to ensure sys.modules has certain white-listed modules, 
 most especially latextools_plugins. This exposes some of the modules in 
 LaTeXTools to plugins. It is intended primarily to expose library-esque 
 functionality, such as the getTeXRoot module, but can be configured by 
 the user as-needed.'"
"def not_friends(user, other_user): 
    return all((not_the_same(friend, other_user) for friend in user['friends']))"," 'Returns True if the user is not friends with the other_user, 
 False otherwise.'","'other_user is not a friend if he\'s not in user[""friends""]; 
 that is, if he\'s not_the_same as all the people in user[""friends""]'"
"def writeOutput(fileName=''): 
    fileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName) 
   if (fileName != ''): 
      skeinforge_craft.writeChainTextWithNounMessage(fileName, 'splodge')", 'Write the output to a file.','Splodge a gcode linear move file.'
"def main(): 
    module = AnsibleModule(argument_spec=dict(name=dict(type='str', required=True), new_name=dict(type='str'), config=dict(type='dict'), description=dict(type='str'), devices=dict(type='dict'), state=dict(choices=PROFILES_STATES, default='present'), url=dict(type='str', default='unix:/var/lib/lxd/unix.socket'), key_file=dict(type='str', default='{}/.config/lxc/client.key'.format(os.environ['HOME'])), cert_file=dict(type='str', default='{}/.config/lxc/client.crt'.format(os.environ['HOME'])), trust_password=dict(type='str')), supports_check_mode=False) 
   lxd_manage = LXDProfileManagement(module=module) 
   lxd_manage.run()", 'Manage LXD profiles.','Ansible Main module.'
"def _LC(f): 
    ring = f.ring 
   k = ring.ngens 
   yring = ring.clone(symbols=ring.symbols[(k - 1)]) 
   y = yring.gens[0] 
   degf = _deg(f) 
   lcf = yring.zero 
   for (monom, coeff) in f.iterterms(): 
      if (monom[:(-1)] == degf): 
         lcf += (coeff * (y ** monom[(-1)])) 
   return lcf"," 'Returns the leading coefficient of a polynomial. 
 Examples 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.densebasic import _LC 
 >>> _LC([[1, 2], [3, 4], [5, 6]]) 
 2'","'Compute the leading coefficient of a multivariate polynomial 
 `f \in K[x_0, \ldots, x_{k-2}, y] \cong K[y][x_0, \ldots, x_{k-2}]`. 
 Parameters 
 f : PolyElement 
 polynomial in `K[x_0, \ldots, x_{k-2}, y]` 
 Returns 
 lcf : PolyElement 
 polynomial in `K[y]`, leading coefficient of `f` 
 Examples 
 >>> from sympy.polys.modulargcd import _LC 
 >>> from sympy.polys import ring, ZZ 
 >>> R, x, y = ring(""x, y"", ZZ) 
 >>> f = x**2*y**2 + x**2*y - 1 
 >>> _LC(f) 
 y**2 + y 
 >>> R, x, y, z = ring(""x, y, z"", ZZ) 
 >>> f = x**2*y**2 + x**2*y - 1 
 >>> _LC(f) 
 1 
 >>> f = x*y*z - y**2*z**2 
 >>> _LC(f) 
 z'"
"def initialize_plugin(pelican_obj): 
    if (_MAIN_SETTINGS is None): 
      initialize_dbs(pelican_obj.settings) 
      subscribe_filter_to_signals(pelican_obj.settings)", 'Initialize the plugin.','Initialize plugin variables and Pelican settings'
"def shutdown_datastore(): 
    logging.info('Shutting   down   Cassandra.') 
   monit_interface.stop(cassandra_interface.CASSANDRA_MONIT_WATCH_NAME, is_group=False) 
   logging.warning('Done!') 
   return True", 'Shutdown the Cassandra cluster.',"'Top level function for bringing down Cassandra. 
 Returns: 
 True on success, False otherwise.'"
"def find_lemmata(tokens): 
    for token in tokens: 
      (word, pos, lemma) = (token[0], token[1], token[0]) 
      if pos.startswith(('DT',)): 
         lemma = singularize(word, pos='DT') 
      if pos.startswith('JJ'): 
         lemma = predicative(word) 
      if (pos == 'NNS'): 
         lemma = singularize(word) 
      if pos.startswith(('VB', 'MD')): 
         lemma = (conjugate(word, INFINITIVE) or word) 
      token.append(lemma.lower()) 
   return tokens"," 'Find lemmas for all tokens in the corpus. 
 :param tokens: list of tuples with token, pos, lemma 
 :return: list of tuples with token, pos, lemma'","'Annotates the tokens with lemmata for plural nouns and conjugated verbs, 
 where each token is a [word, part-of-speech] list.'"
"def test_raises_value_error_non_sym(): 
    P = np.array([[0.4, 0.6]]) 
   assert_raises(ValueError, MarkovChain, P) 
   assert_raises(ValueError, MarkovChain, sparse.csr_matrix(P))"," 'Test that a MarkovChain is raised if the transition matrix is not 
 symmetric.'",'Test with non symmetric input'
"def inherit_from_std_ex(node): 
    if ((node.name in ('Exception', 'BaseException')) and (node.root().name == EXCEPTIONS_MODULE)): 
      return True 
   return any((inherit_from_std_ex(parent) for parent in node.ancestors(recurs=False)))"," 'Check if this node is a subclass of an exception in the stdlib. 
 If so, return True, otherwise return False.'","'Return true if the given class node is subclass of 
 exceptions.Exception.'"
"def build_arg_list(fn, env): 
    kw = {} 
   argspec = inspect.getargspec(fn) 
   if argspec[2]: 
      kw = env 
   else: 
      argnames = argspec[0][1:] 
      for name in argnames: 
         if (name in env): 
            kw[name] = env[name] 
   return kw"," 'Builds a kwargs dict from the given function and environment. 
 The kwargs dict is used to build the kwargs for the call to the 
 function. 
 If the function has a positional argument list, the environment 
 is used to fill it. 
 If the function has keyword arguments, the environment is used 
 to fill them.'","'given a fn and and environment the builds a keyword argument list 
 for fn'"
"def is_python_proxy(parameter): 
    try: 
      is_proxy = (len(parameter.Java.implements) > 0) 
   except Exception: 
      is_proxy = False 
   return is_proxy", 'Returns true if the parameter is a Python proxy.',"'Determines whether parameter is a Python Proxy, i.e., it has a Java 
 internal class with an `implements` member. 
 :param parameter: the object to check. 
 :rtype: True if the parameter is a Python Proxy'"
"def interpret_size(si): 
    m = re.match(u'\\s*(\\d+)\\s*([ac-z]?)(b?)\\s*$', str(si), re.I) 
   if m: 
      if ((not m.group(2)) and m.group(3)): 
         times = 1 
      else: 
         times = (const.SIPrefixTimes[m.group(2).upper()] if m.group(2) else 1) 
      return (int(m.group(1)) * times) 
   else: 
      raise ValueError"," 'Interpret a string in the form of a size. 
 Examples: 
 \'1k\', \'1M\', \'1G\', \'1T\', \'1P\', \'1E\', \'1Z\', \'1Y\', \'1B\', \'1Ti\', \'1Gi\', \'1Mi\', \'1Ki\', \'1Bi\', \'1TiB\', \'1GiB\', \'1MiB\', \'1KiB\', \'1BiB\', \'1TiB\', \'1GiB\', \'1MiB\', \'1KiB\', \'1BiB\', \'1TiB\', \'1GiB\', \'1MiB\', \'1KiB\', \'1BiB\', \'1TiB\', \'1GiB\', \'1MiB\', \'1KiB\', \'1BiB","'>>> interpret_size(10) 
 10 
 >>> interpret_size(\'10\') 
 10 
 >>> interpret_size(\'10b\') 
 10 
 >>> interpret_size(\'10k\') 
 10240 
 >>> interpret_size(\'10K\') 
 10240 
 >>> interpret_size(\'10kb\') 
 10240 
 >>> interpret_size(\'10kB\') 
 10240 
 >>> interpret_size(\'a10\') 
 Traceback (most recent call last): 
 ValueError 
 >>> interpret_size(\'10a\') 
 Traceback (most recent call last): 
 KeyError: \'A\''"
"def main(): 
    salt_vars = get_salt_vars() 
   def salt_outputter(value): 
      ""\n                        Use   Salt's   outputters   to   print   values   to   the   shell\n                        "" 
      if (value is not None): 
         builtins._ = value 
         salt.output.display_output(value, '', salt_vars['__opts__']) 
   sys.displayhook = salt_outputter 
   readline.set_history_length(300) 
   if os.path.exists(HISTFILE): 
      readline.read_history_file(HISTFILE) 
   atexit.register(savehist) 
   atexit.register((lambda : sys.stdout.write('Salt   you   later!\n'))) 
   saltrepl = InteractiveConsole(locals=salt_vars) 
   saltrepl.interact(banner=__doc__)"," 'Set the displayhook to use Salt\'s outputters to print values to the shell. 
 This is useful for displaying values that are too long for the shell\'s 
 default output.'",'The main entry point'
"def node_degree_xy(G, x='out', y='in', weight=None, nodes=None): 
    if (nodes is None): 
      nodes = set(G) 
   else: 
      nodes = set(nodes) 
   xdeg = G.degree 
   ydeg = G.degree 
   if G.is_directed(): 
      direction = {'out': G.out_degree, 'in': G.in_degree} 
      xdeg = direction[x] 
      ydeg = direction[y] 
   for (u, degu) in xdeg(nodes, weight=weight): 
      neighbors = (nbr for (_, nbr) in G.edges(u) if (nbr in nodes)) 
      for (v, degv) in ydeg(neighbors, weight=weight): 
         (yield (degu, degv))"," 'Return a tuple of (degree, degree) for each node in G.'","'Generate node degree-degree pairs for edges in G. 
 Parameters 
 G: NetworkX graph 
 x: string (\'in\',\'out\') 
 The degree type for source node (directed graphs only). 
 y: string (\'in\',\'out\') 
 The degree type for target node (directed graphs only). 
 weight: string or None, optional (default=None) 
 The edge attribute that holds the numerical value used 
 as a weight.  If None, then each edge has weight 1. 
 The degree is the sum of the edge weights adjacent to the node. 
 nodes: list or iterable (optional) 
 Use only edges that are adjacency to specified nodes. 
 The default is all nodes. 
 Returns 
 (x,y): 2-tuple 
 Generates 2-tuple of (degree,degree) values. 
 Examples 
 >>> G = nx.DiGraph() 
 >>> G.add_edge(1,2) 
 >>> list(nx.node_degree_xy(G,x=\'out\',y=\'in\')) 
 [(1, 1)] 
 >>> list(nx.node_degree_xy(G,x=\'in\',y=\'out\')) 
 [(0, 0)] 
 Notes 
 For undirected graphs each edge is produced twice, once for each edge 
 representation (u,v) and (v,u), with the exception of self-loop edges 
 which only appear once.'"
"def setup_platform(hass, config, add_devices, discovery_info=None): 
    if (discovery_info is None): 
      _LOGGER.error('Configure   Qwikswitch   component') 
      return False 
   add_devices(qwikswitch.QSUSB['switch']) 
   return True", 'Configure Qwikswitch component','Add switched from the main Qwikswitch component.'
"def get_asset_dir_prefix(): 
    asset_dir_prefix = '' 
   if (feconf.IS_MINIFIED or (not feconf.DEV_MODE)): 
      cache_slug = get_cache_slug() 
      asset_dir_prefix = ('/build/%s' % cache_slug) 
   return asset_dir_prefix"," 'Returns the absolute path to the directory where assets are stored. 
 This is used by the frontend to store assets, and by the backend to serve 
 assets.'","'Returns prefix for asset directory depending whether dev or prod. 
 It is used as a prefix in urls for images, css and script files.'"
"def register(dmm, typecls): 
    def wraps(fn): 
      dmm.register(typecls, fn) 
      return fn 
   return wraps", 'Register a type with the dmm',"'Used as decorator to simplify datamodel registration. 
 Returns the object being decorated so that chaining is possible.'"
"def load_meta(prefix, dist): 
    return linked_data(prefix).get(dist)"," 'Load a single meta object for the given distribution. 
 Parameters 
 prefix : str 
 The prefix of the distribution name. 
 dist : str 
 The distribution name. 
 Returns 
 The meta object for the given distribution. 
 Raises 
 ValueError 
 If the distribution is not found.'","'Return the install meta-data for a linked package in a prefix, or None 
 if the package is not linked in the prefix.'"
"def downgrade(migrate_engine): 
    dialect = migrate_engine.url.get_dialect() 
   if (dialect is not postgresql.dialect): 
      meta = MetaData(bind=migrate_engine) 
      for (table, column) in TABLE_COLUMNS: 
         t = Table(table, meta, autoload=True) 
         getattr(t.c, column).alter(type=String(43))"," 'Downgrade a database to a version where the `CHARACTER_MAXIMUM_LENGTH` 
 is 43. 
 This is a workaround for a bug in PostgreSQL 9.4 that prevents 
 migration from 9.3 to 9.4. 
 See: 
 https://github.com/mperham/django-migrations/issues/156'",'Convert columns back to the larger String(43) defined in version 149.'
"def condense_zero_units(css): 
    return re.sub('([\\s:])(0)(px|em|%|in|cm|mm|pc|pt|ex)', '\\1\\2', css)"," 'Condense zero units in a CSS string. 
 :param css: CSS string to be condensed 
 :return: condensed CSS string'","'Replace `0(px, em, %, etc)` with `0`.'"
"def get_file_for_svn_txn(repo_path, txn_id, filename): 
    return subproc_check_output(['svnlook', 'cat', repo_path, '-t', txn_id, filename])"," 'Get the file contents for a specific SVN transaction. 
 :param repo_path: The path to the repository. 
 :param txn_id: The id of the transaction. 
 :param filename: The name of the file. 
 :returns: The file contents. 
 :rtype: str'","'Returns file in an svn transaction. 
 Helper function for hook scripts in the svn commit mission. 
 This function may be mocked in the tests.'"
"def GetReportByName(name): 
    report_class = REGISTRY.GetRegisteredPlugins()[name] 
   report_object = report_class() 
   return report_object"," 'GetReportByName(name) -> Report 
 Get a report by name. 
 :param name: The name of the report. 
 :type name: str 
 :return: The report object. 
 :rtype: Report'","'Maps report plugin names to report objects. 
 Args: 
 name: The name of a plugin class. Also the name field of 
 ApiGetReportArgs and ApiReportDescriptor. 
 Returns: 
 Report plugin object of class corresponding to the given name.'"
"def save_load(jid, clear_load, minions=None, recurse_count=0): 
    if (recurse_count >= 5): 
      err = 'save_load   could   not   write   job   cache   file   after   {0}   retries.'.format(recurse_count) 
      log.error(err) 
      raise salt.exceptions.SaltCacheError(err) 
   jid_dir = salt.utils.jid.jid_dir(jid, _job_dir(), __opts__['hash_type']) 
   serial = salt.payload.Serial(__opts__) 
   try: 
      if (not os.path.exists(jid_dir)): 
         os.makedirs(jid_dir) 
   except OSError as exc: 
      if (exc.errno == errno.EEXIST): 
         pass 
      else: 
         raise 
   try: 
      serial.dump(clear_load, salt.utils.fopen(os.path.join(jid_dir, LOAD_P), 'w+b')) 
   except IOError as exc: 
      log.warning('Could   not   write   job   invocation   cache   file:   %s', exc) 
      time.sleep(0.1) 
      return save_load(jid=jid, clear_load=clear_load, recurse_count=(recurse_count + 1)) 
   if (('tgt' in clear_load) and (clear_load['tgt'] != '')): 
      if (minions is None): 
         ckminions = salt.utils.minions.CkMinions(__opts__) 
         minions = ckminions.check_minions(clear_load['tgt'], clear_load.get('tgt_type', 'glob')) 
      save_minions(jid, minions)"," 'Write the job invocation cache file to disk. 
 :param jid: job id 
 :param clear_load: job invocation 
 :param minions: minions to save 
 :param recurse_count: number of retries'","'Save the load to the specified jid 
 minions argument is to provide a pre-computed list of matched minions for 
 the job, for cases when this function can\'t compute that list itself (such 
 as for salt-ssh)'"
"def _get_service_user(user, service_username): 
    if ((not user) or user.is_anonymous()): 
      try: 
         user = User.objects.get(username=service_username) 
      except User.DoesNotExist: 
         user = None 
   return user", 'Returns the user object of the service user.',"'Retrieve and return the Catalog Integration Service User Object 
 if the passed user is None or anonymous'"
"@jit(nopython=True) 
 def _initialize_tableaux_ig(X, Y, tableaux, bases): 
    m = X.shape[0] 
   min_ = np.zeros(m) 
   for i in range(m): 
      for j in range((2 * m)): 
         if ((j == i) or (j == (i + m))): 
            tableaux[0][(i, j)] = 1 
         else: 
            tableaux[0][(i, j)] = 0 
      tableaux[0][(i, (2 * m))] = 1 
   for i in range(m): 
      for j in range(m): 
         if (j == i): 
            tableaux[1][(i, j)] = 1 
         else: 
            tableaux[1][(i, j)] = 0 
      for j in range(m): 
         d = (X[i] - Y[j]) 
         tableaux[1][(i, (m + j))] = (_square_sum(d) * (-1)) 
         if (tableaux[1][(i, (m + j))] < min_[j]): 
            min_[j] = tableaux[1][(i, (m + j))] 
      tableaux[1][(i, (2 * m))] = 1 
   for i in range(m): 
      for j in range(m): 
         tableaux[1][(i, (m + j))] -= min_[j] 
         tableaux[1][(i, (m + j))] += 1 
   for (pl, start) in enumerate([m, 0]): 
      for i in range(m): 
         bases[pl][i] = (start + i) 
   return (tableaux, bases)"," 'Initialize the tableaux for the ``IG`` algorithm. 
 Parameters 
 X : array_like 
 The data. 
 Y : array_like 
 The target. 
 tableaux : list of list of arrays 
 The tableaux. 
 bases : list of arrays 
 The bases. 
 Returns 
 tableaux : list of list of arrays 
 The tableaux. 
 bases : list of arrays 
 The bases.'","'Given sequences `X` and `Y` of ndarrays, initialize the tableau and 
 basis arrays in place for the ""geometric"" imitation game as defined 
 in McLennan and Tourky (2006), to be passed to `lemke_howson_tbl`. 
 Parameters 
 X, Y : ndarray(float) 
 Arrays of the same shape (m, n). 
 tableaux : tuple(ndarray(float, ndim=2)) 
 Tuple of two arrays to be used to store the tableaux, of shape 
 (2m, 2m). Modified in place. 
 bases : tuple(ndarray(int, ndim=1)) 
 Tuple of two arrays to be used to store the bases, of shape 
 (m,). Modified in place. 
 Returns 
 tableaux : tuple(ndarray(float, ndim=2)) 
 View to `tableaux`. 
 bases : tuple(ndarray(int, ndim=1)) 
 View to `bases`.'"
"def sobel_v(image, mask=None): 
    assert_nD(image, 2) 
   image = img_as_float(image) 
   result = convolve(image, VSOBEL_WEIGHTS) 
   return _mask_filter_result(result, mask)"," 'Apply the Sobel vertical filter to the image. 
 Parameters 
 image : 2D array 
 Input image. 
 mask : 2D array, optional 
 Mask to apply to the image. 
 Returns 
 result : 2D array 
 The result of applying the Sobel vertical filter to the image. 
 Notes 
 The Sobel vertical filter is defined as: 
 .. math:: 
 \mathbf{S} = \frac{1}{2} \left( \mathbf{I} + \mathbf{K} \right) 
 where :math:`\mathbf{I}` is the identity matrix, and :math:`\mathbf{K}` is the 
 derivative matrix of the Gaussian kernel. 
 Examples 
 >>> from skimage import data 
 >>> from skimage.filters.rank import sobel_v 
 >>> img = data.camera() 
 >>> sobel_v(img) 
 array([[[ 0.00000000,  0.00000000,  0.00000000,  0.0000","'Find the vertical edges of an image using the Sobel transform. 
 Parameters 
 image : 2-D array 
 Image to process. 
 mask : 2-D array, optional 
 An optional mask to limit the application to a certain area. 
 Note that pixels surrounding masked regions are also masked to 
 prevent masked regions from affecting the result. 
 Returns 
 output : 2-D array 
 The Sobel edge map. 
 Notes 
 We use the following kernel:: 
 1   0  -1 
 2   0  -2 
 1   0  -1'"
"def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type): 
    (n_samples, n_features) = X.shape 
   (n_components, _) = means.shape 
   log_det = _compute_log_det_cholesky(precisions_chol, covariance_type, n_features) 
   if (covariance_type == 'full'): 
      log_prob = np.empty((n_samples, n_components)) 
      for (k, (mu, prec_chol)) in enumerate(zip(means, precisions_chol)): 
         y = (np.dot(X, prec_chol) - np.dot(mu, prec_chol)) 
         log_prob[:, k] = np.sum(np.square(y), axis=1) 
   elif (covariance_type == 'tied'): 
      log_prob = np.empty((n_samples, n_components)) 
      for (k, mu) in enumerate(means): 
         y = (np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)) 
         log_prob[:, k] = np.sum(np.square(y), axis=1) 
   elif (covariance_type == 'diag'): 
      precisions = (precisions_chol ** 2) 
      log_prob = ((np.sum(((means ** 2) * precisions), 1) - (2.0 * np.dot(X, (means * precisions).T))) + np.dot((X ** 2), precisions.T)) 
   elif (covariance_type == 'spherical'): 
      precisions = (precisions_chol ** 2) 
      log_prob = (((np.sum((means ** 2), 1) * precisions) - (2 * np.dot(X, (means.T * precisions)))) + np.outer(row_norms(X, squared=True), precisions)) 
   return (((-0.5) * ((n_features * np.log((2 * np.pi))) + log_prob)) + log_det)"," 'Estimates the log-probability of a Gaussian mixture model. 
 Parameters 
 X : ndarray, shape (n_samples, n_features) 
 Data to fit to. 
 means : ndarray, shape (n_components, n_features) 
 Means of the Gaussian components. 
 precisions_chol : ndarray, shape (n_components, n_features) 
 Cholesky-factored precision matrices. 
 covariance_type : str 
 Type of covariance matrix to use. 
 Full: A full covariance matrix. 
 Tied: A diagonal covariance matrix with the same diagonal as the precision matrix. 
 Diag: A diagonal covariance matrix. 
 Spherical: A diagonal covariance matrix with the same diagonal as the row norms of X. 
 Returns 
 log_prob : ndarray, shape (n_samples, n_components) 
 The log-probability of the Gaussian mixture model. 
 Notes 
 The log-probability is estimated using the Cholesky factor of the precision matrix. 
 References 
 ..","'Estimate the log Gaussian probability. 
 Parameters 
 X : array-like, shape (n_samples, n_features) 
 means : array-like, shape (n_components, n_features) 
 precisions_chol : array-like, 
 Cholesky decompositions of the precision matrices. 
 \'full\' : shape of (n_components, n_features, n_features) 
 \'tied\' : shape of (n_features, n_features) 
 \'diag\' : shape of (n_components, n_features) 
 \'spherical\' : shape of (n_components,) 
 covariance_type : {\'full\', \'tied\', \'diag\', \'spherical\'} 
 Returns 
 log_prob : array, shape (n_samples, n_components)'"
"def gen_resource(ob, perm=None): 
    res = [] 
   if isinstance(ob, dict): 
      role = ob.get('role') 
      asset_r = ob.get('asset') 
      user = ob.get('user') 
      if (not perm): 
         perm = get_group_user_perm(user) 
      if role: 
         roles = perm.get('role', {}).keys() 
         if (role not in roles): 
            return {} 
         role_assets_all = perm.get('role').get(role).get('asset') 
         assets = (set(role_assets_all) & set(asset_r)) 
         for asset in assets: 
            asset_info = get_asset_info(asset) 
            role_key = get_role_key(user, role) 
            info = {'hostname': asset.hostname, 'ip': asset.ip, 'port': asset_info.get('port', 22), 'ansible_ssh_private_key_file': role_key, 'username': role.name} 
            if os.path.isfile(role_key): 
               info['ssh_key'] = role_key 
            res.append(info) 
      else: 
         for (asset, asset_info) in perm.get('asset').items(): 
            if (asset not in asset_r): 
               continue 
            asset_info = get_asset_info(asset) 
            try: 
               role = sorted(list(perm.get('asset').get(asset).get('role')))[0] 
            except IndexError: 
               continue 
            role_key = get_role_key(user, role) 
            info = {'hostname': asset.hostname, 'ip': asset.ip, 'port': asset_info.get('port', 22), 'username': role.name, 'password': CRYPTOR.decrypt(role.password)} 
            if os.path.isfile(role_key): 
               info['ssh_key'] = role_key 
            res.append(info) 
   elif isinstance(ob, User): 
      if (not perm): 
         perm = get_group_user_perm(ob) 
      for (asset, asset_info) in perm.get('asset').items(): 
         asset_info = get_asset_info(asset) 
         info = {'hostname': asset.hostname, 'ip': asset.ip, 'port': asset_info.get('port', 22)} 
         try: 
            role = sorted(list(perm.get('asset').get(asset).get('role')))[0] 
         except IndexError: 
            continue 
         info['username'] = role.name 
         info['password'] = CRYPTOR.decrypt(role.password) 
         role_key = get_role_key(ob, role) 
         if os.path.isfile(role_key): 
            info['ssh_key'] = role_key 
         res.append(info) 
   elif isinstance(ob, (list, QuerySet)): 
      for asset in ob: 
         info = get_asset_info(asset) 
         res.append(info) 
   logger.debug(('\xe7\x94\x9f\xe6\x88\x90res:   %s' % res)) 
   return res"," 'Generate the list of resources to be used in the playbook. 
 Args: 
 ob: A dictionary, a list, or a user. 
 perm: The permission to use. 
 Returns: 
 A list of dictionaries.'","'obä¸ºç¨æ·æèµäº§åè¡¨æèµäº§queryset, å¦æåæ¶è¾å¥ç¨æ·å{\'role\': role1, \'asset\': []}ï¼åè·åç¨æ·å¨è¿äºèµäº§ä¸çä¿¡æ¯ 
 çæMyInventoryéè¦ç resourceæä»¶'"
"def get_filter(doctype, f): 
    from frappe.model import default_fields, optional_fields 
   if isinstance(f, dict): 
      (key, value) = f.items()[0] 
      f = make_filter_tuple(doctype, key, value) 
   if (not isinstance(f, (list, tuple))): 
      frappe.throw(u'Filter   must   be   a   tuple   or   list   (in   a   list)') 
   if (len(f) == 3): 
      f = (doctype, f[0], f[1], f[2]) 
   elif (len(f) != 4): 
      frappe.throw(u'Filter   must   have   4   values   (doctype,   fieldname,   operator,   value):   {0}'.format(str(f))) 
   f = frappe._dict(doctype=f[0], fieldname=f[1], operator=f[2], value=f[3]) 
   if (not f.operator): 
      f.operator = u'=' 
   valid_operators = (u'=', u'!=', u'>', u'<', u'>=', u'<=', u'like', u'not   like', u'in', u'not   in', u'Between') 
   if (f.operator not in valid_operators): 
      frappe.throw(u'Operator   must   be   one   of   {0}'.format(u',   '.join(valid_operators))) 
   if (f.doctype and (f.fieldname not in (default_fields + optional_fields))): 
      meta = frappe.get_meta(f.doctype) 
      if (not meta.has_field(f.fieldname)): 
         for df in meta.get_table_fields(): 
            if frappe.get_meta(df.options).has_field(f.fieldname): 
               f.doctype = df.options 
               break 
   return f"," 'Converts filter to tuple (doctype, fieldname, operator, value) 
 :param doctype: doctype 
 :param f: filter dict 
 :returns: tuple'","'Returns a _dict like 
 ""doctype"": 
 ""fieldname"": 
 ""operator"": 
 ""value"":'"
"def single_line(text): 
    return re.sub('   +', '   ', normalize_newlines(text).replace('\n', '')).strip()", 'Returns a single line of text','Quick utility to make comparing template output easier.'
"@RegisterWithArgChecks(name='neighbor.attribute_map.get', req_args=[neighbors.IP_ADDRESS], opt_args=[ROUTE_DISTINGUISHER, VRF_RF]) 
 def get_neighbor_attribute_map(neigh_ip_address, route_dist=None, route_family=VRF_RF_IPV4): 
    core = CORE_MANAGER.get_core_service() 
   peer = core.peer_manager.get_by_addr(neigh_ip_address) 
   at_maps_key = const.ATTR_MAPS_LABEL_DEFAULT 
   if (route_dist is not None): 
      at_maps_key = ':'.join([route_dist, route_family]) 
   at_maps = peer.attribute_maps.get(at_maps_key) 
   if at_maps: 
      return at_maps.get(const.ATTR_MAPS_ORG_KEY) 
   else: 
      return []", 'Returns the attribute map of a peer.','Returns a neighbor attribute_map for given ip address if exists.'
"def _get_conn(ret=None): 
    _options = _get_options(ret) 
   dsn = _options.get('dsn') 
   user = _options.get('user') 
   passwd = _options.get('passwd') 
   return pyodbc.connect('DSN={0};UID={1};PWD={2}'.format(dsn, user, passwd))", 'Return a connection to the database.','Return a MSSQL connection.'
"def validate(tax_number): 
    try: 
      verify_vat(tax_number) 
      return u'vat' 
   except VatCannotIdentifyValidationError: 
      pass 
   return u'unknown'"," 'Validate a VAT number. 
 :param tax_number: VAT number to validate 
 :returns: \'vat\' or \'unknown\' 
 :raises: :class:`VatCannotIdentifyValidationError` 
 :rtype: str'","'Validate a tax number. 
 :param tax_number: Tax number to validate 
 :type tax_number: str 
 :return: 
 Type identifier of the tax number, if detected.  Possible 
 values for now are either ""vat"" or ""unknown"". 
 :rtype: str 
 :raise: 
 `ValidationError` if tax number type was detected, but it is 
 somehow malformed.'"
"def get_file_title(files_path_list, filename): 
    fname = os.path.basename(filename) 
   same_name_files = get_same_name_files(files_path_list, fname) 
   if (len(same_name_files) > 1): 
      compare_path = shortest_path(same_name_files) 
      if (compare_path == filename): 
         same_name_files.remove(path_components(filename)) 
         compare_path = shortest_path(same_name_files) 
      diff_path = differentiate_prefix(path_components(filename), path_components(compare_path)) 
      diff_path_length = len(diff_path) 
      path_component = path_components(diff_path) 
      if ((diff_path_length > 20) and (len(path_component) > 2)): 
         if ((path_component[0] != '/') and (path_component[0] != '')): 
            path_component = [path_component[0], '...', path_component[(-1)]] 
         else: 
            path_component = [path_component[2], '...', path_component[(-1)]] 
         diff_path = os.path.join(*path_component) 
      fname = ((fname + '   -   ') + diff_path) 
   return fname"," 'Returns the file title given the filename and the list of files in the 
 directory. If there are multiple files with the same name, then the longest 
 common prefix is removed from the filename and the file title is updated. 
 If there are no files with the same name, then the filename is returned.'",'Get tab title without ambiguation.'
"def query_chooser(query): 
    ids = [] 
   for (column, operator, value) in _get_query_comparisons(query): 
      if column.shares_lineage(weather_locations.c.continent): 
         if (operator == operators.eq): 
            ids.append(shard_lookup[value]) 
         elif (operator == operators.in_op): 
            ids.extend((shard_lookup[v] for v in value)) 
   if (len(ids) == 0): 
      return ['north_america', 'asia', 'europe', 'south_america'] 
   else: 
      return ids"," 'Returns a list of shard ids that match the query. 
 :param query: The query to match. 
 :return: A list of shard ids that match the query.'","'query chooser. 
 this also returns a list of shard ids, which can 
 just be all of them.  but here we\'ll search into the Query in order 
 to try to narrow down the list of shards to query.'"
"def remove_ignorable_whitespace(node): 
    if (node.tail and (node.tail.strip() == '')): 
      node.tail = None 
   for child in node: 
      if (node.text and (node.text.strip() == '')): 
         node.text = None 
      remove_ignorable_whitespace(child)", 'Remove any whitespace that is ignored by the parser.',"'Remove insignificant whitespace from XML nodes 
 It should only remove whitespace in between elements and sub elements. 
 This should be safe for Jenkins due to how it\'s XML serialization works 
 but may not be valid for other XML documents. So use this method with 
 caution outside of this specific library.'"
"def nopackages(pkg_list): 
    pkg_list = [pkg for pkg in pkg_list if is_installed(pkg)] 
   if pkg_list: 
      uninstall(pkg_list)", 'Uninstall packages from the system',"'Require several opkg packages to be uninstalled. 
 Example:: 
 from fabtools import require 
 require.opkg.nopackages([ 
 \'perl\', 
 \'php5\', 
 \'ruby\','"
"def get_colors(palette, funcs): 
    palettes = import_required('bokeh.palettes', _BOKEH_MISSING_MSG) 
   tz = import_required('toolz', _TOOLZ_MISSING_MSG) 
   unique_funcs = list(sorted(tz.unique(funcs))) 
   n_funcs = len(unique_funcs) 
   palette_lookup = palettes.all_palettes[palette] 
   keys = list(sorted(palette_lookup.keys())) 
   index = keys[min(bisect_left(keys, n_funcs), (len(keys) - 1))] 
   palette = palette_lookup[index] 
   palette = list(tz.unique(palette)) 
   if (len(palette) > n_funcs): 
      random.Random(42).shuffle(palette) 
   color_lookup = dict(zip(unique_funcs, cycle(palette))) 
   return [color_lookup[n] for n in funcs]", 'Returns a list of color mappings for the given palette and functions.',"'Get a dict mapping funcs to colors from palette. 
 Parameters 
 palette : string 
 Name of the bokeh palette to use, must be a member of 
 bokeh.palettes.all_palettes. 
 funcs : iterable 
 Iterable of function names'"
"def get_time_format(format='medium', locale=LC_TIME): 
    return Locale.parse(locale).time_formats[format]"," 'Get the time format string. 
 :param format: 
 :param locale: 
 :return: 
 :rtype: 
 :raises: 
 :param format: 
 :param locale: 
 :rtype: 
 :raises: 
 :return: 
 :rtype: 
 :raises: 
 :param format: 
 :param locale: 
 :return: 
 :rtype: 
 :raises: 
 :param format: 
 :param locale: 
 :return: 
 :rtype: 
 :raises: 
 :param format: 
 :param locale: 
 :return: 
 :rtype: 
 :raises: 
 :param format: 
 :param locale: 
 :return: 
 :rtype: 
 :raises: 
 :param format: 
 :param locale: 
 :return: 
 :rtype: 
 :raises: 
 :param format: 
 :param locale: 
 :return: 
 :rtype: 
 :raises: 
 :param format","'Return the time formatting patterns used by the locale for the specified 
 format. 
 >>> get_time_format(locale=\'en_US\') 
 <DateTimePattern u\'h:mm:ss a\'> 
 >>> get_time_format(\'full\', locale=\'de_DE\') 
 <DateTimePattern u\'HH:mm:ss v\'> 
 :param format: the format to use, one of ""full"", ""long"", ""medium"", or 
 ""short"" 
 :param locale: the `Locale` object, or a locale string 
 :return: the time format pattern 
 :rtype: `DateTimePattern`'"
"def alias(selectable, name=None, flat=False): 
    return _interpret_as_from(selectable).alias(name=name, flat=flat)"," 'Alias a selectable to a new name. 
 :param selectable: 
 :param name: 
 :param flat: 
 :returns: 
 :rtype: 
 :raises: 
 :class:`~sqlalchemy.exc.ArgumentError` 
 :class:`~sqlalchemy.exc.InvalidRequestError` 
 :class:`~sqlalchemy.exc.InvalidRequestError` 
 :class:`~sqlalchemy.exc.ArgumentError` 
 :class:`~sqlalchemy.exc.InvalidRequestError` 
 :class:`~sqlalchemy.exc.ArgumentError` 
 :class:`~sqlalchemy.exc.ArgumentError` 
 :class:`~sqlalchemy.exc.ArgumentError` 
 :class:`~sqlalchemy.exc.ArgumentError` 
 :class:`~sqlalchemy.exc.ArgumentError`'","'Return an :class:`.Alias` object. 
 An :class:`.Alias` represents any :class:`.FromClause` 
 with an alternate name assigned within SQL, typically using the ``AS`` 
 clause when generated, e.g. ``SELECT * FROM table AS aliasname``. 
 Similar functionality is available via the 
 :meth:`~.FromClause.alias` method 
 available on all :class:`.FromClause` subclasses. 
 When an :class:`.Alias` is created from a :class:`.Table` object, 
 this has the effect of the table being rendered 
 as ``tablename AS aliasname`` in a SELECT statement. 
 For :func:`.select` objects, the effect is that of creating a named 
 subquery, i.e. ``(select ...) AS aliasname``. 
 The ``name`` parameter is optional, and provides the name 
 to use in the rendered SQL.  If blank, an ""anonymous"" name 
 will be deterministically generated at compile time. 
 Deterministic means the name is guaranteed to be unique against 
 other constructs used in the same statement, and will also be the 
 same name for each successive compilation of the same statement 
 object. 
 :param selectable: any :class:`.FromClause` subclass, 
 such as a table, select statement, etc. 
 :param name: string name to be assigned as the alias. 
 If ``None``, a name will be deterministically generated 
 at compile time. 
 :param flat: Will be passed through to if the given selectable 
 is an instance of :class:`.Join` - see :meth:`.Join.alias` 
 for details. 
 .. versionadded:: 0.9.0'"
"def clean_html(buf): 
    buf = buf.strip() 
   if (not buf): 
      return buf 
   html_parser = html5lib.HTMLParser(tree=treebuilders.getTreeBuilder('dom'), tokenizer=HTMLSanitizer) 
   dom_tree = html_parser.parseFragment(buf) 
   walker = treewalkers.getTreeWalker('dom') 
   stream = walker(dom_tree) 
   s = serializer.htmlserializer.HTMLSerializer(omit_optional_tags=False, quote_attr_values=True) 
   output = s.render(stream, 'utf-8') 
   while ('toberemoved' in output): 
      oldoutput = output 
      matches = re.findall('&lt;toberemoved.*?&gt;.*?&lt;/toberemoved&gt;', output, re.DOTALL) 
      for s in matches: 
         output = output.replace(s, '') 
      matches = re.findall('&lt;/toberemoved&gt;', output, re.DOTALL) 
      for s in matches: 
         output = output.replace(s, '') 
      matches = re.findall('&lt;toberemoved.*?&gt;', output, re.DOTALL) 
      for s in matches: 
         output = output.replace(s, '') 
      if (output == oldoutput): 
         break 
   return output"," 'Returns cleaned HTML. 
 :param buf: the HTML string to clean 
 :returns: cleaned HTML'",'Cleans HTML of dangerous tags and content.'
"def getDisplayedDialogFromConstructor(repository): 
    try: 
      getReadRepository(repository) 
      return RepositoryDialog(repository, Tkinter.Tk()) 
   except: 
      print 'this   should   never   happen,   getDisplayedDialogFromConstructor   in   settings   could   not   open' 
      print repository 
      traceback.print_exc(file=sys.stdout) 
      return None"," 'Returns a dialog that is currently displayed. 
 :param repository: The repository to use for the dialog. 
 :type repository: str 
 :return: The displayed dialog, or None if no dialog is currently displayed.'",'Display the repository dialog.'
"@cors_enabled('*') 
 def serve_cors(*args, **kwargs): 
    if (not settings.DEBUG): 
      raise RuntimeError(""Don't   use   kitsune.sumo.views.serve_cors   in   production."") 
   from django.views.static import serve 
   return serve(*args, **kwargs)"," 'Serves static files with CORS enabled. 
 This function is only intended for use in production environments. 
 :param args: 
 :param kwargs: 
 :return: 
 :rtype: 
 :raises: 
 :seealso: 
 :version: 0.1'",'A wrapper around django.views.static.serve that adds CORS headers.'
"def getRoundedToThreePlaces(number): 
    return str(round(number, 3))", 'Returns a number rounded to three places.','Get number rounded to three places as a string.'
"def create_read_replica(name, source_name, db_instance_class=None, availability_zone=None, port=None, auto_minor_version_upgrade=None, iops=None, option_group_name=None, publicly_accessible=None, tags=None, db_subnet_group_name=None, storage_type=None, copy_tags_to_snapshot=None, monitoring_interval=None, monitoring_role_arn=None, region=None, key=None, keyid=None, profile=None): 
    if (not backup_retention_period): 
      raise SaltInvocationError('backup_retention_period   is   required') 
   res = __salt__['boto_rds.exists'](source_name, tags, region, key, keyid, profile) 
   if (not res.get('exists')): 
      return {'exists': bool(res), 'message': 'RDS   instance   source   {0}   does   not   exists.'.format(source_name)} 
   res = __salt__['boto_rds.exists'](name, tags, region, key, keyid, profile) 
   if res.get('exists'): 
      return {'exists': bool(res), 'message': 'RDS   replica   instance   {0}   already   exists.'.format(name)} 
   try: 
      conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
      kwargs = {} 
      for key in ('OptionGroupName', 'MonitoringRoleArn'): 
         if (locals()[key] is not None): 
            kwargs[key] = str(locals()[key]) 
      for key in ('MonitoringInterval', 'Iops', 'Port'): 
         if (locals()[key] is not None): 
            kwargs[key] = int(locals()[key]) 
      for key in ('CopyTagsToSnapshot', 'AutoMinorVersionUpgrade'): 
         if (locals()[key] is not None): 
            kwargs[key] = bool(locals()[key]) 
      taglist = _tag_doc(tags) 
      rds_replica = conn.create_db_instance_read_replica(DBInstanceIdentifier=name, SourceDBInstanceIdentifier=source_name, DBInstanceClass=db_instance_class, AvailabilityZone=availability_zone, PubliclyAccessible=publicly_accessible, Tags=taglist, DBSubnetGroupName=db_subnet_group_name, StorageType=storage_type, **kwargs) 
      return {'exists': bool(rds_replica)} 
   except ClientError as e: 
      return {'error': salt.utils.boto3.get_error(e)}"," 'Create a read replica of a database instance. 
 This function creates a read replica of a database instance. 
 If the source_name is not specified, the source is assumed to be the 
 current instance. 
 The replica is created in the specified region, and the specified 
 availability zone. 
 The replica is created with the specified tags, and the specified 
 option group. 
 The replica is created with the specified storage type. 
 The replica is created with the specified port. 
 The replica is created with the specified IOPS. 
 The replica is created with the specified monitoring role. 
 The replica is created with the specified monitoring interval. 
 The replica is created with the specified copy tags to snapshot. 
 The replica is created with the specified auto minor version upgrade. 
 The replica is created with the specified publicly accessible. 
 The replica is created with the specified DB subnet group. 
 The replica is created with the specified tags. 
 The replica is created with the specified profile. 
 :param name: Name of the replica. 
 :param source_name","'Create an RDS read replica 
 CLI example to create an RDS  read replica:: 
 salt myminion boto_rds.create_read_replica replicaname source_name'"
"@task(ignore_result=True) 
 def email_membership_change(group_pk, user_pk, old_status, new_status): 
    from mozillians.groups.models import Group, GroupMembership 
   group = Group.objects.get(pk=group_pk) 
   user = User.objects.get(pk=user_pk) 
   activate('en-us') 
   if (old_status in [GroupMembership.PENDING, GroupMembership.PENDING_TERMS]): 
      if (new_status == GroupMembership.MEMBER): 
         subject = (_('Accepted   to   Mozillians   group   ""%s""') % group.name) 
         template_name = 'groups/email/accepted.txt' 
      elif (new_status is None): 
         subject = (_('Not   accepted   to   Mozillians   group   ""%s""') % group.name) 
         template_name = 'groups/email/rejected.txt' 
      else: 
         raise ValueError('BAD   ARGS   TO   email_membership_change') 
   else: 
      raise ValueError('BAD   ARGS   TO   email_membership_change') 
   context = {'group': group, 'user': user} 
   template = get_template(template_name) 
   body = template.render(context) 
   send_mail(subject, body, settings.FROM_NOREPLY, [user.email], fail_silently=False)"," 'Send an email to a user who has changed their membership status. 
 This is used for new members and for people who have been accepted 
 or rejected from a group. 
 :param group_pk: the pk of the group 
 :param user_pk: the pk of the user 
 :param old_status: the old membership status 
 :param new_status: the new membership status'","'Email user that their group membership status has changed. 
 old_status and new_status can either be a valid value for GroupMembership.status, 
 or None if we\'re going from or to a state where there is no GroupMembership 
 record (e.g. if they\'re being removed from a group). 
 This is queued from Group.add_member() and Group.remove_member().'"
"def cyclic(length=None, alphabet=string.ascii_lowercase, n=None): 
    if (n is None): 
      n = 4 
   if ((len(alphabet) ** n) < length): 
      log.error((""Can't   create   a   pattern   length=%i   with   len(alphabet)==%i   and   n==%i"" % (length, len(alphabet), n))) 
   out = [] 
   for (ndx, c) in enumerate(de_bruijn(alphabet, n)): 
      if ((length != None) and (ndx >= length)): 
         break 
      else: 
         out.append(c) 
   if isinstance(alphabet, str): 
      return ''.join(out) 
   else: 
      return out"," 'Return a cyclic de Bruijn sequence of length length. 
 If length is None, the length of the de Bruijn sequence is chosen to 
 be the smallest length for which the de Bruijn sequence has length 
 greater than or equal to length. 
 If length is None and n is not None, the length of the de Bruijn 
 sequence is chosen to be the smallest length for which the de Bruijn 
 sequence has length greater than or equal to n. 
 If length is None and n is None, the length of the de Bruijn sequence 
 is chosen to be the smallest length for which the de Bruijn sequence 
 has length greater than or equal to the length of the alphabet. 
 If length is not None and n is None, the length of the de Bruijn 
 sequence is chosen to be the smallest length for which the de Bruijn 
 sequence has length greater than or equal to length. 
 If length is not None and n is not None, the length of the de Bruijn 
 sequence is chosen to be the smallest length for which the de Bruijn 
 sequence has length greater than or equal to n. 
 If length is not None and n","'cyclic(length = None, alphabet = string.ascii_lowercase, n = 4) -> list/str 
 A simple wrapper over :func:`de_bruijn`. This function returns at most 
 `length` elements. 
 If the given alphabet is a string, a string is returned from this function. Otherwise 
 a list is returned. 
 Arguments: 
 length: The desired length of the list or None if the entire sequence is desired. 
 alphabet: List or string to generate the sequence over. 
 n(int): The length of subsequences that should be unique. 
 Example: 
 >>> cyclic(alphabet = ""ABC"", n = 3) 
 \'AAABAACABBABCACBACCBBBCBCCC\' 
 >>> cyclic(20) 
 \'aaaabaaacaaadaaaeaaa\' 
 >>> alphabet, n = range(30), 3 
 >>> len(alphabet)**n, len(cyclic(alphabet = alphabet, n = n)) 
 (27000, 27000)'"
"def _process_dataset(name, images, vocab, num_shards): 
    images = [ImageMetadata(image.image_id, image.filename, [caption]) for image in images for caption in image.captions] 
   random.seed(12345) 
   random.shuffle(images) 
   num_threads = min(num_shards, FLAGS.num_threads) 
   spacing = np.linspace(0, len(images), (num_threads + 1)).astype(np.int) 
   ranges = [] 
   threads = [] 
   for i in xrange((len(spacing) - 1)): 
      ranges.append([spacing[i], spacing[(i + 1)]]) 
   coord = tf.train.Coordinator() 
   decoder = ImageDecoder() 
   print(('Launching   %d   threads   for   spacings:   %s' % (num_threads, ranges))) 
   for thread_index in xrange(len(ranges)): 
      args = (thread_index, ranges, name, images, decoder, vocab, num_shards) 
      t = threading.Thread(target=_process_image_files, args=args) 
      t.start() 
      threads.append(t) 
   coord.join(threads) 
   print((""%s:   Finished   processing   all   %d   image-caption   pairs   in   data   set   '%s'."" % (datetime.now(), len(images), name)))"," 'Processes an image dataset. 
 :param name: name of the dataset 
 :param images: list of images 
 :param vocab: vocabulary of the dataset 
 :param num_shards: number of shards'","'Processes a complete data set and saves it as a TFRecord. 
 Args: 
 name: Unique identifier specifying the dataset. 
 images: List of ImageMetadata. 
 vocab: A Vocabulary object. 
 num_shards: Integer number of shards for the output files.'"
"def detach_user_policy(policy_name, user_name, region=None, key=None, keyid=None, profile=None): 
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
   policy_arn = _get_policy_arn(policy_name, region, key, keyid, profile) 
   try: 
      conn.detach_user_policy(policy_arn, user_name) 
      log.info('Detached   {0}   policy   to   user   {1}.'.format(policy_name, user_name)) 
   except boto.exception.BotoServerError as e: 
      log.debug(e) 
      msg = 'Failed   to   detach   {0}   policy   to   user   {1}.' 
      log.error(msg.format(policy_name, user_name)) 
      return False 
   return True"," 'Detaches a policy from a user. 
 :param policy_name: The name of the policy to detach. 
 :param user_name: The name of the user to detach the policy from. 
 :param region: The region of the user and policy. 
 :param key: The access key ID. 
 :param keyid: The access key ID. 
 :param profile: The profile name. 
 :return: True if the policy was successfully detached, False otherwise.'","'Detach a managed policy to a user. 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_iam.detach_user_policy mypolicy myuser'"
"def scan(): 
    ret = [] 
   devices = bluetooth.discover_devices(lookup_names=True) 
   for device in devices: 
      ret.append({device[0]: device[1]}) 
   return ret", 'Scan for Bluetooth devices.',"'Scan for bluetooth devices in the area 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' bluetooth.scan'"
"def from_castra(x, columns=None, index=False): 
    from castra import Castra 
   if (not isinstance(x, Castra)): 
      x = Castra(x, readonly=True) 
   elif (not x._readonly): 
      x = Castra(x.path, readonly=True) 
   if (columns is None): 
      columns = x.columns 
   name = ('from-castra-' + tokenize(os.path.getmtime(x.path), x.path, columns, index)) 
   dsk = dict((((name, i), (load_castra_partition, x, part, columns, index)) for (i, part) in enumerate(x.partitions))) 
   return Bag(dsk, name, len(x.partitions))"," 'Loads a Castra object into a Bag. 
 Parameters 
 x : Castra 
 The Castra object to load. 
 columns : list of str, optional 
 The columns to load. 
 index : bool, optional 
 If True, the index is loaded. 
 Returns 
 Bag 
 The loaded Bag.'","'Load a dask Bag from a Castra. 
 Parameters 
 x : filename or Castra 
 columns: list or string, optional 
 The columns to load. Default is all columns. 
 index: bool, optional 
 If True, the index is included as the first element in each tuple. 
 Default is False.'"
"def execute_manager(settings_mod, argv=None): 
    warnings.warn(""The   'execute_manager'   function   is   deprecated,   you   likely   need   to   update   your   'manage.py';   please   see   the   Django   1.4   release   notes   (https://docs.djangoproject.com/en/dev/releases/1.4/)."", PendingDeprecationWarning) 
   setup_environ(settings_mod) 
   utility = ManagementUtility(argv) 
   utility.execute()", 'Execute the django management command line tool.',"'Like execute_from_command_line(), but for use by manage.py, a 
 project-specific django-admin.py utility.'"
"def _DefaultValueConstructorForField(field): 
    if (field.label == _FieldDescriptor.LABEL_REPEATED): 
      if (field.has_default_value and (field.default_value != [])): 
         raise ValueError(('Repeated   field   default   value   not   empty   list:   %s' % field.default_value)) 
      if (field.cpp_type == _FieldDescriptor.CPPTYPE_MESSAGE): 
         message_type = field.message_type 
         def MakeRepeatedMessageDefault(message): 
            return containers.RepeatedCompositeFieldContainer(message._listener_for_children, field.message_type) 
         return MakeRepeatedMessageDefault 
      else: 
         type_checker = type_checkers.GetTypeChecker(field) 
         def MakeRepeatedScalarDefault(message): 
            return containers.RepeatedScalarFieldContainer(message._listener_for_children, type_checker) 
         return MakeRepeatedScalarDefault 
   if (field.cpp_type == _FieldDescriptor.CPPTYPE_MESSAGE): 
      message_type = field.message_type 
      def MakeSubMessageDefault(message): 
         result = message_type._concrete_class() 
         result._SetListener(message._listener_for_children) 
         return result 
      return MakeSubMessageDefault 
   def MakeScalarDefault(message): 
      return field.default_value 
   return MakeScalarDefault"," 'Returns a default value constructor for a field. 
 The default value constructor is used to initialize the field of 
 a message when it is created. 
 :param field: The field. 
 :returns: A function that constructs a default value for the field. 
 :rtype: function'","'Returns a function which returns a default value for a field. 
 Args: 
 field: FieldDescriptor object for this field. 
 The returned function has one argument: 
 message: Message instance containing this field, or a weakref proxy 
 of same. 
 That function in turn returns a default value for this field.  The default 
 value may refer back to |message| via a weak reference.'"
"def new_figure_manager_given_figure(num, figure): 
    canvas = FigureCanvasWebAgg(figure) 
   manager = core.FigureManagerWebAgg(canvas, num) 
   return manager"," 'Create a new FigureManagerWebAgg with a given figure. 
 Parameters 
 num : int 
 The number of the manager. 
 figure : Figure 
 The figure to be managed. 
 Returns 
 FigureManagerWebAgg 
 The new FigureManagerWebAgg.'",'Create a new figure manager instance for the given figure.'
"def tetrahedral_graph(create_using=None): 
    G = complete_graph(4, create_using) 
   G.name = 'Platonic   Tetrahedral   graph' 
   return G", 'Returns a tetrahedral graph.','Return the 3-regular Platonic Tetrahedral graph.'
"def get_data_path(f=''): 
    (_, filename, _, _, _, _) = inspect.getouterframes(inspect.currentframe())[1] 
   base_dir = os.path.abspath(os.path.dirname(filename)) 
   return os.path.join(base_dir, 'data', f)"," 'Returns the data path for the given file name. 
 The data path is the absolute path of the file in the data directory. 
 The data directory is the directory containing the data files, which is 
 normally the directory containing the python script. 
 Parameters 
 f : str 
 The name of the file to get the data path for. 
 Returns 
 str 
 The data path of the file.'","'Return the path of a data file, these are relative to the current test 
 directory.'"
"def statusEnquiry(): 
    a = TpPd(pd=3) 
   b = MessageType(mesType=52) 
   packet = (a / b) 
   return packet", 'Returns a packet for status enquiry.','STATUS ENQUIRY Section 9.3.28'
"def encryptMessage(key, message): 
    return translateMessage(key, message, 'encrypt')"," 'Encrypt a message using the given key. 
 :param key: The key used to encrypt the message. 
 :param message: The message to encrypt. 
 :return: The encrypted message.'","'>>> encryptMessage(\'LFWOAYUISVKMNXPBDCRJTQEGHZ\', \'Harshil Darji\') 
 \'Ilcrism Olcvs\''"
"def python_version(): 
    return _sys_version()[1]", 'Returns the Python version as a string.',"'Returns the Python version as string \'major.minor.patchlevel\' 
 Note that unlike the Python sys.version, the returned value 
 will always include the patchlevel (it defaults to 0).'"
"def _trial(factors, n, candidates, verbose=False): 
    if verbose: 
      factors0 = list(factors.keys()) 
   nfactors = len(factors) 
   for d in candidates: 
      if ((n % d) == 0): 
         m = multiplicity(d, n) 
         n //= (d ** m) 
         factors[d] = m 
   if verbose: 
      for k in sorted(set(factors).difference(set(factors0))): 
         print((factor_msg % (k, factors[k]))) 
   return (int(n), (len(factors) != nfactors))"," 'Find the largest prime factor of n and the number of factors 
 of n. 
 Parameters 
 factors : dict 
 A dictionary of prime factors of n. 
 n : int 
 The number to factor. 
 candidates : list of int 
 The prime factors to try. 
 verbose : bool 
 If True, print the factors. 
 Returns 
 (n, factors) 
 The largest prime factor of n and the number of factors.'","'Helper function for integer factorization. Trial factors ``n` 
 against all integers given in the sequence ``candidates`` 
 and updates the dict ``factors`` in-place. Returns the reduced 
 value of ``n`` and a flag indicating whether any factors were found.'"
"def tokenize_wrapper(input): 
    skip = set((token.NEWLINE, token.INDENT, token.DEDENT)) 
   tokens = tokenize.generate_tokens(StringIO.StringIO(input).readline) 
   for quintuple in tokens: 
      (type, value, start, end, line_text) = quintuple 
      if (type not in skip): 
         (yield quintuple)"," 'Tokenize the input string. 
 :param input: Input string. 
 :return: Tuples of (type, value, start, end, line_text).'",'Tokenizes a string suppressing significant whitespace.'
"@mock_autoscaling 
 def test_execute_policy_small_percent_change_in_capacity(): 
    setup_autoscale_group() 
   conn = boto.connect_autoscale() 
   policy = ScalingPolicy(name=u'ScaleUp', adjustment_type=u'PercentChangeInCapacity', as_name=u'tester_group', scaling_adjustment=1) 
   conn.create_scaling_policy(policy) 
   conn.execute_policy(u'ScaleUp') 
   instances = list(conn.get_all_autoscaling_instances()) 
   instances.should.have.length_of(3)"," 'Test that a scaling policy with a small percentage change in capacity 
 succeeds.'","'http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/as-scale-based-on-demand.html 
 If PercentChangeInCapacity returns a value between 0 and 1, 
 Auto Scaling will round it off to 1.'"
"def get_prediction(self, exog=None, transform=True, weights=None, row_labels=None, pred_kwds=None): 
    if (transform and hasattr(self.model, 'formula') and (exog is not None)): 
      from patsy import dmatrix 
      exog = dmatrix(self.model.data.design_info.builder, exog) 
   if (exog is not None): 
      if (row_labels is None): 
         if hasattr(exog, 'index'): 
            row_labels = exog.index 
         else: 
            row_labels = None 
      exog = np.asarray(exog) 
      if ((exog.ndim == 1) and ((self.model.exog.ndim == 1) or (self.model.exog.shape[1] == 1))): 
         exog = exog[:, None] 
      exog = np.atleast_2d(exog) 
   else: 
      exog = self.model.exog 
      if (weights is None): 
         weights = getattr(self.model, 'weights', None) 
      if (row_labels is None): 
         row_labels = getattr(self.model.data, 'row_labels', None) 
   if (weights is not None): 
      weights = np.asarray(weights) 
      if ((weights.size > 1) and ((weights.ndim != 1) or (weights.shape[0] == exog.shape[1]))): 
         raise ValueError('weights   has   wrong   shape') 
   if (pred_kwds is None): 
      pred_kwds = {} 
   predicted_mean = self.model.predict(self.params, exog, **pred_kwds) 
   covb = self.cov_params() 
   var_pred_mean = (exog * np.dot(covb, exog.T).T).sum(1) 
   var_resid = (self.scale / weights) 
   if (self.cov_type == 'fixed   scale'): 
      var_resid = (self.cov_kwds['scale'] / weights) 
   dist = ['norm', 't'][self.use_t] 
   return PredictionResults(predicted_mean, var_pred_mean, var_resid, df=self.df_resid, dist=dist, row_labels=row_labels)"," 'Returns prediction results for the fitted model. 
 Parameters 
 exog : array-like, optional 
 The design matrix. If None, the design matrix is obtained from 
 self.model.exog. 
 transform : bool, optional 
 If True, the design matrix is transformed according to the formula 
 specified in self.model.formula. 
 weights : array-like, optional 
 The weights. If None, the weights are obtained from self.model.weights. 
 row_labels : array-like, optional 
 The row labels of the design matrix. If None, the row labels are obtained 
 from self.model.data.row_labels. 
 pred_kwds : dict, optional 
 Additional keyword arguments passed to self.model.predict. 
 Returns 
 A PredictionResults object. 
 Raises 
 ValueError 
 If the design matrix is not 2-dimensional, or if the number of 
 weights is not equal to the number of rows of the design matrix. 
 See Also 
 PredictionResults'","'compute prediction results 
 Parameters 
 exog : array-like, optional 
 The values for which you want to predict. 
 transform : bool, optional 
 If the model was fit via a formula, do you want to pass 
 exog through the formula. Default is True. E.g., if you fit 
 a model y ~ log(x1) + log(x2), and transform is True, then 
 you can pass a data structure that contains x1 and x2 in 
 their original form. Otherwise, you\'d need to log the data 
 first. 
 weights : array_like, optional 
 Weights interpreted as in WLS, used for the variance of the predicted 
 residual. 
 args, kwargs : 
 Some models can take additional arguments or keywords, see the 
 predict method of the model for the details. 
 Returns 
 prediction_results : instance 
 The prediction results instance contains prediction and prediction 
 variance and can on demand calculate confidence intervals and summary 
 tables for the prediction of the mean and of new observations.'"
"def list_bucket(bucket_name): 
    url = 'https://www.googleapis.com/storage/v1/b/{0}/o'.format(bucket_name) 
   try: 
      response = gcs_get_request(url) 
      if (response.status_code != HTTP_OK): 
         logging.error('Error   on   listing   objects   in   GCS   bucket:   {0}.   Error:   {1}'.format(bucket_name, response.status_code)) 
         return [] 
      content = json.loads(response.content) 
   except requests.HTTPError as error: 
      logging.error('Error   on   listing   objects   in   GCS   bucket:   {0}.   Error:   {1}'.format(bucket_name, error)) 
      return [] 
   if ('items' not in content.keys()): 
      return [] 
   objects = [] 
   for item in content['items']: 
      objects.append(item['name']) 
   logging.debug('Bucket   contents:   {0}'.format(objects)) 
   return objects"," 'Lists the objects in a bucket. 
 :param bucket_name: The name of the bucket to list. 
 :return: A list of the objects in the bucket. 
 :rtype: list'","'Lists all the files that are in the designated GCS bucket. 
 Args: 
 bucket_name: A str, the name of the GCS bucket to look up. 
 Returns: 
 A list of str, the names of the files in the bucket.'"
"def liveobj_changed(obj, other): 
    return ((obj != other) or (type(obj) != type(other)))"," 'Return True if obj and other are different objects or if they are 
 different types of objects.'","'Check whether obj and other are not equal, properly handling lost weakrefs. 
 Use this whenever you cache a Live API object in some variable, and want to check 
 whether you need to update the cached object.'"
"def site_enabled(config): 
    enable_site(config) 
   reload_service('apache2')"," 'Enable the Apache2 service. 
 :param config: the configuration dictionary 
 :type config: dict'","'Require an Apache site to be enabled. 
 This will cause Apache to reload its configuration. 
 from fabtools import require 
 require.apache.site_enabled(\'mysite\')'"
"def read_weighted_edgelist(path, comments='#', delimiter=None, create_using=None, nodetype=None, encoding='utf-8'): 
    return read_edgelist(path, comments=comments, delimiter=delimiter, create_using=create_using, nodetype=nodetype, data=(('weight', float),), encoding=encoding)"," 'Read a weighted edgelist from a file. 
 :param path: Path to the file to read. 
 :param comments: A string of comments to ignore. 
 :param delimiter: A character to use as a delimiter. 
 :param create_using: A function to create a new edge from a tuple of 
 two node objects. 
 :param nodetype: A function to create a new node from a tuple of 
 two node objects. 
 :param encoding: The encoding of the file. 
 :return: A dictionary of edge tuples where the key is the node id and the 
 value is the edge tuple. 
 :rtype: dict'","'Read a graph as list of edges with numeric weights. 
 Parameters 
 path : file or string 
 File or filename to read. If a file is provided, it must be 
 opened in \'rb\' mode. 
 Filenames ending in .gz or .bz2 will be uncompressed. 
 comments : string, optional 
 The character used to indicate the start of a comment. 
 delimiter : string, optional 
 The string used to separate values.  The default is whitespace. 
 create_using : Graph container, optional, 
 Use specified container to build graph.  The default is networkx.Graph, 
 an undirected graph. 
 nodetype : int, float, str, Python type, optional 
 Convert node data from strings to specified type 
 encoding: string, optional 
 Specify which encoding to use when reading file. 
 Returns 
 G : graph 
 A networkx Graph or other type specified with create_using 
 Notes 
 Since nodes must be hashable, the function nodetype must return hashable 
 types (e.g. int, float, str, frozenset - or tuples of those, etc.) 
 Example edgelist file format. 
 With numeric edge data:: 
 # read with 
 # >>> G=nx.read_weighted_edgelist(fh) 
 # source target data 
 a b 1 
 a c 3.14159 
 d e 42'"
"def _d(n, j, prec, sq23pi, sqrt8): 
    j = from_int(j) 
   pi = mpf_pi(prec) 
   a = mpf_div(sq23pi, j, prec) 
   b = mpf_sub(from_int(n), from_rational(1, 24, prec), prec) 
   c = mpf_sqrt(b, prec) 
   (ch, sh) = mpf_cosh_sinh(mpf_mul(a, c), prec) 
   D = mpf_div(mpf_sqrt(j, prec), mpf_mul(mpf_mul(sqrt8, b), pi), prec) 
   E = mpf_sub(mpf_mul(a, ch), mpf_div(sh, c, prec), prec) 
   return mpf_mul(D, E)"," 'Returns the value of the 23rd-order Jacobi polynomial 
 at the given value of n. 
 The 23rd-order Jacobi polynomial is defined as 
 .. math:: 
 P_{n}^{(23)}(x) = \frac{1}{2^{15/2} \sqrt{\pi}} \sum_{k=0}^{n} 
 \frac{(2k+1)!}{k! (n-k)! (23 k+1)!} x^{23 k} 
 (1-x)^{23 n} 
 where the sum is over nonnegative integers. 
 The values of the 23rd-order Jacobi polynomial at integer values of n are 
 .. math:: 
 P_{n}^{(23)}(x) = \frac{1}{2^{15/2} \sqrt{\pi}} 
 \frac{(2n+1)!}{n! (23 n)!} x^{23 n} (1-x)^{23 n} 
 for n = 0, 1, 2, 3, ... 
 Parameters 
 n : the value of the","'Compute the sinh term in the outer sum of the HRR formula. 
 The constants sqrt(2/3*pi) and sqrt(8) must be precomputed.'"
"def t_BOOLCONSTANT(t): 
    t.value = (t.value == 'true') 
   return t", 'BOOLCONSTANT: boolean constant','\btrue\b|\bfalse\b'
"def write_excellon(): 
    filename = string_cam_file.get() 
   file = open(filename, 'wb') 
   units = cad.inches_per_unit 
   file.write('%FSLAX24Y24*%\n') 
   file.write('%MOIN*%\n') 
   file.write('%OFA0B0*%\n') 
   ixs = cad.x[::2] 
   xs = (cad.xmin + (((cad.xmax - cad.xmin) * (ixs + 0.5)) / float(cad.nx))) 
   ixe = cad.x[1::2] 
   xe = (cad.xmin + (((cad.xmax - cad.xmin) * (ixe + 0.5)) / float(cad.nx))) 
   idx = (ixe - ixs) 
   dx = (xe - xs) 
   iys = cad.y[::2] 
   ys = (cad.ymin + (((cad.ymax - cad.ymin) * (iys + 0.5)) / float(cad.ny))) 
   iye = cad.y[1::2] 
   ye = (cad.ymin + (((cad.ymax - cad.ymin) * (iye + 0.5)) / float(cad.ny))) 
   idy = (iye - iys) 
   dy = (ye - ys) 
   mins = where((idx < idy), idx, idy) 
   uniques = unique(mins) 
   apertures = (((cad.xmax - cad.xmin) * uniques) / float(cad.nx)) 
   index = searchsorted(uniques, mins) 
   for i in range(len(uniques)): 
      file.write(('%%ADD%dR,%.4fX%.4f*%%\n' % ((i + 10), apertures[i], apertures[i]))) 
   coords = arange(len(mins)) 
   for i in range(len(uniques)): 
      file.write(('D%d*\n' % (i + 10))) 
      coord = coords[(index == i)] 
      delta = (apertures[i] / 2.0) 
      ixs = (10000 * (xs + delta)).astype(int32) 
      ixe = (10000 * (xe - delta)).astype(int32) 
      iys = (10000 * (ys + delta)).astype(int32) 
      iye = (10000 * (ye - delta)).astype(int32) 
      for j in range(len(coord)): 
         n = coord[j] 
         if (idx[n] == idy[n]): 
            file.write(('X%dY%dD03*\n' % (ixs[n], iys[n]))) 
         elif (idx[n] > idy[n]): 
            file.write(('X%dY%dD02*\n' % (ixs[n], iys[n]))) 
            file.write(('X%dY%dD01*\n' % (ixe[n], iys[n]))) 
         else: 
            file.write(('X%dY%dD02*\n' % (ixs[n], iys[n]))) 
            file.write(('X%dY%dD01*\n' % (ixs[n], iye[n]))) 
   file.write('M02*\n') 
   file.close() 
   string_msg.set(('wrote   %s   (RS-274X)' % filename)) 
   root.update()"," 'Writes the Excellon file to disk. 
 Parameters 
 filename : str 
 The name of the file to write. 
 Returns 
 None 
 Notes 
 This function writes the Excellon file to disk. 
 Examples 
 >>> write_excellon() 
 >>> write_excellon(string_cam_file.get())'","'%       Rewind and Stop 
 X#Y#    Move and Drill 
 T#      Tool Selection 
 M30     End of Program 
 M00     End of Program 
 R#X#Y#  Repeat Hole 
 G05, G81        Select Drill Mode 
 G90     Absolute Mode 
 G91     Incremental Mode 
 G92 X#Y#        Set Zero 
 G93 X#Y#        Set Zero 
 M48     Program Header to first ""%"" 
 M72     English-Imperial Mode'"
"def oas(X, assume_centered=False): 
    X = np.asarray(X) 
   if ((len(X.shape) == 2) and (X.shape[1] == 1)): 
      if (not assume_centered): 
         X = (X - X.mean()) 
      return (np.atleast_2d((X ** 2).mean()), 0.0) 
   if (X.ndim == 1): 
      X = np.reshape(X, (1, (-1))) 
      warnings.warn('Only   one   sample   available.   You   may   want   to   reshape   your   data   array') 
      n_samples = 1 
      n_features = X.size 
   else: 
      (n_samples, n_features) = X.shape 
   emp_cov = empirical_covariance(X, assume_centered=assume_centered) 
   mu = (np.trace(emp_cov) / n_features) 
   alpha = np.mean((emp_cov ** 2)) 
   num = (alpha + (mu ** 2)) 
   den = ((n_samples + 1.0) * (alpha - ((mu ** 2) / n_features))) 
   shrinkage = (1.0 if (den == 0) else min((num / den), 1.0)) 
   shrunk_cov = ((1.0 - shrinkage) * emp_cov) 
   shrunk_cov.flat[::(n_features + 1)] += (shrinkage * mu) 
   return (shrunk_cov, shrinkage)"," 'Calculate the OAS for a given data matrix. 
 Parameters 
 X : array-like 
 The data matrix. 
 assume_centered : bool, optional 
 If True, the data will be centered before calculating the OAS. 
 Returns 
 cov : array, shape (n_features, n_features) 
 The estimated covariance matrix. 
 shrinkage : float 
 The shrinkage parameter. 
 Examples 
 >>> from sklearn.covariance import oas 
 >>> X = np.array([[1, 2, 3], [4, 5, 6]]) 
 >>> oas(X) 
 (array([[0.0, 0.0, 0.0], 
 [0.0, 0.0, 0.0], 
 [0.0, 0.0, 0.0]]), 
 0.0)'","'Estimate covariance with the Oracle Approximating Shrinkage algorithm. 
 Parameters 
 X : array-like, shape (n_samples, n_features) 
 Data from which to compute the covariance estimate. 
 assume_centered : boolean 
 If True, data are not centered before computation. 
 Useful to work with data whose mean is significantly equal to 
 zero but is not exactly zero. 
 If False, data are centered before computation. 
 Returns 
 shrunk_cov : array-like, shape (n_features, n_features) 
 Shrunk covariance. 
 shrinkage : float 
 Coefficient in the convex combination used for the computation 
 of the shrunk estimate. 
 Notes 
 The regularised (shrunk) covariance is: 
 (1 - shrinkage)*cov 
 + shrinkage * mu * np.identity(n_features) 
 where mu = trace(cov) / n_features 
 The formula we used to implement the OAS 
 does not correspond to the one given in the article. It has been taken 
 from the MATLAB program available from the author\'s webpage 
 (http://tbayes.eecs.umich.edu/yilun/covestimation).'"
"def virtualenv_no_global(): 
    site_mod_dir = os.path.dirname(os.path.abspath(site.__file__)) 
   no_global_file = os.path.join(site_mod_dir, 'no-global-site-packages.txt') 
   if (running_under_virtualenv() and os.path.isfile(no_global_file)): 
      return True"," 'Check if we are running under a virtualenv and if so, if the no-global-site-packages.txt file exists.'",'Return True if in a venv and no system site packages.'
"def pkt_line(data): 
    if (data is None): 
      return '0000' 
   return (('%04x' % (len(data) + 4)).encode('ascii') + data)"," 'Returns a string representation of a packet line. 
 :param data: Packet data. 
 :type data: bytes 
 :return: String representation of the packet line. 
 :rtype: bytes'","'Wrap data in a pkt-line. 
 :param data: The data to wrap, as a str or None. 
 :return: The data prefixed with its length in pkt-line format; if data was 
 None, returns the flush-pkt (\'0000\').'"
"def assert_raises_regexp(exception, reg, run, *args, **kwargs): 
    __tracebackhide__ = True 
   try: 
      run(*args, **kwargs) 
      assert False, ('%s   should   have   been   thrown' % exception) 
   except exception: 
      e = sys.exc_info()[1] 
      p = re.compile(reg) 
      assert p.search(str(e)), str(e)"," 'Raise an exception if the given exception is raised with the given 
 regexp in the error message. 
 :param exception: The exception to check for 
 :param reg: The regexp to match the error message against 
 :param run: The function to run 
 :param *args: Additional arguments to pass to run 
 :param **kwargs: Additional keyword arguments to pass to run'",'Like assertRaisesRegexp in unittest'
"def network_list(call=None, **kwargs): 
    conn = get_conn() 
   return conn.network_list()", 'Lists all networks.','List private networks'
"@fixture 
 def patch_network_functions(monkeypatch): 
    import inbox.actions.backends 
   for backend in inbox.actions.backends.module_registry.values(): 
      for method_name in backend.__all__: 
         monkeypatch.setattr(((backend.__name__ + '.') + method_name), (lambda *args, **kwargs: None))", 'Patch all network functions to return None.',"'Monkeypatch syncback functions that actually talk to Gmail so that the 
 tests can run faster.'"
"def mkdir_p(path): 
    try: 
      os.makedirs(path) 
   except OSError as exc: 
      if ((exc.errno == errno.EEXIST) and os.path.isdir(path)): 
         pass 
      else: 
         raise"," 'Create a directory if it does not exist. 
 Raises OSError if the directory cannot be created.'",'like `mkdir -p`'
"def get_docstring_and_rest(filename): 
    with open(filename, 'rb') as fid: 
      content = fid.read() 
   content = content.replace('\r\n', '\n') 
   try: 
      node = ast.parse(content) 
   except SyntaxError: 
      return (SYNTAX_ERROR_DOCSTRING, content.decode('utf-8')) 
   if (not isinstance(node, ast.Module)): 
      raise TypeError('This   function   only   supports   modules.   You   provided   {0}'.format(node.__class__.__name__)) 
   if (node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Str)): 
      docstring_node = node.body[0] 
      docstring = docstring_node.value.s 
      if hasattr(docstring, 'decode'): 
         docstring = docstring.decode('utf-8') 
      rest = content.decode('utf-8').split('\n', docstring_node.lineno)[(-1)] 
      return (docstring, rest) 
   else: 
      raise ValueError('Could   not   find   docstring   in   file   ""{0}"".   A   docstring   is   required   by   sphinx-gallery'.format(filename))"," 'Returns docstring and rest of the file, where rest is the part of the 
 file after the docstring. 
 This function parses the file and looks for the first docstring node in 
 the file, then returns the docstring and the rest of the file. 
 The docstring is returned as a string, and the rest of the file is 
 returned as a string. 
 The docstring is returned as a string, and the rest of the file is 
 returned as a string.'","'Separate `filename` content between docstring and the rest 
 Strongly inspired from ast.get_docstring. 
 Returns 
 docstring: str 
 docstring of `filename` 
 rest: str 
 `filename` content without the docstring'"
"@pytest.mark.parametrize('constructor,   attrs,   expected', [(False, {}, '<test_utils.Obj>'), (False, {'foo': None}, '<test_utils.Obj   foo=None>'), (False, {'foo': ""b'ar"", 'baz': 2}, '<test_utils.Obj   baz=2   foo=""b\'ar"">'), (True, {}, 'test_utils.Obj()'), (True, {'foo': None}, 'test_utils.Obj(foo=None)'), (True, {'foo': ""te'st"", 'bar': 2}, 'test_utils.Obj(bar=2,   foo=""te\'st"")')]) 
 def test_get_repr(constructor, attrs, expected): 
    assert (utils.get_repr(Obj(), constructor, **attrs) == expected)", 'Test get_repr()','Test get_repr().'
"@treeio_login_required 
 @handle_response_format 
 def ordered_product_delete(request, ordered_product_id, response_format='html'): 
    ordered_product = get_object_or_404(OrderedProduct, pk=ordered_product_id) 
   if ((not request.user.profile.has_permission(ordered_product, mode='w')) and (not request.user.profile.is_admin('treeio.sales'))): 
      return user_denied(request, ""You   don't   have   access   to   this   Sale   Status"", response_format) 
   if request.POST: 
      if ('delete' in request.POST): 
         order_id = ordered_product.order_id 
         if ('trash' in request.POST): 
            ordered_product.trash = True 
            ordered_product.save() 
         else: 
            ordered_product.delete() 
         ordered_product.order.update_total() 
         return HttpResponseRedirect(reverse('sales_order_view', args=[order_id])) 
      elif ('cancel' in request.POST): 
         return HttpResponseRedirect(reverse('sales_ordered_product_view', args=[ordered_product.id])) 
   order = ordered_product.order 
   return render_to_response('sales/ordered_product_delete', {'ordered_product': ordered_product, 'order': order}, context_instance=RequestContext(request), response_format=response_format)"," 'Delete an ordered product. 
 :param request: The current request. 
 :param ordered_product_id: The id of the ordered product to delete. 
 :param response_format: The response format to use. 
 :return: The response. 
 :rtype: :class:`django.http.HttpResponse`'",'OrderedProduct delete'
"def snmp_preprocessor(a_device, oid='.1.3.6.1.2.1.1.1.0'): 
    if (not (a_device.snmp_credentials.snmp_mode == 'snmp3')): 
      raise ValueError('Invalid   SNMP   mode   in   config_detect   {}'.format(a_device.snmp_credentials.snmp_mode)) 
   snmp_device = (a_device.ip_address, a_device.snmp_port) 
   snmp_user = (a_device.snmp_credentials.username, a_device.snmp_credentials.auth_key, a_device.snmp_credentials.encrypt_key) 
   auth_proto = a_device.snmp_credentials.auth_proto 
   encrypt_proto = a_device.snmp_credentials.encrypt_proto 
   return {'snmp_device': snmp_device, 'snmp_user': snmp_user, 'oid': oid, 'auth_proto': auth_proto, 'encrypt_proto': encrypt_proto}", 'Returns a dictionary with the SNMP credentials for the device',"'Extract snmp parameters from NetworkDevice object 
 Only supports SNMPv3'"
"def _can_do_sum_of_squares(n, k): 
    if (k < 1): 
      return False 
   if (n < 0): 
      return False 
   if (n == 0): 
      return True 
   if (k == 1): 
      return is_square(n) 
   if (k == 2): 
      if (n in (1, 2)): 
         return True 
      if isprime(n): 
         if ((n % 4) == 1): 
            return 1 
         return False 
      else: 
         f = factorint(n) 
         for (p, m) in f.items(): 
            if (((p % 4) == 3) and (m % 2)): 
               return False 
         return True 
   if (k == 3): 
      if (((n // (4 ** multiplicity(4, n))) % 8) == 7): 
         return False 
   return True"," 'Check if n can be expressed as the sum of k squares. 
 The function checks if n can be expressed as the sum of k squares 
 where k is a non-negative integer. 
 Examples 
 >>> from sympy.ntheory import _can_do_sum_of_squares 
 >>> _can_do_sum_of_squares(16, 2) 
 True 
 >>> _can_do_sum_of_squares(16, 3) 
 False 
 >>> _can_do_sum_of_squares(16, 4) 
 False 
 >>> _can_do_sum_of_squares(16, 5) 
 True 
 >>> _can_do_sum_of_squares(16, 6) 
 False 
 >>> _can_do_sum_of_squares(16, 7) 
 False 
 >>> _can_do_sum_of_squares(16, 8) 
 True 
 >>> _can_do_sum_of_squares(16, 9)","'Return True if n can be written as the sum of k squares, 
 False if it can\'t, or 1 if k == 2 and n is prime (in which 
 case it *can* be written as a sum of two squares). A False 
 is returned only if it can\'t be written as k-squares, even 
 if 0s are allowed.'"
"@require_context 
 @require_volume_exists 
 def volume_glance_metadata_create(context, volume_id, key, value, session=None): 
    if (session is None): 
      session = get_session() 
   with session.begin(): 
      rows = session.query(models.VolumeGlanceMetadata).filter_by(volume_id=volume_id).filter_by(key=key).filter_by(deleted=False).all() 
      if (len(rows) > 0): 
         raise exception.GlanceMetadataExists(key=key, volume_id=volume_id) 
      vol_glance_metadata = models.VolumeGlanceMetadata() 
      vol_glance_metadata.volume_id = volume_id 
      vol_glance_metadata.key = key 
      vol_glance_metadata.value = value 
      vol_glance_metadata.save(session=session) 
   return", 'Create a volume glance metadata entry.',"'Update the Glance metadata for a volume by adding a new key:value pair. 
 This API does not support changing the value of a key once it has been 
 created.'"
"def _tile_perimeter_width(coord, projection): 
    perimeter = _tile_perimeter(coord, projection, False) 
   return (perimeter[8][0] - perimeter[0][0])", 'Return the width of the tile perimeter.',"'Get the width in projected coordinates of the coordinate tile polygon. 
 Uses _tile_perimeter().'"
"def update_connection_pool(maxsize=1): 
    get_pool().connection_pool_kw.update(maxsize=maxsize)", 'Update the connection pool\'s maxsize to the given value.',"'Update the global connection pool manager parameters. 
 maxsize: Number of connections to save that can be reused (default=1). 
 More than 1 is useful in multithreaded situations.'"
"@LocalContext 
 def alphanumeric(raw_bytes, *a, **kw): 
    return encode(raw_bytes, expr=re_alphanumeric, *a, **kw)"," 'Convert bytes to alphanumeric characters. 
 The ``re_alphanumeric`` regular expression matches alphanumeric 
 characters, and is defined in :func:`re_compile`. 
 The ``encoding`` argument is a :class:`~.codecs.Codec` object, 
 and is used to encode the input bytes into Unicode. 
 :param bytes raw_bytes: the bytes to convert. 
 :param encoding: the encoding to use. 
 :param flags: the encoding flags. 
 :param errors: the encoding errors. 
 :param bytes encoding: the encoding to use. 
 :param bytes errors: the encoding errors. 
 :param bytes flags: the encoding flags. 
 :param bytes raw_bytes: the bytes to convert. 
 :param encoding: the encoding to use. 
 :param flags: the encoding flags. 
 :param errors: the encoding errors. 
 :param bytes encoding: the encoding to use. 
 :param bytes errors: the encoding errors. 
 :param bytes flags: the encoding flags. 
 :param bytes raw_bytes: the bytes to convert. 
 :param encoding: the encoding to","'alphanumeric(raw_bytes) -> str 
 Encode the shellcode ``raw_bytes`` such that it does not 
 contain any bytes except for [A-Za-z0-9]. 
 Accepts the same arguments as :func:`encode`.'"
"def libvlc_media_list_set_media(p_ml, p_md): 
    f = (_Cfunctions.get('libvlc_media_list_set_media', None) or _Cfunction('libvlc_media_list_set_media', ((1,), (1,)), None, None, MediaList, Media)) 
   return f(p_ml, p_md)"," 'Set the media in the media list. 
 @param p_ml: media list. 
 @param p_md: media. 
 @version: LibVLC 2.0.0 or later.'","'Associate media instance with this media list instance. 
 If another media instance was present it will be released. 
 The L{libvlc_media_list_lock} should NOT be held upon entering this function. 
 @param p_ml: a media list instance. 
 @param p_md: media instance to add.'"
"def _GetPdbPath(target_dict, config_name, vars): 
    config = target_dict['configurations'][config_name] 
   msvs = config.setdefault('msvs_settings', {}) 
   linker = msvs.get('VCLinkerTool', {}) 
   pdb_path = linker.get('ProgramDatabaseFile') 
   if pdb_path: 
      return pdb_path 
   variables = target_dict.get('variables', {}) 
   pdb_path = variables.get('msvs_large_pdb_path', None) 
   if pdb_path: 
      return pdb_path 
   pdb_base = target_dict.get('product_name', target_dict['target_name']) 
   pdb_base = ('%s.%s.pdb' % (pdb_base, TARGET_TYPE_EXT[target_dict['type']])) 
   pdb_path = ((vars['PRODUCT_DIR'] + '/') + pdb_base) 
   return pdb_path"," 'Returns the path to the large pdb file for the given target. 
 The path is returned in the form of a string.'","'Returns the path to the PDB file that will be generated by a given 
 configuration. 
 The lookup proceeds as follows: 
 - Look for an explicit path in the VCLinkerTool configuration block. 
 - Look for an \'msvs_large_pdb_path\' variable. 
 - Use \'<(PRODUCT_DIR)/<(product_name).(exe|dll).pdb\' if \'product_name\' is 
 specified. 
 - Use \'<(PRODUCT_DIR)/<(target_name).(exe|dll).pdb\'. 
 Arguments: 
 target_dict: The target dictionary to be searched. 
 config_name: The name of the configuration of interest. 
 vars: A dictionary of common GYP variables with generator-specific values. 
 Returns: 
 The path of the corresponding PDB file.'"
"@constructor 
 def max(x, axis=None, keepdims=False): 
    try: 
      out = max_and_argmax(x, axis)[0] 
   except Exception: 
      out = CAReduce(scal.maximum, axis)(x) 
   if keepdims: 
      out = makeKeepDims(x, out, axis) 
   return out"," 'Return the maximum of the input array. 
 Parameters 
 x : array_like 
 Input array. 
 axis : int, optional 
 The axis along which to compute the maximum. 
 If None, the maximum over all axes is returned. 
 If `axis` is a list of integers, the maximum along each axis 
 is returned. 
 keepdims : bool, optional 
 If `True`, the axes reduced along are retained. 
 Returns 
 out : ndarray 
 The maximum of `x`.'","'Returns maximum elements obtained by iterating over given axis. 
 When axis is None (the default value), the max is performed 
 over the flattened tensor. 
 Parameters 
 keepdims: bool 
 If this is set to True, the axes which are reduced are left in 
 the result as dimensions with size one. With this option, the result 
 will broadcast correctly against the original tensor. 
 Notes 
 We return an error as numpy when we reduce a dim with a shape of 0.'"
"def staff_member_required(view_func): 
    def _checklogin(request, *args, **kwargs): 
      if (request.user.is_authenticated() and request.user.is_staff): 
         return view_func(request, *args, **kwargs) 
      assert hasattr(request, 'session'), ""The   Django   admin   requires   session   middleware   to   be   installed.   Edit   your   MIDDLEWARE_CLASSES   setting   to   insert   'django.contrib.sessions.middleware.SessionMiddleware'."" 
      if (not request.POST.has_key(LOGIN_FORM_KEY)): 
         if request.POST: 
            message = _('Please   log   in   again,   because   your   session   has   expired.') 
         else: 
            message = '' 
         return _display_login_form(request, message) 
      if (not request.session.test_cookie_worked()): 
         message = _(""Looks   like   your   browser   isn't   configured   to   accept   cookies.   Please   enable   cookies,   reload   this   page,   and   try   again."") 
         return _display_login_form(request, message) 
      username = request.POST.get('username', None) 
      password = request.POST.get('password', None) 
      user = authenticate(username=username, password=password) 
      if (user is None): 
         message = ERROR_MESSAGE 
         if ('@' in username): 
            try: 
               user = User.objects.get(email=username) 
            except User.DoesNotExist: 
               message = _(""Usernames   cannot   contain   the   '@'   character."") 
            else: 
               message = (_(""Your   e-mail   address   is   not   your   username.   Try   '%s'   instead."") % user.username) 
         return _display_login_form(request, message) 
      elif (user.is_active and user.is_staff): 
         login(request, user) 
         user.last_login = datetime.datetime.now() 
         user.save() 
         return http.HttpResponseRedirect(request.path) 
      else: 
         return _display_login_form(request, ERROR_MESSAGE) 
   return _checklogin"," 'Adds a staff_member_required decorator to a view. 
 The decorator ensures that only staff members can access the view. 
 The view must be a function, and the decorator will be applied to that 
 function. 
 This decorator is used by the Django admin to ensure that only staff 
 members can access the admin interface. 
 .. versionchanged:: 0.7 
 The decorator now takes a view function as an argument.'","'Decorator for views that checks that the user is logged in and is a staff 
 member, displaying the login page if necessary.'"
"def _is_axial_coil(coil): 
    is_ax = (coil['coil_class'] in (FIFF.FWD_COILC_MAG, FIFF.FWD_COILC_AXIAL_GRAD, FIFF.FWD_COILC_AXIAL_GRAD2)) 
   return is_ax", 'Returns True if the coil is an axial gradiometer.','Determine if the coil is axial.'
"@public 
 def guess_generating_function_rational(v, X=Symbol('x')): 
    q = find_simple_recurrence_vector(v) 
   n = len(q) 
   if (n <= 1): 
      return None 
   p = [sum(((v[(i - k)] * q[k]) for k in range(min((i + 1), n)))) for i in range(len(v))] 
   return (sum(((p[k] * (X ** k)) for k in range(len(p)))) / sum(((q[k] * (X ** k)) for k in range(n))))"," 'Guess generating function of the recurrence relation. 
 Examples 
 >>> from sympy.recurrence import guess_generating_function_rational 
 >>> v = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20] 
 >>> guess_generating_function_rational(v) 
 2/5*x**5 + 2/3*x**3 + 1/5*x 
 >>> v = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30] 
 >>> guess_generating_function_rational(v) 
 2/5*x**5 + 2/3*x**3 + 1/5*x + 2/5*x**3 + 2/3*x**3 + 1/5*x 
 See Also 
 find_simple_recurrence_","'Tries to ""guess"" a rational generating function for a sequence of rational 
 numbers v. 
 Examples 
 >>> from sympy.concrete.guess import guess_generating_function_rational 
 >>> from sympy import fibonacci 
 >>> l = [fibonacci(k) for k in range(5,15)] 
 >>> guess_generating_function_rational(l) 
 (3*x + 5)/(-x**2 - x + 1) 
 See also 
 See function sympy.series.approximants and mpmath.pade'"
"def generate_strings(total_strings, string_length=20): 
    statements = [] 
   for _ in range(0, total_strings): 
      text = ''.join((random.choice(((string.ascii_letters + string.digits) + '   ')) for _ in range(string_length))) 
      statements.append(text) 
   return statements", 'Generate a list of strings of length `string_length`','Generate a list of random strings.'
"def parse_requirements(strs): 
    lines = iter(yield_lines(strs)) 
   def scan_list(ITEM, TERMINATOR, line, p, groups, item_name): 
      items = [] 
      while (not TERMINATOR(line, p)): 
         if CONTINUE(line, p): 
            try: 
               line = next(lines) 
               p = 0 
            except StopIteration: 
               raise ValueError('\\   must   not   appear   on   the   last   nonblank   line') 
         match = ITEM(line, p) 
         if (not match): 
            msg = (('Expected   ' + item_name) + '   in') 
            raise ValueError(msg, line, 'at', line[p:]) 
         items.append(match.group(*groups)) 
         p = match.end() 
         match = COMMA(line, p) 
         if match: 
            p = match.end() 
         elif (not TERMINATOR(line, p)): 
            msg = ""Expected   ','   or   end-of-list   in"" 
            raise ValueError(msg, line, 'at', line[p:]) 
      match = TERMINATOR(line, p) 
      if match: 
         p = match.end() 
      return (line, p, items) 
   for line in lines: 
      match = DISTRO(line) 
      if (not match): 
         raise ValueError('Missing   distribution   spec', line) 
      project_name = match.group(1) 
      p = match.end() 
      extras = [] 
      match = OBRACKET(line, p) 
      if match: 
         p = match.end() 
         (line, p, extras) = scan_list(DISTRO, CBRACKET, line, p, (1,), ""'extra'   name"") 
      (line, p, specs) = scan_list(VERSION, LINE_END, line, p, (1, 2), 'version   spec') 
      specs = [(op, safe_version(val)) for (op, val) in specs] 
      (yield Requirement(project_name, specs, extras))"," 'Parse a requirements.txt file into a list of requirements. 
 :param strs: a file-like object with a newline-delimited stream of strings 
 :return: a list of requirements'","'Yield ``Requirement`` objects for each specification in `strs` 
 `strs` must be a string, or a (possibly-nested) iterable thereof.'"
"def dynamize_value(val): 
    dynamodb_type = get_dynamodb_type(val) 
   if (dynamodb_type == 'N'): 
      val = {dynamodb_type: serialize_num(val)} 
   elif (dynamodb_type == 'S'): 
      val = {dynamodb_type: val} 
   elif (dynamodb_type == 'NS'): 
      val = {dynamodb_type: list(map(serialize_num, val))} 
   elif (dynamodb_type == 'SS'): 
      val = {dynamodb_type: [n for n in val]} 
   elif (dynamodb_type == 'B'): 
      if isinstance(val, bytes): 
         val = Binary(val) 
      val = {dynamodb_type: val.encode()} 
   elif (dynamodb_type == 'BS'): 
      val = {dynamodb_type: [n.encode() for n in val]} 
   return val"," 'Returns the value of a DynamoDB attribute as a dictionary. 
 :param val: the value to be dynamized 
 :type val: any 
 :return: a dictionary with the dynamized value 
 :rtype: dict'","'Take a scalar Python value and return a dict consisting 
 of the Amazon DynamoDB type specification and the value that 
 needs to be sent to Amazon DynamoDB.  If the type of the value 
 is not supported, raise a TypeError'"
"def add_completer(widget, items): 
    completer = QtWidgets.QCompleter(items, widget) 
   completer.setCaseSensitivity(Qt.CaseInsensitive) 
   completer.setCompletionMode(QtWidgets.QCompleter.InlineCompletion) 
   widget.setCompleter(completer)"," 'Add a completer to a QLineEdit. 
 :param widget: the QLineEdit to add the completer to 
 :param items: a list of strings to use as the completer\'s items 
 :type items: list(str)'",'Add simple completion to a widget'
"def _compute_hash_v1(get_deps_dict, hash): 
    uniquedeps = get_deps_dict['uniquedeps'] 
   spec = get_deps_dict['spec'] 
   hash.update(spec.text) 
   for d in uniquedeps: 
      hash.update(roslib.msgs.get_registered(d).text) 
   return hash.hexdigest()"," 'Compute the hash of the spec and uniquedeps, and return the hash.'","'subroutine of compute_md5_v1() 
 @param get_deps_dict: dictionary returned by get_dependencies call 
 @type  get_deps_dict: dict 
 @param hash: hash instance 
 @type  hash: hash instance'"
"def negative_sampling(x, t, W, sampler, sample_size): 
    return NegativeSamplingFunction(sampler, sample_size)(x, t, W)"," 'Apply negative sampling to x. 
 Parameters 
 x : array 
 The input data. 
 t : array 
 The target values. 
 W : array 
 The weighting matrix. 
 sampler : sampler 
 The sampler to use. 
 sample_size : int 
 The number of negative samples to use.'","'Negative sampling loss function. 
 In natural language processing, especially language modeling, the number of 
 words in a vocabulary can be very large. 
 Therefore, you need to spend a lot of time calculating the gradient of the 
 embedding matrix. 
 By using the negative sampling trick you only need to calculate the 
 gradient for a few sampled negative examples. 
 The objective function is below: 
 .. math:: 
 f(x, p) = \log \sigma(x^\top w_p) + \ 
 k E_{i \sim P(i)}[\log \sigma(- x^\top w_i)], 
 where :math:`\sigma(\cdot)` is a sigmoid function, :math:`w_i` is the 
 weight vector for the word :math:`i`, and :math:`p` is a positive example. 
 It is approximeted with :math:`k` examples :math:`N` sampled from 
 probability :math:`P(i)`, like this: 
 .. math:: 
 f(x, p) \approx \log \sigma(x^\top w_p) + \ 
 \sum_{n \in N} \log \sigma(-x^\top w_n). 
 Each sample of :math:`N` is drawn from the word distribution :math:`P(w)`. 
 This is calculated as :math:`P(w) = \frac{1}{Z} c(w)^\alpha`, where 
 :math:`c(w)` is the unigram count of the word :math:`w`, :math:`\alpha` is 
 a hyper-parameter, and :math:`Z` is the normalization constant. 
 Args: 
 x (~chainer.Variable): Batch of input vectors. 
 t (~chainer.Variable): Vector of groundtruth labels. 
 W (~chainer.Variable): Weight matrix. 
 sampler (function): Sampling function. It takes a shape and returns an 
 integer array of the shape. Each element of this array is a sample 
 from the word distribution. A :class:`~chainer.utils.WalkerAlias` 
 object built with the power distribution of word frequency is 
 recommended. 
 sample_size (int): Number of samples. 
 See: `Distributed Representations of Words and Phrases and their         Compositionality <http://arxiv.org/abs/1310.4546>`_ 
 .. seealso:: :class:`~chainer.links.NegativeSampling`.'"
"@sensitive_post_parameters() 
 @csrf_protect 
 @never_cache 
 def login(request, template_name='registration/login.html', redirect_field_name=REDIRECT_FIELD_NAME, authentication_form=AuthenticationForm, current_app=None, extra_context=None): 
    redirect_to = request.REQUEST.get(redirect_field_name, '') 
   if (request.method == 'POST'): 
      form = authentication_form(data=request.POST) 
      if form.is_valid(): 
         if (not is_safe_url(url=redirect_to, host=request.get_host())): 
            redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL) 
         auth_login(request, form.get_user()) 
         if request.session.test_cookie_worked(): 
            request.session.delete_test_cookie() 
         return HttpResponseRedirect(redirect_to) 
   else: 
      form = authentication_form(request) 
   request.session.set_test_cookie() 
   current_site = get_current_site(request) 
   context = {'form': form, redirect_field_name: redirect_to, 'site': current_site, 'site_name': current_site.name} 
   if (extra_context is not None): 
      context.update(extra_context) 
   return TemplateResponse(request, template_name, context, current_app=current_app)"," 'Log in a user and return a redirect. 
 This view is used to handle the login process. 
 It is called by the ``login_url`` in the urls.py file. 
 The view is protected by csrf_protect and never_cache. 
 :param request: The current request. 
 :param template_name: The template to use. 
 :param redirect_field_name: The name of the POST variable to use for 
 redirecting the user. 
 :param authentication_form: The form to use for authentication. 
 :param current_app: The current application. 
 :param extra_context: Any additional context to add to the template. 
 :return: A redirect response. 
 :rtype: HttpResponseRedirect'",'Displays the login form and handles the login action.'
"def split_title(title, width, title_fs): 
    titles = [] 
   if (not title): 
      return titles 
   size = reverse_text_len(width, (title_fs * 1.1)) 
   title_lines = title.split('\n') 
   for title_line in title_lines: 
      while (len(title_line) > size): 
         title_part = title_line[:size] 
         i = title_part.rfind('   ') 
         if (i == (-1)): 
            i = len(title_part) 
         titles.append(title_part[:i]) 
         title_line = title_line[i:].strip() 
      titles.append(title_line) 
   return titles"," 'Split a title into lines that are no longer than `width` characters. 
 If the title is longer than `width`, the lines will be split at the first 
 space. 
 :param title: The title to split. 
 :param width: The maximum length of each line. 
 :param title_fs: The font size of the title. 
 :returns: A list of titles.'",'Split a string for a specified width and font size'
"def hpsModelSynth(hfreq, hmag, hphase, stocEnv, N, H, fs): 
    yh = SM.sineModelSynth(hfreq, hmag, hphase, N, H, fs) 
   yst = STM.stochasticModelSynth(stocEnv, H, (H * 2)) 
   y = (yh[:min(yh.size, yst.size)] + yst[:min(yh.size, yst.size)]) 
   return (y, yh, yst)"," 'Generate a sine model and a stochastic model. 
 Parameters 
 hfreq : array 
 The frequency of the sine model. 
 hmag : array 
 The magnitude of the sine model. 
 hphase : array 
 The phase of the sine model. 
 stocEnv : array 
 The stochastic model. 
 N : int 
 The number of samples in the sine model. 
 H : int 
 The number of samples in the stochastic model. 
 fs : int 
 The sampling rate. 
 Returns 
 y : array 
 The generated signal. 
 yh : array 
 The sine model. 
 yst : array 
 The stochastic model. 
 Examples 
 >>> from scipy.signal import hpsModelSynth 
 >>> hfreq = [100, 1000, 10000, 100000, 1000000] 
 >>> hmag = [1, 1, 1, 1, 1] 
 >>> hphase = [0, ","'Synthesis of a sound using the harmonic plus stochastic model 
 hfreq, hmag: harmonic frequencies and amplitudes; stocEnv: stochastic envelope 
 Ns: synthesis FFT size; H: hop size, fs: sampling rate 
 returns y: output sound, yh: harmonic component, yst: stochastic component'"
"def to_nnf(expr, simplify=True): 
    if is_nnf(expr, simplify): 
      return expr 
   return expr.to_nnf(simplify)"," 'Return a normal form of the expression. 
 Examples 
 >>> from sympy import to_nnf, sqrt, Symbol 
 >>> to_nnf(sqrt(2*x)) 
 sqrt(2*x) 
 >>> to_nnf(sqrt(Symbol(1))) 
 sqrt(1) 
 >>> to_nnf(sqrt(Symbol(1))) 
 sqrt(1) 
 >>> to_nnf(sqrt(2*x)) 
 sqrt(2*x) 
 >>> to_nnf(sqrt(2*x), simplify=False) 
 sqrt(2*x) 
 >>> to_nnf(sqrt(2*x), simplify=True) 
 sqrt(2*x) 
 >>> to_nnf(sqrt(2*x), simplify=True) 
 sqrt(2*x) 
 >>> to_nnf(sqrt(2*x), simplify=True) 
 sqrt(2*x) 
 >>> to_nnf(sqrt(2*x), simplify=True) 
 sqrt(2*x) 
 >>> to","'Converts expr to Negation Normal Form. 
 A logical expression is in Negation Normal Form (NNF) if it 
 contains only And, Or and Not, and Not is applied only to literals. 
 If simpify is True, the result contains no redundant clauses. 
 Examples 
 >>> from sympy.abc import A, B, C, D 
 >>> from sympy.logic.boolalg import Not, Equivalent, to_nnf 
 >>> to_nnf(Not((~A & ~B) | (C & D))) 
 And(Or(A, B), Or(Not(C), Not(D))) 
 >>> to_nnf(Equivalent(A >> B, B >> A)) 
 And(Or(A, And(A, Not(B)), Not(B)), Or(And(B, Not(A)), B, Not(A)))'"
"def fromstring(*args, **kwargs): 
    global ET 
   _bootstrap() 
   return ET.fromstring(*args, **kwargs)"," 'fromstring(string) -> ElementTree 
 :param string: 
 :type string: str 
 :param kwargs: 
 :type kwargs: dict 
 :return: 
 :rtype: ElementTree'","'Helper func to provide easy access to the (possibly) moving target that is 
 C{ET}.'"
"@pytest.mark.django_db 
 def test_submit_with_suggestion_and_comment(client, request_users, settings): 
    settings.POOTLE_CAPTCHA_ENABLED = False 
   Comment = get_comment_model() 
   unit = Unit.objects.filter(suggestion__state='pending', state=UNTRANSLATED)[0] 
   sugg = Suggestion.objects.filter(unit=unit, state='pending')[0] 
   user = request_users['user'] 
   if (user.username != 'nobody'): 
      client.login(username=user.username, password=request_users['password']) 
   url = ('/xhr/units/%d/' % unit.id) 
   edited_target = ('Edited   %s' % sugg.target_f) 
   comment = 'This   is   a   comment!' 
   response = client.post(url, {'state': False, 'target_f_0': edited_target, 'suggestion': sugg.id, 'comment': comment}, HTTP_X_REQUESTED_WITH='XMLHttpRequest') 
   if check_permission('translate', response.wsgi_request): 
      assert (response.status_code == 200) 
      content = json.loads(response.content) 
      assert (content['newtargets'] == [edited_target]) 
      assert (content['user_score'] == response.wsgi_request.user.public_score) 
      assert (content['checks'] is None) 
      accepted_suggestion = Suggestion.objects.get(id=sugg.id) 
      updated_unit = Unit.objects.get(id=unit.id) 
      assert (accepted_suggestion.state == 'accepted') 
      assert (str(updated_unit.target) == edited_target) 
      assert (Comment.objects.for_model(accepted_suggestion).get().comment == comment) 
   else: 
      assert (response.status_code == 403)", 'Submit a suggestion with comment','Tests translation can be applied after suggestion is accepted.'
"def get_app(services, registry_path=forms.DEFAULT_REGISTRY_PATH, debug=False, config=None): 
    mappings = service_mapping(services, registry_path=registry_path) 
   return webapp2.WSGIApplication(routes=mappings, debug=debug, config=config)", 'Get a WSGI application from a list of services.',"'Returns a WSGI application configured for the given services. 
 Parameters are the same as :func:`service_mapping`, plus: 
 :param debug: 
 WSGI application debug flag: True to enable debug mode. 
 :param config: 
 WSGI application configuration dictionary.'"
"def getMatrixTetragridMatrix(matrixTetragrid, prefix, xmlElement): 
    matrixKey = (prefix + 'matrix') 
   evaluatedDictionary = evaluate.getEvaluatedDictionary([matrixKey], xmlElement) 
   if (len(evaluatedDictionary.keys()) < 1): 
      return matrixTetragrid 
   value = evaluatedDictionary[matrixKey] 
   if ((value == None) or (value == 'None')): 
      print 'Warning,   value   in   getMatrixTetragridMatrix   in   matrix   is   None   for   matrixKey   for   dictionary:' 
      print matrixKey 
      print evaluatedDictionary 
   else: 
      matrixTetragrid = getIdentityMatrixTetragrid(matrixTetragrid) 
      for (rowIndex, row) in enumerate(value): 
         for (elementIndex, element) in enumerate(row): 
            matrixTetragrid[rowIndex][elementIndex] = element 
   euclidean.removeListFromDictionary(xmlElement.attributeDictionary, [matrixKey]) 
   return matrixTetragrid"," 'Returns the matrix of the tetragrid, as a numpy matrix. 
 This is the same as getMatrixTetragridMatrix but returns a numpy matrix 
 instead of a list of lists.'",'Get the matrix Tetragrid from the xmlElement matrix value.'
"def get_feature_permission(request, feature, operation=None): 
    network_config = getattr(settings, 'OPENSTACK_NEUTRON_NETWORK', {}) 
   feature_info = FEATURE_MAP.get(feature) 
   if (not feature_info): 
      raise ValueError(_(""The   requested   feature   '%(feature)s'   is   unknown.   Please   make   sure   to   specify   a   feature   defined   in   FEATURE_MAP."")) 
   feature_config = feature_info.get('config') 
   if feature_config: 
      if (not network_config.get(feature_config['name'], feature_config['default'])): 
         return False 
   feature_policies = feature_info.get('policies') 
   if feature_policies: 
      policy_name = feature_policies.get(operation) 
      if (not policy_name): 
         raise ValueError((_(""The   'operation'   parameter   for   get_feature_permission   '%(feature)s'   is   invalid.   It   should   be   one   of   %(allowed)s"") % {'feature': feature, 'allowed': '   '.join(feature_policies.keys())})) 
      role = (('network', policy_name),) 
      if (not policy.check(role, request)): 
         return False 
   feature_extension = feature_info.get('extension') 
   if feature_extension: 
      try: 
         return is_extension_supported(request, feature_extension) 
      except Exception: 
         msg = (_(""Failed   to   check   Neutron   '%s'   extension   is   not   supported"") % feature_extension) 
         LOG.info(msg) 
         return False 
   return True"," 'Checks if the given operation is allowed for the given feature. 
 If the operation is not allowed, the request is not authorized. 
 :param request: The request object. 
 :param feature: The name of the feature. 
 :param operation: The operation name. 
 :return: True if the operation is allowed, False otherwise.'","'Check if a feature-specific field can be displayed. 
 This method check a permission for a feature-specific field. 
 Such field is usually provided through Neutron extension. 
 :param request: Request Object 
 :param feature: feature name defined in FEATURE_MAP 
 :param operation (optional): Operation type. The valid value should be 
 defined in FEATURE_MAP[feature][\'policies\'] 
 It must be specified if FEATURE_MAP[feature] has \'policies\'.'"
"def _GetTimezone(timezone_string): 
    if (pytz is None): 
      if timezone_string: 
         raise ValueError('need   pytz   in   order   to   specify   a   timezone') 
      return None 
   if timezone_string: 
      return pytz.timezone(timezone_string) 
   else: 
      return pytz.utc", 'Get a pytz timezone object from a string',"'Converts a timezone string to a pytz timezone object. 
 Arguments: 
 timezone_string: a string representing a timezone, or None 
 Returns: 
 a pytz timezone object, or None 
 Raises: 
 ValueError: if timezone_string is specified, but pytz module could not be 
 loaded'"
"def cmp_version(a, b): 
    a = a.split('.') 
   b = b.split('.') 
   for (va, vb) in zip(a, b): 
      ret = (int(va) - int(vb)) 
      if ret: 
         return ret 
   return (len(a) - len(b))"," 'Compare versions. 
 Parameters 
 a : str 
 The first version. 
 b : str 
 The second version. 
 Returns 
 int 
 The result of the comparison.'",'Compare two version strings (eg 0.0.1.10 > 0.0.1.9).'
"def ToScatteredId(v): 
    if (v >= _MAX_SCATTERED_COUNTER): 
      raise datastore_errors.BadArgumentError(('counter   value   too   large   (%d)' % v)) 
   return ((_MAX_SEQUENTIAL_ID + 1) + long(ReverseBitsInt64((v << _SCATTER_SHIFT))))"," 'Convert a counter value to a scattered id. 
 This function is used to convert a counter value to a scattered id, 
 which is a 64-bit integer with the high 32 bits set to the counter value, 
 and the low 32 bits set to the counter offset. 
 :param v: A counter value. 
 :type v: int 
 :return: A scattered id. 
 :rtype: int'","'Map counter value v to the scattered ID space. 
 Translate to scattered ID space, then reverse bits. 
 Args: 
 v: Counter value from which to produce ID. 
 Returns: 
 Integer ID. 
 Raises: 
 datastore_errors.BadArgumentError if counter value exceeds the range of 
 the scattered ID space.'"
"@handle_response_format 
 @treeio_login_required 
 def item_view(request, folderPath, itemPath, response_format='html'): 
    try: 
      item = KnowledgeItem.by_path(folderPath, itemPath) 
   except KnowledgeItem.DoesNotExist: 
      raise Http404 
   if (not item): 
      raise Http404 
   items = Object.filter_permitted(manager=KnowledgeItem.objects, user=request.user.profile, mode='r') 
   if (not request.user.profile.has_permission(item)): 
      return user_denied(request, message=""You   don't   have   access   to   this   Knowledge   Item"") 
   context = _get_default_context(request) 
   context.update({'items': items, 'item': item}) 
   return render_to_response('knowledge/item_view', context, context_instance=RequestContext(request), response_format=response_format)"," 'View a single item. 
 :param request: 
 :param folderPath: 
 :param itemPath: 
 :param response_format: 
 :returns: 
 :rtype: 
 :raises: 
 :version: 0.1'",'Single knowledge item view page'
"def getFirstWord(splitLine): 
    if (len(splitLine) > 0): 
      return splitLine[0] 
   return ''"," 'Get the first word from a line. 
 :param splitLine: A line to split. 
 :return: The first word of the line, or an empty string.'",'Get the first word of a split line.'
"def create_and_check_dir(path): 
    if (not os.path.exists(path)): 
      os.makedirs(path) 
   elif (not os.access(path, os.W_OK)): 
      raise OSError('DATA_DIR   {0}   is   not   writable!'.format(path))", 'Create and check the path to the data directory.','Ensure directory exists and is writable by us'
"def validate_input(trans, error_map, param_values, page_param_map): 
    first = param_values['name1'] 
   second = param_values['name2'] 
   if (first == second): 
      error_map['name1'] = 'The   value   names   should   be   different.'", 'Validate the input to the Transaction.',"'Validates the user input, before execution.'"
"def parse_date(string): 
    return get_i18n().parse_date(string)"," 'Parse a date string. 
 :param string: The date string to parse. 
 :type string: str 
 :return: The parsed date. 
 :rtype: datetime.datetime'",'See :meth:`I18n.parse_date`'
"def test_nonexistent_options_listed_in_order(script, data): 
    result = script.pip('install', '--no-index', ('--find-links=' + data.find_links), 'simplewheel[nonexistent,   nope]', expect_stderr=True) 
   msg = ""      simplewheel   2.0   does   not   provide   the   extra   'nonexistent'\n      simplewheel   2.0   does   not   provide   the   extra   'nope'"" 
   assert (msg in result.stderr)"," 'simplewheel[nonexistent,   nope] should not be installed'",'Warn the user for each extra that doesn\'t exist.'
"def build_dict(): 
    containers = dict([(c, (['all'] + (lxc.Container(c).get_config_item('lxc.group') or []))) for c in lxc.list_containers()]) 
   groups = set(sum([g for g in containers.values()], [])) 
   return dict([(g, {'hosts': [k for (k, v) in containers.items() if (g in v)], 'vars': {'ansible_connection': 'lxc'}}) for g in groups])", 'Generate a dictionary of groups and vars for Ansible',"'Returns a dictionary keyed to the defined LXC groups. All 
 containers, including the ones not in any group, are included in the 
 ""all"" group.'"
"def decryptAndCheckPubkeyPayload(data, address): 
    try: 
      (status, addressVersion, streamNumber, ripe) = decodeAddress(address) 
      readPosition = 20 
      (embeddedAddressVersion, varintLength) = decodeVarint(data[readPosition:(readPosition + 10)]) 
      readPosition += varintLength 
      (embeddedStreamNumber, varintLength) = decodeVarint(data[readPosition:(readPosition + 10)]) 
      readPosition += varintLength 
      storedData = data[20:readPosition] 
      if (addressVersion != embeddedAddressVersion): 
         logger.info('Pubkey   decryption   was   UNsuccessful   due   to   address   version   mismatch.') 
         return 'failed' 
      if (streamNumber != embeddedStreamNumber): 
         logger.info('Pubkey   decryption   was   UNsuccessful   due   to   stream   number   mismatch.') 
         return 'failed' 
      tag = data[readPosition:(readPosition + 32)] 
      readPosition += 32 
      signedData = data[8:readPosition] 
      encryptedData = data[readPosition:] 
      (toAddress, cryptorObject) = shared.neededPubkeys[tag] 
      if (toAddress != address): 
         logger.critical(('decryptAndCheckPubkeyPayload   failed   due   to   toAddress   mismatch.   This   is   very   peculiar.   toAddress:   %s,   address   %s' % (toAddress, address))) 
         return 'failed' 
      try: 
         decryptedData = cryptorObject.decrypt(encryptedData) 
      except: 
         logger.info('Pubkey   decryption   was   unsuccessful.') 
         return 'failed' 
      readPosition = 0 
      bitfieldBehaviors = decryptedData[readPosition:(readPosition + 4)] 
      readPosition += 4 
      publicSigningKey = ('\x04' + decryptedData[readPosition:(readPosition + 64)]) 
      readPosition += 64 
      publicEncryptionKey = ('\x04' + decryptedData[readPosition:(readPosition + 64)]) 
      readPosition += 64 
      (specifiedNonceTrialsPerByte, specifiedNonceTrialsPerByteLength) = decodeVarint(decryptedData[readPosition:(readPosition + 10)]) 
      readPosition += specifiedNonceTrialsPerByteLength 
      (specifiedPayloadLengthExtraBytes, specifiedPayloadLengthExtraBytesLength) = decodeVarint(decryptedData[readPosition:(readPosition + 10)]) 
      readPosition += specifiedPayloadLengthExtraBytesLength 
      storedData += decryptedData[:readPosition] 
      signedData += decryptedData[:readPosition] 
      (signatureLength, signatureLengthLength) = decodeVarint(decryptedData[readPosition:(readPosition + 10)]) 
      readPosition += signatureLengthLength 
      signature = decryptedData[readPosition:(readPosition + signatureLength)] 
      if highlevelcrypto.verify(signedData, signature, hexlify(publicSigningKey)): 
         logger.info('ECDSA   verify   passed   (within   decryptAndCheckPubkeyPayload)') 
      else: 
         logger.info('ECDSA   verify   failed   (within   decryptAndCheckPubkeyPayload)') 
         return 'failed' 
      sha = hashlib.new('sha512') 
      sha.update((publicSigningKey + publicEncryptionKey)) 
      ripeHasher = hashlib.new('ripemd160') 
      ripeHasher.update(sha.digest()) 
      embeddedRipe = ripeHasher.digest() 
      if (embeddedRipe != ripe): 
         logger.info('Pubkey   decryption   was   UNsuccessful   due   to   RIPE   mismatch.') 
         return 'failed' 
      logger.info(('within   decryptAndCheckPubkeyPayload,   addressVersion:   %s,   streamNumber:   %s   \n                                                            ripe   %s\n                                                            publicSigningKey   in   hex:   %s\n                                                            publicEncryptionKey   in   hex:   %s' % (addressVersion, streamNumber, hexlify(ripe), hexlify(publicSigningKey), hexlify(publicEncryptionKey)))) 
      t = (address, addressVersion, storedData, int(time.time()), 'yes') 
      sqlExecute('INSERT   INTO   pubkeys   VALUES   (?,?,?,?,?)', *t) 
      return 'successful' 
   except varintDecodeError as e: 
      logger.info('Pubkey   decryption   was   UNsuccessful   due   to   a   malformed   varint.') 
      return 'failed' 
   except Exception as e: 
      logger.critical(('Pubkey   decryption   was   UNsuccessful   because   of   an   unhandled   exception!   This   is   definitely   a   bug!   \n%s' % traceback.format_exc())) 
      return 'failed'"," 'Decrypts the pubkey payload and checks that the decrypted data matches the 
 public key. 
 The address is decrypted and the public key is verified. 
 :param data: The payload to decrypt. 
 :param address: The address to decrypt. 
 :return: The result of the decryption and verification.'","'Version 4 pubkeys are encrypted. This function is run when we already have the 
 address to which we want to try to send a message. The \'data\' may come either 
 off of the wire or we might have had it already in our inventory when we tried 
 to send a msg to this particular address.'"
"def create_mac_string(num, splitter=u':'): 
    mac = hex(num)[2:] 
   if (mac[(-1)] == u'L'): 
      mac = mac[:(-1)] 
   pad = max((12 - len(mac)), 0) 
   mac = ((u'0' * pad) + mac) 
   mac = splitter.join([mac[x:(x + 2)] for x in range(0, 12, 2)]) 
   mac = mac.upper() 
   return mac"," 'Creates a MAC string from a number. 
 :param num: A number 
 :param splitter: The character to use to split the string 
 :return: A MAC string'","'Return the mac address interpretation of num, 
 in the form eg \'00:11:22:33:AA:BB\'. 
 :param num: a 48-bit integer (eg from uuid.getnode) 
 :param spliiter: a string to join the hex pairs with'"
"def get_rising_items(omit_sr_ids, count=4): 
    all_rising = rising.get_all_rising() 
   candidate_sr_ids = {sr_id for (link, score, sr_id) in all_rising}.difference(omit_sr_ids) 
   link_fullnames = [link for (link, score, sr_id) in all_rising if (sr_id in candidate_sr_ids)] 
   link_fullnames_to_show = random_sample(link_fullnames, count) 
   rising_links = Link._by_fullname(link_fullnames_to_show, return_dict=False, data=True) 
   rising_items = [ExploreItem(TYPE_RISING, 'ris', Subreddit._byID(l.sr_id), l) for l in rising_links] 
   return rising_items"," 'Return a list of ExploreItems for the rising subreddits. 
 :param omit_sr_ids: A list of subreddit IDs to exclude. 
 :param count: The number of items to return. 
 :returns: A list of ExploreItems. 
 :rtype: list(ExploreItem)'",'Get links that are rising right now.'
"def get_datasources(orgname=None, profile='grafana'): 
    if isinstance(profile, string_types): 
      profile = __salt__['config.option'](profile) 
   if orgname: 
      switch_org(orgname, profile) 
   response = requests.get('{0}/api/datasources'.format(profile['grafana_url']), auth=_get_auth(profile), headers=_get_headers(profile), timeout=profile.get('grafana_timeout', 3)) 
   if (response.status_code >= 400): 
      response.raise_for_status() 
   return response.json()"," 'Get all datasources for a Grafana profile. 
 Returns: 
 response: 
 The response from the API.'","'List all datasources in an organisation. 
 orgname 
 Name of the organization. 
 profile 
 Configuration profile used to connect to the Grafana instance. 
 Default is \'grafana\'. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' grafana4.get_datasources <orgname>'"
"def _get_storage_path(path, app_id): 
    (_, _, app_id) = app_id.replace(':', '_').rpartition('~') 
   if (path is None): 
      for path in _generate_storage_paths(app_id): 
         try: 
            os.mkdir(path, 448) 
         except OSError as e: 
            if (e.errno == errno.EEXIST): 
               if ((sys.platform == 'win32') or (((os.stat(path).st_mode & 511) == 448) and os.path.isdir(path))): 
                  return path 
               else: 
                  continue 
            raise 
         else: 
            return path 
   elif (not os.path.exists(path)): 
      os.mkdir(path) 
      return path 
   elif (not os.path.isdir(path)): 
      raise IOError(('the   given   storage   path   %r   is   a   file,   a   directory   was   expected' % path)) 
   else: 
      return path"," 'Returns the absolute path to the given storage path, or None if the 
 path doesn\'t exist or is a file. 
 :param path: The path to the storage directory. 
 :param app_id: The application ID. 
 :return: The absolute path to the storage directory, or None.'",'Returns a path to the directory where stub data can be stored.'
"def url_for(endpoint, **values): 
    appctx = _app_ctx_stack.top 
   reqctx = _request_ctx_stack.top 
   if (appctx is None): 
      raise RuntimeError('Attempted   to   generate   a   URL   without   the   application   context   being   pushed.   This   has   to   be   executed   when   application   context   is   available.') 
   if (reqctx is not None): 
      url_adapter = reqctx.url_adapter 
      blueprint_name = request.blueprint 
      if (not reqctx.request._is_old_module): 
         if (endpoint[:1] == '.'): 
            if (blueprint_name is not None): 
               endpoint = (blueprint_name + endpoint) 
            else: 
               endpoint = endpoint[1:] 
      elif ('.' not in endpoint): 
         if (blueprint_name is not None): 
            endpoint = ((blueprint_name + '.') + endpoint) 
      elif endpoint.startswith('.'): 
         endpoint = endpoint[1:] 
      external = values.pop('_external', False) 
   else: 
      url_adapter = appctx.url_adapter 
      if (url_adapter is None): 
         raise RuntimeError('Application   was   not   able   to   create   a   URL   adapter   for   request   independent   URL   generation.   You   might   be   able   to   fix   this   by   setting   the   SERVER_NAME   config   variable.') 
      external = values.pop('_external', True) 
   anchor = values.pop('_anchor', None) 
   method = values.pop('_method', None) 
   scheme = values.pop('_scheme', None) 
   appctx.app.inject_url_defaults(endpoint, values) 
   if (scheme is not None): 
      if (not external): 
         raise ValueError('When   specifying   _scheme,   _external   must   be   True') 
      url_adapter.url_scheme = scheme 
   try: 
      rv = url_adapter.build(endpoint, values, method=method, force_external=external) 
   except BuildError as error: 
      values['_external'] = external 
      values['_anchor'] = anchor 
      values['_method'] = method 
      return appctx.app.handle_url_build_error(error, endpoint, values) 
   if (anchor is not None): 
      rv += ('#' + url_quote(anchor)) 
   return rv"," 'Generate a URL for the given endpoint. 
 This is a shortcut for app.url_for(endpoint, **values) 
 It is useful when you want to generate a URL and pass 
 some values to the view, but you don\'t want to have to 
 specify the values explicitly. 
 :param endpoint: The endpoint to generate a URL for. 
 :param values: A dictionary of values to pass to the view. 
 :return: A URL for the given endpoint. 
 :rtype: str'","'Generates a URL to the given endpoint with the method provided. 
 Variable arguments that are unknown to the target endpoint are appended 
 to the generated URL as query arguments.  If the value of a query argument 
 is `None`, the whole pair is skipped.  In case blueprints are active 
 you can shortcut references to the same blueprint by prefixing the 
 local endpoint with a dot (``.``). 
 This will reference the index function local to the current blueprint:: 
 url_for(\'.index\') 
 For more information, head over to the :ref:`Quickstart <url-building>`. 
 To integrate applications, :class:`Flask` has a hook to intercept URL build 
 errors through :attr:`Flask.build_error_handler`.  The `url_for` function 
 results in a :exc:`~werkzeug.routing.BuildError` when the current app does 
 not have a URL for the given endpoint and values.  When it does, the 
 :data:`~flask.current_app` calls its :attr:`~Flask.build_error_handler` if 
 it is not `None`, which can return a string to use as the result of 
 `url_for` (instead of `url_for`\'s default to raise the 
 :exc:`~werkzeug.routing.BuildError` exception) or re-raise the exception. 
 An example:: 
 def external_url_handler(error, endpoint, **values): 
 ""Looks up an external URL when `url_for` cannot build a URL."" 
 # This is an example of hooking the build_error_handler. 
 # Here, lookup_url is some utility function you\'ve built 
 # which looks up the endpoint in some external URL registry. 
 url = lookup_url(endpoint, **values) 
 if url is None: 
 # External lookup did not have a URL. 
 # Re-raise the BuildError, in context of original traceback. 
 exc_type, exc_value, tb = sys.exc_info() 
 if exc_value is error: 
 raise exc_type, exc_value, tb 
 else: 
 raise error 
 # url_for will use this result, instead of raising BuildError. 
 return url 
 app.build_error_handler = external_url_handler 
 Here, `error` is the instance of :exc:`~werkzeug.routing.BuildError`, and 
 `endpoint` and `**values` are the arguments passed into `url_for`.  Note 
 that this is for building URLs outside the current application, and not for 
 handling 404 NotFound errors. 
 .. versionadded:: 0.10 
 The `_scheme` parameter was added. 
 .. versionadded:: 0.9 
 The `_anchor` and `_method` parameters were added. 
 .. versionadded:: 0.9 
 Calls :meth:`Flask.handle_build_error` on 
 :exc:`~werkzeug.routing.BuildError`. 
 :param endpoint: the endpoint of the URL (name of the function) 
 :param values: the variable arguments of the URL rule 
 :param _external: if set to `True`, an absolute URL is generated. Server 
 address can be changed via `SERVER_NAME` configuration variable which 
 defaults to `localhost`. 
 :param _scheme: a string specifying the desired URL scheme. The `_external` 
 parameter must be set to `True` or a `ValueError` is raised. 
 :param _anchor: if provided this is added as anchor to the URL. 
 :param _method: if provided this explicitly specifies an HTTP method.'"
"def test_delayed_epochs(): 
    (raw, events, picks) = _get_data() 
   events = events[:10] 
   picks = np.concatenate([pick_types(raw.info, meg=True, eeg=True)[::22], pick_types(raw.info, meg=False, eeg=False, ecg=True, eog=True)]) 
   picks = np.sort(picks) 
   raw.load_data().pick_channels([raw.ch_names[pick] for pick in picks]) 
   raw.info.normalize_proj() 
   del picks 
   n_epochs = 2 
   raw.info['lowpass'] = 40.0 
   for decim in (1, 3): 
      proj_data = Epochs(raw, events, event_id, tmin, tmax, proj=True, reject=reject, decim=decim) 
      use_tmin = proj_data.tmin 
      proj_data = proj_data.get_data() 
      noproj_data = Epochs(raw, events, event_id, tmin, tmax, proj=False, reject=reject, decim=decim).get_data() 
      assert_equal(proj_data.shape, noproj_data.shape) 
      assert_equal(proj_data.shape[0], n_epochs) 
      for preload in (True, False): 
         for proj in (True, False, 'delayed'): 
            for ii in range(3): 
               print (decim, preload, proj, ii) 
               comp = (proj_data if (proj is True) else noproj_data) 
               if (ii in (0, 1)): 
                  epochs = Epochs(raw, events, event_id, tmin, tmax, proj=proj, reject=reject, preload=preload, decim=decim) 
               else: 
                  fake_events = np.zeros((len(comp), 3), int) 
                  fake_events[:, 0] = np.arange(len(comp)) 
                  fake_events[:, 2] = 1 
                  epochs = EpochsArray(comp, raw.info, tmin=use_tmin, event_id=1, events=fake_events, proj=proj) 
                  epochs.info['sfreq'] /= decim 
                  assert_equal(len(epochs), n_epochs) 
               assert_true((raw.proj is False)) 
               assert_true((epochs.proj is (True if (proj is True) else False))) 
               if (ii == 1): 
                  epochs.load_data() 
               picks_data = pick_types(epochs.info, meg=True, eeg=True) 
               evoked = epochs.average(picks=picks_data) 
               assert_equal(evoked.nave, n_epochs, epochs.drop_log) 
               if (proj is True): 
                  evoked.apply_proj() 
               else: 
                  assert_true((evoked.proj is False)) 
               assert_array_equal(evoked.ch_names, np.array(epochs.ch_names)[picks_data]) 
               assert_allclose(evoked.times, epochs.times) 
               epochs_data = epochs.get_data() 
               assert_allclose(evoked.data, epochs_data.mean(axis=0)[picks_data], rtol=1e-05, atol=1e-20) 
               assert_allclose(epochs_data, comp, rtol=1e-05, atol=1e-20)", 'Test delayed epochs with different preloading and projections','Test delayed projection on Epochs.'
"def git_status(path): 
    cmd = (git_cmd_base(path) + ['status', '--porcelain']) 
   return run_subprocess(cmd, stderr=None, universal_newlines=True)[0]", 'Return the status of a git repository.',"'Return a string listing all changes to the working tree in a git 
 repository.'"
"def create_instance(c_instance): 
    return GenericScript(c_instance, Live.MidiMap.MapMode.absolute, Live.MidiMap.MapMode.absolute, DEVICE_CONTROLS, TRANSPORT_CONTROLS, VOLUME_CONTROLS, TRACKARM_CONTROLS, BANK_CONTROLS, CONTROLLER_DESCRIPTIONS)"," 'Create a MIDI script for a generic MIDI instrument. 
 :param c_instance: The MIDI script to create. 
 :type c_instance: :class:`~live.controlsurface.MidiScript` 
 :return: The created MIDI script.'",'The generic script can be customised by using parameters (see config.py).'
"def cloud_query_sinfo(cookie, tokens, source_path): 
    url = ''.join([const.PAN_URL, 'rest/2.0/services/cloud_dl?channel=chunlei&clienttype=0&web=1', '&method=query_sinfo&app_id=250528', '&bdstoken=', tokens['bdstoken'], '&source_path=', encoder.encode_uri_component(source_path), '&type=2', '&t=', util.timestamp()]) 
   req = net.urlopen(url, headers={'Cookie': cookie.header_output()}) 
   if req: 
      content = req.data 
      return json.loads(content.decode()) 
   else: 
      return None", 'Return cloud_dl service info.','source_path - BTç§å­çç»å¯¹è·¯å¾.'
"def build_encoder_bi(tparams, options): 
    embedding = tensor.tensor3('embedding', dtype='float32') 
   embeddingr = embedding[::(-1)] 
   x_mask = tensor.matrix('x_mask', dtype='float32') 
   xr_mask = x_mask[::(-1)] 
   proj = get_layer(options['encoder'])[1](tparams, embedding, options, prefix='encoder', mask=x_mask) 
   projr = get_layer(options['encoder'])[1](tparams, embeddingr, options, prefix='encoder_r', mask=xr_mask) 
   ctx = tensor.concatenate([proj[0][(-1)], projr[0][(-1)]], axis=1) 
   return (embedding, x_mask, ctx)"," 'Build encoder bi-directional layers. 
 Parameters 
 tparams : Theano variable 
 options : dict 
 Returns 
 embedding : Theano variable 
 x_mask : Theano variable 
 ctx : Theano variable'","'build bidirectional encoder, given pre-computed word embeddings'"
"def _minimize_bfgs(fun, x0, args=(), jac=None, callback=None, gtol=1e-05, norm=Inf, eps=_epsilon, maxiter=None, disp=False, return_all=False, **unknown_options): 
    _check_unknown_options(unknown_options) 
   f = fun 
   fprime = jac 
   epsilon = eps 
   retall = return_all 
   x0 = asarray(x0).flatten() 
   if (x0.ndim == 0): 
      x0.shape = (1,) 
   if (maxiter is None): 
      maxiter = (len(x0) * 200) 
   (func_calls, f) = wrap_function(f, args) 
   if (fprime is None): 
      (grad_calls, myfprime) = wrap_function(approx_fprime, (f, epsilon)) 
   else: 
      (grad_calls, myfprime) = wrap_function(fprime, args) 
   gfk = myfprime(x0) 
   k = 0 
   N = len(x0) 
   I = numpy.eye(N, dtype=int) 
   Hk = I 
   old_fval = f(x0) 
   old_old_fval = (old_fval + (np.linalg.norm(gfk) / 2)) 
   xk = x0 
   if retall: 
      allvecs = [x0] 
   sk = [(2 * gtol)] 
   warnflag = 0 
   gnorm = vecnorm(gfk, ord=norm) 
   while ((gnorm > gtol) and (k < maxiter)): 
      pk = (- numpy.dot(Hk, gfk)) 
      try: 
         (alpha_k, fc, gc, old_fval, old_old_fval, gfkp1) = _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval, old_old_fval, amin=1e-100, amax=1e+100) 
      except _LineSearchError: 
         warnflag = 2 
         break 
      xkp1 = (xk + (alpha_k * pk)) 
      if retall: 
         allvecs.append(xkp1) 
      sk = (xkp1 - xk) 
      xk = xkp1 
      if (gfkp1 is None): 
         gfkp1 = myfprime(xkp1) 
      yk = (gfkp1 - gfk) 
      gfk = gfkp1 
      if (callback is not None): 
         callback(xk) 
      k += 1 
      gnorm = vecnorm(gfk, ord=norm) 
      if (gnorm <= gtol): 
         break 
      if (not numpy.isfinite(old_fval)): 
         warnflag = 2 
         break 
      try: 
         rhok = (1.0 / numpy.dot(yk, sk)) 
      except ZeroDivisionError: 
         rhok = 1000.0 
         if disp: 
            print('Divide-by-zero   encountered:   rhok   assumed   large') 
      if isinf(rhok): 
         rhok = 1000.0 
         if disp: 
            print('Divide-by-zero   encountered:   rhok   assumed   large') 
      A1 = (I - ((sk[:, numpy.newaxis] * yk[numpy.newaxis, :]) * rhok)) 
      A2 = (I - ((yk[:, numpy.newaxis] * sk[numpy.newaxis, :]) * rhok)) 
      Hk = (numpy.dot(A1, numpy.dot(Hk, A2)) + ((rhok * sk[:, numpy.newaxis]) * sk[numpy.newaxis, :])) 
   fval = old_fval 
   if np.isnan(fval): 
      warnflag = 2 
   if (warnflag == 2): 
      msg = _status_message['pr_loss'] 
      if disp: 
         print(('Warning:   ' + msg)) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   elif (k >= maxiter): 
      warnflag = 1 
      msg = _status_message['maxiter'] 
      if disp: 
         print(('Warning:   ' + msg)) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   else: 
      msg = _status_message['success'] 
      if disp: 
         print(msg) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   result = OptimizeResult(fun=fval, jac=gfk, hess_inv=Hk, nfev=func_calls[0], njev=grad_calls[0], status=warnflag, success=(warnflag == 0), message=msg, x=xk, nit=k) 
   if retall: 
      result['allvecs'] = allvecs 
   return result"," 'Minimize the objective function using the Broyden-Fletcher-Goldfarb-Shanno 
 algorithm. 
 Parameters 
 fun : callable, 
 The objective function. 
 args : tuple of arguments, 
 The arguments to the objective function. 
 jac : callable, 
 The Jacobian of the objective function. 
 callback : callable, 
 A callback function that is called at each iteration. 
 gtol : float, 
 The tolerance for the line search. 
 norm : float, 
 The norm to use for the line search. 
 eps : float, 
 The tolerance for the line search. 
 maxiter : int, 
 The maximum number of iterations. 
 disp : bool, 
 Whether to print the status of the optimization. 
 return_all : bool, 
 Whether to return all the vectors in the minimization. 
 **unknown_options : dict, 
 Any additional options. 
 Returns 
 OptimizeResult 
 The result of the optimization.'","'Minimization of scalar function of one or more variables using the 
 BFGS algorithm. 
 Options 
 disp : bool 
 Set to True to print convergence messages. 
 maxiter : int 
 Maximum number of iterations to perform. 
 gtol : float 
 Gradient norm must be less than `gtol` before successful 
 termination. 
 norm : float 
 Order of norm (Inf is max, -Inf is min). 
 eps : float or ndarray 
 If `jac` is approximated, use this value for the step size.'"
"def terminal_action(parent, fn): 
    action = cmd_action(parent, cmds.LaunchTerminal, (lambda : utils.select_directory(fn())), hotkeys.TERMINAL) 
   return action", 'Create a terminal action','Launch a terminal -> QAction'
"def present(name, DomainName, ElasticsearchClusterConfig=None, EBSOptions=None, AccessPolicies=None, SnapshotOptions=None, AdvancedOptions=None, Tags=None, region=None, key=None, keyid=None, profile=None, ElasticsearchVersion='1.5'): 
    ret = {'name': DomainName, 'result': True, 'comment': '', 'changes': {}} 
   if (ElasticsearchClusterConfig is None): 
      ElasticsearchClusterConfig = {'DedicatedMasterEnabled': False, 'InstanceCount': 1, 'InstanceType': 'm3.medium.elasticsearch', 'ZoneAwarenessEnabled': False} 
   if (EBSOptions is None): 
      EBSOptions = {'EBSEnabled': False} 
   if (SnapshotOptions is None): 
      SnapshotOptions = {'AutomatedSnapshotStartHour': 0} 
   if (AdvancedOptions is None): 
      AdvancedOptions = {'rest.action.multi.allow_explicit_index': 'true'} 
   if (Tags is None): 
      Tags = {} 
   if ((AccessPolicies is not None) and isinstance(AccessPolicies, six.string_types)): 
      try: 
         AccessPolicies = json.loads(AccessPolicies) 
      except ValueError as e: 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   create   domain:   {0}.'.format(e.message) 
         return ret 
   r = __salt__['boto_elasticsearch_domain.exists'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile) 
   if ('error' in r): 
      ret['result'] = False 
      ret['comment'] = 'Failed   to   create   domain:   {0}.'.format(r['error']['message']) 
      return ret 
   if (not r.get('exists')): 
      if __opts__['test']: 
         ret['comment'] = 'Domain   {0}   is   set   to   be   created.'.format(DomainName) 
         ret['result'] = None 
         return ret 
      r = __salt__['boto_elasticsearch_domain.create'](DomainName=DomainName, ElasticsearchClusterConfig=ElasticsearchClusterConfig, EBSOptions=EBSOptions, AccessPolicies=AccessPolicies, SnapshotOptions=SnapshotOptions, AdvancedOptions=AdvancedOptions, ElasticsearchVersion=str(ElasticsearchVersion), region=region, key=key, keyid=keyid, profile=profile) 
      if (not r.get('created')): 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   create   domain:   {0}.'.format(r['error']['message']) 
         return ret 
      _describe = __salt__['boto_elasticsearch_domain.describe'](DomainName, region=region, key=key, keyid=keyid, profile=profile) 
      ret['changes']['old'] = {'domain': None} 
      ret['changes']['new'] = _describe 
      ret['comment'] = 'Domain   {0}   created.'.format(DomainName) 
      return ret 
   ret['comment'] = os.linesep.join([ret['comment'], 'Domain   {0}   is   present.'.format(DomainName)]) 
   ret['changes'] = {} 
   _status = __salt__['boto_elasticsearch_domain.status'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile)['domain'] 
   if (_status.get('ElasticsearchVersion') != str(ElasticsearchVersion)): 
      ret['result'] = False 
      ret['comment'] = 'Failed   to   update   domain:   version   cannot   be   modified   from   {0}   to   {1}.'.format(_status.get('ElasticsearchVersion'), str(ElasticsearchVersion)) 
      return ret 
   _describe = __salt__['boto_elasticsearch_domain.describe'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile)['domain'] 
   _describe['AccessPolicies'] = json.loads(_describe['AccessPolicies']) 
   if (not _describe.get('EBSOptions', {}).get('EBSEnabled')): 
      opts = _describe.get('EBSOptions', {}) 
      opts.pop('VolumeSize', None) 
      opts.pop('VolumeType', None) 
   comm_args = {} 
   need_update = False 
   es_opts = {'ElasticsearchClusterConfig': ElasticsearchClusterConfig, 'EBSOptions': EBSOptions, 'AccessPolicies': AccessPolicies, 'SnapshotOptions': SnapshotOptions, 'AdvancedOptions': AdvancedOptions} 
   for (k, v) in six.iteritems(es_opts): 
      if (not _compare_json(v, _describe[k])): 
         need_update = True 
         comm_args[k] = v 
         ret['changes'].setdefault('new', {})[k] = v 
         ret['changes'].setdefault('old', {})[k] = _describe[k] 
   if need_update: 
      if __opts__['test']: 
         msg = 'Domain   {0}   set   to   be   modified.'.format(DomainName) 
         ret['comment'] = msg 
         ret['result'] = None 
         return ret 
      ret['comment'] = os.linesep.join([ret['comment'], 'Domain   to   be   modified']) 
      r = __salt__['boto_elasticsearch_domain.update'](DomainName=DomainName, region=region, key=key, keyid=keyid, profile=profile, **comm_args) 
      if (not r.get('updated')): 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   update   domain:   {0}.'.format(r['error']) 
         ret['changes'] = {} 
         return ret 
   return ret"," 'Create a new Elasticsearch domain. 
 :param name: Name of the domain. 
 :param DomainName: Name of the domain. 
 :param ElasticsearchClusterConfig: Configuration for Elasticsearch cluster. 
 :param EBSOptions: Configuration for Elasticsearch EBS. 
 :param AccessPolicies: Access policies for the domain. 
 :param SnapshotOptions: Configuration for Elasticsearch snapshots. 
 :param AdvancedOptions: Configuration for Elasticsearch advanced options. 
 :param Tags: Tags for the domain. 
 :param region: Region where the domain is created. 
 :param key: AWS key. 
 :param keyid: AWS key ID. 
 :param profile: AWS profile. 
 :param ElasticsearchVersion: Elasticsearch version. 
 :returns: Dictionary with status and comment.'","'Ensure domain exists. 
 name 
 The name of the state definition 
 DomainName 
 Name of the domain. 
 ElasticsearchClusterConfig 
 Configuration options for an Elasticsearch domain. Specifies the 
 instance type and number of instances in the domain cluster. 
 InstanceType (string) -- 
 The instance type for an Elasticsearch cluster. 
 InstanceCount (integer) -- 
 The number of instances in the specified domain cluster. 
 DedicatedMasterEnabled (boolean) -- 
 A boolean value to indicate whether a dedicated master node is enabled. 
 See About Dedicated Master Nodes for more information. 
 ZoneAwarenessEnabled (boolean) -- 
 A boolean value to indicate whether zone awareness is enabled. See About 
 Zone Awareness for more information. 
 DedicatedMasterType (string) -- 
 The instance type for a dedicated master node. 
 DedicatedMasterCount (integer) -- 
 Total number of dedicated master nodes, active and on standby, for the 
 cluster. 
 EBSOptions 
 Options to enable, disable and specify the type and size of EBS storage 
 volumes. 
 EBSEnabled (boolean) -- 
 Specifies whether EBS-based storage is enabled. 
 VolumeType (string) -- 
 Specifies the volume type for EBS-based storage. 
 VolumeSize (integer) -- 
 Integer to specify the size of an EBS volume. 
 Iops (integer) -- 
 Specifies the IOPD for a Provisioned IOPS EBS volume (SSD). 
 AccessPolicies 
 IAM access policy 
 SnapshotOptions 
 Option to set time, in UTC format, of the daily automated snapshot. 
 Default value is 0 hours. 
 AutomatedSnapshotStartHour (integer) -- 
 Specifies the time, in UTC format, when the service takes a daily 
 automated snapshot of the specified Elasticsearch domain. Default value 
 is 0 hours. 
 AdvancedOptions 
 Option to allow references to indices in an HTTP request body. Must be 
 false when configuring access to individual sub-resources. By default, 
 the value is true . 
 region 
 Region to connect to. 
 key 
 Secret key to be used. 
 keyid 
 Access key to be used. 
 profile 
 A dict with region, key and keyid, or a pillar key (string) that 
 contains a dict with region, key and keyid. 
 ElasticsearchVersion 
 String of format X.Y to specify version for the Elasticsearch domain eg. 
 ""1.5"" or ""2.3"".'"
"def locate_prefix_by_name(ctx, name): 
    if (name == ROOT_ENV_NAME): 
      return ctx.root_dir 
   for envs_dir in chain((ctx.envs_dirs + (os.getcwd(),))): 
      prefix = join(envs_dir, name) 
      if isdir(prefix): 
         return prefix 
   raise CondaEnvironmentNotFoundError(name)"," 'Locate a prefix directory for an environment name. 
 This function looks for a prefix directory named ``name`` in the 
 ``envs_dirs`` list of the context, and then in the current working 
 directory. 
 :param ctx: A conda context. 
 :param name: The environment name to locate. 
 :returns: The prefix directory. 
 :raises CondaEnvironmentNotFoundError: if the prefix is not found.'","'Find the location of a prefix given a conda env name. 
 Args: 
 ctx (Context): the context object 
 name (str): the name of prefix to find 
 Returns: 
 str: the location of the prefix found, or CondaValueError will raise if not found 
 Raises: 
 CondaValueError: when no prefix is found'"
"def split(line): 
    if (not line.strip()): 
      raise exceptions.MpdNoCommand(u'No   command   given') 
   match = WORD_RE.match(line) 
   if (not match): 
      raise exceptions.MpdUnknownError(u'Invalid   word   character') 
   (whitespace, command, remainder) = match.groups() 
   if whitespace: 
      raise exceptions.MpdUnknownError(u'Letter   expected') 
   result = [command] 
   while remainder: 
      match = PARAM_RE.match(remainder) 
      if (not match): 
         msg = _determine_error_message(remainder) 
         raise exceptions.MpdArgError(msg, command=command) 
      (unquoted, quoted, remainder) = match.groups() 
      result.append((unquoted or UNESCAPE_RE.sub(u'\\g<1>', quoted))) 
   return result"," 'Split a line into a command and its arguments. 
 The command is the first word in the line, and the arguments are the 
 remaining words. 
 If the line contains whitespace, an error is raised. 
 If the line does not contain a command, an error is raised. 
 If the line contains an invalid command, an error is raised. 
 If the line contains an invalid argument, an error is raised.'","'Splits a line into tokens using same rules as MPD. 
 - Lines may not start with whitespace 
 - Tokens are split by arbitrary amount of spaces or tabs 
 - First token must match `[a-z][a-z0-9_]*` 
 - Remaining tokens can be unquoted or quoted tokens. 
 - Unquoted tokens consist of all printable characters except double quotes, 
 single quotes, spaces and tabs. 
 - Quoted tokens are surrounded by a matching pair of double quotes. 
 - The closing quote must be followed by space, tab or end of line. 
 - Any value is allowed inside a quoted token. Including double quotes, 
 assuming it is correctly escaped. 
 - Backslash inside a quoted token is used to escape the following 
 character. 
 For examples see the tests for this function.'"
"def number_aware_alphabetical_cmp(str1, str2): 
    def flatten_tokens(tokens): 
      l = [] 
      for token in tokens: 
         if isinstance(token, str): 
            for char in token: 
               l.append(char) 
         else: 
            assert isinstance(token, float) 
            l.append(token) 
      return l 
   seq1 = flatten_tokens(tokenize_by_number(str1)) 
   seq2 = flatten_tokens(tokenize_by_number(str2)) 
   l = min(len(seq1), len(seq2)) 
   i = 0 
   while (i < l): 
      if (isinstance(seq1[i], float) and isinstance(seq2[i], string_types)): 
         return (-1) 
      elif (isinstance(seq1[i], string_types) and isinstance(seq2[i], float)): 
         return 1 
      elif (seq1[i] < seq2[i]): 
         return (-1) 
      elif (seq1[i] > seq2[i]): 
         return 1 
      i += 1 
   if (len(seq1) < len(seq2)): 
      return (-1) 
   elif (len(seq1) > len(seq2)): 
      return 1 
   return 0"," 'Compares two strings by their number-aware alphabetic order. 
 This is the same as alphabetic order, except that 
 a float is considered to be less than a string. 
 This is used to sort the tokens returned by tokenize_by_number().'","'cmp function for sorting a list of strings by alphabetical 
 order, but with numbers sorted numerically, i.e. `foo1, 
 foo2, foo10, foo11` instead of `foo1, foo10, foo11, foo2`. 
 Parameters 
 str1 : str 
 WRITEME 
 str2 : str 
 WRITEME 
 Returns 
 WRITEME'"
"def get_all_collections(bus): 
    service_obj = bus_get_object(bus, SS_PATH) 
   service_props_iface = dbus.Interface(service_obj, dbus.PROPERTIES_IFACE) 
   for collection_path in service_props_iface.Get(SERVICE_IFACE, 'Collections', signature='ss'): 
      (yield Collection(bus, collection_path))", 'Get all collections from the system.','Returns a generator of all available collections.'
"def kvToSeq(data, strict=False): 
    def err(msg): 
      formatted = ('kvToSeq   warning:   %s:   %r' % (msg, data)) 
      if strict: 
         raise KVFormError(formatted) 
      else: 
         oidutil.log(formatted) 
   lines = data.split('\n') 
   if lines[(-1)]: 
      err('Does   not   end   in   a   newline') 
   else: 
      del lines[(-1)] 
   pairs = [] 
   line_num = 0 
   for line in lines: 
      line_num += 1 
      if (not line.strip()): 
         continue 
      pair = line.split(':', 1) 
      if (len(pair) == 2): 
         (k, v) = pair 
         k_s = k.strip() 
         if (k_s != k): 
            fmt = 'In   line   %d,   ignoring   leading   or   trailing   whitespace   in   key   %r' 
            err((fmt % (line_num, k))) 
         if (not k_s): 
            err(('In   line   %d,   got   empty   key' % (line_num,))) 
         v_s = v.strip() 
         if (v_s != v): 
            fmt = 'In   line   %d,   ignoring   leading   or   trailing   whitespace   in   value   %r' 
            err((fmt % (line_num, v))) 
         pairs.append((k_s.decode('UTF8'), v_s.decode('UTF8'))) 
      else: 
         err(('Line   %d   does   not   contain   a   colon' % line_num)) 
   return pairs"," 'Converts a key-value pair list to a sequence of (key, value) tuples. 
 The key and value are encoded in UTF8. 
 The key and value are not checked for validity. 
 The key and value are not checked for equality. 
 The key and value are not checked for length. 
 The key and value are not checked for leading or trailing whitespace. 
 The key and value are not checked for leading or trailing non-ASCII characters. 
 The key and value are not checked for leading or trailing non-UTF8 characters. 
 The key and value are not checked for leading or trailing non-ASCII whitespace. 
 The key and value are not checked for leading or trailing non-UTF8 whitespace. 
 The key and value are not checked for leading or trailing non-ASCII non-whitespace. 
 The key and value are not checked for leading or trailing non-UTF8 non-whitespace. 
 The key and value are not checked for leading or trailing non-ASCII non-whitespace. 
 The key and value are not checked for leading or trailing non-UTF8 non-whitespace. 
","'After one parse, seqToKV and kvToSeq are inverses, with no warnings:: 
 seq = kvToSeq(s) 
 seqToKV(kvToSeq(seq)) == seq'"
"def detect(source): 
    return (SIGNATURE in source)"," 'Returns True if source contains a signature, False otherwise.'",'Detects MyObfuscate.com packer.'
"def test__FileIO___reduce__(): 
    pass", 'Test that __reduce__ works correctly for FileIO.','TODO'
"def is_valid_csv(parser, file_name, row_limit): 
    row_count = 0 
   for row in csv.reader(open(file_name)): 
      row_count += 1 
   if (row_limit > row_count): 
      parser.error(""The   'row_count'   of   '{}'   is   >   the   number   of   rows   in   '{}'!"".format(row_limit, file_name)) 
      sys.exit(1)"," 'Check if the row_limit is greater than the number of rows in the file. 
 :param parser: The parser object. 
 :param file_name: The file name. 
 :param row_limit: The row limit. 
 :return: True if the row_limit is greater than the number of rows in the file. 
 :rtype: bool'","'Ensure that the # of rows in the input_file 
 is greater than the row_limit.'"
"def parse_certificate_issuer_id(id): 
    return parse_object_id('certificates/issuers', id)"," 'Parse certificate issuer id. 
 :param id: the id of the certificate issuer 
 :type id: str 
 :return: the certificate issuer object 
 :rtype: CertificateIssuer'","':param id: The resource collection type. 
 :type id: str 
 :rtype: KeyVaultId'"
"def _parse_qsub_job_id(qsub_out): 
    return int(qsub_out.split()[2])", 'Parse a qsub output to get the job id.',"'Parse job id from qsub output string. 
 Assume format: 
 ""Your job <job_id> (""<job_name>"") has been submitted""'"
"def not_found(*args, **kwargs): 
    raise falcon.HTTPNotFound()", 'Raise a 404 Not Found error.','Redirects request handling to the not found render'
"def strip_esc_sequence(txt): 
    if isinstance(txt, six.string_types): 
      return txt.replace('\x1b', '?') 
   else: 
      return txt", 'Strip \x1b (ESC) sequences from a string.',"'Replace ESC (ASCII 27/Oct 33) to prevent unsafe strings 
 from writing their own terminal manipulation commands'"
"def compute_use_defs(blocks): 
    var_use_map = {} 
   var_def_map = {} 
   for (offset, ir_block) in blocks.items(): 
      var_use_map[offset] = use_set = set() 
      var_def_map[offset] = def_set = set() 
      for stmt in ir_block.body: 
         if isinstance(stmt, ir.Assign): 
            if isinstance(stmt.value, ir.Inst): 
               rhs_set = set((var.name for var in stmt.value.list_vars())) 
            elif isinstance(stmt.value, ir.Var): 
               rhs_set = set([stmt.value.name]) 
            elif isinstance(stmt.value, (ir.Arg, ir.Const, ir.Global, ir.FreeVar)): 
               rhs_set = () 
            else: 
               raise AssertionError('unreachable', type(stmt.value)) 
            if (stmt.target.name not in rhs_set): 
               def_set.add(stmt.target.name) 
         for var in stmt.list_vars(): 
            if (var.name not in def_set): 
               use_set.add(var.name) 
   return _use_defs_result(usemap=var_use_map, defmap=var_def_map)"," 'Compute use and def sets for each block. 
 :param blocks: A dictionary of block offsets to IR blocks. 
 :return: A tuple of use and def sets. 
 :rtype: tuple(set, set)'",'Find variable use/def per block.'
"@sensitive_post_parameters() 
 @never_cache 
 def password_reset_confirm(request, uidb64=None, token=None, template_name='registration/password_reset_confirm.html', token_generator=default_token_generator, set_password_form=SetPasswordForm, post_reset_redirect=None, current_app=None, extra_context=None): 
    UserModel = get_user_model() 
   assert ((uidb64 is not None) and (token is not None)) 
   if (post_reset_redirect is None): 
      post_reset_redirect = reverse('password_reset_complete') 
   else: 
      post_reset_redirect = resolve_url(post_reset_redirect) 
   try: 
      uid = urlsafe_base64_decode(uidb64) 
      user = UserModel._default_manager.get(pk=uid) 
   except (TypeError, ValueError, OverflowError, UserModel.DoesNotExist): 
      user = None 
   if ((user is not None) and token_generator.check_token(user, token)): 
      validlink = True 
      if (request.method == 'POST'): 
         form = set_password_form(user, request.POST) 
         if form.is_valid(): 
            form.save() 
            return HttpResponseRedirect(post_reset_redirect) 
      else: 
         form = set_password_form(None) 
   else: 
      validlink = False 
      form = None 
   context = {'form': form, 'validlink': validlink} 
   if (extra_context is not None): 
      context.update(extra_context) 
   return TemplateResponse(request, template_name, context, current_app=current_app)"," 'The password reset confirmation view. 
 This view is used to confirm the user\'s new password. 
 If the user\'s new password is correct, the user will be 
 redirected to the post_reset_redirect URL. 
 :param uidb64: The user id as a base64 encoded string 
 :type uidb64: str 
 :param token: The token generated by the token generator 
 :type token: str 
 :param template_name: The template name to use for the view 
 :type template_name: str 
 :param token_generator: The token generator to use 
 :type token_generator: TokenGenerator 
 :param set_password_form: The form to use for the view 
 :type set_password_form: SetPasswordForm 
 :param post_reset_redirect: The URL to redirect to after a successful 
 password reset 
 :type post_reset_redirect: str 
 :param current_app: The current application 
 :type current_app: django.contrib.auth.models.User 
 :param extra_context: Extra context to pass to","'View that checks the hash in a password reset link and presents a 
 form for entering a new password.'"
"def _delAccountRights(sidObject, user_right): 
    try: 
      _polHandle = win32security.LsaOpenPolicy(None, win32security.POLICY_ALL_ACCESS) 
      user_rights_list = [user_right] 
      _ret = win32security.LsaRemoveAccountRights(_polHandle, sidObject, False, user_rights_list) 
      return True 
   except Exception as e: 
      log.error('Error   attempting   to   delete   account   right,   exception   was   {0}'.format(e)) 
      return False"," 'Delete a user account right for a given SID 
 :param sidObject: The SID of the user to delete the right from 
 :type sidObject: str 
 :param user_right: The name of the user right to delete 
 :type user_right: str 
 :return: True if the operation was successful, False otherwise'",'helper function to remove an account right from a user'
"def filter_non_model_columns(data, model): 
    columns = [c.name for c in model.__table__.columns] 
   return dict(((k, v) for (k, v) in six.iteritems(data) if ((k in columns) or isinstance(getattr(model, k, None), associationproxy.AssociationProxy))))"," 'Returns a dict of all the model columns and their values, 
 removing any columns not present in the model.'","'Return the attributes from data which are model columns. 
 Return a new dict with items from data that whose keys are columns in 
 the model or are association proxies of the model.'"
"@gen.coroutine 
 def ShareExisting(client, obj_store, user_id, device_id, request): 
    request['user_id'] = user_id 
   (yield Activity.VerifyActivityId(client, user_id, device_id, request['activity']['activity_id'])) 
   num_photos = 0 
   for ep_dict in request['episodes']: 
      (yield Episode.VerifyEpisodeId(client, user_id, device_id, ep_dict['new_episode_id'])) 
      num_photos += len(ep_dict['photo_ids']) 
   (yield gen.Task(Operation.CreateAndExecute, client, user_id, device_id, 'ShareExistingOperation.Execute', request)) 
   logging.info(('SHARE   EXISTING:   user:   %d,   device:   %d,   viewpoint:   %s,   %d   episodes,   %d   photos' % (user_id, device_id, request['viewpoint_id'], len(request['episodes']), num_photos))) 
   raise gen.Return({})", 'Share existing photos from the given user\'s device.','Shares photos from existing episodes with the followers of an existing viewpoint.'
"@log_call 
 def metadef_resource_type_create(context, values): 
    global DATA 
   resource_type_values = copy.deepcopy(values) 
   resource_type_name = resource_type_values['name'] 
   allowed_attrubites = ['name', 'protected'] 
   for resource_type in DATA['metadef_resource_types']: 
      if (resource_type['name'] == resource_type_name): 
         raise exception.Duplicate() 
   incorrect_keys = (set(resource_type_values.keys()) - set(allowed_attrubites)) 
   if incorrect_keys: 
      raise exception.Invalid(('The   keys   %s   are   not   valid' % str(incorrect_keys))) 
   resource_type = _format_resource_type(resource_type_values) 
   DATA['metadef_resource_types'].append(resource_type) 
   return resource_type", 'Create a new resource type in the metadata.','Create a metadef resource type'
"def __virtual__(): 
    if (get_configured_provider() is False): 
      return False 
   if (get_dependencies() is False): 
      return False 
   return __virtualname__", 'Returns true if the module is available','Check for aliyun configurations'
"def publish_string(source, source_path=None, destination_path=None, reader=None, reader_name='standalone', parser=None, parser_name='restructuredtext', writer=None, writer_name='pseudoxml', settings=None, settings_spec=None, settings_overrides=None, config_section=None, enable_exit_status=False): 
    (output, pub) = publish_programmatically(source_class=io.StringInput, source=source, source_path=source_path, destination_class=io.StringOutput, destination=None, destination_path=destination_path, reader=reader, reader_name=reader_name, parser=parser, parser_name=parser_name, writer=writer, writer_name=writer_name, settings=settings, settings_spec=settings_spec, settings_overrides=settings_overrides, config_section=config_section, enable_exit_status=enable_exit_status) 
   return output"," 'Publish a string to a string output. 
 :param source: The source string to publish. 
 :param source_path: The path to the source string. 
 :param destination_path: The path to the destination string. 
 :param reader: The reader to use for the source string. 
 :param reader_name: The name of the reader to use for the source string. 
 :param parser: The parser to use for the source string. 
 :param parser_name: The name of the parser to use for the source string. 
 :param writer: The writer to use for the destination string. 
 :param writer_name: The name of the writer to use for the destination string. 
 :param settings: The settings to use for the source string. 
 :param settings_spec: The settings specification to use for the source string. 
 :param settings_overrides: The settings overrides to use for the source string. 
 :param config_section: The config section to use for the source string. 
 :param enable_exit_status: If True, the exit status will be set to the exit status of the publisher.","'Set up & run a `Publisher` for programmatic use with string I/O.  Return 
 the encoded string or Unicode string output. 
 For encoded string output, be sure to set the \'output_encoding\' setting to 
 the desired encoding.  Set it to \'unicode\' for unencoded Unicode string 
 output.  Here\'s one way:: 
 publish_string(..., settings_overrides={\'output_encoding\': \'unicode\'}) 
 Similarly for Unicode string input (`source`):: 
 publish_string(..., settings_overrides={\'input_encoding\': \'unicode\'}) 
 Parameters: see `publish_programmatically`.'"
"def Synchronized(f): 
    @functools.wraps(f) 
   def NewFunction(self, *args, **kw): 
      with self.lock: 
         return f(self, *args, **kw) 
   return NewFunction"," 'A decorator that wraps a function with a lock object. 
 The lock object is acquired before the function is called and released 
 after the function returns. 
 This is used to ensure that a function is only called once, and that 
 it is called with a consistent state. 
 Examples: 
 >>> class Foo(object): 
 ...     def __init__(self): 
 ...             self.lock = threading.Lock() 
 ...     @Synchronized 
 ...     def bar(self): 
 ...             self.lock.acquire() 
 ...             print(""bar"", self.lock.locked()) 
 ...             self.lock.release() 
 >>> foo = Foo() 
 >>> foo.bar() 
 bar True 
 >>> foo.bar() 
 bar True 
 >>> foo.bar() 
 bar False 
 >>> foo.bar() 
 bar False 
 >>> foo.bar() 
 bar True 
 >>> foo.bar() 
 bar True 
 >>> foo.bar() 
 bar False 
 >>> foo.bar() 
 bar False 
 >>> foo.bar() 
 bar False",'Synchronization decorator.'
"def get_id(opts, cache_minion_id=False): 
    if (opts['root_dir'] is None): 
      root_dir = salt.syspaths.ROOT_DIR 
   else: 
      root_dir = opts['root_dir'] 
   config_dir = salt.syspaths.CONFIG_DIR 
   if config_dir.startswith(salt.syspaths.ROOT_DIR): 
      config_dir = config_dir.split(salt.syspaths.ROOT_DIR, 1)[(-1)] 
   id_cache = os.path.join(root_dir, config_dir.lstrip(os.path.sep), 'minion_id') 
   if opts.get('minion_id_caching', True): 
      try: 
         with salt.utils.fopen(id_cache) as idf: 
            name = idf.readline().strip() 
            bname = salt.utils.to_bytes(name) 
            if bname.startswith(codecs.BOM): 
               name = salt.utils.to_str(bname.replace(codecs.BOM, '', 1)) 
         if (name and (name != 'localhost')): 
            log.debug('Using   cached   minion   ID   from   {0}:   {1}'.format(id_cache, name)) 
            return (name, False) 
      except (IOError, OSError): 
         pass 
   if (('__role' in opts) and (opts.get('__role') == 'minion')): 
      log.debug('Guessing   ID.   The   id   can   be   explicitly   set   in   {0}'.format(os.path.join(salt.syspaths.CONFIG_DIR, 'minion'))) 
   newid = salt.utils.network.generate_minion_id() 
   if (('__role' in opts) and (opts.get('__role') == 'minion')): 
      log.debug('Found   minion   id   from   generate_minion_id():   {0}'.format(newid)) 
   if (cache_minion_id and opts.get('minion_id_caching', True)): 
      _cache_id(newid, id_cache) 
   is_ipv4 = salt.utils.network.is_ipv4(newid) 
   return (newid, is_ipv4)"," 'Return the minion id and is_ipv4 
 :param opts: Salt configuration 
 :param cache_minion_id: If True, cache the minion id 
 :return: minion id and is_ipv4'","'Guess the id of the minion. 
 If CONFIG_DIR/minion_id exists, use the cached minion ID from that file. 
 If no minion id is configured, use multiple sources to find a FQDN. 
 If no FQDN is found you may get an ip address. 
 Returns two values: the detected ID, and a boolean value noting whether or 
 not an IP address is being used for the ID.'"
"def deg(r): 
    return ((r / pi) * 180)", 'Returns the degree of the angle in radians.','Return the degree value for the given radians (pi = 180 degrees).'
"def parsePWDResponse(response): 
    match = re.search('""(.*)""', response) 
   if match: 
      return match.groups()[0] 
   else: 
      return None"," 'Parse the response of the PWD command. 
 :param response: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :param response: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :param response: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :param response: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :param response: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :param response: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :param response: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :param response: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :param response: 
 :return: 
 :","'Returns the path from a response to a PWD command. 
 Responses typically look like:: 
 257 ""/home/andrew"" is current directory. 
 For this example, I will return C{\'/home/andrew\'}. 
 If I can\'t find the path, I return C{None}.'"
"def cross_entropy_seq(logits, target_seqs, batch_size=1, num_steps=None): 
    loss = tf.nn.seq2seq.sequence_loss_by_example([logits], [tf.reshape(target_seqs, [(-1)])], [tf.ones([(batch_size * num_steps)])]) 
   cost = (tf.reduce_sum(loss) / batch_size) 
   return cost"," 'Cross entropy loss for sequence prediction. 
 Parameters 
 logits : tf.Tensor 
 Predicted logits. 
 target_seqs : tf.Tensor 
 Target sequence. 
 batch_size : int, optional 
 Number of examples in a batch. 
 num_steps : int, optional 
 Number of steps in a sequence. 
 Returns 
 cost : tf.Tensor 
 Cross entropy loss. 
 Notes 
 The loss is calculated for each example in a batch, and the batch_size is 
 used to calculate the loss for each example. 
 The loss is calculated for each example in a sequence, and the num_steps 
 is used to calculate the loss for each example. 
 Examples 
 >>> logits = tf.constant([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], name=""logits"") 
 >>> target_seqs = tf.constant([[1, 2, 3], [4, 5, 6]], name=""target_seqs"") 
 >>> batch_size = 2","'Returns the expression of cross-entropy of two sequences, implement 
 softmax internally. Normally be used for Fixed Length RNN outputs. 
 Parameters 
 logits : Tensorflow variable 
 2D tensor, ``network.outputs``, [batch_size*n_steps (n_examples), number of output units] 
 target_seqs : Tensorflow variable 
 target : 2D tensor [batch_size, n_steps], if the number of step is dynamic, please use ``cross_entropy_seq_with_mask`` instead. 
 batch_size : a int, default is 1 
 RNN batch_size, number of concurrent processes, divide the loss by batch_size. 
 num_steps : a int 
 sequence length 
 Examples 
 >>> see PTB tutorial for more details 
 >>> input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) 
 >>> targets = tf.placeholder(tf.int32, [batch_size, num_steps]) 
 >>> cost = tf.cost.cross_entropy_seq(network.outputs, targets, batch_size, num_steps)'"
"def assert_crypto_availability(f): 
    @functools.wraps(f) 
   def wrapper(*args, **kwds): 
      if (AES is None): 
         raise CryptoUnavailableError() 
      return f(*args, **kwds) 
   return wrapper", 'Decorator for assert_crypto_availability','Ensure Crypto module is available.'
"def test_color_array(): 
    x = ColorArray(['r', 'g', 'b']) 
   assert_array_equal(x.rgb, np.eye(3)) 
   assert isinstance(x[0], ColorArray) 
   assert isinstance(x[:], ColorArray) 
   assert_array_equal(x.rgba[:], x[:].rgba) 
   assert_array_equal(x.rgba[0], x[0].rgba.squeeze()) 
   assert_array_equal(x.rgba[1:3], x[1:3].rgba) 
   assert_raises(ValueError, x.__getitem__, (0, 1)) 
   x[0] = 0 
   assert_array_equal(x.rgba[0, :], np.zeros(4)) 
   assert_array_equal(x.rgba, x[:].rgba) 
   x[1] = 1 
   assert_array_equal(x[1].rgba, np.ones((1, 4))) 
   x[:] = 0.5 
   assert_array_equal(x.rgba, (0.5 * np.ones((3, 4)))) 
   assert_raises(ValueError, x.__setitem__, (0, 1), 0) 
   x = ColorArray(color_space='hsv', color=[(0, 0, 1), (0, 0, 0.5), (0, 0, 0)]) 
   assert_array_equal(x.rgba[0], [1, 1, 1, 1]) 
   assert_array_equal(x.rgba[1], [0.5, 0.5, 0.5, 1]) 
   assert_array_equal(x.rgba[2], [0, 0, 0, 1]) 
   x = ColorArray(color_space='hsv') 
   assert_array_equal(x.rgba[0], [0, 0, 0, 1])", 'Test ColorArray class','Basic tests for ColorArray class'
"def create_urllib3_context(ssl_version=None, cert_reqs=None, options=None, ciphers=None): 
    context = SSLContext((ssl_version or ssl.PROTOCOL_SSLv23)) 
   cert_reqs = (ssl.CERT_REQUIRED if (cert_reqs is None) else cert_reqs) 
   if (options is None): 
      options = 0 
      options |= OP_NO_SSLv2 
      options |= OP_NO_SSLv3 
      options |= OP_NO_COMPRESSION 
   context.options |= options 
   if getattr(context, 'supports_set_ciphers', True): 
      context.set_ciphers((ciphers or DEFAULT_CIPHERS)) 
   context.verify_mode = cert_reqs 
   if (getattr(context, 'check_hostname', None) is not None): 
      context.check_hostname = False 
   return context", 'Creates a new urllib3 context with the specified options.',"'All arguments have the same meaning as ``ssl_wrap_socket``. 
 By default, this function does a lot of the same work that 
 ``ssl.create_default_context`` does on Python 3.4+. It: 
 - Disables SSLv2, SSLv3, and compression 
 - Sets a restricted set of server ciphers 
 If you wish to enable SSLv3, you can do:: 
 from urllib3.util import ssl_ 
 context = ssl_.create_urllib3_context() 
 context.options &= ~ssl_.OP_NO_SSLv3 
 You can do the same to enable compression (substituting ``COMPRESSION`` 
 for ``SSLv3`` in the last line above). 
 :param ssl_version: 
 The desired protocol version to use. This will default to 
 PROTOCOL_SSLv23 which will negotiate the highest protocol that both 
 the server and your installation of OpenSSL support. 
 :param cert_reqs: 
 Whether to require the certificate verification. This defaults to 
 ``ssl.CERT_REQUIRED``. 
 :param options: 
 Specific OpenSSL options. These default to ``ssl.OP_NO_SSLv2``, 
 ``ssl.OP_NO_SSLv3``, ``ssl.OP_NO_COMPRESSION``. 
 :param ciphers: 
 Which cipher suites to allow the server to select. 
 :returns: 
 Constructed SSLContext object with specified options 
 :rtype: SSLContext'"
"@verbose 
 def _merge_info(infos, force_update_to_first=False, verbose=None): 
    for info in infos: 
      info._check_consistency() 
   if (force_update_to_first is True): 
      infos = deepcopy(infos) 
      _force_update_info(infos[0], infos[1:]) 
   info = Info() 
   info['chs'] = [] 
   for this_info in infos: 
      info['chs'].extend(this_info['chs']) 
   info._update_redundant() 
   duplicates = set([ch for ch in info['ch_names'] if (info['ch_names'].count(ch) > 1)]) 
   if (len(duplicates) > 0): 
      msg = ('The   following   channels   are   present   in   more   than   one   input   measurement   info   objects:   %s' % list(duplicates)) 
      raise ValueError(msg) 
   transforms = ['ctf_head_t', 'dev_head_t', 'dev_ctf_t'] 
   for trans_name in transforms: 
      trans = [i[trans_name] for i in infos if i[trans_name]] 
      if (len(trans) == 0): 
         info[trans_name] = None 
      elif (len(trans) == 1): 
         info[trans_name] = trans[0] 
      elif all(((np.all((trans[0]['trans'] == x['trans'])) and (trans[0]['from'] == x['from']) and (trans[0]['to'] == x['to'])) for x in trans[1:])): 
         info[trans_name] = trans[0] 
      else: 
         msg = ('Measurement   infos   provide   mutually   inconsistent   %s' % trans_name) 
         raise ValueError(msg) 
   kit_sys_ids = [i['kit_system_id'] for i in infos if i['kit_system_id']] 
   if (len(kit_sys_ids) == 0): 
      info['kit_system_id'] = None 
   elif (len(set(kit_sys_ids)) == 1): 
      info['kit_system_id'] = kit_sys_ids[0] 
   else: 
      raise ValueError('Trying   to   merge   channels   from   different   KIT   systems') 
   other_fields = ['acq_pars', 'acq_stim', 'bads', 'buffer_size_sec', 'comps', 'custom_ref_applied', 'description', 'dig', 'experimenter', 'file_id', 'highpass', 'hpi_results', 'hpi_meas', 'hpi_subsystem', 'events', 'line_freq', 'lowpass', 'meas_date', 'meas_id', 'proj_id', 'proj_name', 'projs', 'sfreq', 'subject_info', 'sfreq', 'xplotter_layout'] 
   for k in other_fields: 
      info[k] = _merge_dict_values(infos, k) 
   info._check_consistency() 
   return info"," 'Merge two measurement infos into one. 
 :param infos: two measurement infos to be merged 
 :param force_update_to_first: if True, update the info in the first 
 measurement info to the values in the second info. 
 :param verbose: if True, print out some info on the merge 
 :returns: the merged info object 
 :raises ValueError: if the two infos are inconsistent in some way 
 :raises TypeError: if the two infos are not infos 
 :raises ValueError: if the two infos are mutually inconsistent 
 :raises ValueError: if the two infos have different kit_system_ids 
 :raises ValueError: if the two infos have different acq_pars 
 :raises ValueError: if the two infos have different acq_stim 
 :raises ValueError: if the two infos have different bads 
 :raises ValueError: if the two infos have different buffer_size_sec 
 :raises ValueError: if the two infos have different comps 
 :raises Value","'Merge multiple measurement info dictionaries. 
 - Fields that are present in only one info object will be used in the 
 merged info. 
 - Fields that are present in multiple info objects and are the same 
 will be used in the merged info. 
 - Fields that are present in multiple info objects and are different 
 will result in a None value in the merged info. 
 - Channels will be concatenated. If multiple info objects contain 
 channels with the same name, an exception is raised. 
 Parameters 
 infos | list of instance of Info 
 Info objects to merge into one info object. 
 force_update_to_first : bool 
 If True, force the fields for objects in `info` will be updated 
 to match those in the first item. Use at your own risk, as this 
 may overwrite important metadata. 
 verbose : bool, str, int, or NonIe 
 If not None, override default verbose level (see :func:`mne.verbose` 
 and :ref:`Logging documentation <tut_logging>` for more). 
 Returns 
 info : instance of Info 
 The merged info object.'"
"def _get_block_count_url_string(xblock_type_set): 
    block_url = '' 
   if (len(xblock_type_set) > 0): 
      block_url += '&all_blocks=true&block_counts=' 
      for (index, block_type) in enumerate(xblock_type_set): 
         block_url += block_type 
         if (index < (len(xblock_type_set) - 1)): 
            block_url += ',' 
   return block_url"," 'Returns the URL string for the block count endpoint. 
 :param xblock_type_set: A list of XBlock types. 
 :return: The block count URL string. 
 :rtype: str'","'Build the string from the xBlock type set to append to the Block url for block_count types 
 Arguments: 
 xblock_type_set (set): A set of strings for all the block types 
 Returns: 
 str: The portion to append to the block url'"
"def create_territories(): 
    from frappe.utils.nestedset import get_root_of 
   country = frappe.db.get_default(u'country') 
   root_territory = get_root_of(u'Territory') 
   for name in (country, _(u'Rest   Of   The   World')): 
      if (name and (not frappe.db.exists(u'Territory', name))): 
         frappe.get_doc({u'doctype': u'Territory', u'territory_name': name.replace(u""'"", u''), u'parent_territory': root_territory, u'is_group': u'No'}).insert()", 'Create root territories for countries.',"'create two default territories, one for home country and one named Rest of the World'"
"@task 
 def GitHub_release_text(): 
    shortversion = get_sympy_short_version() 
   htmltable = table() 
   out = 'See   https://github.com/sympy/sympy/wiki/release-notes-for-{shortversion}   for   the   release   notes.\n\n{htmltable}\n\n**Note**:   Do   not   download   the   **Source   code   (zip)**   or   the   **Source   code   (tar.gz)**\nfiles   below.\n' 
   out = out.format(shortversion=shortversion, htmltable=htmltable) 
   print(blue('Here   are   the   release   notes   to   copy   into   the   GitHub   release   Markdown   form:', bold=True)) 
   print() 
   print(out) 
   return out", 'Generate the release notes for the current release.','Generate text to put in the GitHub release Markdown box'
"def build_desired_iface_config(module): 
    module.custom_desired_config = {'addr_family': None, 'auto': True, 'config': {}, 'name': module.params.get('name')} 
   build_addr_method(module) 
   build_address(module) 
   build_vids(module) 
   build_pvid(module) 
   build_speed(module) 
   build_alias_name(module) 
   build_vrr(module) 
   for _attr in ['mtu', 'mstpctl_portnetwork', 'mstpctl_portadminedge', 'mstpctl_bpduguard', 'clagd_enable', 'clagd_priority', 'clagd_peer_ip', 'clagd_sys_mac', 'clagd_args']: 
      build_generic_attr(module, _attr)", 'Build the desired configuration for an interface.','take parameters defined and build ifupdown2 compatible hash'
"def _extract_id_token(id_token): 
    if (type(id_token) == bytes): 
      segments = id_token.split('.') 
   else: 
      segments = id_token.split(u'.') 
   if (len(segments) != 3): 
      raise VerifyJwtTokenError('Wrong   number   of   segments   in   token:   {0}'.format(id_token)) 
   return json.loads(_helpers._from_bytes(_helpers._urlsafe_b64decode(segments[1])))"," 'Extracts the ID token from the JWT. 
 :param id_token: The JWT to extract the ID token from. 
 :returns: The ID token. 
 :raises VerifyJwtTokenError: If the ID token cannot be extracted from the JWT.'","'Extract the JSON payload from a JWT. 
 Does the extraction w/o checking the signature. 
 Args: 
 id_token: string or bytestring, OAuth 2.0 id_token. 
 Returns: 
 object, The deserialized JSON payload.'"
"def unwatch_log(): 
    log_root = logging.getLogger(None).logger 
   to_replace = [h for h in log_root.handlers if isinstance(h, handlers.WatchedFileHandler)] 
   for handler in to_replace: 
      new_handler = std_logging.FileHandler(handler.baseFilename, mode=handler.mode, encoding=handler.encoding) 
      log_root.removeHandler(handler) 
      log_root.addHandler(new_handler)"," 'Remove the watched file handler from the root logger. 
 This is necessary because the watched file handler is created 
 when the log is opened, and it is not safe to remove it 
 while the log is open. 
 :rtype: None'","'Replace WatchedFileHandler handlers by FileHandler ones. 
 Neutron logging uses WatchedFileHandler handlers but they do not 
 support privileges drop, this method replaces them by FileHandler 
 handlers supporting privileges drop.'"
"def compress(body, compress_level): 
    import zlib 
   (yield '\x1f\x8b') 
   (yield '\x08') 
   (yield '\x00') 
   (yield struct.pack('<L', (int(time.time()) & 4294967295L))) 
   (yield '\x02') 
   (yield '\xff') 
   crc = zlib.crc32('') 
   size = 0 
   zobj = zlib.compressobj(compress_level, zlib.DEFLATED, (- zlib.MAX_WBITS), zlib.DEF_MEM_LEVEL, 0) 
   for line in body: 
      size += len(line) 
      crc = zlib.crc32(line, crc) 
      (yield zobj.compress(line)) 
   (yield zobj.flush()) 
   (yield struct.pack('<L', (crc & 4294967295L))) 
   (yield struct.pack('<L', (size & 4294967295L)))", 'Compress a body and yield it as a bytestring.','Compress \'body\' at the given compress_level.'
"def alignment_summary(alignment, index='      ', vertical_threshold=5): 
    answer = [] 
   alignment_len = alignment.get_alignment_length() 
   rec_count = len(alignment) 
   if (rec_count < vertical_threshold): 
      for record in alignment: 
         answer.append(('%s%s   %s' % (index, str_summary(str(record.seq)), record.id))) 
   else: 
      for i in range(min(5, alignment_len)): 
         answer.append(((index + str_summary(alignment[:, i])) + ('   alignment   column   %i' % i))) 
      if (alignment_len > 5): 
         i = (alignment_len - 1) 
         answer.append(((index + str_summary(('|' * rec_count))) + '   ...')) 
         answer.append(((index + str_summary(alignment[:, i])) + ('   alignment   column   %i' % i))) 
   return '\n'.join(answer)", 'Return a summary of the alignment.','Returns a concise summary of an Alignment object as a string.'
"def _check_module_dependencies(is_imbalanced_dataset_installing=False): 
    for (module_name, module_metadata) in REQUIRED_MODULE_METADATA: 
      if (not (is_imbalanced_dataset_installing and (not module_metadata['required_at_installation']))): 
         _import_module_with_version_check(module_name=module_name, minimum_version=module_metadata['min_version'], install_info=module_metadata.get('install_info'))"," 'Check that all required modules are installed and importable. 
 :param is_imbalanced_dataset_installing: If True, then the function 
 only checks the dependencies of the imbalanced dataset installer. 
 :return: 
 :rtype: None 
 :raises: 
 :py:class:`ImbalancedDatasetError` 
 :raises: 
 :py:class:`ModuleNotFoundError` 
 :raises: 
 :py:class:`ImportError` 
 :raises: 
 :py:class:`ImportWarning`'","'Throw an exception if imbalanced-learn dependencies are not installed. 
 Parameters 
 is_imbalanced_dataset_installing: boolean 
 if True, only error on missing packages that cannot be auto-installed. 
 if False, error on any missing package. 
 Throws 
 ImportError'"
"def setup_platform(hass, config, add_devices, discovery_info=None): 
    import pyowm 
   longitude = config.get(CONF_LONGITUDE, round(hass.config.longitude, 5)) 
   latitude = config.get(CONF_LATITUDE, round(hass.config.latitude, 5)) 
   name = config.get(CONF_NAME) 
   try: 
      owm = pyowm.OWM(config.get(CONF_API_KEY)) 
   except pyowm.exceptions.api_call_error.APICallError: 
      _LOGGER.error('Error   while   connecting   to   OpenWeatherMap') 
      return False 
   data = WeatherData(owm, latitude, longitude) 
   add_devices([OpenWeatherMapWeather(name, data, hass.config.units.temperature_unit)], True)", 'Setup OpenWeatherMap weather data.','Setup the OpenWeatherMap weather platform.'
"def restart(name, jail=None): 
    cmd = '{0}   {1}   onerestart'.format(_cmd(jail), name) 
   return (not __salt__['cmd.retcode'](cmd, python_shell=False))"," 'Restart the specified service. 
 :param name: Name of the service. 
 :param jail: Jail to run the command in. 
 :returns: True if the service was restarted successfully.'","'Restart the named service 
 .. versionchanged:: 2016.3.4 
 jail: optional jid or jail name 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' service.restart <service name>'"
"@pytest.mark.parametrize('fast_writer', [True, False]) 
 def test_byte_string_output(fast_writer): 
    t = table.Table([['Hello', 'World']], dtype=['S10']) 
   out = StringIO() 
   ascii.write(t, out, fast_writer=fast_writer) 
   assert (out.getvalue().splitlines() == ['col0', 'Hello', 'World'])", 'Test writing ascii table to a StringIO',"'Test the fix for #4350 where byte strings were output with a 
 leading `b` on Py3.'"
"def _name_value_to_bson(name, value, check_keys, opts): 
    try: 
      return _ENCODERS[type(value)](name, value, check_keys, opts) 
   except KeyError: 
      pass 
   marker = getattr(value, '_type_marker', None) 
   if (isinstance(marker, int) and (marker in _MARKERS)): 
      func = _MARKERS[marker] 
      _ENCODERS[type(value)] = func 
      return func(name, value, check_keys, opts) 
   for base in _ENCODERS: 
      if isinstance(value, base): 
         func = _ENCODERS[base] 
         _ENCODERS[type(value)] = func 
         return func(name, value, check_keys, opts) 
   raise InvalidDocument(('cannot   convert   value   of   type   %s   to   bson' % type(value)))"," 'Convert a name-value pair to a bson type. 
 :param name: The name of the field. 
 :param value: The value to convert. 
 :param check_keys: If True, check that the key exists in the dict. 
 :param opts: The options to use. 
 :returns: The bson type. 
 :raises: :exc:`InvalidDocument` if the value cannot be converted to bson.'","'Encode a single name, value pair.'"
"def getGeometryOutput(derivation, elementNode): 
    if (derivation == None): 
      derivation = ExtrudeDerivation(elementNode) 
   if (len(euclidean.getConcatenatedList(derivation.target)) == 0): 
      print 'Warning,   in   extrude   there   are   no   paths.' 
      print elementNode.attributes 
      return None 
   return getGeometryOutputByLoops(derivation, derivation.target)"," 'getGeometryOutput(derivation, elementNode) -> Geometry 
 Returns the geometry of the extruded element.'",'Get triangle mesh from attribute dictionary.'
"def getRandomRange(a, b, randfunc=None): 
    range_ = ((b - a) - 1) 
   bits = size(range_) 
   value = getRandomInteger(bits, randfunc) 
   while (value > range_): 
      value = getRandomInteger(bits, randfunc) 
   return (a + value)"," 'Generate a random value in a range. 
 Parameters 
 a : int 
 The minimum value in the range. 
 b : int 
 The maximum value in the range. 
 randfunc : callable 
 A function that takes a single integer and returns a random integer. 
 Returns 
 value : int 
 A random integer in the range. 
 Notes 
 If randfunc is not supplied, a random integer is generated using 
 the randint function.'","'getRandomRange(a:int, b:int, randfunc:callable):long 
 Return a random number n so that a <= n < b. 
 If randfunc is omitted, then Random.new().read is used. 
 This function is for internal use only and may be renamed or removed in 
 the future.'"
"def convert_tree(beautiful_soup_tree, makeelement=None): 
    root = _convert_tree(beautiful_soup_tree, makeelement) 
   children = root.getchildren() 
   for child in children: 
      root.remove(child) 
   return children"," 'Convert a BeautifulSoup tree to a list of Elements. 
 :param BeautifulSoup tree: 
 :param makeelement: 
 :return: 
 :rtype: list'","'Convert a BeautifulSoup tree to a list of Element trees. 
 Returns a list instead of a single root Element to support 
 HTML-like soup with more than one root element. 
 You can pass a different Element factory through the `makeelement` 
 keyword.'"
"def AddUpdateOptions(parser): 
    parser.add_option('--retain_upload_dir', action='store_true', dest='retain_upload_dir', default=False, help='Do   not   delete   temporary   (staging)   directory   used   in   uploading   Java   apps') 
   parser.add_option('--no_symlinks', action='store_true', dest='no_symlinks', default=False, help='Do   not   use   symbolic   links   when   making   the   temporary   (staging)   directory   for   uploading   Java   apps') 
   parser.add_option('--compile_encoding', action='store', dest='compile_encoding', default='UTF-8', help='Set   the   encoding   to   be   used   when   compiling   Java   source   files   (default   ""UTF-8"").') 
   parser.add_option('--disable_jar_jsps', action='store_false', dest='jar_jsps', default=True, help='Do   not   jar   the   classes   generated   from   JSPs.') 
   parser.add_option('--delete_jsps', action='store_true', dest='delete_jsps', default=False, help='Delete   the   JSP   source   files   after   compilation.') 
   parser.add_option('--enable_jar_classes', action='store_true', dest='do_jar_classes', default=False, help='Jar   the   WEB-INF/classes   content.') 
   parser.add_option('--enable_jar_splitting', action='store_true', dest='do_jar_splitting', default=False, help='Split   large   jar   files   (>   32M)   into   smaller   fragments.') 
   parser.add_option('--jar_splitting_excludes', action='store', dest='jar_splitting_exclude_suffixes', default='', help='When   --enable_jar_splitting   is   specified   and   --jar_splitting_excludes   specifies   a   comma-separated   list   of   suffixes,   a   file   in   a   jar   whose   name   ends   with   one   of   the   suffixes   will   not   be   included   in   the   split   jar   fragments.')"," 'Add options to control the compilation of Java applications. 
 :param parser: The parser to add options to. 
 :returns: The parser with the options added.'","'Adds options specific to the \'update\' command on Java apps to \'parser\'. 
 Args: 
 parser: An instance of OptionsParser.'"
"def _determine_toggles(payload, toggles): 
    for (toggle, definition) in six.iteritems(toggles): 
      if (definition['value'] is not None): 
         if (((definition['value'] is True) or (definition['value'] == 'yes')) and (definition['type'] == 'yes_no')): 
            payload[toggle] = 'yes' 
         elif (((definition['value'] is False) or (definition['value'] == 'no')) and (definition['type'] == 'yes_no')): 
            payload[toggle] = 'no' 
         if (((definition['value'] is True) or (definition['value'] == 'yes')) and (definition['type'] == 'true_false')): 
            payload[toggle] = True 
         elif (((definition['value'] is False) or (definition['value'] == 'no')) and (definition['type'] == 'true_false')): 
            payload[toggle] = False 
   return payload"," 'Determine toggles in payload. 
 :param payload: Payload dictionary 
 :param toggles: Toggles dictionary 
 :return: Payload dictionary'","'BigIP can\'t make up its mind if it likes yes / no or true or false. 
 Figure out what it likes to hear without confusing the user.'"
"def create_mpl_fig(fig=None, figsize=None): 
    if (fig is None): 
      plt = _import_mpl() 
      fig = plt.figure(figsize=figsize) 
   return fig"," 'Create a matplotlib figure. 
 Parameters 
 fig : matplotlib figure 
 figsize : (width, height) 
 Returns 
 matplotlib figure'","'Helper function for when multiple plot axes are needed. 
 Those axes should be created in the functions they are used in, with 
 ``fig.add_subplot()``. 
 Parameters 
 fig : Matplotlib figure instance, optional 
 If given, this figure is simply returned.  Otherwise a new figure is 
 created. 
 Returns 
 fig : Matplotlib figure instance 
 If `fig` is None, the created figure.  Otherwise the input `fig` is 
 returned. 
 See Also 
 create_mpl_ax'"
"def serialize_revision(node, record, version, index, anon=False): 
    if anon: 
      user = None 
   else: 
      user = {u'name': version.creator.fullname, u'url': version.creator.url} 
   return {u'user': user, u'index': (index + 1), u'date': version.date_created.isoformat(), u'downloads': record.get_download_count(version=index), u'md5': version.metadata.get(u'md5'), u'sha256': version.metadata.get(u'sha256')}", 'Returns a dict with revision information.',"'Serialize revision for use in revisions table. 
 :param Node node: Root node 
 :param FileRecord record: Root file record 
 :param FileVersion version: The version to serialize 
 :param int index: One-based index of version'"
"def now(parser, token): 
    bits = token.contents.split('""') 
   if (len(bits) != 3): 
      raise TemplateSyntaxError(""'now'   statement   takes   one   argument"") 
   format_string = bits[1] 
   return NowNode(format_string)"," 'Parse a now statement. 
 The now statement takes a format string and returns a NowNode. 
 .. versionchanged:: 0.7 
 The format string now takes a format string as its first argument. 
 .. versionchanged:: 0.6 
 The format string now takes a format string as its first argument. 
 .. versionchanged:: 0.5 
 The format string now takes a format string as its first argument. 
 .. versionchanged:: 0.4 
 The format string now takes a format string as its first argument. 
 .. versionchanged:: 0.3 
 The format string now takes a format string as its first argument. 
 .. versionchanged:: 0.2 
 The format string now takes a format string as its first argument. 
 .. versionchanged:: 0.1 
 The format string now takes a format string as its first argument. 
 .. versionchanged:: 0.0 
 The now statement takes a format string and returns a NowNode.'","'Displays the date, formatted according to the given string. 
 Uses the same format as PHP\'s ``date()`` function; see http://php.net/date 
 for all the possible values. 
 Sample usage:: 
 It is {% now ""jS F Y H:i"" %}'"
"def _validate_list(key, value): 
    for (ind, element) in enumerate(value): 
      if (not (isinstance(element, basestring) or isinstance(element, datetime.date) or isinstance(element, datetime.datetime) or isinstance(element, numbers.Number))): 
         raise ValueError(('All   values   of   a   multi-valued   field   must   be   numbers,   strings,   date   or   datetime   instances,   The   %dth   value   for   field   %s   has   type   %s.' % (ind, key, type(element))))"," 'Validate a list of values for a field. 
 :param key: The name of the field. 
 :param value: The value of the field. 
 :raises: ValueError'","'Validates a list to be included as document fields. The key is just 
 passed in to make better error messages.'"
"def test_continuous_error(): 
    y = np.linspace(0, 1, 20) 
   cnn = CondensedNearestNeighbour(random_state=RND_SEED) 
   assert_warns(UserWarning, cnn.fit, X, y)", 'Test continuous error',"'Test either if an error is raised when the target are continuous 
 type'"
"def ntohl(bs): 
    return struct.unpack('!I', bs)[0]", 'Convert a byte string to a 32-bit integer.','Convert integer in \'n\' from network-byte order to host-byte order.'
"def mbruteforce(func, alphabet, length, method='upto', start=None, threads=None): 
    def bruteforcewrap(func, alphabet, length, method, start, databag): 
      oldloglevel = context.log_level 
      context.log_level = 'critical' 
      res = bruteforce(func, alphabet, length, method=method, start=start, databag=databag) 
      context.log_level = oldloglevel 
      databag['result'] = res 
   if (start == None): 
      start = (1, 1) 
   if (threads == None): 
      try: 
         threads = multiprocessing.cpu_count() 
      except NotImplementedError: 
         threads = 1 
   h = log.waitfor('MBruteforcing') 
   processes = ([None] * threads) 
   shareddata = ([None] * threads) 
   (i2, N2) = start 
   totalchunks = (threads * N2) 
   for i in range(threads): 
      shareddata[i] = multiprocessing.Manager().dict() 
      shareddata[i]['result'] = None 
      shareddata[i]['current_item'] = '' 
      shareddata[i]['items_done'] = 0 
      shareddata[i]['items_total'] = 0 
      chunkid = (((i2 - 1) + (i * N2)) + 1) 
      processes[i] = multiprocessing.Process(target=bruteforcewrap, args=(func, alphabet, length, method, (chunkid, totalchunks), shareddata[i])) 
      processes[i].start() 
   done = False 
   while (not done): 
      current_item_list = ','.join([('""%s""' % x['current_item']) for x in shareddata if (x != None)]) 
      items_done = sum([x['items_done'] for x in shareddata if (x != None)]) 
      items_total = sum([x['items_total'] for x in shareddata if (x != None)]) 
      progress = (((100.0 * items_done) / items_total) if (items_total != 0) else 0.0) 
      h.status(('Trying   %s   --   %0.3f%%' % (current_item_list, progress))) 
      for i in range(threads): 
         if (processes[i] and (processes[i].exitcode != None)): 
            res = shareddata[i]['result'] 
            processes[i].join() 
            processes[i] = None 
            if (res != None): 
               for i in range(threads): 
                  if (processes[i] != None): 
                     processes[i].terminate() 
                     processes[i].join() 
                     processes[i] = None 
               h.success(('Found   key:   ""%s""' % res)) 
               return res 
            if all([(x == None) for x in processes]): 
               done = True 
      time.sleep(0.3) 
   h.failure('No   matches   found')"," 'A brute force attack on a password. 
 This is a wrapper for bruteforce() that allows you to use 
 a multiprocessing pool to speed things up. 
 Parameters 
 func : callable 
 A function that takes a string and returns a boolean 
 (True if it is the password, False otherwise). 
 alphabet : string 
 The alphabet to use. 
 length : int 
 The length of the password. 
 method : str 
 The method to use to bruteforce. 
 start : tuple 
 The start of the password. 
 threads : int 
 The number of threads to use. 
 Returns 
 A tuple of (key, success) where key is the password and 
 success is True if the password was found, False otherwise. 
 Notes 
 This function is very slow and will only be useful if you 
 have a lot of CPU power. 
 Examples 
 >>> from cryptography.hazmat.backends import default_backend 
 >>> from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC 
 >>>","'mbruteforce(func, alphabet, length, method = \'upto\', start = None, threads = None) 
 Same functionality as bruteforce(), but multithreaded. 
 Arguments: 
 func, alphabet, length, method, start: same as for bruteforce() 
 threads: Amount of threads to spawn, default is the amount of cores.'"
"def get_dataset_file(dataset, default_dataset, origin): 
    (data_dir, data_file) = os.path.split(dataset) 
   if ((data_dir == '') and (not os.path.isfile(dataset))): 
      new_path = os.path.join(os.path.split(__file__)[0], '..', 'data', dataset) 
      if (os.path.isfile(new_path) or (data_file == default_dataset)): 
         dataset = new_path 
   if ((not os.path.isfile(dataset)) and (data_file == default_dataset)): 
      from six.moves import urllib 
      print(('Downloading   data   from   %s' % origin)) 
      urllib.request.urlretrieve(origin, dataset) 
   return dataset"," 'Get the dataset file for a dataset. 
 If the dataset is not a file, it will be downloaded from the origin. 
 :param dataset: The dataset to download. 
 :param default_dataset: The default dataset to use if the dataset is not 
 a file. 
 :param origin: The origin to use for downloading the dataset. 
 :returns: The dataset file path.'","'Look for it as if it was a full path, if not, try local file, 
 if not try in the data directory. 
 Download dataset if it is not present'"
"def test_epochs_hash(): 
    (raw, events) = _get_data()[:2] 
   epochs = Epochs(raw, events, event_id, tmin, tmax) 
   assert_raises(RuntimeError, epochs.__hash__) 
   epochs = Epochs(raw, events, event_id, tmin, tmax, preload=True) 
   assert_equal(hash(epochs), hash(epochs)) 
   epochs_2 = Epochs(raw, events, event_id, tmin, tmax, preload=True) 
   assert_equal(hash(epochs), hash(epochs_2)) 
   assert_true((pickle.dumps(epochs) == pickle.dumps(epochs_2))) 
   epochs_2._data[(0, 0, 0)] -= 1 
   assert_not_equal(hash(epochs), hash(epochs_2))", 'Test that epochs have a consistent hash','Test epoch hashing.'
"def read_plain_int32(file_obj, count): 
    length = (4 * count) 
   data = file_obj.read(length) 
   if (len(data) != length): 
      raise EOFError(u'Expected   {0}   bytes   but   got   {1}   bytes'.format(length, len(data))) 
   res = struct.unpack('<{0}i'.format(count).encode(u'utf-8'), data) 
   return res"," 'Reads an int32 from a file_obj. 
 :param file_obj: The file_obj to read from. 
 :param count: The number of bytes to read from the file_obj. 
 :return: The int32 read from the file_obj. 
 :rtype: int32'",'Read `count` 32-bit ints using the plain encoding.'
"def load_stored_item(cache, path, item): 
    return cache.load_parser(path, (item.change_time - 1))"," 'Loads a stored item from the cache. 
 :param cache: The cache to use. 
 :param path: The path to the item. 
 :param item: The item to load. 
 :return: The item loaded from the cache.'",'Load `item` stored at `path` in `cache`.'
"def stash_conf_values(): 
    conf = {'bind_host': CONF.bind_host, 'bind_port': CONF.bind_port, 'tcp_keepidle': CONF.cert_file, 'backlog': CONF.backlog, 'key_file': CONF.key_file, 'cert_file': CONF.cert_file} 
   return conf"," 'Returns a dictionary of conf values for use in the server. 
 :returns: dictionary of conf values'","'Make a copy of some of the current global CONF\'s settings. 
 Allows determining if any of these values have changed 
 when the config is reloaded.'"
"def _strxfrm(s): 
    return s", 'Convert a string to a byte string.',"'strxfrm(string) -> string. 
 Returns a string that behaves for cmp locale-aware.'"
"def rjust(s, width): 
    n = (width - len(s)) 
   if (n <= 0): 
      return s 
   return (('   ' * n) + s)"," 'Return a string right-justified to a given width. 
 If the width is negative, the string is left-justified. 
 >>> rjust(""foo"", 3) 
 \'   foo\''","'rjust(s, width) -> string 
 Return a right-justified version of s, in a field of the 
 specified width, padded with spaces as needed.  The string is 
 never truncated.'"
"def WRatio(s1, s2, force_ascii=True): 
    p1 = utils.full_process(s1, force_ascii=force_ascii) 
   p2 = utils.full_process(s2, force_ascii=force_ascii) 
   if (not utils.validate_string(p1)): 
      return 0 
   if (not utils.validate_string(p2)): 
      return 0 
   try_partial = True 
   unbase_scale = 0.95 
   partial_scale = 0.9 
   base = ratio(p1, p2) 
   len_ratio = (float(max(len(p1), len(p2))) / min(len(p1), len(p2))) 
   if (len_ratio < 1.5): 
      try_partial = False 
   if (len_ratio > 8): 
      partial_scale = 0.6 
   if try_partial: 
      partial = (partial_ratio(p1, p2) * partial_scale) 
      ptsor = ((partial_token_sort_ratio(p1, p2, full_process=False) * unbase_scale) * partial_scale) 
      ptser = ((partial_token_set_ratio(p1, p2, full_process=False) * unbase_scale) * partial_scale) 
      return utils.intr(max(base, partial, ptsor, ptser)) 
   else: 
      tsor = (token_sort_ratio(p1, p2, full_process=False) * unbase_scale) 
      tser = (token_set_ratio(p1, p2, full_process=False) * unbase_scale) 
      return utils.intr(max(base, tsor, tser))"," 'Calculate the WRatio of two strings. 
 :param s1: string 1 
 :param s2: string 2 
 :param force_ascii: if True, force ASCII 
 :return: WRatio 
 :rtype: float'","'Return a measure of the sequences\' similarity between 0 and 100, using different algorithms. 
 **Steps in the order they occur** 
 #. Run full_process from utils on both strings 
 #. Short circuit if this makes either string empty 
 #. Take the ratio of the two processed strings (fuzz.ratio) 
 #. Run checks to compare the length of the strings 
 * If one of the strings is more than 1.5 times as long as the other 
 use partial_ratio comparisons - scale partial results by 0.9 
 (this makes sure only full results can return 100) 
 * If one of the strings is over 8 times as long as the other 
 instead scale by 0.6 
 #. Run the other ratio functions 
 * if using partial ratio functions call partial_ratio, 
 partial_token_sort_ratio and partial_token_set_ratio 
 scale all of these by the ratio based on length 
 * otherwise call token_sort_ratio and token_set_ratio 
 * all token based comparisons are scaled by 0.95 
 (on top of any partial scalars) 
 #. Take the highest value from these results 
 round it and return it as an integer. 
 :param s1: 
 :param s2: 
 :param force_ascii: Allow only ascii characters 
 :type force_ascii: bool 
 :return:'"
"def create_tags(filesystemid, tags, keyid=None, key=None, profile=None, region=None, **kwargs): 
    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region) 
   new_tags = [] 
   for (k, v) in tags.iteritems(): 
      new_tags.append({'Key': k, 'Value': v}) 
   client.create_tags(FileSystemId=filesystemid, Tags=new_tags)", 'Create tags for a file system.',"'Creates or overwrites tags associated with a file system. 
 Each tag is a key-value pair. If a tag key specified in the request 
 already exists on the file system, this operation overwrites 
 its value with the value provided in the request. 
 filesystemid 
 (string) - ID of the file system for whose tags will be modified. 
 tags 
 (dict) - The tags to add to the file system 
 CLI Example: 
 .. code-block:: 
 salt \'my-minion\' boto_efs.create_tags'"
"def has_flag(conf, atom, flag): 
    if (flag in get_flags_from_package_conf(conf, atom)): 
      return True 
   return False", 'Returns true if the given flag is set for the given atom',"'Verify if the given package or DEPEND atom has the given flag. 
 Warning: This only works if the configuration files tree is in the correct 
 format (the one enforced by enforce_nice_config) 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' portage_config.has_flag license salt Apache-2.0'"
"def read_template(tmplf): 
    if pkg_resources.resource_exists('roscreate', tmplf): 
      f = pkg_resources.resource_stream('roscreate', tmplf) 
      t = f.read() 
   else: 
      r = rospkg.RosPack() 
      with open(os.path.join(r.get_path('roscreate'), 'templates', tmplf)) as f: 
         t = f.read() 
   try: 
      t = t.decode('utf-8') 
   except AttributeError: 
      pass 
   return t"," 'Read the template file for the given template name. 
 :param tmplf: Name of the template file to read. 
 :returns: The template text as a string.'","'Read resource template from egg installation, or fallback on rospkg otherwise. 
 :returns: text of template file'"
"@celery_app.task(base=ArchiverTask, ignore_result=False) 
 @logged('make_copy_request') 
 def make_copy_request(job_pk, url, data): 
    create_app_context() 
   job = ArchiveJob.load(job_pk) 
   (src, dst, user) = job.info() 
   provider = data['source']['provider'] 
   logger.info('Sending   copy   request   for   addon:   {0}   on   node:   {1}'.format(provider, dst._id)) 
   res = requests.post(url, data=json.dumps(data)) 
   if (res.status_code not in (http.OK, http.CREATED, http.ACCEPTED)): 
      raise HTTPError(res.status_code)"," 'Request a copy of a file from a remote provider. 
 :param job_pk: The ID of the job to copy from. 
 :param url: The URL of the remote provider. 
 :param data: The data to send to the remote provider. 
 :returns: The response from the remote provider. 
 :rtype: requests.Response'","'Make the copy request to the WaterBulter API and handle 
 successful and failed responses 
 :param job_pk: primary key of ArchiveJob 
 :param url: URL to send request to 
 :param data: <dict> of setting to send in POST to WaterBulter API 
 :return: None'"
"def obtain_lock_id_to_hog(): 
    for id in board_ids(): 
      if _obtain_lock(id): 
         return id 
   return (-1)"," 'Obtains a lock for the current board. 
 Returns -1 if no board is currently locked.'","'Finds a free id, locks it and returns integer id, or -1 if none free. 
 * Lock must be freed manually *'"
"def create_instance(options): 
    project = get_project(options) 
   print 'Creating   instance   {project}/{zone}/{instance}'.format(project=project, zone=get_zone(options), instance=options.instance) 
   print '      with   --machine_type={type}   and   --disk_size={disk_size}...'.format(type=options.machine_type, disk_size=options.disk_size) 
   google_dev_dir = os.path.join(os.path.dirname(__file__), '../google/dev') 
   dev_dir = os.path.dirname(__file__) 
   project_dir = os.path.join(dev_dir, '..') 
   install_dir = '{dir}/../install'.format(dir=dev_dir) 
   startup_command = ['/opt/spinnaker/install/install_spinnaker.sh   --dependencies_only', '/opt/spinnaker/install/install_development.sh'] 
   (fd, temp_startup) = tempfile.mkstemp() 
   os.write(fd, ';'.join(startup_command)) 
   os.close(fd) 
   metadata_files = ['startup-script={google_dev_dir}/google_install_loader.py,sh_bootstrap_dev={dev_dir}/bootstrap_dev.sh,sh_install_spinnaker={project_dir}/InstallSpinnaker.sh,sh_install_development={dev_dir}/install_development.sh,startup_command={temp_startup}'.format(google_dev_dir=google_dev_dir, dev_dir=dev_dir, project_dir=project_dir, temp_startup=temp_startup)] 
   metadata = ','.join(['startup_loader_files=sh_install_spinnaker+sh_install_development+sh_bootstrap_dev']) 
   command = ['gcloud', 'compute', 'instances', 'create', options.instance, '--project', get_project(options), '--zone', get_zone(options), '--machine-type', options.machine_type, '--image', 'ubuntu-14-04', '--scopes', 'compute-rw,storage-rw', '--boot-disk-size={size}'.format(size=options.disk_size), '--boot-disk-type={type}'.format(type=options.disk_type), '--metadata', metadata, '--metadata-from-file={files}'.format(files=','.join(metadata_files))] 
   if options.address: 
      command.extend(['--address', options.address]) 
   check_run_quick('   '.join(command), echo=False)"," 'Creates an instance in the given project and zone. 
 :param options: The options object'",'Creates new GCE VM instance for development.'
"def make_or_verify_needed_dirs(config): 
    make_or_verify_core_dir(config.config_dir, constants.CONFIG_DIRS_MODE, os.geteuid(), config.strict_permissions) 
   make_or_verify_core_dir(config.work_dir, constants.CONFIG_DIRS_MODE, os.geteuid(), config.strict_permissions) 
   make_or_verify_core_dir(config.logs_dir, 448, os.geteuid(), config.strict_permissions)"," 'Make or verify the existence of the core directories. 
 :param config: The config to use 
 :type config: Config'","'Create or verify existance of config, work, or logs directories'"
"def name_validator(value, context): 
    if (not isinstance(value, basestring)): 
      raise Invalid(_('Names   must   be   strings')) 
   if (value in ['new', 'edit', 'search']): 
      raise Invalid(_('That   name   cannot   be   used')) 
   if (len(value) < 2): 
      raise Invalid((_('Must   be   at   least   %s   characters   long') % 2)) 
   if (len(value) > PACKAGE_NAME_MAX_LENGTH): 
      raise Invalid((_('Name   must   be   a   maximum   of   %i   characters   long') % PACKAGE_NAME_MAX_LENGTH)) 
   if (not name_match.match(value)): 
      raise Invalid(_('Must   be   purely   lowercase   alphanumeric   (ascii)   characters   and   these   symbols:   -_')) 
   return value"," 'Validates a package name. 
 :param value: 
 :type value: 
 :param context: 
 :type context: 
 :return: 
 :rtype: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :raises: 
 :ra","'Return the given value if it\'s a valid name, otherwise raise Invalid. 
 If it\'s a valid name, the given value will be returned unmodified. 
 This function applies general validation rules for names of packages, 
 groups, users, etc. 
 Most schemas also have their own custom name validator function to apply 
 custom validation rules after this function, for example a 
 ``package_name_validator()`` to check that no package with the given name 
 already exists. 
 :raises ckan.lib.navl.dictization_functions.Invalid: if ``value`` is not 
 a valid name'"
"def repeat(x, repeats, axis=None): 
    repeats = tensor.as_tensor_variable(repeats) 
   if (repeats.ndim > 1): 
      raise ValueError('The   dimension   of   repeats   should   not   exceed   1.') 
   if ((repeats.ndim == 1) and (not repeats.broadcastable[0])): 
      return RepeatOp(axis=axis)(x, repeats) 
   else: 
      if (repeats.ndim == 1): 
         repeats = repeats[0] 
      if (x.dtype == 'uint64'): 
         raise TypeError(""theano.tensor.repeat   don't   support   dtype   uint64"") 
      if (axis is None): 
         axis = 0 
         x = x.flatten() 
      else: 
         if (axis >= x.ndim): 
            raise ValueError('Axis   should   not   exceed   x.ndim-1.') 
         if (axis < 0): 
            axis = (x.ndim + axis) 
      shape = [x.shape[i] for i in xrange(x.ndim)] 
      shape_ = shape[:] 
      shape_.insert((axis + 1), repeats) 
      shape[axis] = (shape[axis] * repeats) 
      dims_ = list(numpy.arange(x.ndim)) 
      dims_.insert((axis + 1), 'x') 
      z = tensor.alloc(x.dimshuffle(*dims_), *shape_).reshape(shape) 
      return z"," 'Repeat an input tensor ``x`` ``repeats`` times along the given axis. 
 The ``repeats`` argument must be a scalar or a tensor of shape 
 ``(1,)``. 
 Parameters 
 x : Theano tensor 
 The input tensor. 
 repeats : int or scalar tensor 
 The number of times to repeat ``x`` along the given axis. 
 axis : int, optional 
 The axis to repeat along. If None, repeat along the first axis. 
 Returns 
 out : Theano tensor 
 The output tensor. 
 Examples 
 >>> x = theano.tensor.matrix(""i"", shape=(3, 4)) 
 >>> repeat(x, 2) 
 TensorShape([3, 4, 2]) 
 >>> repeat(x, 2, axis=1) 
 TensorShape([3, 2, 4]) 
 >>> repeat(x, 2, axis=0) 
 TensorShape([3, 4, 2]) 
 >>> repeat(x, 2, axis=None) 
 TensorShape([3, ","'Repeat elements of an array. 
 It returns an array which has the same shape as `x`, except 
 along the given axis. The axis is used to speficy along which 
 axis to repeat values. By default, use the flattened input 
 array, and return a flat output array. 
 The number of repetitions for each element is `repeat`. 
 `repeats` is broadcasted to fit the length of the given `axis`. 
 Parameters 
 x 
 Input data, tensor variable. 
 repeats 
 int, scalar or tensor variable 
 axis : int, optional 
 See Also 
 tensor.tile 
 .. versionadded:: 0.6'"
"@pytest.mark.hasgpu 
 def test_hist(nbin_offset_dim_dtype_inp, backend_pair): 
    ((nbins, offset), dim, dtype, (name, inp_gen)) = nbin_offset_dim_dtype_inp 
   gpuflag = (check_gpu.get_compute_capability(0) >= 3.0) 
   if (gpuflag is False): 
      raise RuntimeError('Device   does   not   have   CUDA   compute   capability   3.0   or   greater') 
   (ng, nc) = backend_pair 
   ng.set_hist_buffers(nbins, offset) 
   nc.set_hist_buffers(nbins, offset) 
   np_inp = inp_gen(dim).astype(dtype) 
   np_hist = ref_hist(np_inp, nbins=nbins, offset=offset) 
   for be in [ng, nc]: 
      be_inp = be.array(np_inp, dtype=dtype) 
      be_hist = be_inp.hist(name) 
      assert tensors_allclose(np_hist, be_hist)", 'Test histogram function',"'Compare the nervanagpu and nervanacpu hist implementation to the reference 
 implementation above. 
 Parameterized test case, uses pytest_generate_test to enumerate dim_dtype_inp 
 tuples that drive the test.'"
"def _estimate_rank_meeg_cov(data, info, scalings, tol='auto', return_singular=False): 
    picks_list = _picks_by_type(info) 
   scalings = _handle_default('scalings_cov_rank', scalings) 
   _apply_scaling_cov(data, picks_list, scalings) 
   if (data.shape[1] < data.shape[0]): 
      ValueError(""You've   got   fewer   samples   than   channels,   your   rank   estimate   might   be   inaccurate."") 
   out = estimate_rank(data, tol=tol, norm=False, return_singular=return_singular) 
   rank = (out[0] if isinstance(out, tuple) else out) 
   ch_type = '   +   '.join(list(zip(*picks_list))[0]) 
   logger.info(('estimated   rank   (%s):   %d' % (ch_type, rank))) 
   _undo_scaling_cov(data, picks_list, scalings) 
   return out"," 'Estimate the rank of the covariance matrix. 
 Parameters 
 data : (n_samples, n_channels, n_times) ndarray 
 The data. 
 info : (n_channels,) ndarray 
 The info field from the info structure. 
 scalings : (n_channels,) ndarray 
 The scalings from the info structure. 
 tol : float 
 The tolerance for the rank estimate. 
 return_singular : bool 
 If True, return the singular values. 
 Returns 
 out : (n_channels,) ndarray 
 The estimated rank of the covariance matrix. 
 Examples 
 >>> from mne import read_raw_meeg 
 >>> data = read_raw_meeg(\'/path/to/data/meeg.fif\') 
 >>> info = read_raw_info(\'/path/to/data/meeg.fif\') 
 >>> scalings = read_raw_info(\'/path/to/data/meeg.fif\', preload=True).info.source","'Estimate rank of M/EEG covariance data, given the covariance. 
 Parameters 
 data : np.ndarray of float, shape (n_channels, n_channels) 
 The M/EEG covariance. 
 info : Info 
 The measurment info. 
 scalings : dict | \'norm\' | np.ndarray | None 
 The rescaling method to be applied. If dict, it will override the 
 following default dict: 
 dict(mag=1e12, grad=1e11, eeg=1e5) 
 If \'norm\' data will be scaled by channel-wise norms. If array, 
 pre-specified norms will be used. If None, no scaling will be applied. 
 tol : float | str 
 Tolerance. See ``estimate_rank``. 
 return_singular : bool 
 If True, also return the singular values that were used 
 to determine the rank. 
 Returns 
 rank : int 
 Estimated rank of the data. 
 s : array 
 If return_singular is True, the singular values that were 
 thresholded to determine the rank are also returned.'"
"def _force_mutable(x): 
    if getattr(x, 'is_Matrix', False): 
      return x.as_mutable() 
   elif isinstance(x, Basic): 
      return x 
   elif hasattr(x, '__array__'): 
      a = x.__array__() 
      if (len(a.shape) == 0): 
         return sympify(a) 
      return Matrix(x) 
   return x"," 'Returns a Matrix if x is a Matrix, a Basic if x is a Basic, or 
 a number if x is a number.'","'Return a matrix as a Matrix, otherwise return x.'"
"def _check_cron_env(user, name, value=None): 
    if (value is None): 
      value = '' 
   lst = __salt__['cron.list_tab'](user) 
   for env in lst['env']: 
      if (name == env['name']): 
         if (value != env['value']): 
            return 'update' 
         return 'present' 
   return 'absent'"," 'Check if the specified environment variable is set 
 This function checks if the specified environment variable is set 
 in the crontab of the user. If it is not set, it will return 
 \'absent\'. If it is set, it will return \'present\' or \'update\' 
 depending on if the value is the same as the current value. 
 :param user: The user\'s name or ID 
 :param name: The name of the environment variable to check 
 :param value: The value of the environment variable to check 
 :return: \'absent\', \'present\', or \'update\''",'Return the environment changes'
"def test_daophot_indef(): 
    table = ascii.read('t/daophot2.dat', Reader=ascii.Daophot) 
   for colname in table.colnames: 
      mask_value = (colname in ('OTIME', 'MAG', 'MERR', 'XAIRMASS')) 
      assert np.all((table[colname].mask == mask_value))", 'Test the Daophot table','Test that INDEF is correctly interpreted as a missing value'
"def delete_vpc_peering_connection(name, conn_id=None, conn_name=None, region=None, key=None, keyid=None, profile=None): 
    log.debug('Called   state   to   delete   VPC   peering   connection') 
   ret = {'name': name, 'result': True, 'changes': {}, 'comment': 'Boto   VPC   peering   state'} 
   if conn_name: 
      vpc_ids = __salt__['boto_vpc.describe_vpc_peering_connection'](conn_name, region=region, key=key, keyid=keyid, profile=profile).get('VPC-Peerings', []) 
   else: 
      vpc_ids = [conn_id] 
   if (not vpc_ids): 
      ret['comment'] = 'No   VPC   connection   found,   nothing   to   be   done.' 
      return ret 
   if __opts__['test']: 
      if vpc_ids: 
         ret['comment'] = 'VPC   peering   connection   would   be   deleted' 
      return ret 
   log.debug('Called   module   to   delete   VPC   peering   connection') 
   result = __salt__['boto_vpc.delete_vpc_peering_connection'](conn_id=conn_id, conn_name=conn_name, region=region, key=key, keyid=keyid, profile=profile) 
   if ('error' in result): 
      ret['comment'] = 'Failed   to   delete   VPC   peering:   {0}'.format(result['error']) 
      ret['result'] = False 
      return ret 
   ret['changes'].update({'old': '', 'new': result['msg']}) 
   return ret", 'Delete a VPC peering connection',"'name 
 Name of the state 
 conn_id 
 ID of the peering connection to delete.  Exlusive with conn_name. 
 conn_name 
 The name of the peering connection to delete.  Exlusive with conn_id. 
 region 
 Region to connect to. 
 key 
 Secret key to be used. 
 keyid 
 Access key to be used. 
 profile 
 A dict with region, key and keyid, or a pillar key (string) that 
 contains a dict with region, key and keyid. 
 .. versionadded:: 2016.11.0 
 Example: 
 .. code-block:: yaml 
 delete a vpc peering connection: 
 boto_vpc.delete_vpc_peering_connection: 
 - region: us-west-2 
 - conn_id: pcx-4613b12e 
 Connection name can be specified (instead of ID). 
 Specifying both conn_name and conn_id will result in an 
 error. 
 .. code-block:: yaml 
 delete a vpc peering connection: 
 boto_vpc.delete_vpc_peering_connection: 
 - conn_name: salt_vpc_peering'"
"def _type_check(arg, msg): 
    if (arg is None): 
      return type(None) 
   if isinstance(arg, basestring): 
      arg = _ForwardRef(arg) 
   if ((isinstance(arg, _TypingBase) and (type(arg).__name__ == u'_ClassVar')) or ((not isinstance(arg, (type, _TypingBase))) and (not callable(arg)))): 
      raise TypeError((msg + (u'   Got   %.100r.' % (arg,)))) 
   if (((type(arg).__name__ in (u'_Union', u'_Optional')) and (not getattr(arg, u'__origin__', None))) or (isinstance(arg, TypingMeta) and (_gorg(arg) in (Generic, _Protocol)))): 
      raise TypeError((u'Plain   %s   is   not   valid   as   type   argument' % arg)) 
   return arg"," 'Type check arguments. 
 :param arg: 
 :param msg: 
 :return: 
 :raises: 
 :rtype: 
 :type msg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg: 
 :type arg:","'Check that the argument is a type, and return it (internal helper). 
 As a special case, accept None and return type(None) instead. 
 Also, _TypeAlias instances (e.g. Match, Pattern) are acceptable. 
 The msg argument is a human-readable error message, e.g. 
 ""Union[arg, ...]: arg should be a type."" 
 We append the repr() of the actual value (truncated to 100 chars).'"
"def test_multiclass_error(): 
    y = np.linspace(0, 1, 15) 
   iht = InstanceHardnessThreshold(random_state=RND_SEED) 
   assert_warns(UserWarning, iht.fit, X, y) 
   y = np.array(((([0] * 10) + ([1] * 3)) + ([2] * 2))) 
   iht = InstanceHardnessThreshold(random_state=RND_SEED) 
   assert_warns(UserWarning, iht.fit, X, y)"," 'Test that InstanceHardnessThreshold.fit() raises a warning when 
 the y values are not of the same shape as X.'","'Test either if an error is raised when the target are not binary 
 type.'"
"def add_password_arg(cmd, psw, required=False): 
    if (UNRAR_TOOL == ALT_TOOL): 
      return 
   if (psw is not None): 
      cmd.append(('-p' + psw)) 
   else: 
      cmd.append('-p-')", 'Add password arg to command line.','Append password switch to commandline.'
"def getNewDerivation(elementNode): 
    return SolidDerivation(elementNode)"," 'Returns a SolidDerivation object for the given element node. 
 :param elementNode: element node to derive from 
 :type elementNode: ElementNode 
 :return: SolidDerivation object 
 :rtype: SolidDerivation'",'Get new derivation.'
"def _set_sentinel(): 
    return LockType()"," 'Set the sentinel lock type. 
 :return: The sentinel lock type.'",'Dummy implementation of _thread._set_sentinel().'
"def formset_factory(form, formset=BaseFormSet, extra=1, can_order=False, can_delete=False, max_num=None, validate_max=False, min_num=None, validate_min=False): 
    if (min_num is None): 
      min_num = DEFAULT_MIN_NUM 
   if (max_num is None): 
      max_num = DEFAULT_MAX_NUM 
   absolute_max = (max_num + DEFAULT_MAX_NUM) 
   attrs = {'form': form, 'extra': extra, 'can_order': can_order, 'can_delete': can_delete, 'min_num': min_num, 'max_num': max_num, 'absolute_max': absolute_max, 'validate_min': validate_min, 'validate_max': validate_max} 
   return type((form.__name__ + 'FormSet'), (formset,), attrs)"," 'Return a formset factory for the given form. 
 :param form: The form to create a formset for. 
 :param formset: The formset class to use. 
 :param extra: The number of extra formsets to create. 
 :param can_order: Whether the formset can be ordered. 
 :param can_delete: Whether the formset can be deleted. 
 :param min_num: The minimum number of formsets to create. 
 :param validate_min: Whether to validate that the minimum number of formsets are created. 
 :param max_num: The maximum number of formsets to create. 
 :param validate_max: Whether to validate that the maximum number of formsets are created. 
 :param absolute_max: The absolute maximum number of formsets to create. 
 :return: A formset factory. 
 :rtype: type'",'Return a FormSet for the given form class.'
"def onLoggerAppShutDown(): 
    INFO_MSG('onLoggerAppShutDown()')", 'Logs a message when the application is shut down.',"'KBEngine method. 
 è¿ä¸ªloggerè¢«å³é­åçåè°å½æ°'"
"def project_get_networks(context, project_id, associate=True): 
    return IMPL.project_get_networks(context, project_id, associate)", 'Get networks associated with the project.',"'Return the network associated with the project. 
 If associate is true, it will attempt to associate a new 
 network if one is not found, otherwise it returns None.'"
"def new_test_client(cls, **kwargs): 
    client = cls(debug_logging=True) 
   client.login(**kwargs) 
   return client"," 'Create a new test client with the given credentials. 
 :param kwargs: 
 :param kwargs.username: The username to use for authentication 
 :param kwargs.password: The password to use for authentication 
 :param kwargs.debug_logging: If True, enable debug logging.'","'Make an instance of a client, login, and return it. 
 kwargs are passed through to cls.login().'"
"def relpath_to_site(lang, target_lang): 
    path = _SITES_RELPATH_DB.get((lang, target_lang), None) 
   if (path is None): 
      siteurl = _SITE_DB.get(lang, _MAIN_SITEURL) 
      target_siteurl = _SITE_DB.get(target_lang, _MAIN_SITEURL) 
      path = posixpath.relpath(get_site_path(target_siteurl), get_site_path(siteurl)) 
      _SITES_RELPATH_DB[(lang, target_lang)] = path 
   return path", 'Returns a relative path from the source site to the target site.',"'Get relative path from siteurl of lang to siteurl of base_lang 
 the output is cached in _SITES_RELPATH_DB'"
"def add_nic(si, vm, network): 
    spec = vim.vm.ConfigSpec() 
   nic_changes = [] 
   nic_spec = vim.vm.device.VirtualDeviceSpec() 
   nic_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add 
   nic_spec.device = vim.vm.device.VirtualE1000() 
   nic_spec.device.deviceInfo = vim.Description() 
   nic_spec.device.deviceInfo.summary = 'vCenter   API   test' 
   nic_spec.device.backing = vim.vm.device.VirtualEthernetCard.NetworkBackingInfo() 
   nic_spec.device.backing.useAutoDetect = False 
   content = si.RetrieveContent() 
   nic_spec.device.backing.network = get_obj(content, [vim.Network], network) 
   nic_spec.device.backing.deviceName = network 
   nic_spec.device.connectable = vim.vm.device.VirtualDevice.ConnectInfo() 
   nic_spec.device.connectable.startConnected = True 
   nic_spec.device.connectable.startConnected = True 
   nic_spec.device.connectable.allowGuestControl = True 
   nic_spec.device.connectable.connected = False 
   nic_spec.device.connectable.status = 'untried' 
   nic_spec.device.wakeOnLanEnabled = True 
   nic_spec.device.addressType = 'assigned' 
   nic_changes.append(nic_spec) 
   spec.deviceChange = nic_changes 
   e = vm.ReconfigVM_Task(spec=spec) 
   print 'NIC   CARD   ADDED'", 'Adds a NIC to a VM',"':param si: Service Instance 
 :param vm: Virtual Machine Object 
 :param network: Virtual Network'"
"def load_tests(loader, tests, pattern): 
    test_dir = os.path.join(os.path.dirname(__file__), TESTS_DIR) 
   return driver.build_tests(test_dir, loader, host=None, intercept=fixture_module.setup_app, fixture_module=fixture_module)"," 'Load tests from the tests directory. 
 :param loader: The test loader. 
 :param tests: The tests to load. 
 :param pattern: The pattern to load. 
 :return: The tests to load. 
 :rtype: list'",'Provide a TestSuite to the discovery process.'
"def format_value(val, limit=100, level=10): 
    return _format_value(val, limit, level)", 'Format a value for display.','Wrapper around _format_value().'
"def default_formats(): 
    return {'html': {'nbconvert_template': 'basic', 'label': 'Notebook', 'icon': 'book'}, 'slides': {'nbconvert_template': 'slides_reveal', 'label': 'Slides', 'icon': 'gift', 'test': (lambda nb, json: ('""slideshow""' in json))}, 'script': {'label': 'Code', 'icon': 'code', 'content_type': 'text/plain;   charset=UTF-8'}}", 'Returns a dictionary of default formats.',"'Return the currently-implemented formats. 
 These are not classes, but maybe should be: would they survive pickling? 
 - exporter: 
 an Exporter subclass. 
 if not defined, and key is in nbconvert.export.exporter_map, it will be added 
 automatically 
 - nbconvert_template: 
 the name of the nbconvert template to add to config.ExporterClass 
 - test: 
 a function(notebook_object, notebook_json) 
 conditionally offer a format based on content if truthy. see 
 `RenderingHandler.filter_exporters` 
 - postprocess: 
 a function(html, resources) 
 perform any modifications to html and resources after nbconvert 
 - content_Type: 
 a string specifying the Content-Type of the response from this format. 
 Defaults to  text/html; charset=UTF-8'"
"def get_latest_flexget_version_number(): 
    try: 
      page = requests.get(u'http://download.flexget.com/latestversion') 
   except requests.RequestException: 
      return 
   ver = page.text.strip() 
   return ver", 'Returns the latest version of Flexget','Return latest Flexget version from http://download.flexget.com/latestversion'
"def edit_service(protocol=None, service_address=None, scheduler=None): 
    cmd = '{0}   -E   {1}'.format(__detect_os(), _build_cmd(protocol=protocol, service_address=service_address, scheduler=scheduler)) 
   out = __salt__['cmd.run_all'](cmd, python_shell=False) 
   if out['retcode']: 
      ret = out['stderr'].strip() 
   else: 
      ret = True 
   return ret"," 'Edit the service configuration for a given protocol and service address. 
 This will edit the configuration file for the service and reload the 
 service. 
 :param protocol: The protocol to edit, defaults to the default protocol. 
 :param service_address: The address to edit, defaults to the default 
 service address. 
 :param scheduler: The scheduler to edit, defaults to the default scheduler. 
 :return: True if the service was edited successfully, False otherwise.'","'Edit the virtual service. 
 protocol 
 The service protocol(only support tcp, udp and fwmark service). 
 service_address 
 The LVS service address. 
 scheduler 
 Algorithm for allocating TCP connections and UDP datagrams to real servers. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' lvs.edit_service tcp 1.1.1.1:80 rr'"
"@register_vcs_handler('git', 'keywords') 
 def git_versions_from_keywords(keywords, tag_prefix, verbose): 
    if (not keywords): 
      raise NotThisMethod('no   keywords   at   all,   weird') 
   date = keywords.get('date') 
   if (date is not None): 
      date = date.strip().replace('   ', 'T', 1).replace('   ', '', 1) 
   refnames = keywords['refnames'].strip() 
   if refnames.startswith('$Format'): 
      if verbose: 
         print('keywords   are   unexpanded,   not   using') 
      raise NotThisMethod('unexpanded   keywords,   not   a   git-archive   tarball') 
   refs = set([r.strip() for r in refnames.strip('()').split(',')]) 
   TAG = 'tag:   ' 
   tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)]) 
   if (not tags): 
      tags = set([r for r in refs if re.search('\\d', r)]) 
      if verbose: 
         print((""discarding   '%s',   no   digits"" % ','.join((refs - tags)))) 
   if verbose: 
      print(('likely   tags:   %s' % ','.join(sorted(tags)))) 
   for ref in sorted(tags): 
      if ref.startswith(tag_prefix): 
         r = ref[len(tag_prefix):] 
         if verbose: 
            print(('picking   %s' % r)) 
         return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date} 
   if verbose: 
      print('no   suitable   tags,   using   unknown   +   full   revision   id') 
   return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no   suitable   tags', 'date': None}"," 'Parses the keywords file, extracting the version, date, and full-revisionid 
 from the keywords. 
 :param keywords: A dictionary of keywords from the keywords file 
 :param tag_prefix: The prefix to use to extract the tag name from a 
 refname 
 :param verbose: If True, print out information about what was found'",'Get version information from git keywords.'
"def value_present(name, datastore, path, config): 
    ret = {'name': name, 'result': False, 'changes': {}, 'comment': ''} 
   existing = __salt__['cisconso.get_data'](datastore, path) 
   if cmp(existing, config): 
      ret['result'] = True 
      ret['comment'] = 'Config   is   already   set' 
   elif (__opts__['test'] is True): 
      ret['result'] = None 
      ret['comment'] = 'Config   will   be   added' 
      diff = _DictDiffer(existing, config) 
      ret['changes']['new'] = diff.added() 
      ret['changes']['removed'] = diff.removed() 
      ret['changes']['changed'] = diff.changed() 
   else: 
      __salt__['cisconso.set_data_value'](datastore, path, config) 
      ret['result'] = True 
      ret['comment'] = 'Successfully   added   config' 
      diff = _DictDiffer(existing, config) 
      ret['changes']['new'] = diff.added() 
      ret['changes']['removed'] = diff.removed() 
      ret['changes']['changed'] = diff.changed() 
   return ret"," 'Check if a value is present in the config file. 
 If the value is present in the config file, return True. 
 If the value is not present in the config file, return False. 
 If the value is present in the config file, but with different value, return None. 
 :param name: The name of the config value to check. 
 :param datastore: The name of the datastore to check. 
 :param path: The path of the config file to check. 
 :param config: The expected value of the config. 
 :return: The result of the check.'","'Ensure a specific value exists at a given path 
 :param name: The name for this rule 
 :type  name: ``str`` 
 :param datastore: The datastore, e.g. running, operational. 
 One of the NETCONF store IETF types 
 :type  datastore: :class:`DatastoreType` (``str`` enum). 
 :param path: The device path to set the value at, 
 a list of element names in order, / seperated 
 :type  path: ``list``, ``str`` OR ``tuple`` 
 :param config: The new value at the given path 
 :type  config: ``dict`` 
 Examples: 
 .. code-block:: yaml 
 enable pap auth: 
 cisconso.config_present: 
 - name: enable_pap_auth 
 - datastore: running 
 - path: devices/device/ex0/config/sys/interfaces/serial/ppp0/authentication 
 - config: 
 authentication: 
 method: pap 
 ""list-name"": foobar'"
"def test_cache_deactivated_get_data(config_stub, tmpdir): 
    config_stub.data = {'storage': {'cache-size': 1024}, 'general': {'private-browsing': True}} 
   disk_cache = cache.DiskCache(str(tmpdir)) 
   url = QUrl('http://www.example.com/') 
   assert (disk_cache.data(url) is None)", 'Test that the disk cache is deactivated when private browsing is enabled.','Query some data from a deactivated cache.'
"@hook.command('wordusage', 'wordexample', 'usage') 
 def word_usage(text): 
    if (not api_key): 
      return 'This   command   requires   an   API   key   from   wordnik.com.' 
   word = sanitize(text) 
   url = (API_URL + 'word.json/{}/examples'.format(word)) 
   params = {'api_key': api_key, 'limit': 10} 
   json = requests.get(url, params=params).json() 
   if json: 
      out = '\x02{}\x02:   '.format(word) 
      example = random.choice(json['examples']) 
      out += '{}   '.format(example['text']) 
      return '   '.join(out.split()) 
   else: 
      return 'I   could   not   find   any   usage   examples   for   \x02{}\x02.'.format(word)"," 'Example usage: 
 /wordusage ""hello"" 
 /wordusage ""world"" 
 /wordusage ""hello world"" 
 /wordusage ""hello   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,   world"" 
 /wordusage ""hello,",'<word> -- Returns an example sentence showing the usage of <word>.'
"def ui_open(*files): 
    if files: 
      osname = get_os_name() 
      opener = _OPENER_BY_OS.get(osname) 
      if opener: 
         opener(files) 
      else: 
         raise OpenError((u'Open   currently   not   supported   for   ' + osname))", 'Open the given file(s) in the default application.',"'Attempts to open the given files using the preferred desktop viewer or editor. 
 :raises :class:`OpenError`: if there is a problem opening any of the files.'"
"def is_pure_elemwise(graph, inputs): 
    allowed_ops = (tensor.basic.DimShuffle, tensor.basic.Elemwise) 
   owner = graph.owner 
   op = (graph.owner.op if (graph.owner is not None) else None) 
   if ((owner is None) and (graph in inputs)): 
      return True 
   elif ((owner is None) and isinstance(graph, tensor.basic.TensorConstant)): 
      return True 
   elif ((owner is None) and (graph not in inputs)): 
      return False 
   elif ((op is not None) and (not isinstance(op, allowed_ops))): 
      return False 
   else: 
      if isinstance(graph.owner.op, tensor.basic.DimShuffle): 
         shuffled = graph.owner.inputs[0] 
         if (not isinstance(shuffled, tensor.basic.TensorConstant)): 
            return False 
      for inp in graph.owner.inputs: 
         if (not is_pure_elemwise(inp, inputs)): 
            return False 
      return True", 'Return True if the graph is a pure elemwise operation.',"'Checks whether a graph is purely elementwise and containing only 
 inputs from a given list. 
 Parameters 
 graph : TensorVariable object 
 Graph to perform checks against. 
 inputs : list 
 List of acceptable inputs to the graph. 
 Returns 
 elemwise_or_not : bool 
 Returns `True` if 
 a) everything in the graph is an Elemwise or a DimShuffle 
 (DimShuffles are only acceptable to broadcast up constants) 
 and 
 b) all nodes without an owner appear in `inputs` or are 
 constants. 
 Returns `False` otherwise.'"
"def audio_codec(): 
    rebulk = Rebulk().regex_defaults(flags=re.IGNORECASE, abbreviations=[dash]).string_defaults(ignore_case=True) 
   def audio_codec_priority(match1, match2): 
      '\n                        Gives   priority   to   audio_codec\n                        :param   match1:\n                        :type   match1:\n                        :param   match2:\n                        :type   match2:\n                        :return:\n                        :rtype:\n                        ' 
      if ((match1.name == 'audio_codec') and (match2.name in ['audio_profile', 'audio_channels'])): 
         return match2 
      if ((match1.name in ['audio_profile', 'audio_channels']) and (match2.name == 'audio_codec')): 
         return match1 
      return '__default__' 
   rebulk.defaults(name='audio_codec', conflict_solver=audio_codec_priority) 
   rebulk.regex('MP3', 'LAME', 'LAME(?:\\d)+-?(?:\\d)+', value='MP3') 
   rebulk.regex('Dolby', 'DolbyDigital', 'Dolby-Digital', 'DD', value='DolbyDigital') 
   rebulk.regex('DolbyAtmos', 'Dolby-Atmos', 'Atmos', value='DolbyAtmos') 
   rebulk.regex('AAC', value='AAC') 
   rebulk.regex('AC3D?', value='AC3') 
   rebulk.regex('Flac', value='FLAC') 
   rebulk.regex('DTS', value='DTS') 
   rebulk.regex('True-?HD', value='TrueHD') 
   rebulk.defaults(name='audio_profile') 
   rebulk.string('HD', value='HD', tags='DTS') 
   rebulk.regex('HD-?MA', value='HDMA', tags='DTS') 
   rebulk.string('HE', value='HE', tags='AAC') 
   rebulk.string('LC', value='LC', tags='AAC') 
   rebulk.string('HQ', value='HQ', tags='AC3') 
   rebulk.defaults(name='audio_channels') 
   rebulk.regex('(7[\\W_][01](?:ch)?)(?:[^\\d]|$)', value='7.1', children=True) 
   rebulk.regex('(5[\\W_][01](?:ch)?)(?:[^\\d]|$)', value='5.1', children=True) 
   rebulk.regex('(2[\\W_]0(?:ch)?)(?:[^\\d]|$)', value='2.0', children=True) 
   rebulk.string('7ch', '8ch', value='7.1') 
   rebulk.string('5ch', '6ch', value='5.1') 
   rebulk.string('2ch', 'stereo', value='2.0') 
   rebulk.string('1ch', 'mono', value='1.0') 
   rebulk.rules(DtsRule, AacRule, Ac3Rule, AudioValidatorRule, HqConflictRule) 
   return rebulk", 'Returns a rebulk instance with default audio codecs.',"'Builder for rebulk object. 
 :return: Created Rebulk object 
 :rtype: Rebulk'"
"def get_bucket(conn, bucket_id): 
    bucket = conn.get_bucket(bucket_id) 
   if (not bucket): 
      msg = (_('Could   not   find   bucket   with   ID   %(bucket_id)s') % locals()) 
      LOG.debug(msg) 
      raise exception.NotFound(msg) 
   return bucket", 'Get bucket from the connection.',"'Get a bucket from an s3 connection 
 :param conn: The ``boto.s3.connection.S3Connection`` 
 :param bucket_id: ID of the bucket to fetch 
 :raises ``glance.exception.NotFound`` if bucket is not found.'"
"def range_error_message(error_message, what_to_enter, minimum, maximum): 
    if (error_message is None): 
      error_message = ('Enter   ' + what_to_enter) 
      if ((minimum is not None) and (maximum is not None)): 
         error_message += '   between   %(min)g   and   %(max)g' 
      elif (minimum is not None): 
         error_message += '   greater   than   or   equal   to   %(min)g' 
      elif (maximum is not None): 
         error_message += '   less   than   or   equal   to   %(max)g' 
   if (type(maximum) in [int, long]): 
      maximum -= 1 
   return (translate(error_message) % dict(min=minimum, max=maximum))"," 'Generate an error message for a range error. 
 The error message will be translated into the user's language. 
 Parameters 
 error_message : str or None 
 The error message to use. 
 what_to_enter : str 
 The object that the user is supposed to enter. 
 minimum : float 
 The minimum value of the range. 
 maximum : float 
 The maximum value of the range. 
 Returns 
 str 
 The error message.'",'build the error message for the number range validators'
"def instance_get_floating_address(context, instance_id): 
    return IMPL.instance_get_floating_address(context, instance_id)", 'Get floating IP address for instance.','Get the first floating ip address of an instance.'
"def _authenticate_mongo_cr(credentials, sock_info): 
    source = credentials.source 
   username = credentials.username 
   password = credentials.password 
   response = sock_info.command(source, {'getnonce': 1}) 
   nonce = response['nonce'] 
   key = _auth_key(nonce, username, password) 
   query = SON([('authenticate', 1), ('user', username), ('nonce', nonce), ('key', key)]) 
   sock_info.command(source, query)", 'Authenticate a Mongo connection with the given credentials.','Authenticate using MONGODB-CR.'
"def entails(expr, formula_set={}): 
    formula_set = list(formula_set) 
   formula_set.append(Not(expr)) 
   return (not satisfiable(And(*formula_set)))"," 'Return True if the formula expr is not satisfiable, False otherwise. 
 If expr is a conjunction, it is tested against the conjuncts of formula_set. 
 If expr is a disjunction, it is tested against the disjuncts of formula_set. 
 If expr is a negation, it is tested against the negated formula in formula_set. 
 If expr is a conditional, it is tested against the formula in formula_set 
 that is the antecedent of expr. 
 If expr is a conjunction of conditions, it is tested against the formulas in 
 formula_set that are the antecedents of the conditions. 
 If expr is a disjunction of conditions, it is tested against the formulas in 
 formula_set that are the antecedents of the conditions. 
 If expr is a negation of a condition, it is tested against the formula in 
 formula_set that is the antecedent of expr. 
 If expr is a conditional of a conjunction, it is tested against the formula in 
 formula_set that is the antecedent of expr. 
 If expr is a conditional of a dis","'Check whether the given expr_set entail an expr. 
 If formula_set is empty then it returns the validity of expr. 
 Examples 
 >>> from sympy.abc import A, B, C 
 >>> from sympy.logic.inference import entails 
 >>> entails(A, [A >> B, B >> C]) 
 False 
 >>> entails(C, [A >> B, B >> C, A]) 
 True 
 >>> entails(A >> B) 
 False 
 >>> entails(A >> (B >> A)) 
 True 
 References 
 .. [1] http://en.wikipedia.org/wiki/Logical_consequence'"
"def spawn_raw(function, *args, **kwargs): 
    if (not callable(function)): 
      raise TypeError('function   must   be   callable') 
   hub = get_hub() 
   if kwargs: 
      function = _functools_partial(function, *args, **kwargs) 
      g = RawGreenlet(function, hub) 
      hub.loop.run_callback(g.switch) 
   else: 
      g = RawGreenlet(function, hub) 
      hub.loop.run_callback(g.switch, *args) 
   return g"," 'Spawn a greenlet and return it. 
 This is a convenience function to spawn a greenlet with the given function. 
 It is similar to the ``spawn`` function, but does not take any arguments. 
 The function is passed to the greenlet, and the greenlet is returned. 
 The greenlet is spawned with the ``hub.loop.run_callback`` function. 
 :param function: the function to be executed. 
 :param args: arguments to pass to the function. 
 :param kwargs: keyword arguments to pass to the function. 
 :returns: the greenlet that was spawned. 
 :rtype: :class:`greenlet.greenlet.RawGreenlet`'","'Create a new :class:`greenlet.greenlet` object and schedule it to 
 run ``function(*args, **kwargs)``. 
 This returns a raw :class:`~greenlet.greenlet` which does not have all the useful 
 methods that :class:`gevent.Greenlet` has. Typically, applications 
 should prefer :func:`~gevent.spawn`, but this method may 
 occasionally be useful as an optimization if there are many 
 greenlets involved. 
 .. versionchanged:: 1.1b1 
 If *function* is not callable, immediately raise a :exc:`TypeError` 
 instead of spawning a greenlet that will raise an uncaught TypeError. 
 .. versionchanged:: 1.1rc2 
 Accept keyword arguments for ``function`` as previously (incorrectly) 
 documented. Note that this may incur an additional expense. 
 .. versionchanged:: 1.1a3 
 Verify that ``function`` is callable, raising a TypeError if not. Previously, 
 the spawned greenlet would have failed the first time it was switched to.'"
"def isproxy(obj): 
    return orig_isinstance(obj, NetProxy)", 'Returns True if obj is a NetProxy object','indicates whether the given object is a NetProxy'
"def _to_app_identity_error(error): 
    error_map = {app_identity_service_pb.AppIdentityServiceError.NOT_A_VALID_APP: InternalError, app_identity_service_pb.AppIdentityServiceError.DEADLINE_EXCEEDED: BackendDeadlineExceeded, app_identity_service_pb.AppIdentityServiceError.BLOB_TOO_LARGE: BlobSizeTooLarge, app_identity_service_pb.AppIdentityServiceError.UNKNOWN_ERROR: InternalError, app_identity_service_pb.AppIdentityServiceError.UNKNOWN_SCOPE: InvalidScope, app_identity_service_pb.AppIdentityServiceError.NOT_ALLOWED: NotAllowed, app_identity_service_pb.AppIdentityServiceError.NOT_IMPLEMENTED: OperationNotImplemented} 
   if (error.application_error in error_map): 
      return error_map[error.application_error](error.error_detail) 
   else: 
      return InternalError(('%s:   %s' % (error.application_error, error.error_detail)))", 'Convert an AppIdentityServiceError to a more useful error.',"'Translate an application error to an external Error, if possible. 
 Args: 
 error: An ApplicationError to translate. 
 Returns: 
 error: app identity API specific error message.'"
"def get_argument_from_call(callfunc_node, position=None, keyword=None): 
    if ((position is None) and (keyword is None)): 
      raise ValueError('Must   specify   at   least   one   of:   position   or   keyword.') 
   try: 
      if ((position is not None) and (not isinstance(callfunc_node.args[position], astroid.Keyword))): 
         return callfunc_node.args[position] 
   except IndexError as error: 
      raise NoSuchArgumentError(error) 
   if keyword: 
      for arg in callfunc_node.args: 
         if (isinstance(arg, astroid.Keyword) and (arg.arg == keyword)): 
            return arg.value 
   raise NoSuchArgumentError"," 'Returns the argument that was passed to the call, or None if it 
 was not found. 
 :param callfunc_node: the call node 
 :param position: the position of the argument to find, or None 
 :param keyword: the keyword of the argument to find, or None 
 :return: the argument that was passed to the call, or None'","'Returns the specified argument from a function call. 
 :param callfunc_node: Node representing a function call to check. 
 :param int position: position of the argument. 
 :param str keyword: the keyword of the argument. 
 :returns: The node representing the argument, None if the argument is not found. 
 :raises ValueError: if both position and keyword are None. 
 :raises NoSuchArgumentError: if no argument at the provided position or with 
 the provided keyword.'"
"def dynamic_class_import(class_path): 
    try: 
      tmp = class_path.split('.') 
      module_path = '.'.join(tmp[0:(-1)]) 
      package = __import__(module_path) 
      return reduce(getattr, tmp[1:], package) 
   except Exception as e: 
      log.error(LOGMSG_ERR_FAB_ADDON_IMPORT.format(class_path, e))"," 'Import a class from a dynamic class path. 
 :param class_path: The class path to import. 
 :return: The imported class. 
 :raises: :py:exc:`FabricAddonImportError` if the class cannot be imported.'","'Will dynamically import a class from a string path 
 :param class_path: string with class path 
 :return: class'"
"def _decomp_quad(P, cond=None, rcond=None, lower=True, check_finite=True): 
    (w, V) = LA.eigh(P, lower=lower, check_finite=check_finite) 
   if (rcond is not None): 
      cond = rcond 
   if (cond in (None, (-1))): 
      t = V.dtype.char.lower() 
      factor = {'f': 1000.0, 'd': 1000000.0} 
      cond = (factor[t] * np.finfo(t).eps) 
   scale = max(np.absolute(w)) 
   w_scaled = (w / scale) 
   maskp = (w_scaled > cond) 
   maskn = (w_scaled < (- cond)) 
   if (np.any(maskp) and np.any(maskn)): 
      warnings.warn('Forming   a   nonconvex   expression   quad_form(x,   indefinite).') 
   M1 = (V[:, maskp] * np.sqrt(w_scaled[maskp])) 
   M2 = (V[:, maskn] * np.sqrt((- w_scaled[maskn]))) 
   return (scale, M1, M2)"," 'Decompose the symmetric positive definite matrix ``P`` into 
 ``M1`` and ``M2``, where ``M1`` and ``M2`` are the matrices 
 defined as follows: 
 .. math:: 
 M_1 = \sqrt{w} V_{1,1} 
 M_2 = \sqrt{w} V_{2,2} 
 The decomposition is performed using the QZ algorithm. 
 Parameters 
 P : ndarray 
 The symmetric positive definite matrix. 
 cond : float, optional 
 The relative condition number of the matrix. 
 rcond : float, optional 
 The relative condition number of the matrix. 
 lower : bool, optional 
 If ``True``, the lower triangular part of ``P`` is used. 
 check_finite : bool, optional 
 If ``True``, the matrix is checked for numerical problems. 
 Returns 
 scale : float 
 The scale factor of the matrix. 
 M1 : ndarray 
 The first part of the matrix. 
 M2 : ndarray 
 The second part of the matrix. 
 Examples 
 >>>","'Compute a matrix decomposition. 
 Compute sgn, scale, M such that P = sgn * scale * dot(M, M.T). 
 The strategy of determination of eigenvalue negligibility follows 
 the pinvh contributions from the scikit-learn project to scipy. 
 Parameters 
 P : matrix or ndarray 
 A real symmetric positive or negative (semi)definite input matrix 
 cond, rcond : float, optional 
 Cutoff for small eigenvalues. 
 Singular values smaller than rcond * largest_eigenvalue 
 are considered negligible. 
 If None or -1, suitable machine precision is used (default). 
 lower : bool, optional 
 Whether the array data is taken from the lower or upper triangle of P. 
 The default is to take it from the lower triangle. 
 check_finite : bool, optional 
 Whether to check that the input matrix contains only finite numbers. 
 The default is True; disabling may give a performance gain 
 but may result in problems (crashes, non-termination) if the inputs 
 contain infinities or NaNs. 
 Returns 
 scale : float 
 induced matrix 2-norm of P 
 M1, M2 : 2d ndarray 
 A rectangular ndarray such that P = scale * (dot(M1, M1.T) - dot(M2, M2.T))'"
"def writeOutput(fileName=''): 
    fileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName) 
   if (fileName != ''): 
      skeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')", 'Write the output to the specified file.','Unpause a gcode linear move file.'
"def _putResultInDeferred(reactor, deferred, f, args, kwargs): 
    try: 
      result = f(*args, **kwargs) 
   except Exception: 
      f = failure.Failure() 
      reactor.callFromThread(deferred.errback, f) 
   else: 
      reactor.callFromThread(deferred.callback, result)", 'Put the result of a function call in a deferred.','Run a function and give results to a Deferred.'
"def fft(x, n=None, axis=(-1), overwrite_x=False): 
    tmp = _asfarray(x) 
   try: 
      work_function = _DTYPE_TO_FFT[tmp.dtype] 
   except KeyError: 
      raise ValueError(('type   %s   is   not   supported' % tmp.dtype)) 
   if (not (istype(tmp, numpy.complex64) or istype(tmp, numpy.complex128))): 
      overwrite_x = 1 
   overwrite_x = (overwrite_x or _datacopied(tmp, x)) 
   if (n is None): 
      n = tmp.shape[axis] 
   elif (n != tmp.shape[axis]): 
      (tmp, copy_made) = _fix_shape(tmp, n, axis) 
      overwrite_x = (overwrite_x or copy_made) 
   if (n < 1): 
      raise ValueError(('Invalid   number   of   FFT   data   points   (%d)   specified.' % n)) 
   if ((axis == (-1)) or (axis == (len(tmp.shape) - 1))): 
      return work_function(tmp, n, 1, 0, overwrite_x) 
   tmp = swapaxes(tmp, axis, (-1)) 
   tmp = work_function(tmp, n, 1, 0, overwrite_x) 
   return swapaxes(tmp, axis, (-1))"," 'Fast Fourier Transform of an array. 
 Parameters 
 x : array_like 
 Input array. 
 n : int, optional 
 The number of data points to use in the FFT. If `n` is not specified, 
 the length of the input array is used. 
 axis : int, optional 
 The axis along which to perform the FFT. If `axis` is `None`, the 
 FFT is performed along the last axis. 
 overwrite_x : bool, optional 
 If `True`, the input array `x` is overwritten. 
 Returns 
 out : ndarray 
 The FFT of `x`. 
 Raises 
 ValueError 
 If `n` is not an integer or if `n` is less than 1. 
 See Also 
 fftconvolve 
 Notes 
 The FFT is performed using the FFTW library. 
 Examples 
 >>> x = np.arange(10) 
 >>> fft(x) 
 array([ 1.,  1.,  1.,  1.,  1.,  1.,  1","'Return discrete Fourier transform of real or complex sequence. 
 The returned complex array contains ``y(0), y(1),..., y(n-1)`` where 
 ``y(j) = (x * exp(-2*pi*sqrt(-1)*j*np.arange(n)/n)).sum()``. 
 Parameters 
 x : array_like 
 Array to Fourier transform. 
 n : int, optional 
 Length of the Fourier transform.  If ``n < x.shape[axis]``, `x` is 
 truncated.  If ``n > x.shape[axis]``, `x` is zero-padded. The 
 default results in ``n = x.shape[axis]``. 
 axis : int, optional 
 Axis along which the fft\'s are computed; the default is over the 
 last axis (i.e., ``axis=-1``). 
 overwrite_x : bool, optional 
 If True, the contents of `x` can be destroyed; the default is False. 
 Returns 
 z : complex ndarray 
 with the elements:: 
 [y(0),y(1),..,y(n/2),y(1-n/2),...,y(-1)]        if n is even 
 [y(0),y(1),..,y((n-1)/2),y(-(n-1)/2),...,y(-1)]  if n is odd 
 where:: 
 y(j) = sum[k=0..n-1] x[k] * exp(-sqrt(-1)*j*k* 2*pi/n), j = 0..n-1 
 Note that ``y(-j) = y(n-j).conjugate()``. 
 See Also 
 ifft : Inverse FFT 
 rfft : FFT of a real sequence 
 Notes 
 The packing of the result is ""standard"": If ``A = fft(a, n)``, then 
 ``A[0]`` contains the zero-frequency term, ``A[1:n/2]`` contains the 
 positive-frequency terms, and ``A[n/2:]`` contains the negative-frequency 
 terms, in order of decreasingly negative frequency. So for an 8-point 
 transform, the frequencies of the result are [0, 1, 2, 3, -4, -3, -2, -1]. 
 To rearrange the fft output so that the zero-frequency component is 
 centered, like [-4, -3, -2, -1,  0,  1,  2,  3], use `fftshift`. 
 For `n` even, ``A[n/2]`` contains the sum of the positive and 
 negative-frequency terms.  For `n` even and `x` real, ``A[n/2]`` will 
 always be real. 
 Both single and double precision routines are implemented.  Half precision 
 inputs will be converted to single precision.  Non floating-point inputs 
 will be converted to double precision.  Long-double precision inputs are 
 not supported. 
 This function is most efficient when `n` is a power of two, and least 
 efficient when `n` is prime. 
 If the data type of `x` is real, a ""real FFT"" algorithm is automatically 
 used, which roughly halves the computation time.  To increase efficiency 
 a little further, use `rfft`, which does the same calculation, but only 
 outputs half of the symmetrical spectrum.  If the data is both real and 
 symmetrical, the `dct` can again double the efficiency, by generating 
 half of the spectrum from half of the signal. 
 Examples 
 >>> from scipy.fftpack import fft, ifft 
 >>> x = np.arange(5) 
 >>> np.allclose(fft(ifft(x)), x, atol=1e-15)  # within numerical accuracy. 
 True'"
"def setup(): 
    os.makedirs(os.path.join(HOME_TEST_DIR, 'ipython'))", 'Create a directory for IPython tests',"'Setup testenvironment for the module: 
 - Adds dummy home dir tree'"
"def make_logged_in_client(username='test', password='test', is_superuser=True, recreate=False, groupname=None): 
    try: 
      user = User.objects.get(username=username) 
      if recreate: 
         user.delete() 
         raise User.DoesNotExist 
   except User.DoesNotExist: 
      user = User.objects.create_user(username, (username + '@localhost'), password) 
      user.is_superuser = is_superuser 
      user.save() 
   else: 
      if (user.is_superuser != is_superuser): 
         user.is_superuser = is_superuser 
         user.save() 
   if (groupname is not None): 
      (group, created) = Group.objects.get_or_create(name=groupname) 
      if (not user.groups.filter(name=group.name).exists()): 
         user.groups.add(group) 
         user.save() 
   c = Client() 
   ret = c.login(username=username, password=password) 
   assert ret, (""Login   failed   (user   '%s')."" % username) 
   return c"," 'Create a logged-in client. 
 :param username: Username to use for login. 
 :param password: Password to use for login. 
 :param is_superuser: Whether the user is a superuser. 
 :param recreate: Whether to delete the user before creating it. 
 :param groupname: Name of the group to add to the user. 
 :returns: A client for the logged-in user. 
 :rtype: :class:`django.contrib.auth.models.User` 
 :raises User.DoesNotExist: If the user does not exist. 
 :raises django.contrib.auth.exceptions.AuthenticationFailed: If the 
 login fails.'","'Create a client with a user already logged in. 
 Sometimes we recreate the user, because some tests like to 
 mess with is_active and such.'"
"def test_invalid_sigma_clip(): 
    data = np.ones((5, 5)) 
   data[(2, 2)] = 1000 
   data[(3, 4)] = np.nan 
   data[(1, 1)] = np.inf 
   result = sigma_clip(data) 
   assert result.mask[(2, 2)] 
   assert result.mask[(3, 4)] 
   assert result.mask[(1, 1)]"," 'Test that the sigma_clip function does not clip values outside the 
 specified sigma range'",'Test sigma_clip of data containing invalid values.'
"def create_gzip_cache(pelican): 
    for (dirpath, _, filenames) in os.walk(pelican.settings['OUTPUT_PATH']): 
      for name in filenames: 
         if should_compress(name): 
            filepath = os.path.join(dirpath, name) 
            create_gzip_file(filepath, should_overwrite(pelican.settings))"," 'Create a gzip cache for the output directory. 
 :param pelican: Pelican instance. 
 :return: None.'","'Create a gzip cache file for every file that a webserver would 
 reasonably want to cache (e.g., text type files). 
 :param pelican: The Pelican instance'"
"@removals.remove(message='Use   keystoneclient.session.request   instead.', version='1.7.0', removal_version='2.0.0') 
 def request(*args, **kwargs): 
    return client_session.request(*args, **kwargs)", 'Use keystoneclient.session.request instead.',"'Make a request. 
 This function is deprecated as of the 1.7.0 release in favor of 
 :func:`keystoneclient.session.request` and may be removed in the 
 2.0.0 release.'"
"def unique_substrings(l, min_size=1): 
    return [unique_substring(s, [x for x in l if (x is not s)], min_size) for s in l]"," 'Return a list of unique substrings of ``l`` of length at least ``min_size``. 
 The substrings are not necessarily contiguous. 
 >>> unique_substrings([1, 2, 3, 1, 2, 3]) 
 [1, 2, 3, 1, 2, 3] 
 >>> unique_substrings([1, 2, 3, 1, 2, 3], min_size=3) 
 [1, 2, 3] 
 >>> unique_substrings([1, 2, 3, 1, 2, 3], min_size=1) 
 [1, 2, 3] 
 >>> unique_substrings([1, 2, 3, 1, 2, 3], min_size=0) 
 []'","'.. todo:: 
 WRITEME'"
"def event_source_mapping_present(name, EventSourceArn, FunctionName, StartingPosition, Enabled=True, BatchSize=100, region=None, key=None, keyid=None, profile=None): 
    ret = {'name': None, 'result': True, 'comment': '', 'changes': {}} 
   r = __salt__['boto_lambda.event_source_mapping_exists'](EventSourceArn=EventSourceArn, FunctionName=FunctionName, region=region, key=key, keyid=keyid, profile=profile) 
   if ('error' in r): 
      ret['result'] = False 
      ret['comment'] = 'Failed   to   create   event   source   mapping:   {0}.'.format(r['error']['message']) 
      return ret 
   if (not r.get('exists')): 
      if __opts__['test']: 
         ret['comment'] = 'Event   source   mapping   {0}   is   set   to   be   created.'.format(FunctionName) 
         ret['result'] = None 
         return ret 
      r = __salt__['boto_lambda.create_event_source_mapping'](EventSourceArn=EventSourceArn, FunctionName=FunctionName, StartingPosition=StartingPosition, Enabled=Enabled, BatchSize=BatchSize, region=region, key=key, keyid=keyid, profile=profile) 
      if (not r.get('created')): 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   create   event   source   mapping:   {0}.'.format(r['error']['message']) 
         return ret 
      _describe = __salt__['boto_lambda.describe_event_source_mapping'](EventSourceArn=EventSourceArn, FunctionName=FunctionName, region=region, key=key, keyid=keyid, profile=profile) 
      ret['name'] = _describe['event_source_mapping']['UUID'] 
      ret['changes']['old'] = {'event_source_mapping': None} 
      ret['changes']['new'] = _describe 
      ret['comment'] = 'Event   source   mapping   {0}   created.'.format(ret['name']) 
      return ret 
   ret['comment'] = os.linesep.join([ret['comment'], 'Event   source   mapping   is   present.']) 
   ret['changes'] = {} 
   _describe = __salt__['boto_lambda.describe_event_source_mapping'](EventSourceArn=EventSourceArn, FunctionName=FunctionName, region=region, key=key, keyid=keyid, profile=profile)['event_source_mapping'] 
   need_update = False 
   options = {'BatchSize': 'BatchSize'} 
   for (val, var) in six.iteritems(options): 
      if (_describe[val] != locals()[var]): 
         need_update = True 
         ret['changes'].setdefault('new', {})[var] = locals()[var] 
         ret['changes'].setdefault('old', {})[var] = _describe[val] 
   function_arn = _get_function_arn(FunctionName, region=region, key=key, keyid=keyid, profile=profile) 
   if (_describe['FunctionArn'] != function_arn): 
      need_update = True 
      ret['changes'].setdefault('new', {})['FunctionArn'] = function_arn 
      ret['changes'].setdefault('old', {})['FunctionArn'] = _describe['FunctionArn'] 
   if need_update: 
      ret['comment'] = os.linesep.join([ret['comment'], 'Event   source   mapping   to   be   modified']) 
      if __opts__['test']: 
         msg = 'Event   source   mapping   {0}   set   to   be   modified.'.format(_describe['UUID']) 
         ret['comment'] = msg 
         ret['result'] = None 
         return ret 
      _r = __salt__['boto_lambda.update_event_source_mapping'](UUID=_describe['UUID'], FunctionName=FunctionName, Enabled=Enabled, BatchSize=BatchSize, region=region, key=key, keyid=keyid, profile=profile) 
      if (not _r.get('updated')): 
         ret['result'] = False 
         ret['comment'] = 'Failed   to   update   mapping:   {0}.'.format(_r['error']['message']) 
         ret['changes'] = {} 
   return ret"," 'Modify an event source mapping. 
 This function is used to modify an event source mapping. 
 This function returns a dictionary containing the following keys: 
 name: The name of the event source mapping. 
 result: True if the event source mapping was modified, False otherwise. 
 comment: A comment about the result. 
 changes: A dictionary containing the old and new values of the event source mapping. 
 FunctionArn: The ARN of the Lambda function. 
 FunctionName: The name of the Lambda function. 
 StartingPosition: The starting position for the event source mapping. 
 Enabled: True if the event source mapping is enabled, False otherwise. 
 BatchSize: The batch size for the event source mapping. 
 region: The AWS region. 
 key: The AWS access key ID. 
 keyid: The AWS secret access key. 
 profile: The AWS profile. 
 Returns: A dictionary containing the above keys. 
 Examples: 
 # Create an event source mapping. 
 event_source_mapping_present(name=""test"", 
                               EventSourceArn=""arn:aws:sqs:us","'Ensure event source mapping exists. 
 name 
 The name of the state definition. 
 EventSourceArn 
 The Amazon Resource Name (ARN) of the Amazon Kinesis or the Amazon 
 DynamoDB stream that is the event source. 
 FunctionName 
 The Lambda function to invoke when AWS Lambda detects an event on the 
 stream. 
 You can specify an unqualified function name (for example, ""Thumbnail"") 
 or you can specify Amazon Resource Name (ARN) of the function (for 
 example, ""arn:aws:lambda:us-west-2:account-id:function:ThumbNail""). AWS 
 Lambda also allows you to specify only the account ID qualifier (for 
 example, ""account-id:Thumbnail""). Note that the length constraint 
 applies only to the ARN. If you specify only the function name, it is 
 limited to 64 character in length. 
 StartingPosition 
 The position in the stream where AWS Lambda should start reading. 
 (TRIM_HORIZON | LATEST) 
 Enabled 
 Indicates whether AWS Lambda should begin polling the event source. By 
 default, Enabled is true. 
 BatchSize 
 The largest number of records that AWS Lambda will retrieve from your 
 event source at the time of invoking your function. Your function 
 receives an event with all the retrieved records. The default is 100 
 records. 
 region 
 Region to connect to. 
 key 
 Secret key to be used. 
 keyid 
 Access key to be used. 
 profile 
 A dict with region, key and keyid, or a pillar key (string) that 
 contains a dict with region, key and keyid.'"
"def getCraftedTextFromText(gcodeText, exportRepository=None): 
    if gcodec.isProcedureDoneOrFileIsEmpty(gcodeText, 'export'): 
      return gcodeText 
   if (exportRepository == None): 
      exportRepository = settings.getReadRepository(ExportRepository()) 
   if (not exportRepository.activateExport.value): 
      return gcodeText 
   return ExportSkein().getCraftedGcode(exportRepository, gcodeText)"," 'Returns the text of the exported GCode from the given GCode text. 
 :param gcodeText: Text of the GCode to export. 
 :param exportRepository: The repository to use for exporting. 
 :return: The exported GCode text.'",'Export a gcode linear move text.'
"def test_gnb_pfit_wrong_nb_features(): 
    clf = GaussianNB() 
   clf.fit(X, y) 
   assert_raises(ValueError, clf.partial_fit, np.hstack((X, X)), y)", 'Test that a GaussianNB can\'t be used with a wrong number of features.',"'Test whether an error is raised when the number of feature changes 
 between two partial fit'"
"def _handle_key(key): 
    code = int(key[(key.index(u'k') + 1):]) 
   value = chr(code) 
   if ((code >= 65) and (code <= 90)): 
      if (u'shift+' in key): 
         key = key.replace(u'shift+', u'') 
      else: 
         value = value.lower() 
   elif ((code >= 48) and (code <= 57)): 
      if (u'shift+' in key): 
         value = u')!@#$%^&*('[int(value)] 
         key = key.replace(u'shift+', u'') 
   elif ((code >= 112) and (code <= 123)): 
      value = (u'f%s' % (code - 111)) 
   elif ((code >= 96) and (code <= 105)): 
      value = (u'%s' % (code - 96)) 
   elif ((code in _SHIFT_LUT) and (u'shift+' in key)): 
      key = key.replace(u'shift+', u'') 
      value = _SHIFT_LUT[code] 
   elif (code in _LUT): 
      value = _LUT[code] 
   key = (key[:key.index(u'k')] + value) 
   return key"," 'Process a keypress, returning a keypress string. 
 :param key: A string of the keypress. 
 :return: A string of the keypress.'",'Handle key codes'
"@mock_ec2 
 def test_igw_desribe(): 
    conn = boto.connect_vpc(u'the_key', u'the_secret') 
   igw = conn.create_internet_gateway() 
   igw_by_search = conn.get_all_internet_gateways([igw.id])[0] 
   igw.id.should.equal(igw_by_search.id)"," 'Test that the create_internet_gateway method returns a valid 
 InternetGateway object'",'internet gateway fetch by id'
"def rpc(cmd=None, dest=None, format='xml', *args, **kwargs): 
    conn = __proxy__['junos.conn']() 
   ret = dict() 
   ret['out'] = True 
   op = dict() 
   if ('__pub_arg' in kwargs): 
      if isinstance(kwargs['__pub_arg'][(-1)], dict): 
         op.update(kwargs['__pub_arg'][(-1)]) 
   else: 
      op.update(kwargs) 
   if ((dest is None) and (format != 'xml')): 
      log.warning('Format   ignored   as   it   is   only   used   for   output   which   is   dumped   in   the   file.') 
   write_response = '' 
   try: 
      if (cmd in ['get-config', 'get_config']): 
         filter_reply = None 
         if ('filter' in op): 
            filter_reply = etree.XML(op['filter']) 
         xml_reply = getattr(conn.rpc, cmd.replace('-', '_'))(filter_reply, options=op) 
         ret['message'] = jxmlease.parse(etree.tostring(xml_reply)) 
         write_response = etree.tostring(xml_reply) 
         if ((dest is not None) and (format != 'xml')): 
            op.update({'format': format}) 
            rpc_reply = getattr(conn.rpc, cmd.replace('-', '_'))(filter_reply, options=op) 
            if (format == 'json'): 
               write_response = json.dumps(rpc_reply, indent=1) 
            else: 
               write_response = rpc_reply.text 
      else: 
         xml_reply = getattr(conn.rpc, cmd.replace('-', '_'))(**op) 
         ret['message'] = jxmlease.parse(etree.tostring(xml_reply)) 
         write_response = etree.tostring(xml_reply) 
         if ((dest is not None) and (format != 'xml')): 
            rpc_reply = getattr(conn.rpc, cmd.replace('-', '_'))({'format': format}, **op) 
            if (format == 'json'): 
               write_response = json.dumps(rpc_reply, indent=1) 
            else: 
               write_response = rpc_reply.text 
   except Exception as exception: 
      ret['message'] = 'Execution   failed   due   to   ""{0}""'.format(exception) 
      ret['out'] = False 
   if (dest is not None): 
      with fopen(dest, 'w') as fp: 
         fp.write(write_response) 
   return ret"," 'RPC function to send commands to the Junos device. 
 :param cmd: Command to send. 
 :param dest: Destination for the command. 
 :param format: Format to send the command. 
 :return: Dictionary with the response.'","'This function executes the rpc provided as arguments on the junos device. 
 The returned data can be stored in a file whose destination can be 
 specified with \'dest\' keyword in the arguments. 
 Usage: 
 .. code-block:: bash 
 salt \'device\' junos.rpc \'get_config\' \'text\' filter=\'<configuration><system/></configuration>\' 
 salt \'device\' junos.rpc \'get-interface-information\' \'/home/user/interface.log\' interface_name=\'lo0\' terse=True 
 Options: 
 * cmd: the rpc to be executed 
 * dest: destination file where the rpc ouput is dumped 
 * format: the format in which the rpc reply must be stored in file specified in the dest (used only when dest is specified) 
 * args: other arguments as taken by rpc call of PyEZ 
 * kwargs: keyworded arguments taken by rpc call of PyEZ'"
"def read(results_file): 
    results = {} 
   if (not os.path.exists(results_file)): 
      raise IOError('Results   file   does   not   exist.') 
   with open(results_file) as handle: 
      lines = handle.readlines() 
   (results, num_params) = _parse_baseml.parse_basics(lines, results) 
   results = _parse_baseml.parse_parameters(lines, results, num_params) 
   if (results.get('version') is None): 
      raise ValueError('Invalid   results   file') 
   return results"," 'Reads the results file and returns the results dictionary. 
 :param results_file: Path to the results file. 
 :return: A dictionary with the results.'",'Parse a BASEML results file.'
"def export_stats(request, project, subproject): 
    subprj = get_subproject(request, project, subproject) 
   data = [trans.get_stats() for trans in subprj.translation_set.all()] 
   return export_response(request, ('stats-%s-%s.csv' % (subprj.project.slug, subprj.slug)), ('name', 'code', 'total', 'translated', 'translated_percent', 'total_words', 'translated_words', 'failing', 'failing_percent', 'fuzzy', 'fuzzy_percent', 'url_translate', 'url', 'last_change', 'last_author'), data)", 'Export stats for a subproject','Exports stats in JSON format.'
"@bp.route('/') 
 def nodes(): 
    nodes = Node.query.order_by(Node.updated.desc()).all() 
   return render_template('node/nodes.html', nodes=nodes)", 'List of nodes','Nodes pages.'
"@register.filter(is_safe=True) 
 @stringfilter 
 def striptags(value): 
    return strip_tags(value)", 'Strip tags from a string.','Strips all [X]HTML tags.'
"def delete_image(gce, name, module): 
    try: 
      gce.ex_delete_image(name) 
      return True 
   except ResourceNotFoundError: 
      return False 
   except GoogleBaseError as e: 
      module.fail_json(msg=str(e), changed=False)", 'Delete an image','Delete a specific image resource by name.'
"def _write_proc_history(fid, info): 
    if ('proc_history' not in info): 
      return 
   if (len(info['proc_history']) > 0): 
      start_block(fid, FIFF.FIFFB_PROCESSING_HISTORY) 
      for record in info['proc_history']: 
         start_block(fid, FIFF.FIFFB_PROCESSING_RECORD) 
         for (key, id_, writer) in zip(_proc_keys, _proc_ids, _proc_writers): 
            if (key in record): 
               writer(fid, id_, record[key]) 
         _write_maxfilter_record(fid, record['max_info']) 
         if ('smartshield' in record): 
            for ss in record['smartshield']: 
               start_block(fid, FIFF.FIFFB_SMARTSHIELD) 
               end_block(fid, FIFF.FIFFB_SMARTSHIELD) 
         end_block(fid, FIFF.FIFFB_PROCESSING_RECORD) 
      end_block(fid, FIFF.FIFFB_PROCESSING_HISTORY)"," 'Write the processing history. 
 Parameters 
 fid : file handle 
 info : dict 
 info dictionary 
 Returns 
 None 
 Notes 
 The processing history is a list of processing records. 
 Each processing record contains a list of processing steps, 
 which are either a dictionary or a list of dictionaries. 
 The processing steps are either a dictionary or a list of 
 dictionaries. 
 Each processing step is either a dictionary or a list of 
 dictionaries. 
 Each dictionary contains a key and a value. 
 The key is either a string or a list of strings. 
 The value is either a string or a list of strings. 
 The list of strings is a list of strings. 
 The list of dictionaries is a list of dictionaries. 
 The list of processing steps is a list of processing steps. 
 The list of processing records is a list of processing records.'",'Write processing history to file.'
"def test_finder_installs_pre_releases(data): 
    req = InstallRequirement.from_line('bar', None) 
   finder = PackageFinder([], [data.index_url('pre')], allow_all_prereleases=True, session=PipSession()) 
   link = finder.find_requirement(req, False) 
   assert link.url.endswith('bar-2.0b1.tar.gz'), link.url 
   links = ['https://foo/bar-1.0.tar.gz', 'https://foo/bar-2.0b1.tar.gz'] 
   finder = PackageFinder(links, [], allow_all_prereleases=True, session=PipSession()) 
   with patch.object(finder, '_get_pages', (lambda x, y: [])): 
      link = finder.find_requirement(req, False) 
      assert (link.url == 'https://foo/bar-2.0b1.tar.gz') 
   links.reverse() 
   finder = PackageFinder(links, [], allow_all_prereleases=True, session=PipSession()) 
   with patch.object(finder, '_get_pages', (lambda x, y: [])): 
      link = finder.find_requirement(req, False) 
      assert (link.url == 'https://foo/bar-2.0b1.tar.gz')", 'Tests that a pre-release can be found with the right URL.','Test PackageFinder finds pre-releases if asked to.'
"def get_config(): 
    profiles = {} 
   curr = None 
   cmd = ['netsh', 'advfirewall', 'show', 'allprofiles'] 
   for line in __salt__['cmd.run'](cmd, python_shell=False).splitlines(): 
      if (not curr): 
         tmp = re.search('(.*)   Profile   Settings:', line) 
         if tmp: 
            curr = tmp.group(1) 
      elif line.startswith('State'): 
         profiles[curr] = (line.split()[1] == 'ON') 
         curr = None 
   return profiles", 'Return a dict of the current firewall profiles',"'Get the status of all the firewall profiles 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' firewall.get_config'"
"def _fastq_solexa_convert_fastq_solexa(in_handle, out_handle, alphabet=None): 
    mapping = ''.join((([chr(0) for ascii in range(0, 59)] + [chr(ascii) for ascii in range(59, 127)]) + [chr(0) for ascii in range(127, 256)])) 
   assert (len(mapping) == 256) 
   return _fastq_generic(in_handle, out_handle, mapping)"," 'Convert fastq to fastq-solexa format. 
 :param in_handle: Input handle. 
 :param out_handle: Output handle. 
 :param alphabet: Alphabet to use.'","'Fast Solexa FASTQ to Solexa FASTQ conversion (PRIVATE). 
 Useful for removing line wrapping and the redundant second identifier 
 on the plus lines. Will check also check the quality string is valid. 
 Avoids creating SeqRecord and Seq objects in order to speed up this 
 conversion.'"
"def _comp_match(item, filter_, scope='collection'): 
    filter_length = len(filter_) 
   if (scope == 'collection'): 
      tag = item.collection.get_meta('tag') 
   else: 
      for component in item.components(): 
         if (component.name in ('VTODO', 'VEVENT', 'VJOURNAL')): 
            tag = component.name 
            break 
      else: 
         return False 
   if (filter_length == 0): 
      return (filter_.get('name') == tag) 
   else: 
      if (filter_length == 1): 
         if (filter_[0].tag == _tag('C', 'is-not-defined')): 
            return (filter_.get('name') != tag) 
      if (filter_[0].tag == _tag('C', 'time-range')): 
         if (not _time_range_match(item.item, filter_[0], tag)): 
            return False 
         filter_ = filter_[1:] 
      return all(((_prop_match(item, child) if (child.tag == _tag('C', 'prop-filter')) else _comp_match(item, child, scope='component')) for child in filter_))", 'Check if item matches filter_',"'Check whether the ``item`` matches the comp ``filter_``. 
 If ``scope`` is ``""collection""``, the filter is applied on the 
 item\'s collection. Otherwise, it\'s applied on the item. 
 See rfc4791-9.7.1.'"
"def register_translation(src_image, target_image, upsample_factor=1, space='real'): 
    if (src_image.shape != target_image.shape): 
      raise ValueError('Error:   images   must   be   same   size   for   register_translation') 
   if ((src_image.ndim != 2) and (upsample_factor > 1)): 
      raise NotImplementedError('Error:   register_translation   only   supports   subpixel   registration   for   2D   images') 
   if (space.lower() == 'fourier'): 
      src_freq = src_image 
      target_freq = target_image 
   elif (space.lower() == 'real'): 
      src_image = np.array(src_image, dtype=np.complex128, copy=False) 
      target_image = np.array(target_image, dtype=np.complex128, copy=False) 
      src_freq = np.fft.fftn(src_image) 
      target_freq = np.fft.fftn(target_image) 
   else: 
      raise ValueError('Error:   register_translation   only   knows   the   ""real""   and   ""fourier""   values   for   the   ``space``   argument.') 
   shape = src_freq.shape 
   image_product = (src_freq * target_freq.conj()) 
   cross_correlation = np.fft.ifftn(image_product) 
   maxima = np.unravel_index(np.argmax(np.abs(cross_correlation)), cross_correlation.shape) 
   midpoints = np.array([np.fix((axis_size / 2)) for axis_size in shape]) 
   shifts = np.array(maxima, dtype=np.float64) 
   shifts[(shifts > midpoints)] -= np.array(shape)[(shifts > midpoints)] 
   if (upsample_factor == 1): 
      src_amp = (np.sum((np.abs(src_freq) ** 2)) / src_freq.size) 
      target_amp = (np.sum((np.abs(target_freq) ** 2)) / target_freq.size) 
      CCmax = cross_correlation.max() 
   else: 
      shifts = (np.round((shifts * upsample_factor)) / upsample_factor) 
      upsampled_region_size = np.ceil((upsample_factor * 1.5)) 
      dftshift = np.fix((upsampled_region_size / 2.0)) 
      upsample_factor = np.array(upsample_factor, dtype=np.float64) 
      normalization = (src_freq.size * (upsample_factor ** 2)) 
      sample_region_offset = (dftshift - (shifts * upsample_factor)) 
      cross_correlation = _upsampled_dft(image_product.conj(), upsampled_region_size, upsample_factor, sample_region_offset).conj() 
      cross_correlation /= normalization 
      maxima = np.array(np.unravel_index(np.argmax(np.abs(cross_correlation)), cross_correlation.shape), dtype=np.float64) 
      maxima -= dftshift 
      shifts = (shifts + (maxima / upsample_factor)) 
      CCmax = cross_correlation.max() 
      src_amp = _upsampled_dft((src_freq * src_freq.conj()), 1, upsample_factor)[(0, 0)] 
      src_amp /= normalization 
      target_amp = _upsampled_dft((target_freq * target_freq.conj()), 1, upsample_factor)[(0, 0)] 
      target_amp /= normalization 
   for dim in range(src_freq.ndim): 
      if (shape[dim] == 1): 
         shifts[dim] = 0 
   return (shifts, _compute_error(CCmax, src_amp, target_amp), _compute_phasediff(CCmax))"," 'Registers a translation between two images. 
 Parameters 
 src_image : ndarray 
 Source image. 
 target_image : ndarray 
 Target image. 
 upsample_factor : int, optional 
 Upsample the source image by this factor. 
 space : \'real\' or \'fourier\', optional 
 The space in which to register the images. 
 Returns 
 shifts : ndarray 
 Translation vectors. 
 error : float 
 Translation error. 
 phasediff : float 
 Phase difference. 
 Examples 
 >>> from scipy.ndimage import shift_image 
 >>> from scipy.ndimage.filters import gaussian_filter 
 >>> from scipy.ndimage.filters import median_filter 
 >>> from scipy.ndimage.filters import gaussian_filter_laplace 
 >>> from scipy.ndimage.filters import median_filter_laplace 
 >>> from scipy.ndimage.filters import gaussian_filter_gauss_view 
 >>> from scipy.ndimage.filters import median_filter_gauss","'Efficient subpixel image translation registration by cross-correlation. 
 This code gives the same precision as the FFT upsampled cross-correlation 
 in a fraction of the computation time and with reduced memory requirements. 
 It obtains an initial estimate of the cross-correlation peak by an FFT and 
 then refines the shift estimation by upsampling the DFT only in a small 
 neighborhood of that estimate by means of a matrix-multiply DFT. 
 Parameters 
 src_image : ndarray 
 Reference image. 
 target_image : ndarray 
 Image to register.  Must be same dimensionality as ``src_image``. 
 upsample_factor : int, optional 
 Upsampling factor. Images will be registered to within 
 ``1 / upsample_factor`` of a pixel. For example 
 ``upsample_factor == 20`` means the images will be registered 
 within 1/20th of a pixel.  Default is 1 (no upsampling) 
 space : string, one of ""real"" or ""fourier"", optional 
 Defines how the algorithm interprets input data.  ""real"" means data 
 will be FFT\'d to compute the correlation, while ""fourier"" data will 
 bypass FFT of input data.  Case insensitive. 
 Returns 
 shifts : ndarray 
 Shift vector (in pixels) required to register ``target_image`` with 
 ``src_image``.  Axis ordering is consistent with numpy (e.g. Z, Y, X) 
 error : float 
 Translation invariant normalized RMS error between ``src_image`` and 
 ``target_image``. 
 phasediff : float 
 Global phase difference between the two images (should be 
 zero if images are non-negative). 
 References 
 .. [1] Manuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup, 
 ""Efficient subpixel image registration algorithms,"" 
 Optics Letters 33, 156-158 (2008). DOI:10.1364/OL.33.000156 
 .. [2] James R. Fienup, ""Invariant error metrics for image reconstruction"" 
 Optics Letters 36, 8352-8357 (1997). DOI:10.1364/AO.36.008352'"
"@task() 
 @timeit 
 def maybe_award_badge(badge_template, year, user): 
    badge = get_or_create_badge(badge_template, year) 
   if badge.is_awarded_to(user): 
      return 
   from kitsune.questions.models import Answer 
   qs = Answer.objects.filter(creator=user, created__gte=date(year, 1, 1), created__lt=date((year + 1), 1, 1)) 
   if (qs.count() >= 30): 
      badge.award_to(user) 
      return True"," 'Award a badge to a user if they have answered at least 30 questions 
 in a year.'",'Award the specific badge to the user if they\'ve earned it.'
"def GetRegisteredExe(exeAlias): 
    return win32api.RegQueryValue(GetRootKey(), ((GetAppPathsKey() + '\\') + exeAlias))"," 'Get the path to the exe registered for the given exeAlias. 
 :param exeAlias: The alias of the exe. 
 :type exeAlias: str 
 :return: The path to the exe or None if not found. 
 :rtype: str or None'",'Get a registered .exe'
"def json_decode(x): 
    return json.loads(x, object_hook=json_hook)"," 'Decode a JSON string into a Python object. 
 This function is used to decode a JSON string into a Python object. 
 It uses the ``json.loads`` function, but provides an object_hook, which 
 allows the caller to define a custom object decoder. 
 :param x: The JSON string to decode. 
 :type x: str 
 :return: The decoded object. 
 :rtype: Any'",'JSON decoder that does ugly first-level datetime handling'
"def xy_color(color): 
    rgb_cie_converter = rgb_cie.Converter() 
   hex_color_re = re.compile('(?<!\\w)([a-f0-9]){2}([a-f0-9]){2}([a-f0-9]){2}\\b', re.IGNORECASE) 
   if (color in CSS_LITERALS): 
      color = CSS_LITERALS[color] 
   color = color.lstrip('#') 
   if (not hex_color_re.match(color)): 
      print 'Invalid   color.   Please   use   a   6-digit   hex   color.' 
      sys.exit() 
   return rgb_cie_converter.hexToCIE1931(color)"," 'Convert a hex color to CIE1931. 
 :param color: Hex color string. 
 :return: CIE1931 color string. 
 :rtype: str'",'Validate and convert hex color to XY space.'
"def zero_value_config_set(kodi_setting, all_settings): 
    try: 
      if (int(kodi_setting) == 0): 
         return 'remove_this_line' 
   except: 
      pass 
   return kodi_setting"," 'Returns a value to be set in the config.xml file. 
 This is used to set the value of a setting to 0 if it is not already 
 set to 0. 
 :param kodi_setting: The setting to check 
 :param all_settings: The list of all the settings in the config.xml file'","'If the value of the kodi setting is zero, then remove the 
 entry from the config.txt. This should be used where zero is 
 the default (pi natural) value.'"
"@public 
 def sring(exprs, *symbols, **options): 
    single = False 
   if (not is_sequence(exprs)): 
      (exprs, single) = ([exprs], True) 
   exprs = list(map(sympify, exprs)) 
   opt = build_options(symbols, options) 
   (reps, opt) = _parallel_dict_from_expr(exprs, opt) 
   if (opt.domain is None): 
      coeffs = sum([list(rep.values()) for rep in reps], []) 
      (opt.domain, _) = construct_domain(coeffs, opt=opt) 
   _ring = PolyRing(opt.gens, opt.domain, opt.order) 
   polys = list(map(_ring.from_dict, reps)) 
   if single: 
      return (_ring, polys[0]) 
   else: 
      return (_ring, polys)"," 'Returns a poly ring containing the given polynomials. 
 Examples 
 >>> from sympy import sring 
 >>> from sympy.abc import x 
 >>> sring([x**2 + 1, 2*x**2 + 3*x + 2, x**2 + 4*x + 1], x) 
 PolyRing([x**2 + 1, 2*x**2 + 3*x + 2, x**2 + 4*x + 1], x, 3) 
 >>> sring([x**2 + 1, 2*x**2 + 3*x + 2, x**2 + 4*x + 1], x, 2) 
 PolyRing([x**2 + 1, 2*x**2 + 3*x + 2, x**2 + 4*x + 1], x, 3) 
 >>> sring([x**2 + 1, 2*x**2 + 3*x + 2, x**2 + 4*x + 1], x, 2,","'Construct a ring deriving generators and domain from options and input expressions. 
 Parameters 
 exprs : :class:`Expr` or sequence of :class:`Expr` (sympifiable) 
 symbols : sequence of :class:`Symbol`/:class:`Expr` 
 options : keyword arguments understood by :class:`Options` 
 Examples 
 >>> from sympy.core import symbols 
 >>> from sympy.polys.rings import sring 
 >>> from sympy.polys.domains import ZZ 
 >>> from sympy.polys.orderings import lex 
 >>> x, y, z = symbols(""x,y,z"") 
 >>> R, f = sring(x + 2*y + 3*z) 
 >>> R 
 Polynomial ring in x, y, z over ZZ with lex order 
 >>> f 
 x + 2*y + 3*z 
 >>> type(_) 
 <class \'sympy.polys.rings.PolyElement\'>'"
"def getInsetLoopsFromLoops(loops, radius): 
    insetLoops = [] 
   for loop in loops: 
      insetLoops += getInsetLoopsFromLoop(loop, radius) 
   return insetLoops", 'Returns all inset loops for a given loop.',"'Get the inset loops, which might overlap.'"
"def setup_logging(): 
    logger = logging.getLogger() 
   logger.setLevel(LOG_LEVEL) 
   fmt = logging.Formatter('[%(asctime)s]   %(levelname)s:   %(message)s') 
   handler = logging.StreamHandler() 
   handler.setFormatter(fmt) 
   logger.addHandler(handler)", 'Setup logging.','Setup logging for the script'
"def guess_net_inet_tcp_sendbuf_max(): 
    return (16 * MB)"," 'Returns the maximum size of the TCP send buffer in bytes. 
 This is the maximum amount of data that can be sent in a single 
 send() call. 
 This is a guess based on the size of a default Linux kernel 
 buffer. 
 The actual value used may vary depending on the system and 
 kernel configuration.'","'Maximum size for TCP send buffers 
 See guess_kern_ipc_maxsockbuf().'"
"def convert_opt(key, val): 
    if (key == 'env'): 
      val = env_to_str(val) 
   elif (val is None): 
      val = '' 
   else: 
      val = str(val) 
   return val", 'Convert an option to the correct type.','get opt'
"def _generate_zip_package(target, sources, sources_dir): 
    zip = zipfile.ZipFile(target, 'w', zipfile.ZIP_DEFLATED) 
   manifest = _archive_package_sources(zip.write, sources, sources_dir) 
   zip.writestr(_PACKAGE_MANIFEST, ('\n'.join(manifest) + '\n')) 
   zip.close() 
   return None"," 'Generate a zip package for the given sources. 
 :param target: the name of the zip file to create 
 :param sources: the list of source files to include in the zip file 
 :param sources_dir: the directory containing the source files 
 :return: None'",'Generate a zip archive containing all of the source files.'
"def getNewRepository(): 
    return PostscriptRepository()", 'Returns a new PostscriptRepository.','Get the repository constructor.'
"def url_decode(s, charset='utf-8', decode_keys=False, include_empty=True, errors='replace', separator='&', cls=None): 
    if (cls is None): 
      cls = MultiDict 
   if (isinstance(s, text_type) and (not isinstance(separator, text_type))): 
      separator = separator.decode((charset or 'ascii')) 
   elif (isinstance(s, bytes) and (not isinstance(separator, bytes))): 
      separator = separator.encode((charset or 'ascii')) 
   return cls(_url_decode_impl(s.split(separator), charset, decode_keys, include_empty, errors))"," 'Decode a URL-encoded string. 
 :param s: the string to be decoded 
 :param charset: the charset to use when decoding the string 
 :param decode_keys: if True, decode the keys of the resulting dict 
 :param include_empty: if True, include the empty string as a key in the 
 resulting dict 
 :param errors: the error handling mode, defaults to ""replace"" 
 :param separator: the separator to use when splitting the string, defaults to 
 ""&"" 
 :return: a dict of the decoded string'","'Parse a querystring and return it as :class:`MultiDict`.  There is a 
 difference in key decoding on different Python versions.  On Python 3 
 keys will always be fully decoded whereas on Python 2, keys will 
 remain bytestrings if they fit into ASCII.  On 2.x keys can be forced 
 to be unicode by setting `decode_keys` to `True`. 
 If the charset is set to `None` no unicode decoding will happen and 
 raw bytes will be returned. 
 Per default a missing value for a key will default to an empty key.  If 
 you don\'t want that behavior you can set `include_empty` to `False`. 
 Per default encoding errors are ignored.  If you want a different behavior 
 you can set `errors` to ``\'replace\'`` or ``\'strict\'``.  In strict mode a 
 `HTTPUnicodeError` is raised. 
 .. versionchanged:: 0.5 
 In previous versions "";"" and ""&"" could be used for url decoding. 
 This changed in 0.5 where only ""&"" is supported.  If you want to 
 use "";"" instead a different `separator` can be provided. 
 The `cls` parameter was added. 
 :param s: a string with the query string to decode. 
 :param charset: the charset of the query string.  If set to `None` 
 no unicode decoding will take place. 
 :param decode_keys: Used on Python 2.x to control whether keys should 
 be forced to be unicode objects.  If set to `True` 
 then keys will be unicode in all cases. Otherwise, 
 they remain `str` if they fit into ASCII. 
 :param include_empty: Set to `False` if you don\'t want empty values to 
 appear in the dict. 
 :param errors: the decoding error behavior. 
 :param separator: the pair separator to be used, defaults to ``&`` 
 :param cls: an optional dict class to use.  If this is not specified 
 or `None` the default :class:`MultiDict` is used.'"
"def _tagAttr(key, fullpath): 
    attrs = {} 
   _escapedKey = escape4xml(key) 
   if (fullpath in TAGS_TO_MODIFY): 
      (tagName, useTitle) = TAGS_TO_MODIFY[fullpath] 
      if useTitle: 
         attrs['key'] = _escapedKey 
   elif (not isinstance(key, unicode)): 
      if isinstance(key, str): 
         tagName = unicode(key, 'ascii', 'ignore') 
      else: 
         strType = str(type(key)).replace(""<type   '"", '').replace(""'>"", '') 
         attrs['keytype'] = strType 
         tagName = unicode(key) 
   else: 
      tagName = key 
   if isinstance(key, int): 
      attrs['keytype'] = 'int' 
   origTagName = tagName 
   tagName = tagName.lower().replace('   ', '-') 
   tagName = str(tagName).translate(_allchars, _keepchars) 
   if (origTagName != tagName): 
      if ('key' not in attrs): 
         attrs['key'] = _escapedKey 
   if ((not tagName) or tagName[0].isdigit() or (tagName[0] == '-')): 
      tagName = 'item' 
      _utils_logger.error(('invalid   tag:   %s   [%s]' % (_escapedKey, fullpath))) 
      attrs['key'] = _escapedKey 
   return (tagName, u'   '.join([(u'%s=""%s""' % i) for i in attrs.items()]))"," 'Returns the tag name and the attributes for the tag. 
 :param key: The key of the tag. 
 :param fullpath: The full path to the tag. 
 :return: The tag name and the attributes for the tag.'","'Return a tuple with a tag name and a (possibly empty) attribute, 
 applying the conversions specified in TAGS_TO_MODIFY and checking 
 that the tag is safe for a XML document.'"
"def load_boston(return_X_y=False): 
    module_path = dirname(__file__) 
   fdescr_name = join(module_path, 'descr', 'boston_house_prices.rst') 
   with open(fdescr_name) as f: 
      descr_text = f.read() 
   data_file_name = join(module_path, 'data', 'boston_house_prices.csv') 
   with open(data_file_name) as f: 
      data_file = csv.reader(f) 
      temp = next(data_file) 
      n_samples = int(temp[0]) 
      n_features = int(temp[1]) 
      data = np.empty((n_samples, n_features)) 
      target = np.empty((n_samples,)) 
      temp = next(data_file) 
      feature_names = np.array(temp) 
      for (i, d) in enumerate(data_file): 
         data[i] = np.asarray(d[:(-1)], dtype=np.float64) 
         target[i] = np.asarray(d[(-1)], dtype=np.float64) 
   if return_X_y: 
      return (data, target) 
   return Bunch(data=data, target=target, feature_names=feature_names[:(-1)], DESCR=descr_text)"," 'Load the Boston Housing dataset. 
 Returns a Bunch of data. 
 Parameters 
 return_X_y : bool 
 If True, returns a tuple of (X, y) where X is the data matrix and 
 y is the target vector. 
 Examples 
 >>> from sklearn.datasets import load_boston 
 >>> X, y = load_boston() 
 >>> X.shape 
 (506, 13) 
 >>> y.shape 
 (506,)'","'Load and return the boston house-prices dataset (regression). 
 Samples total                 506 
 Dimensionality                 13 
 Features           real, positive 
 Targets             real 5. - 50. 
 Parameters 
 return_X_y : boolean, default=False. 
 If True, returns ``(data, target)`` instead of a Bunch object. 
 See below for more information about the `data` and `target` object. 
 .. versionadded:: 0.18 
 Returns 
 data : Bunch 
 Dictionary-like object, the interesting attributes are: 
 \'data\', the data to learn, \'target\', the regression targets, 
 and \'DESCR\', the full description of the dataset. 
 (data, target) : tuple if ``return_X_y`` is True 
 .. versionadded:: 0.18 
 Examples 
 >>> from sklearn.datasets import load_boston 
 >>> boston = load_boston() 
 >>> print(boston.data.shape) 
 (506, 13)'"
"def disinherit(name, objectType, copy_inherited_acl=True): 
    ret = {'name': name, 'result': True, 'changes': {}, 'comment': ''} 
   tRet = __salt__['win_dacl.check_inheritance'](name, objectType) 
   if tRet['result']: 
      if tRet['Inheritance']: 
         if __opts__['test']: 
            ret['result'] = None 
            ret['changes']['Inheritance'] = 'Disabled' 
            ret['comment'] = 'Inheritance   is   set   to   be   disabled.' 
            ret['changes']['Inherited   ACLs'] = ('Are   set   to   be   kept' if copy_inherited_acl else 'Are   set   to   be   removed') 
            return ret 
         eRet = __salt__['win_dacl.disable_inheritance'](name, objectType, copy_inherited_acl) 
         ret['result'] = eRet['result'] 
         if eRet['result']: 
            ret['changes'] = dict(ret['changes'], **eRet['changes']) 
         else: 
            ret['comment'] = '   '.join([ret['comment'], eRet['comment']]) 
      elif __opts__['test']: 
         ret['result'] = None 
         ret['comment'] = 'Inheritance   is   disabled.' 
   else: 
      ret['result'] = False 
      ret['comment'] = tRet['comment'] 
   return ret"," 'Disable inheritance for the specified object. 
 :param name: Name of the object. 
 :param objectType: Type of the object. 
 :param copy_inherited_acl: Copy inherited ACLs. 
 :return: Dictionary of the results.'",'Ensure an object is not inheriting ACLs from its parent'
"def get_load(jid): 
    query = 'SELECT   load   FROM   salt.jids   WHERE   jid   =   ?;' 
   ret = {} 
   try: 
      data = __salt__['cassandra_cql.cql_query_with_prepare'](query, 'get_load', [jid]) 
      if data: 
         load = data[0].get('load') 
         if load: 
            ret = json.loads(load) 
   except CommandExecutionError: 
      log.critical('Could   not   get   load   from   jids   table.') 
      raise 
   except Exception as e: 
      log.critical('Unexpected   error   while   getting   load   from   jids:   {0}'.format(str(e))) 
      raise 
   return ret", 'Get the load for a given jid.','Return the load data that marks a specified jid'
"def colnum2name(n): 
    assert (n > 0) 
   s = '' 
   while n: 
      (n, m) = divmod((n - 1), 26) 
      s = (chr((m + ord('A'))) + s) 
   return s", 'Convert a column number to a column name.',"'Translate a column number to name (e.g. 1->\'A\', etc.).'"
"def task_reserved(request): 
    reserved_requests.add(request)", 'Add a task to the reserved request list.','Updates global state when a task has been reserved.'
"def pretty_css(container, name, raw): 
    sheet = container.parse_css(raw) 
   return serialize(sheet, u'text/css')", 'Returns a CSS string from a raw CSS string.','Pretty print the CSS represented as a string in raw'
"@csrf_exempt 
 @require_POST 
 def cspreport(request): 
    report = ('blocked-uri', 'violated-directive', 'original-policy') 
   if (not waffle.sample_is_active('csp-store-reports')): 
      return HttpResponse() 
   try: 
      v = json.loads(request.body)['csp-report'] 
      meta = request.META.copy() 
      meta['PATH_INFO'] = v.get('document-uri', meta['PATH_INFO']) 
      v = [(k, v[k]) for k in report if (k in v)] 
      log_cef('CSPViolation', 5, meta, signature='CSPREPORT', msg='A   client   reported   a   CSP   violation', cs6=v, cs6Label='ContentPolicy') 
   except (KeyError, ValueError) as e: 
      log.debug(('Exception   in   CSP   report:   %s' % e), exc_info=True) 
      return HttpResponseBadRequest() 
   return HttpResponse()", 'Report CSP violations to the server.','Accept CSP reports and log them.'
"def extract_bool(name, value): 
    if (str(value).lower() not in ('true', 'false')): 
      raise ValueError((_('Unrecognized   value   ""%(value)s""   for   ""%(name)s"",   acceptable   values   are:   true,   false.') % {'value': value, 'name': name})) 
   return strutils.bool_from_string(value, strict=True)", 'Extract a boolean value from a string.',"'Convert any true/false string to its corresponding boolean value. 
 Value is case insensitive.'"
"def format_version(module, attr, call=False): 
    try: 
      if call: 
         version = getattr(module, attr)() 
      else: 
         version = getattr(module, attr) 
   except Exception as e: 
      print e 
      version = 'Version   could   not   be   acquired!' 
   if (not isinstance(version, str)): 
      version = list2string(version) 
   return version"," 'Returns a formatted version string of a module. 
 :param module: The module to get the version from. 
 :param attr: The attribute to get the version from. 
 :param call: If True, the module\'s version will be called. 
 :return: A formatted version string.'",'Format the version.'
"def require_open(func): 
    def wrapper(self, *args, **kwargs): 
      if self.closed: 
         raise IOError(errno.EBADF, 'I/O   operation   on   closed   file') 
      return func(self, *args, **kwargs) 
   return wrapper", 'Wrapper for :func:`open` that raises an exception if the file is closed.',"'Decorator that ensures that the file instance isn\'t closed when the 
 function is run.'"
"def evaluate(hps, logdir, traindir, subset='valid', return_val=False): 
    hps.batch_size = 100 
   with tf.Graph().as_default(): 
      with tf.device('/cpu:0'): 
         with tf.variable_scope('model') as var_scope: 
            eval_model = RealNVP(hps) 
            summary_writer = tf.summary.FileWriter(logdir) 
            var_scope.reuse_variables() 
         saver = tf.train.Saver() 
         sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) 
         tf.train.start_queue_runners(sess) 
         previous_global_step = 0 
         with sess.as_default(): 
            while True: 
               ckpt_state = tf.train.get_checkpoint_state(traindir) 
               if (not (ckpt_state and ckpt_state.model_checkpoint_path)): 
                  print ('No   model   to   eval   yet   at   %s' % traindir) 
                  time.sleep(30) 
                  continue 
               print ('Loading   file   %s' % ckpt_state.model_checkpoint_path) 
               saver.restore(sess, ckpt_state.model_checkpoint_path) 
               current_step = tf.train.global_step(sess, eval_model.step) 
               if (current_step == previous_global_step): 
                  print 'Waiting   for   the   checkpoint   to   be   updated.' 
                  time.sleep(30) 
                  continue 
               previous_global_step = current_step 
               print 'Evaluating...' 
               bit_per_dim = eval_model.eval_epoch(hps) 
               print ('Epoch:   %d,   %s   ->   %.3f   bits/dim' % (current_step, subset, bit_per_dim)) 
               print 'Writing   summary...' 
               summary = tf.Summary() 
               summary.value.extend([tf.Summary.Value(tag='bit_per_dim', simple_value=bit_per_dim)]) 
               summary_writer.add_summary(summary, current_step) 
               if return_val: 
                  return (current_step, bit_per_dim)"," 'Evaluate the model on the given checkpoint. 
 Args: 
 hps: Hyper-parameters 
 logdir: Log directory 
 traindir: Training directory 
 subset: Subset of data to evaluate 
 return_val: Return the value instead of just printing it'",'Evaluation.'
"def comment(parser, token): 
    parser.skip_past('endcomment') 
   return CommentNode()"," 'Parse a comment. 
 :param parser: The parser. 
 :param token: The token to parse. 
 :return: A CommentNode.'",'Ignores everything between ``{% comment %}`` and ``{% endcomment %}``.'
"@dec.skip_win32 
 def test_find_cmd_ls(): 
    path = find_cmd('ls') 
   nt.assert_true(path.endswith('ls'))", 'Test find_cmd_ls','Make sure we can find the full path to ls.'
"def get_area(y, x): 
    return trapz(y=y, x=x)"," 'Get the area under a curve. 
 Parameters 
 y : 1D array 
 The values of the curve. 
 x : 1D array 
 The x-coordinates of the curve. 
 Returns 
 area : float 
 The area of the curve. 
 Examples 
 >>> from sympy import Symbol, Function, integrate, get_area 
 >>> from sympy.abc import x 
 >>> from sympy.abc import y 
 >>> x = Symbol(""x"") 
 >>> y = Symbol(""y"") 
 >>> f = Function(""f"") 
 >>> integrate(f(x), x) 
 1/2*x**2 + 1/2*x**3 + 1/3*x**4 + 1/4*x**5 + 1/6*x**6 
 >>> get_area(f(x), x) 
 1/2*x**2 + 1/2*x**3 + 1/3*x**4 + 1/4*x**5 + 1/6*x**6 
 >>> get_area(y=f",'Get the area under the curve'
"def py_encode_basestring_ascii(s, _PY3=PY3): 
    if _PY3: 
      if isinstance(s, binary_type): 
         s = s.decode('utf-8') 
      if (type(s) is not text_type): 
         s = text_type(s) 
   else: 
      if (isinstance(s, str) and (HAS_UTF8.search(s) is not None)): 
         s = s.decode('utf-8') 
      if (type(s) not in string_types): 
         s = text_type(s) 
   def replace(match): 
      s = match.group(0) 
      try: 
         return ESCAPE_DCT[s] 
      except KeyError: 
         n = ord(s) 
         if (n < 65536): 
            return ('\\u%04x' % (n,)) 
         else: 
            n -= 65536 
            s1 = (55296 | ((n >> 10) & 1023)) 
            s2 = (56320 | (n & 1023)) 
            return ('\\u%04x\\u%04x' % (s1, s2)) 
   return (('""' + str(ESCAPE_ASCII.sub(replace, s))) + '""')"," 'Returns a string that represents the given string in ASCII, encoded with 
 UTF-8. 
 This is a Python 3-only function, but it is also available for Python 2 
 as a compatibility layer. 
 If the input is a bytes object, it is first decoded to a unicode object. 
 This is done because the bytes type in Python 2 does not have a .decode() 
 method, and the bytes type in Python 3 does not have a .encode() method. 
 The unicode object is then encoded with UTF-8. 
 This function is a compatibility layer for Python 2, because the 
 `unicode` type is not supported in Python 2. 
 The `unicode` type is a Python 3-only type. 
 :param s: A string to encode. 
 :param _PY3: If True, this is a Python 3-only function.'",'Return an ASCII-only JSON representation of a Python string'
"def set_reboot_required_witnessed(): 
    return __salt__['reg.set_value'](hive='HKLM', key=MINION_VOLATILE_KEY, volatile=True, vname=REBOOT_REQUIRED_NAME, vdata=1, vtype='REG_DWORD')"," 'Set reboot required flag in HKLM\Software\Microsoft\Windows\CurrentVersion\Policies\System\'. 
 This is a volatile flag, so it will be reset when the minion reboots. 
 This flag is set to 1 if the minion is rebooted and the agent is not 
 running. 
 .. versionadded:: 0.13.1'","'.. versionadded:: 2016.11.0 
 This function is used to remember that 
 an event indicating that a reboot is required was witnessed. 
 This function relies on the salt-minion\'s ability to create the following 
 volatile registry key in the *HKLM* hive: 
 *SYSTEM\\CurrentControlSet\\Services\\salt-minion\\Volatile-Data* 
 Because this registry key is volatile, it will not persist 
 beyond the current boot session. 
 Also, in the scope of this key, the name *\'Reboot required\'* will be 
 assigned the value of *1*. 
 (For the time being, this this function is being used 
 whenever an install completes with exit code 3010 and 
 this usage can be extended where appropriate in the future.) 
 :return: A boolean indicating whether or not the salt minion was 
 able to perform the necessary registry operations. 
 :rtype: bool 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' system.set_reboot_required_witnessed'"
"def preprocess_for_eval(image, height, width, central_fraction=0.875, scope=None): 
    with tf.name_scope(scope, 'eval_image', [image, height, width]): 
      if (image.dtype != tf.float32): 
         image = tf.image.convert_image_dtype(image, dtype=tf.float32) 
      if central_fraction: 
         image = tf.image.central_crop(image, central_fraction=central_fraction) 
      if (height and width): 
         image = tf.expand_dims(image, 0) 
         image = tf.image.resize_bilinear(image, [height, width], align_corners=False) 
         image = tf.squeeze(image, [0]) 
      image = tf.sub(image, 0.5) 
      image = tf.mul(image, 2.0) 
      return image"," 'Preprocesses an image for evaluation. 
 Args: 
 image: A 3-D tensor of shape [height, width, channels]. 
 height: The height of the image. 
 width: The width of the image. 
 central_fraction: The central fraction of the image to crop. 
 scope: Optional scope. 
 Returns: 
 A 3-D tensor of shape [height, width, channels]. 
 Raises: 
 ValueError: If the image has more than 3 dimensions.'","'Prepare one image for evaluation. 
 If height and width are specified it would output an image with that size by 
 applying resize_bilinear. 
 If central_fraction is specified it would cropt the central fraction of the 
 input image. 
 Args: 
 image: 3-D Tensor of image. If dtype is tf.float32 then the range should be 
 [0, 1], otherwise it would converted to tf.float32 assuming that the range 
 is [0, MAX], where MAX is largest positive representable number for 
 int(8/16/32) data type (see `tf.image.convert_image_dtype` for details) 
 height: integer 
 width: integer 
 central_fraction: Optional Float, fraction of the image to crop. 
 scope: Optional scope for name_scope. 
 Returns: 
 3-D float Tensor of prepared image.'"
"def user_registered_cb(user, **kwargs): 
    siteconfig = SiteConfiguration.objects.get_current() 
   if siteconfig.get(u'mail_send_new_user_mail'): 
      mail_new_user(user)"," 'Called when a user registers. 
 :param user: The new user. 
 :param kwargs: Keyword arguments.'","'Send e-mail when a user is registered. 
 Listens for new user registrations and sends a new user registration 
 e-mail to administrators, if this type of notification is enabled (through 
 ``mail_send_new_user_mail`` site configuration).'"
"def _is_astropy_source(path=None): 
    if (path is None): 
      path = os.path.join(os.path.dirname(__file__), os.pardir) 
   elif os.path.isfile(path): 
      path = os.path.dirname(path) 
   source_dir = os.path.abspath(path) 
   return os.path.exists(os.path.join(source_dir, '.astropy-root'))", 'Returns True if the path is an astropy source directory.',"'Returns whether the source for this module is directly in an astropy 
 source distribution or checkout.'"
"def data_to_tfrecord(images, labels, filename): 
    print ('Converting   data   into   %s   ...' % filename) 
   cwd = os.getcwd() 
   writer = tf.python_io.TFRecordWriter(filename) 
   for (index, img) in enumerate(images): 
      img_raw = img.tobytes() 
      label = int(labels[index]) 
      example = tf.train.Example(features=tf.train.Features(feature={'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])), 'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))})) 
      writer.write(example.SerializeToString()) 
   writer.close()"," 'Convert data to TFRecord format. 
 Parameters 
 images : numpy array 
 The images to be converted. 
 labels : numpy array 
 The labels to be converted. 
 filename : str 
 The filename to which the TFRecord will be written. 
 Returns 
 None'",'Save data into TFRecord'
"def save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL): 
    new_fd = False 
   if isinstance(f, str): 
      new_fd = True 
      f = open(f, 'wb') 
   try: 
      return _save(obj, f, pickle_module, pickle_protocol) 
   finally: 
      if new_fd: 
         f.close()"," 'Saves an object to a file. 
 :param obj: 
 :param f: 
 :param pickle_module: 
 :param pickle_protocol: 
 :return: 
 :rtype: 
 :raises: 
 :version: 0.1'","'Saves an object to a disk file. 
 Args: 
 obj: saved object 
 f: a file-like object (has to implement fileno that returns a file descriptor) 
 or a string containing a file name 
 pickle_module: module used for pickling metadata and objects 
 pickle_protocol: can be specified to override the default protocol'"
"def pearson_score(list1, list2): 
    size = len(list1) 
   sum1 = sum(list1) 
   sum2 = sum(list2) 
   sum_sq1 = sum([pow(l, 2) for l in list1]) 
   sum_sq2 = sum([pow(l, 2) for l in list2]) 
   prod_sum = sum([(list1[i] * list2[i]) for i in range(size)]) 
   num = (prod_sum - ((sum1 * sum2) / float(size))) 
   den = sqrt(((sum_sq1 - (pow(sum1, 2.0) / size)) * (sum_sq2 - (pow(sum2, 2.0) / size)))) 
   return (num / den)"," 'Compute Pearson correlation coefficient between two lists. 
 Parameters 
 list1 : list 
 The first list. 
 list2 : list 
 The second list. 
 Returns 
 float 
 The Pearson correlation coefficient.'",'Compute the Pearson\' score between 2 lists of vectors.'
"def AskString(prompt, default='', id=261, ok=None, cancel=None): 
    _initialize() 
   _interact() 
   d = GetNewDialog(id, (-1)) 
   if (not d): 
      print ""EasyDialogs:   Can't   get   DLOG   resource   with   id   ="", id, '   (missing   resource   file?)' 
      return 
   h = d.GetDialogItemAsControl(3) 
   SetDialogItemText(h, lf2cr(prompt)) 
   h = d.GetDialogItemAsControl(4) 
   SetDialogItemText(h, lf2cr(default)) 
   d.SelectDialogItemText(4, 0, 999) 
   if (ok is not None): 
      h = d.GetDialogItemAsControl(1) 
      h.SetControlTitle(ok) 
   if (cancel is not None): 
      h = d.GetDialogItemAsControl(2) 
      h.SetControlTitle(cancel) 
   d.SetDialogDefaultItem(1) 
   d.SetDialogCancelItem(2) 
   d.AutoSizeDialog() 
   d.GetDialogWindow().ShowWindow() 
   while 1: 
      n = ModalDialog(None) 
      if (n == 1): 
         h = d.GetDialogItemAsControl(4) 
         return cr2lf(GetDialogItemText(h)) 
      if (n == 2): 
         return None"," 'Asks a question, returning the user's answer. 
 This is a simple dialog that allows the user to enter a string. 
 The prompt is displayed in a dialog window, and the user can 
 enter a string in the dialog window. 
 If the user presses OK, the string is returned. 
 If the user presses Cancel, the string is returned as None. 
 The prompt is a string, and the default value is a string. 
 The OK button is a string, and the Cancel button is a string. 
 The OK and Cancel buttons are optional. 
 Examples 
 >>> import easygui 
 >>> easygui.AskString(""What's your name?"", ""John"", ""Ok"", ""Cancel"") 
 \'John\' 
 >>> easygui.AskString(""What's your name?"", ""John"", ""Ok"", ""Cancel"", ""Okay"") 
 \'John\' 
 >>> easygui.AskString(""What's your name?"", ""John"", ""Ok"", ""Cancel"", ""Cancel"") 
 None'","'Display a PROMPT string and a text entry field with a DEFAULT string. 
 Return the contents of the text entry field when the user clicks the 
 OK button or presses Return. 
 Return None when the user clicks the Cancel button. 
 If omitted, DEFAULT is empty. 
 The PROMPT and DEFAULT strings, as well as the return value, 
 can be at most 255 characters long.'"
"def detect(stream): 
    for fmt in formats.available: 
      try: 
         if fmt.detect(stream): 
            return (fmt, stream) 
      except AttributeError: 
         pass 
   return (None, stream)"," 'Detect the first format that can be played by the given stream. 
 :param stream: A stream to be played. 
 :type stream: :py:class:`~pymedia.stream.Stream` 
 :rtype: tuple(format, stream)'","'Return (format, stream) of given stream.'"
"@flake8ext 
 def validate_log_translations(logical_line, physical_line, filename): 
    if ('neutron/tests' in filename): 
      return 
   if pep8.noqa(physical_line): 
      return 
   msg = 'N320:   Log   messages   require   translation   hints!' 
   if log_translation_hint.match(logical_line): 
      (yield (0, msg))"," 'Validate that the logical line has a log translation hint. 
 :param logical_line: The logical line. 
 :param physical_line: The physical line. 
 :param filename: The filename. 
 :return: None'",'N320 - Log messages require translation.'
"def file_join(nzo, workdir, workdir_complete, delete, joinables): 
    newfiles = [] 
   bufsize = ((24 * 1024) * 1024) 
   joinable_sets = {} 
   joinable_set = None 
   for joinable in joinables: 
      (head, tail) = os.path.splitext(joinable) 
      if (tail == '.ts'): 
         head = match_ts(joinable)[1] 
      if (head not in joinable_sets): 
         joinable_sets[head] = [] 
      joinable_sets[head].append(joinable) 
   logging.debug('joinable_sets:   %s', joinable_sets) 
   try: 
      for joinable_set in joinable_sets: 
         current = joinable_sets[joinable_set] 
         joinable_sets[joinable_set].sort() 
         if os.path.exists(joinable_set): 
            logging.debug('file_join():   Skipping   %s,   (probably)   joined   by   par2', joinable_set) 
            if delete: 
               clean_up_joinables(current) 
            continue 
         size = len(current) 
         if (size < 2): 
            continue 
         filename = joinable_set 
         if workdir_complete: 
            filename = filename.replace(workdir, workdir_complete) 
         logging.debug('file_join():   Assembling   %s', filename) 
         joined_file = open(filename, 'ab') 
         n = get_seq_number(current[0]) 
         seq_error = (n > 1) 
         for joinable in current: 
            if (get_seq_number(joinable) != n): 
               seq_error = True 
            perc = ((100.0 / size) * n) 
            logging.debug('Processing   %s', joinable) 
            nzo.set_action_line(T('Joining'), ('%.0f%%' % perc)) 
            f = open(joinable, 'rb') 
            shutil.copyfileobj(f, joined_file, bufsize) 
            f.close() 
            if delete: 
               logging.debug('Deleting   %s', joinable) 
               os.remove(joinable) 
            n += 1 
         clean_up_joinables(current) 
         joined_file.flush() 
         joined_file.close() 
         newfiles.append(filename) 
         if seq_error: 
            msg = T('Incomplete   sequence   of   joinable   files') 
            nzo.fail_msg = (T('File   join   of   %s   failed') % unicoder(joinable_set)) 
            nzo.set_unpack_info('Filejoin', (T('[%s]   Error   ""%s""   while   joining   files') % (unicoder(joinable_set), msg))) 
            logging.error(T('Error   ""%s""   while   running   file_join   on   %s'), msg, nzo.final_name) 
         else: 
            msg = (T('[%s]   Joined   %s   files') % (unicoder(joinable_set), size)) 
            nzo.set_unpack_info('Filejoin', msg, set=joinable_set) 
   except: 
      msg = sys.exc_info()[1] 
      nzo.fail_msg = (T('File   join   of   %s   failed') % msg) 
      nzo.set_unpack_info('Filejoin', (T('[%s]   Error   ""%s""   while   joining   files') % (unicoder(joinable_set), msg))) 
      logging.error(T('Error   ""%s""   while   running   file_join   on   %s'), msg, nzo.final_name) 
      return (True, []) 
   return (False, newfiles)"," 'Assemble a set of files into a single file. 
 This is a utility function that is used by par2. 
 It is called by par2 to assemble a set of files into a single 
 file. 
 The files are assembled into a single file by copying the contents 
 of each file into a temporary file. 
 The temporary file is then closed, and the contents of the temporary 
 file are written to the final file. 
 The function returns a tuple of (True, newfiles) if the file join 
 was successful, or (False, newfiles) if the file join failed. 
 newfiles is a list of the new files created by the file join.'","'Join and joinable files in \'workdir\' to \'workdir_complete\' and 
 when successful, delete originals'"
"def _ValidateClientId(client_id): 
    if (not isinstance(client_id, basestring)): 
      raise InvalidChannelClientIdError(('""%s""   is   not   a   string.' % client_id)) 
   if isinstance(client_id, unicode): 
      client_id = client_id.encode('utf-8') 
   if (len(client_id) > MAXIMUM_CLIENT_ID_LENGTH): 
      msg = ('Client   id   length   %d   is   greater   than   max   length   %d' % (len(client_id), MAXIMUM_CLIENT_ID_LENGTH)) 
      raise InvalidChannelClientIdError(msg) 
   return client_id"," 'Validate client_id. 
 :param client_id: Client id. 
 :type client_id: basestring 
 :rtype: basestring'","'Validates a client id. 
 Args: 
 client_id: The client id provided by the application. 
 Returns: 
 If the client id is of type str, returns the original client id. 
 If the client id is of type unicode, returns the id encoded to utf-8. 
 Raises: 
 InvalidChannelClientIdError: if client id is not an instance of str or 
 unicode, or if the (utf-8 encoded) string is longer than 64 characters.'"
"def isvector(X): 
    warnings.warn('isvector   has   been   moved   to   matplotlib.mlab   --   please   import   it   from   there', DeprecationWarning) 
   import matplotlib.mlab as mlab 
   return mlab.isvector(x, y, xi, extrap=extrap)"," 'Returns True if the data is a vector. 
 Parameters 
 X : array-like 
 The data. 
 extrap : boolean 
 If True, extrapolate beyond the data. 
 Returns 
 True if the data is a vector. 
 Examples 
 >>> from scipy import interpolate 
 >>> from scipy.interpolate import isvector 
 >>> x = [1, 2, 3, 4, 5] 
 >>> y = [1, 2, 3, 4, 5] 
 >>> interpolate.interp1d(x, y, extrap=False) 
 1.0 
 >>> interpolate.interp1d(x, y, extrap=True) 
 1.0 
 >>> interpolate.interp1d(x, y, extrap=True) 
 1.0 
 >>> interpolate.interp1d(x, y, extrap=True) 
 1.0'","'This function has been moved to matplotlib.mlab -- please import 
 it from there'"
"@register_canonicalize 
 @register_specialize 
 @gof.local_optimizer([T.int_div, T.true_div]) 
 def local_zero_div(node): 
    if (isinstance(node.op, T.Elemwise) and isinstance(node.op.scalar_op, (theano.scalar.IntDiv, theano.scalar.TrueDiv))): 
      if (local_mul_canonizer.get_constant(node.inputs[0]) == 0): 
         ret = broadcast_like(0, node.outputs[0], node.fgraph) 
         ret.tag.values_eq_approx = values_eq_approx_remove_nan 
         return [ret]"," 'If the divisor is zero, return a zero. 
 This is a local optimizer, i.e. it is only applied to the nodes 
 in the graph. 
 It is a local optimizer because it may be applied to a node in a 
 different graph, e.g. a graph with a different input shape. 
 For example, if we have a graph with input shape (2, 3, 4) and 
 we have a node with input shape (2, 3, 4) and output shape (2, 3, 4), 
 this optimizer may be applied to the node and it will return a 
 node with input shape (2, 3, 4) and output shape (2, 3, 4). 
 The optimizer will not be applied to a node in a graph with 
 different input shape (2, 3, 4) and output shape (2, 3, 4). 
 The optimizer will also not be applied to a node in a graph with 
 different input shape (2, 3, 4) and output shape (2, 3, 4) if ",'0 / x -> 0'
"def test_nested(a, b, c): 
    def one(): 
      return a 
   def two(): 
      return b 
   def three(): 
      return c 
   def new_closure(a, b): 
      def sum(): 
         return (a + b) 
      return sum 
   (yield one) 
   (yield two) 
   (yield three) 
   (yield new_closure(a, c))", 'Test the nested function generator.',"'>>> obj = test_nested(1, 2, 3) 
 >>> [i() for i in obj] 
 [1, 2, 3, 4]'"
"def make_fna(sff_fp, output_fp, use_sfftools=False, no_trim=False): 
    if use_sfftools: 
      _fail_on_gzipped_sff(sff_fp) 
      check_sffinfo() 
      if no_trim: 
         _check_call(['sffinfo', '-notrim', '-s', sff_fp], stdout=open(output_fp, 'w')) 
      else: 
         _check_call(['sffinfo', '-s', sff_fp], stdout=open(output_fp, 'w')) 
   else: 
      try: 
         format_binary_sff_as_fna(qiime_open(sff_fp, 'rb'), open(output_fp, 'w')) 
      except: 
         raise IOError(('Could   not   parse   SFF   %s' % sff_fp))", 'Convert an SFF file to FNA format.','Makes fna file from sff file.'
"def worker_e_step(input_queue, result_queue): 
    logger.debug('worker   process   entering   E-step   loop') 
   while True: 
      logger.debug('getting   a   new   job') 
      (chunk_no, chunk, worker_lda) = input_queue.get() 
      logger.debug('processing   chunk   #%i   of   %i   documents', chunk_no, len(chunk)) 
      worker_lda.state.reset() 
      worker_lda.do_estep(chunk) 
      del chunk 
      logger.debug('processed   chunk,   queuing   the   result') 
      result_queue.put(worker_lda.state) 
      del worker_lda 
      logger.debug('result   put')"," 'Worker process for the E-step. 
 :param input_queue: queue to put new chunks into 
 :param result_queue: queue to put results into'","'Perform E-step for each (chunk_no, chunk, model) 3-tuple from the 
 input queue, placing the resulting state into the result queue.'"
"def _sl_fit(estimator, X, y): 
    from sklearn.base import clone 
   estimators_ = list() 
   for ii in range(X.shape[(-1)]): 
      est = clone(estimator) 
      est.fit(X[..., ii], y) 
      estimators_.append(est) 
   return estimators_"," 'Return a list of estimators. 
 Parameters 
 estimator : estimator object 
 X : array 
 y : array 
 Returns 
 estimators_ : list 
 A list of estimators.'","'Aux. function to fit _SearchLight in parallel. 
 Fit a clone estimator to each slice of data. 
 Parameters 
 base_estimator : object 
 The base estimator to iteratively fit on a subset of the dataset. 
 X : array, shape (n_samples, nd_features, n_estimators) 
 The target data. The feature dimension can be multidimensional e.g. 
 X.shape = (n_samples, n_features_1, n_features_2, n_estimators) 
 y : array, shape (n_sample, ) 
 The target values. 
 Returns 
 estimators_ : list of estimators 
 The fitted estimators.'"
"def perm2tensor(t, g, canon_bp=False): 
    if (not isinstance(t, TensExpr)): 
      return t 
   new_tids = get_tids(t).perm2tensor(g, canon_bp) 
   coeff = get_coeff(t) 
   if (g[(-1)] != (len(g) - 1)): 
      coeff = (- coeff) 
   res = TensMul.from_TIDS(coeff, new_tids, is_canon_bp=canon_bp) 
   return res"," 'Construct a tensor from a permuted tensor. 
 Parameters 
 t : TensExpr 
 tensor to permute. 
 g : list of ints 
 permutation. 
 canon_bp : bool 
 whether to construct a canonical tensor. 
 Returns 
 TensExpr 
 tensor.'","'Returns the tensor corresponding to the permutation ``g`` 
 For further details, see the method in ``TIDS`` with the same name.'"
"def get_build_results(build): 
    r_url = get_results_raw_url(build) 
   if (not r_url): 
      return 
   return convert_json_to_df(r_url)"," 'Get build results from a build\'s url. 
 :param build: Build object 
 :type build: Build'",'Returns a df with the results of the VBENCH job associated with the travis build'
"def get_hash(f): 
    import hashlib 
   m = hashlib.md5() 
   m.update(f) 
   return m.hexdigest()", 'Returns the hash of the given file.','Gets hexadmecimal md5 hash of a string'
"@requires_sklearn_0_15 
 def test_SearchLight(): 
    from sklearn.linear_model import Ridge, LogisticRegression 
   from sklearn.pipeline import make_pipeline 
   from sklearn.metrics import roc_auc_score, get_scorer, make_scorer 
   (X, y) = make_data() 
   (n_epochs, _, n_time) = X.shape 
   assert_raises(ValueError, _SearchLight, 'foo') 
   sl = _SearchLight(Ridge()) 
   sl = _SearchLight(LogisticRegression()) 
   assert_equal(sl.__repr__()[:14], '<_SearchLight(') 
   sl.fit(X, y) 
   assert_equal(sl.__repr__()[(-28):], ',   fitted   with   10   estimators>') 
   assert_raises(ValueError, sl.fit, X[1:], y) 
   assert_raises(ValueError, sl.fit, X[:, :, 0], y) 
   assert_raises(ValueError, sl.predict, X[:, :, :2]) 
   y_pred = sl.predict(X) 
   assert_true((y_pred.dtype == int)) 
   assert_array_equal(y_pred.shape, [n_epochs, n_time]) 
   y_proba = sl.predict_proba(X) 
   assert_true((y_proba.dtype == float)) 
   assert_array_equal(y_proba.shape, [n_epochs, n_time, 2]) 
   score = sl.score(X, y) 
   assert_array_equal(score.shape, [n_time]) 
   assert_true((np.sum(np.abs(score)) != 0)) 
   assert_true((score.dtype == float)) 
   sl = _SearchLight(LogisticRegression()) 
   assert_equal(sl.scoring, None) 
   for (err, scoring) in [(ValueError, 'foo'), (TypeError, 999)]: 
      sl = _SearchLight(LogisticRegression(), scoring=scoring) 
      sl.fit(X, y) 
      assert_raises(err, sl.score, X, y) 
   sl = _SearchLight(LogisticRegression(), scoring='roc_auc') 
   y = (np.arange(len(X)) % 3) 
   sl.fit(X, y) 
   assert_raises(ValueError, sl.score, X, y) 
   y = ((np.arange(len(X)) % 2) + 1) 
   sl.fit(X, y) 
   score = sl.score(X, y) 
   assert_array_equal(score, [roc_auc_score((y - 1), (_y_pred - 1)) for _y_pred in sl.decision_function(X).T]) 
   y = (np.arange(len(X)) % 2) 
   for (method, scoring) in [('predict_proba', 'roc_auc'), ('predict', roc_auc_score)]: 
      sl1 = _SearchLight(LogisticRegression(), scoring=scoring) 
      sl1.fit(X, y) 
      np.random.seed(0) 
      X = np.random.randn(*X.shape) 
      score_sl = sl1.score(X, y) 
      assert_array_equal(score_sl.shape, [n_time]) 
      assert_true((score_sl.dtype == float)) 
      if isinstance(scoring, str): 
         scoring = get_scorer(scoring) 
      else: 
         scoring = make_scorer(scoring) 
      score_manual = [scoring(est, x, y) for (est, x) in zip(sl1.estimators_, X.transpose(2, 0, 1))] 
      assert_array_equal(score_manual, score_sl) 
   sl = _SearchLight(LogisticRegression(), n_jobs=2) 
   sl.fit(X, y) 
   sl.predict(X) 
   sl.score(X, y) 
   sl.fit(X[..., [0]], y) 
   sl.predict(X[..., [0]]) 
   class _LogRegTransformer(LogisticRegression, ): 
      def transform(self, X): 
         return super(_LogRegTransformer, self).predict_proba(X)[..., 1] 
   pipe = make_pipeline(_SearchLight(_LogRegTransformer()), LogisticRegression()) 
   pipe.fit(X, y) 
   pipe.predict(X) 
   X = np.random.rand(10, 3, 4, 2) 
   y = (np.arange(10) % 2) 
   y_preds = list() 
   for n_jobs in [1, 2]: 
      pipe = _SearchLight(make_pipeline(Vectorizer(), LogisticRegression()), n_jobs=n_jobs) 
      y_preds.append(pipe.fit(X, y).predict(X)) 
      features_shape = pipe.estimators_[0].steps[0][1].features_shape_ 
      assert_array_equal(features_shape, [3, 4]) 
   assert_array_equal(y_preds[0], y_preds[1])", 'Test SearchLight class.','Test _SearchLight'
"def _parse_output(output, template): 
    ret = {} 
   index = 0 
   if (not (output and template)): 
      return ret 
   if ('translate' in template): 
      ret = _translate_output(output) 
   else: 
      output_list = output.strip().replace('\n', '').split('   ') 
      if (sum(template.values()) != len(output_list)): 
         raise ipmiexcept.IPMIException(_('ipmitool   output   length   mismatch')) 
      for item in template.items(): 
         index_end = (index + item[1]) 
         update_value = output_list[index:index_end] 
         ret[item[0]] = update_value 
         index = index_end 
   return ret", 'Parse output from ipmitool',"'Parse the return value of IPMI command into dict 
 :param output: output of the execution of IPMI command 
 :param template: a dict that contains the expected items of 
 IPMI command and its length.'"
"def timestamp(x): 
    if (x.tzinfo is None): 
      x = x.replace(tzinfo=utc) 
   if hasattr(x, 'timestamp'): 
      return x.timestamp() 
   else: 
      return (x - datetime(1970, 1, 1, tzinfo=utc)).total_seconds()"," 'Convert a datetime object to a timestamp. 
 If x is a datetime, return its timestamp. 
 Otherwise, return the number of seconds since 1970-01-01 00:00:00 UTC.'",'Get a timestamp from a date in python 3 and python 2'
"def get_latest_repository_metadata(app, decoded_repository_id, downloadable=False): 
    sa_session = app.model.context.current 
   repository = sa_session.query(app.model.Repository).get(decoded_repository_id) 
   repo = hg_util.get_repo_for_repository(app, repository=repository, repo_path=None, create=False) 
   if downloadable: 
      changeset_revision = get_latest_downloadable_changeset_revision(app, repository, repo) 
   else: 
      changeset_revision = get_latest_changeset_revision(app, repository, repo) 
   return get_repository_metadata_by_changeset_revision(app, app.security.encode_id(repository.id), changeset_revision)"," 'Returns the latest repository metadata for a given repository. 
 :param downloadable: Whether to return the latest downloadable changeset 
 :param app: The current application 
 :param decoded_repository_id: The repository id encoded in the URL 
 :param repo: The repository object 
 :return: The latest repository metadata for the given repository'",'Get last metadata defined for a specified repository from the database.'
"def test_string(): 
    schema = vol.Schema(cv.string) 
   with pytest.raises(vol.MultipleInvalid): 
      schema(None) 
   for value in (True, 1, 'hello'): 
      schema(value)", 'Test that a string schema does not accept None.','Test string validation.'
"@gen.engine 
 def ListRecursively(store, pattern, callback): 
    results = (yield gen.Task(ListAllKeys, store, prefix=PrefixFromPattern(pattern))) 
   callback(results)"," 'Lists all keys in a store recursively. 
 :param store: Store to list keys from. 
 :param pattern: Pattern to use for listing keys. 
 :param callback: Callback to call with the results.'",'Recursively list all files matching \'pattern\'. This does not return directories.'
"def _extract_doc_comment(content, line, column, markers): 
    if (markers[1] == ''): 
      return _extract_doc_comment_simple(content, line, column, markers) 
   elif (markers[1] == markers[2]): 
      return _extract_doc_comment_continuous(content, line, column, markers) 
   else: 
      return _extract_doc_comment_standard(content, line, column, markers)"," 'Extract a doc comment from the given content. 
 :param content: The content to extract the doc comment from. 
 :param line: The line number of the content. 
 :param column: The column number of the content. 
 :param markers: The markers to extract the doc comment from. 
 :returns: The doc comment. 
 :rtype: str'","'Delegates depending on the given markers to the right extraction method. 
 :param content: Presplitted lines of the source-code-string. 
 :param line:    Line where the documentation comment starts (behind the 
 start marker). Zero-based. 
 :param column:  Column where the documentation comment starts (behind the 
 start marker). Zero-based. 
 :param markers: The documentation identifying markers. 
 :return:        If the comment matched layout a triple with end-of-comment 
 line, column and the extracted documentation. If not 
 matched, returns None.'"
"def Time2Internaldate(date_time): 
    if isinstance(date_time, (int, float)): 
      tt = time.localtime(date_time) 
   elif isinstance(date_time, (tuple, time.struct_time)): 
      tt = date_time 
   elif (isinstance(date_time, str) and ((date_time[0], date_time[(-1)]) == ('""', '""'))): 
      return date_time 
   else: 
      raise ValueError('date_time   not   of   a   known   type') 
   dt = time.strftime('%d-%b-%Y   %H:%M:%S', tt) 
   if (dt[0] == '0'): 
      dt = ('   ' + dt[1:]) 
   if (time.daylight and tt[(-1)]): 
      zone = (- time.altzone) 
   else: 
      zone = (- time.timezone) 
   return ((('""' + dt) + ('   %+03d%02d' % divmod((zone // 60), 60))) + '""')"," 'Converts a time object into a string suitable for use in an SQL 
 query. 
 :param date_time: a time object or a string representing a time object 
 :return: a string representing the time object 
 :rtype: str'","'Convert \'date_time\' to IMAP4 INTERNALDATE representation. 
 Return string in form: \'""DD-Mmm-YYYY HH:MM:SS +HHMM""\''"
"def parseSdr(s): 
    assert isinstance(s, basestring) 
   sdr = [int(c) for c in s if (c in ('0', '1'))] 
   if (len(sdr) != len(s)): 
      raise ValueError(""The   provided   string   %s   is   malformed.   The   string   should   have   only   0's   and   1's."") 
   return sdr"," 'Parses a string in the S-D-R format, i.e. a string of 0'",'Parses a string containing only 0\'s and 1\'s and return a Python list object.'
"def _relpath(path, start='.'): 
    if (not path): 
      raise ValueError('no   path   specified') 
   startList = os.path.abspath(start).split(os.path.sep) 
   pathList = os.path.abspath(path).split(os.path.sep) 
   i = len(os.path.commonprefix([startList, pathList])) 
   relList = ((['..'] * (len(startList) - i)) + pathList[i:]) 
   if (not relList): 
      return path 
   return os.path.join(*relList)"," 'Return a relative path from start to path. 
 If path is not specified, return the current working directory. 
 If start is not specified, return the current working directory. 
 If start is specified, it is relative to the current working directory. 
 The return value is a string.'","'This code is based on os.path.relpath in the Python 2.6 distribution, 
 included here for compatibility with Python 2.5'"
"def packages(pkg_list, update=False): 
    pkg_list = [pkg for pkg in pkg_list if (not is_installed(pkg))] 
   if pkg_list: 
      install(pkg_list, update)"," 'Install packages. 
 :param pkg_list: List of packages to install. 
 :param update: Install the packages even if they are already installed. 
 :type pkg_list: list 
 :type update: bool'","'Require several Arch Linux packages to be installed. 
 Example:: 
 from fabtools import require 
 require.arch.packages([ 
 \'foo\', 
 \'bar\', 
 \'baz\','"
"def construct_mirror_name(volume): 
    return ('mirror_' + six.text_type(volume.id))", 'Returns a unique mirror name for a volume.','Constructs MirrorView name for volume.'
"def floating_ip_create(kwargs, call=None): 
    if (call != 'function'): 
      raise SaltCloudSystemExit('The   floating_ip_create   action   must   be   called   with   -f   or   --function') 
   if ('pool' not in kwargs): 
      log.error('pool   is   required') 
      return False 
   conn = get_conn() 
   return conn.floating_ip_create(kwargs['pool'])", 'Create a floating IP',"'Allocate a floating IP 
 .. versionadded:: 2016.3.0'"
"def top_contributors_l10n(start=None, end=None, locale=None, product=None, count=10, page=1): 
    query = RevisionMetricsMappingType.search().facet('creator_id', filtered=True, size=BIG_NUMBER) 
   if (locale is None): 
      query = query.filter((~ F(locale=settings.WIKI_DEFAULT_LANGUAGE))) 
   query = _apply_filters(query, start, end, locale, product) 
   return _get_creator_counts(query, count, page)"," 'Returns a list of top contributors to a locale, product, or 
 date range. 
 :param start: start date (ISO 8601) 
 :param end: end date (ISO 8601) 
 :param locale: locale to return 
 :param product: product to return 
 :param count: number of contributors to return 
 :param page: page number to return 
 :returns: a list of contributors 
 :rtype: list of :class:`~django.contrib.auth.models.User`'",'Get the top l10n contributors for the KB.'
"def assert_array_max_ulp(a, b, maxulp=1, dtype=None): 
    numpy.testing.assert_array_max_ulp(cupy.asnumpy(a), cupy.asnumpy(b), maxulp=maxulp, dtype=dtype)"," 'Assert that the absolute difference between two arrays is less than maxulp. 
 Parameters 
 a, b : ndarray 
 The two arrays to compare. 
 maxulp : int 
 The maximum relative difference between the two arrays. 
 dtype : dtype, optional 
 The dtype to use for the comparison. 
 Returns 
 None'","'Check that all items of arrays differ in at most N Units in the Last Place. 
 Args: 
 a(numpy.ndarray or cupy.ndarray): The actual object to check. 
 b(numpy.ndarray or cupy.ndarray): The desired, expected object. 
 maxulp(int): The maximum number of units in the last place 
 that elements of ``a`` and ``b`` can differ. 
 dtype(numpy.dtype): Data-type to convert ``a`` and ``b`` to if given. 
 .. seealso:: :func:`numpy.testing.assert_array_max_ulp`'"
"@login_required 
 @ensure_csrf_cookie 
 def assets_handler(request, course_key_string=None, asset_key_string=None): 
    course_key = CourseKey.from_string(course_key_string) 
   if (not has_course_author_access(request.user, course_key)): 
      raise PermissionDenied() 
   response_format = (request.GET.get('format') or request.POST.get('format') or 'html') 
   if ((response_format == 'json') or ('application/json' in request.META.get('HTTP_ACCEPT', 'application/json'))): 
      if (request.method == 'GET'): 
         return _assets_json(request, course_key) 
      else: 
         asset_key = (AssetKey.from_string(asset_key_string) if asset_key_string else None) 
         return _update_asset(request, course_key, asset_key) 
   elif (request.method == 'GET'): 
      return _asset_index(request, course_key) 
   else: 
      return HttpResponseNotFound()"," 'View assets for a course. 
 This view is used to display assets for a course. 
 If the course is not public, the user must be logged in and 
 have author access to the course. 
 If the course is public, the user must be logged in and have 
 access to the course. 
 If the user is not logged in, the user will be redirected to the 
 login page. 
 If the user is not authorized to access the course, the user will 
 be redirected to the course home page. 
 If the user is authorized to access the course, but does not have 
 access to the assets, the user will be redirected to the course 
 home page. 
 If the user is authorized to access the course and has access to 
 the assets, the user will be redirected to the assets page. 
 If the user is authorized to access the course and does not have 
 access to the assets, the user will be redirected to the course 
 home page. 
 If the user is not authorized to access the course, the user will 
 be redirected to the course home page. 
","'The restful handler for assets. 
 It allows retrieval of all the assets (as an HTML page), as well as uploading new assets, 
 deleting assets, and changing the ""locked"" state of an asset. 
 GET 
 html: return an html page which will show all course assets. Note that only the asset container 
 is returned and that the actual assets are filled in with a client-side request. 
 json: returns a page of assets. The following parameters are supported: 
 page: the desired page of results (defaults to 0) 
 page_size: the number of items per page (defaults to 50) 
 sort: the asset field to sort by (defaults to ""date_added"") 
 direction: the sort direction (defaults to ""descending"") 
 POST 
 json: create (or update?) an asset. The only updating that can be done is changing the lock state. 
 PUT 
 json: update the locked state of an asset 
 DELETE 
 json: delete an asset'"
"def savgol_coeffs(window_length, polyorder, deriv=0, delta=1.0, pos=None, use='conv'): 
    if (polyorder >= window_length): 
      raise ValueError('polyorder   must   be   less   than   window_length.') 
   (halflen, rem) = divmod(window_length, 2) 
   if (rem == 0): 
      raise ValueError('window_length   must   be   odd.') 
   if (pos is None): 
      pos = halflen 
   if (not (0 <= pos < window_length)): 
      raise ValueError('pos   must   be   nonnegative   and   less   than   window_length.') 
   if (use not in ['conv', 'dot']): 
      raise ValueError(""`use`   must   be   'conv'   or   'dot'"") 
   x = np.arange((- pos), (window_length - pos), dtype=float) 
   if (use == 'conv'): 
      x = x[::(-1)] 
   order = np.arange((polyorder + 1)).reshape((-1), 1) 
   A = (x ** order) 
   y = np.zeros((polyorder + 1)) 
   y[deriv] = (factorial(deriv) / (delta ** deriv)) 
   (coeffs, _, _, _) = lstsq(A, y) 
   return coeffs"," 'Returns the Savitzky-Golay coefficients for a given window size. 
 The coefficients are calculated using the least squares method. 
 Parameters 
 window_length : int 
 The window length. 
 polyorder : int 
 The polynomial order. 
 deriv : int 
 The polynomial order of the derivative. 
 delta : float 
 The smoothing factor. 
 pos : int 
 The position of the first coefficient. 
 use : \'conv\' or \'dot\' 
 The type of the Savitzky-Golay filter. 
 Returns 
 coeffs : array 
 The coefficients of the Savitzky-Golay filter. 
 References 
 .. [1] http://en.wikipedia.org/wiki/Savitzky-Golay_filter 
 Examples 
 >>> from scipy import signal 
 >>> from scipy.signal import savgol_coeffs 
 >>> x = np.arange(-10, 11) 
 >>> y = signal.savgol_filter(x, 5, 3, delta=1.0) ","'Compute the coefficients for a 1-d Savitzky-Golay FIR filter. 
 Parameters 
 window_length : int 
 The length of the filter window (i.e. the number of coefficients). 
 `window_length` must be an odd positive integer. 
 polyorder : int 
 The order of the polynomial used to fit the samples. 
 `polyorder` must be less than `window_length`. 
 deriv : int, optional 
 The order of the derivative to compute.  This must be a 
 nonnegative integer.  The default is 0, which means to filter 
 the data without differentiating. 
 delta : float, optional 
 The spacing of the samples to which the filter will be applied. 
 This is only used if deriv > 0. 
 pos : int or None, optional 
 If pos is not None, it specifies evaluation position within the 
 window.  The default is the middle of the window. 
 use : str, optional 
 Either \'conv\' or \'dot\'.  This argument chooses the order of the 
 coefficients.  The default is \'conv\', which means that the 
 coefficients are ordered to be used in a convolution.  With 
 use=\'dot\', the order is reversed, so the filter is applied by 
 dotting the coefficients with the data set. 
 Returns 
 coeffs : 1-d ndarray 
 The filter coefficients. 
 References 
 A. Savitzky, M. J. E. Golay, Smoothing and Differentiation of Data by 
 Simplified Least Squares Procedures. Analytical Chemistry, 1964, 36 (8), 
 pp 1627-1639. 
 See Also 
 savgol_filter 
 Notes 
 .. versionadded:: 0.14.0 
 Examples 
 >>> from scipy.signal import savgol_coeffs 
 >>> savgol_coeffs(5, 2) 
 array([-0.08571429,  0.34285714,  0.48571429,  0.34285714, -0.08571429]) 
 >>> savgol_coeffs(5, 2, deriv=1) 
 array([  2.00000000e-01,   1.00000000e-01,   2.00607895e-16, 
 -1.00000000e-01,  -2.00000000e-01]) 
 Note that use=\'dot\' simply reverses the coefficients. 
 >>> savgol_coeffs(5, 2, pos=3) 
 array([ 0.25714286,  0.37142857,  0.34285714,  0.17142857, -0.14285714]) 
 >>> savgol_coeffs(5, 2, pos=3, use=\'dot\') 
 array([-0.14285714,  0.17142857,  0.34285714,  0.37142857,  0.25714286]) 
 `x` contains data from the parabola x = t**2, sampled at 
 t = -1, 0, 1, 2, 3.  `c` holds the coefficients that will compute the 
 derivative at the last position.  When dotted with `x` the result should 
 be 6. 
 >>> x = np.array([1, 0, 1, 4, 9]) 
 >>> c = savgol_coeffs(5, 2, pos=4, deriv=1, use=\'dot\') 
 >>> c.dot(x) 
 6.0000000000000018'"
"def clamav(registry, xml_parent, data): 
    clamav = XML.SubElement(xml_parent, 'org.jenkinsci.plugins.clamav.ClamAvRecorder') 
   clamav.set('plugin', 'clamav') 
   mappings = [('includes', 'includes', ''), ('excludes', 'excludes', '')] 
   helpers.convert_mapping_to_xml(clamav, data, mappings, fail_required=True)", 'Configure ClamAV scanning.',"'yaml: clamav 
 Check files with ClamAV, an open source antivirus engine. 
 Requires the Jenkins :jenkins-wiki:`ClamAV Plugin <ClamAV+Plugin>`. 
 :arg str includes: Comma seperated list of files that should be scanned. 
 Must be set for ClamAV to check for artifacts. (default \'\') 
 :arg str excludes: Comma seperated list of files that should be ignored 
 (default \'\') 
 Full Example: 
 .. literalinclude:: /../../tests/publishers/fixtures/clamav-full.yaml 
 :language: yaml 
 Minimal Example: 
 .. literalinclude:: /../../tests/publishers/fixtures/clamav-minimal.yaml 
 :language: yaml'"
"def __virtual__(): 
    if salt.utils.which('nft'): 
      return 'nftables' 
   return (False, 'The   nftables   execution   module   failed   to   load:   nftables   is   not   installed.')", 'Load nftables execution module.','Only load the module if nftables is installed'
"def tree_support(master, subsampled_tree): 
    master_tipnames = set(master.getTipNames()) 
   subsampled_tree_trimmed = copy.deepcopy(subsampled_tree) 
   def delete_test(node): 
      if (not node.isTip()): 
         return False 
      else: 
         return (node.Name not in master_tipnames) 
   subsampled_tree_trimmed.removeDeleted(delete_test) 
   subsampled_tree_trimmed.prune() 
   subsampled_tree_nodes_names = [] 
   for node in subsampled_tree_trimmed.iterNontips(include_self=True): 
      subsampled_tree_nodes_names.append(node.getTipNames()) 
   subsampled_tree_nodes_names = map(set, subsampled_tree_nodes_names) 
   for master_node in master.iterNontips(include_self=True): 
      if (set(master_node.getTipNames()) in subsampled_tree_nodes_names): 
         try: 
            master_node.bootstrap_support += 1 
         except AttributeError: 
            master_node.bootstrap_support = 1"," 'Test whether subsampled tree has same tip names as master. 
 If so, the bootstrap support is updated.'","'compares master tree to subsampled_tree, modifies master in place 
 this calculates bootstrap support of each nontip node in the master tree 
 a given master_tree_node is supported if there exists a node in subsampled 
 tree where sub_tree_node.tips == master_tree_node.tips (by name) 
 each subsampled tree is first modified to remove tips and branches leading 
 to them if the tip isn\'t in the master tree 
 not specific to bootstrap, does node support for trees generated in any 
 manner (e.g.: jackknifing) 
 master is modified to have node.bootstrap_support incremented by 1 if 
 subsampled tree has support for that node'"
"def filter_factory(global_conf, **local_conf): 
    conf = global_conf.copy() 
   conf.update(local_conf) 
   register_swift_info('formpost') 
   return (lambda app: FormPost(app, conf))", 'Filter factory for FormPost.','Returns the WSGI filter for use with paste.deploy.'
"def dlcs_api_request(path, params='', user='', passwd='', throttle=True): 
    if throttle: 
      Waiter() 
   if params: 
      url = ('%s/%s?%s' % (DLCS_API, path, urllib.urlencode(dict0(params)))) 
   else: 
      url = ('%s/%s' % (DLCS_API, path)) 
   if DEBUG: 
      print >>sys.stderr, ('dlcs_api_request:   %s' % url) 
   try: 
      return http_auth_request(url, DLCS_API_HOST, user, passwd, USER_AGENT) 
   except DefaultErrorHandler as e: 
      print >>sys.stderr, ('%s' % e)"," 'Make an HTTP request to the DLCS API. 
 :param path: the path to the API 
 :param params: the parameters to pass to the API 
 :param user: the username to use for authentication 
 :param passwd: the password to use for authentication 
 :param throttle: if True, wait for the rate limit to reset 
 :return: the response from the API'","'Retrieve/query a path within the del.icio.us API. 
 This implements a minimum interval between calls to avoid 
 throttling. [#]_ Use param \'throttle\' to turn this behaviour off. 
 @todo: back off on 503\'s (HTTPError, URLError? @todo: testing). 
 Returned XML does not always correspond with given del.icio.us examples 
 @todo: (cf. help/api/... and post\'s attributes) 
 .. [#] http://del.icio.us/help/api/'"
"def conserve_mpmath_dps(func): 
    import functools 
   import mpmath 
   def func_wrapper(*args, **kwargs): 
      dps = mpmath.mp.dps 
      try: 
         return func(*args, **kwargs) 
      finally: 
         mpmath.mp.dps = dps 
   func_wrapper = functools.update_wrapper(func_wrapper, func) 
   return func_wrapper", 'conserve mpmath dps after each call to a function',"'After the function finishes, resets the value of mpmath.mp.dps to 
 the value it had before the function was run.'"
"def pathMatchPatterns(path, repos): 
    for repo in repos: 
      if fnmatch(path, repo): 
         return True 
   return False"," 'Return True if the given path matches any of the patterns in the given 
 repos. 
 :param path: The path to test. 
 :param repos: The list of paths to match against. 
 :returns: True if the path matches any of the patterns, False otherwise.'","'Check existence of given path against list of path patterns 
 The pattern definition is the as fnmatch.fnmatch.'"
"def minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None): 
    x0 = np.asarray(x0) 
   if (x0.dtype.kind in np.typecodes['AllInteger']): 
      x0 = np.asarray(x0, dtype=float) 
   if (not isinstance(args, tuple)): 
      args = (args,) 
   if (method is None): 
      if constraints: 
         method = 'SLSQP' 
      elif (bounds is not None): 
         method = 'L-BFGS-B' 
      else: 
         method = 'BFGS' 
   if callable(method): 
      meth = '_custom' 
   else: 
      meth = method.lower() 
   if (options is None): 
      options = {} 
   if ((meth in ['nelder-mead', 'powell', 'cobyla']) and bool(jac)): 
      warn(('Method   %s   does   not   use   gradient   information   (jac).' % method), RuntimeWarning) 
   if ((meth not in ('newton-cg', 'dogleg', 'trust-ncg', '_custom')) and (hess is not None)): 
      warn(('Method   %s   does   not   use   Hessian   information   (hess).' % method), RuntimeWarning) 
   if ((meth not in ('newton-cg', 'dogleg', 'trust-ncg', '_custom')) and (hessp is not None)): 
      warn(('Method   %s   does   not   use   Hessian-vector   product   information   (hessp).' % method), RuntimeWarning) 
   if ((meth in ['nelder-mead', 'powell', 'cg', 'bfgs', 'newton-cg', 'dogleg', 'trust-ncg']) and ((bounds is not None) or np.any(constraints))): 
      warn(('Method   %s   cannot   handle   constraints   nor   bounds.' % method), RuntimeWarning) 
   if ((meth in ['l-bfgs-b', 'tnc']) and np.any(constraints)): 
      warn(('Method   %s   cannot   handle   constraints.' % method), RuntimeWarning) 
   if ((meth == 'cobyla') and (bounds is not None)): 
      warn(('Method   %s   cannot   handle   bounds.' % method), RuntimeWarning) 
   if ((meth in ['cobyla']) and (callback is not None)): 
      warn(('Method   %s   does   not   support   callback.' % method), RuntimeWarning) 
   if ((meth in ['l-bfgs-b', 'tnc', 'cobyla', 'slsqp']) and options.get('return_all', False)): 
      warn(('Method   %s   does   not   support   the   return_all   option.' % method), RuntimeWarning) 
   if (not callable(jac)): 
      if bool(jac): 
         fun = MemoizeJac(fun) 
         jac = fun.derivative 
      else: 
         jac = None 
   if (tol is not None): 
      options = dict(options) 
      if (meth == 'nelder-mead'): 
         options.setdefault('xatol', tol) 
         options.setdefault('fatol', tol) 
      if (meth in ['newton-cg', 'powell', 'tnc']): 
         options.setdefault('xtol', tol) 
      if (meth in ['powell', 'l-bfgs-b', 'tnc', 'slsqp']): 
         options.setdefault('ftol', tol) 
      if (meth in ['bfgs', 'cg', 'l-bfgs-b', 'tnc', 'dogleg', 'trust-ncg']): 
         options.setdefault('gtol', tol) 
      if (meth in ['cobyla', '_custom']): 
         options.setdefault('tol', tol) 
   if (meth == '_custom'): 
      return method(fun, x0, args=args, jac=jac, hess=hess, hessp=hessp, bounds=bounds, constraints=constraints, callback=callback, **options) 
   elif (meth == 'nelder-mead'): 
      return _minimize_neldermead(fun, x0, args, callback, **options) 
   elif (meth == 'powell'): 
      return _minimize_powell(fun, x0, args, callback, **options) 
   elif (meth == 'cg'): 
      return _minimize_cg(fun, x0, args, jac, callback, **options) 
   elif (meth == 'bfgs'): 
      return _minimize_bfgs(fun, x0, args, jac, callback, **options) 
   elif (meth == 'newton-cg'): 
      return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback, **options) 
   elif (meth == 'l-bfgs-b'): 
      return _minimize_lbfgsb(fun, x0, args, jac, bounds, callback=callback, **options) 
   elif (meth == 'tnc'): 
      return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback, **options) 
   elif (meth == 'cobyla'): 
      return _minimize_cobyla(fun, x0, args, constraints, **options) 
   elif (meth == 'slsqp'): 
      return _minimize_slsqp(fun, x0, args, jac, bounds, constraints, callback=callback, **options) 
   elif (meth == 'dogleg'): 
      return _minimize_dogleg(fun, x0, args, jac, hess, callback=callback, **options) 
   elif (meth == 'trust-ncg'): 
      return _minimize_trust_ncg(fun, x0, args, jac, hess, hessp, callback=callback, **options) 
   else: 
      raise ValueError(('Unknown   solver   %s' % method))"," 'Minimize a function. 
 Parameters 
 fun : callable 
 Function to minimize. 
 x0 : array 
 Initial guess. 
 args : tuple of callables, optional 
 Arguments to pass to the objective function. 
 method : str, optional 
 Method to use. 
 jac : callable, optional 
 Jacobian of the objective function. 
 hess : callable, optional 
 Hessian of the objective function. 
 hessp : callable, optional 
 Hessian-vector product of the objective function. 
 bounds : array, optional 
 Bounds to use. 
 constraints : array, optional 
 Constraints to use. 
 tol : float, optional 
 Tolerance for the optimization. 
 callback : callable, optional 
 Callback function to be called during the optimization. 
 options : dict, optional 
 Options to pass to the solver. 
 Returns 
 x : array 
 Optimal value of the objective function. 
 Notes 
 This function calls the minimize function of the solver. 
 See Also 
 minimize_","'Minimization of scalar function of one or more variables. 
 In general, the optimization problems are of the form:: 
 minimize f(x) subject to 
 g_i(x) >= 0,  i = 1,...,m 
 h_j(x)  = 0,  j = 1,...,p 
 where x is a vector of one or more variables. 
 ``g_i(x)`` are the inequality constraints. 
 ``h_j(x)`` are the equality constrains. 
 Optionally, the lower and upper bounds for each element in x can also be 
 specified using the `bounds` argument. 
 Parameters 
 fun : callable 
 Objective function. 
 x0 : ndarray 
 Initial guess. 
 args : tuple, optional 
 Extra arguments passed to the objective function and its 
 derivatives (Jacobian, Hessian). 
 method : str or callable, optional 
 Type of solver.  Should be one of 
 - \'Nelder-Mead\' :ref:`(see here) <optimize.minimize-neldermead>` 
 - \'Powell\'      :ref:`(see here) <optimize.minimize-powell>` 
 - \'CG\'          :ref:`(see here) <optimize.minimize-cg>` 
 - \'BFGS\'        :ref:`(see here) <optimize.minimize-bfgs>` 
 - \'Newton-CG\'   :ref:`(see here) <optimize.minimize-newtoncg>` 
 - \'L-BFGS-B\'    :ref:`(see here) <optimize.minimize-lbfgsb>` 
 - \'TNC\'         :ref:`(see here) <optimize.minimize-tnc>` 
 - \'COBYLA\'      :ref:`(see here) <optimize.minimize-cobyla>` 
 - \'SLSQP\'       :ref:`(see here) <optimize.minimize-slsqp>` 
 - \'dogleg\'      :ref:`(see here) <optimize.minimize-dogleg>` 
 - \'trust-ncg\'   :ref:`(see here) <optimize.minimize-trustncg>` 
 - custom - a callable object (added in version 0.14.0), 
 see below for description. 
 If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``, 
 depending if the problem has constraints or bounds. 
 jac : bool or callable, optional 
 Jacobian (gradient) of objective function. Only for CG, BFGS, 
 Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. 
 If `jac` is a Boolean and is True, `fun` is assumed to return the 
 gradient along with the objective function. If False, the 
 gradient will be estimated numerically. 
 `jac` can also be a callable returning the gradient of the 
 objective. In this case, it must accept the same arguments as `fun`. 
 hess, hessp : callable, optional 
 Hessian (matrix of second-order derivatives) of objective function or 
 Hessian of objective function times an arbitrary vector p.  Only for 
 Newton-CG, dogleg, trust-ncg. 
 Only one of `hessp` or `hess` needs to be given.  If `hess` is 
 provided, then `hessp` will be ignored.  If neither `hess` nor 
 `hessp` is provided, then the Hessian product will be approximated 
 using finite differences on `jac`. `hessp` must compute the Hessian 
 times an arbitrary vector. 
 bounds : sequence, optional 
 Bounds for variables (only for L-BFGS-B, TNC and SLSQP). 
 ``(min, max)`` pairs for each element in ``x``, defining 
 the bounds on that parameter. Use None for one of ``min`` or 
 ``max`` when there is no bound in that direction. 
 constraints : dict or sequence of dict, optional 
 Constraints definition (only for COBYLA and SLSQP). 
 Each constraint is defined in a dictionary with fields: 
 type : str 
 Constraint type: \'eq\' for equality, \'ineq\' for inequality. 
 fun : callable 
 The function defining the constraint. 
 jac : callable, optional 
 The Jacobian of `fun` (only for SLSQP). 
 args : sequence, optional 
 Extra arguments to be passed to the function and Jacobian. 
 Equality constraint means that the constraint function result is to 
 be zero whereas inequality means that it is to be non-negative. 
 Note that COBYLA only supports inequality constraints. 
 tol : float, optional 
 Tolerance for termination. For detailed control, use solver-specific 
 options. 
 options : dict, optional 
 A dictionary of solver options. All methods accept the following 
 generic options: 
 maxiter : int 
 Maximum number of iterations to perform. 
 disp : bool 
 Set to True to print convergence messages. 
 For method-specific options, see :func:`show_options()`. 
 callback : callable, optional 
 Called after each iteration, as ``callback(xk)``, where ``xk`` is the 
 current parameter vector. 
 Returns 
 res : OptimizeResult 
 The optimization result represented as a ``OptimizeResult`` object. 
 Important attributes are: ``x`` the solution array, ``success`` a 
 Boolean flag indicating if the optimizer exited successfully and 
 ``message`` which describes the cause of the termination. See 
 `OptimizeResult` for a description of other attributes. 
 See also 
 minimize_scalar : Interface to minimization algorithms for scalar 
 univariate functions 
 show_options : Additional options accepted by the solvers 
 Notes 
 This section describes the available solvers that can be selected by the 
 \'method\' parameter. The default method is *BFGS*. 
 **Unconstrained minimization** 
 Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the 
 Simplex algorithm [1]_, [2]_. This algorithm is robust in many 
 applications. However, if numerical computation of derivative can be 
 trusted, other algorithms using the first and/or second derivatives 
 information might be preferred for their better performance in 
 general. 
 Method :ref:`Powell <optimize.minimize-powell>` is a modification 
 of Powell\'s method [3]_, [4]_ which is a conjugate direction 
 method. It performs sequential one-dimensional minimizations along 
 each vector of the directions set (`direc` field in `options` and 
 `info`), which is updated at each iteration of the main 
 minimization loop. The function need not be differentiable, and no 
 derivatives are taken. 
 Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate 
 gradient algorithm by Polak and Ribiere, a variant of the 
 Fletcher-Reeves method described in [5]_ pp.  120-122. Only the 
 first derivatives are used. 
 Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton 
 method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_ 
 pp. 136. It uses the first derivatives only. BFGS has proven good 
 performance even for non-smooth optimizations. This method also 
 returns an approximation of the Hessian inverse, stored as 
 `hess_inv` in the OptimizeResult object. 
 Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a 
 Newton-CG algorithm [5]_ pp. 168 (also known as the truncated 
 Newton method). It uses a CG method to the compute the search 
 direction. See also *TNC* method for a box-constrained 
 minimization with a similar algorithm. 
 Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg 
 trust-region algorithm [5]_ for unconstrained minimization. This 
 algorithm requires the gradient and Hessian; furthermore the 
 Hessian is required to be positive definite. 
 Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the 
 Newton conjugate gradient trust-region algorithm [5]_ for 
 unconstrained minimization. This algorithm requires the gradient 
 and either the Hessian or a function that computes the product of 
 the Hessian with a given vector. 
 **Constrained minimization** 
 Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B 
 algorithm [6]_, [7]_ for bound constrained minimization. 
 Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton 
 algorithm [5]_, [8]_ to minimize a function with variables subject 
 to bounds. This algorithm uses gradient information; it is also 
 called Newton Conjugate-Gradient. It differs from the *Newton-CG* 
 method described above as it wraps a C implementation and allows 
 each variable to be given upper and lower bounds. 
 Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the 
 Constrained Optimization BY Linear Approximation (COBYLA) method 
 [9]_, [10]_, [11]_. The algorithm is based on linear 
 approximations to the objective function and each constraint. The 
 method wraps a FORTRAN implementation of the algorithm. The 
 constraints functions \'fun\' may return either a single number 
 or an array or list of numbers. 
 Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential 
 Least SQuares Programming to minimize a function of several 
 variables with any combination of bounds, equality and inequality 
 constraints. The method wraps the SLSQP Optimization subroutine 
 originally implemented by Dieter Kraft [12]_. Note that the 
 wrapper handles infinite values in bounds by converting them into 
 large floating values. 
 **Custom minimizers** 
 It may be useful to pass a custom minimization method, for example 
 when using a frontend to this method such as `scipy.optimize.basinhopping` 
 or a different library.  You can simply pass a callable as the ``method`` 
 parameter. 
 The callable is called as ``method(fun, x0, args, **kwargs, **options)`` 
 where ``kwargs`` corresponds to any other parameters passed to `minimize` 
 (such as `callback`, `hess`, etc.), except the `options` dict, which has 
 its contents also passed as `method` parameters pair by pair.  Also, if 
 `jac` has been passed as a bool type, `jac` and `fun` are mangled so that 
 `fun` returns just the function values and `jac` is converted to a function 
 returning the Jacobian.  The method shall return an ``OptimizeResult`` 
 object. 
 The provided `method` callable must be able to accept (and possibly ignore) 
 arbitrary parameters; the set of parameters accepted by `minimize` may 
 expand in future versions and then these parameters will be passed to 
 the method.  You can find an example in the scipy.optimize tutorial. 
 .. versionadded:: 0.11.0 
 References 
 .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function 
 Minimization. The Computer Journal 7: 308-13. 
 .. [2] Wright M H. 1996. Direct search methods: Once scorned, now 
 respectable, in Numerical Analysis 1995: Proceedings of the 1995 
 Dundee Biennial Conference in Numerical Analysis (Eds. D F 
 Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK. 
 191-208. 
 .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of 
 a function of several variables without calculating derivatives. The 
 Computer Journal 7: 155-162. 
 .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery. 
 Numerical Recipes (any edition), Cambridge University Press. 
 .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization. 
 Springer New York. 
 .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory 
 Algorithm for Bound Constrained Optimization. SIAM Journal on 
 Scientific and Statistical Computing 16 (5): 1190-1208. 
 .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm 
 778: L-BFGS-B, FORTRAN routines for large scale bound constrained 
 optimization. ACM Transactions on Mathematical Software 23 (4): 
 550-560. 
 .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method. 
 1984. SIAM Journal of Numerical Analysis 21: 770-778. 
 .. [9] Powell, M J D. A direct search optimization method that models 
 the objective and constraint functions by linear interpolation. 
 1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez 
 and J-P Hennart, Kluwer Academic (Dordrecht), 51-67. 
 .. [10] Powell M J D. Direct search algorithms for optimization 
 calculations. 1998. Acta Numerica 7: 287-336. 
 .. [11] Powell M J D. A view of algorithms for optimization without 
 derivatives. 2007.Cambridge University Technical Report DAMTP 
 2007/NA03 
 .. [12] Kraft, D. A software package for sequential quadratic 
 programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace 
 Center -- Institute for Flight Mechanics, Koln, Germany. 
 Examples 
 Let us consider the problem of minimizing the Rosenbrock function. This 
 function (and its respective derivatives) is implemented in `rosen` 
 (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`. 
 >>> from scipy.optimize import minimize, rosen, rosen_der 
 A simple application of the *Nelder-Mead* method is: 
 >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2] 
 >>> res = minimize(rosen, x0, method=\'Nelder-Mead\', tol=1e-6) 
 >>> res.x 
 array([ 1.,  1.,  1.,  1.,  1.]) 
 Now using the *BFGS* algorithm, using the first derivative and a few 
 options: 
 >>> res = minimize(rosen, x0, method=\'BFGS\', jac=rosen_der, 
 ...                options={\'gtol\': 1e-6, \'disp\': True}) 
 Optimization terminated successfully. 
 Current function value: 0.000000 
 Iterations: 26 
 Function evaluations: 31 
 Gradient evaluations: 31 
 >>> res.x 
 array([ 1.,  1.,  1.,  1.,  1.]) 
 >>> print(res.message) 
 Optimization terminated successfully. 
 >>> res.hess_inv 
 array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary 
 [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269], 
 [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151], 
 [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ], 
 [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]]) 
 Next, consider a minimization problem with several constraints (namely 
 Example 16.4 from [5]_). The objective function is: 
 >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2 
 There are three constraints defined as: 
 >>> cons = ({\'type\': \'ineq\', \'fun\': lambda x:  x[0] - 2 * x[1] + 2}, 
 ...         {\'type\': \'ineq\', \'fun\': lambda x: -x[0] - 2 * x[1] + 6}, 
 ...         {\'type\': \'ineq\', \'fun\': lambda x: -x[0] + 2 * x[1] + 2}) 
 And variables must be positive, hence the following bounds: 
 >>> bnds = ((0, None), (0, None)) 
 The optimization problem is solved using the SLSQP method as: 
 >>> res = minimize(fun, (2, 0), method=\'SLSQP\', bounds=bnds, 
 ...                constraints=cons) 
 It should converge to the theoretical solution (1.4 ,1.7).'"
"@domain_constructor(loss_target=(-2)) 
 def distractor(): 
    x = hp.uniform('x', (-15), 15) 
   f1 = old_div(1.0, (1.0 + scope.exp((- x)))) 
   f2 = (2 * scope.exp((- ((x + 10) ** 2)))) 
   return {'loss': ((- f1) - f2), 'status': base.STATUS_OK}", 'Distractor',"'This is a nasty function: it has a max in a spike near -10, and a long 
 asymptote that is easy to find, but guides hill-climbing approaches away 
 from the true max. 
 The second peak is at x=-10. 
 The prior mean is 0.'"
"def is_inside_except(node): 
    current = node 
   while (current and (not isinstance(current.parent, astroid.ExceptHandler))): 
      current = current.parent 
   return (current and (current is current.parent.name))"," 'Returns True if node is inside an except block. 
 If node is inside a try block, returns True if it is inside an 
 except block. 
 :param node: A node to check. 
 :returns: True if node is inside an except block.'",'Returns true if node is inside the name of an except handler.'
"def dump(obj, fp, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding='utf-8', default=None, use_decimal=True, **kw): 
    if ((not skipkeys) and ensure_ascii and check_circular and allow_nan and (cls is None) and (indent is None) and (separators is None) and (encoding == 'utf-8') and (default is None) and (not kw)): 
      iterable = _default_encoder.iterencode(obj) 
   else: 
      if (cls is None): 
         cls = JSONEncoder 
      iterable = cls(skipkeys=skipkeys, ensure_ascii=ensure_ascii, check_circular=check_circular, allow_nan=allow_nan, indent=indent, separators=separators, encoding=encoding, default=default, use_decimal=use_decimal, **kw).iterencode(obj) 
   for chunk in iterable: 
      fp.write(chunk)"," 'Dumps an object to a file-like object. 
 The ``fp`` argument should be an object that supports the ``write()`` 
 method. 
 If ``skipkeys`` is ``True``, the ``dict`` keys will not be written. 
 If ``ensure_ascii`` is ``True``, the output will be encoded as UTF-8. 
 If ``check_circular`` is ``True``, circular references will be detected. 
 If ``allow_nan`` is ``True``, ``None`` values will be written as the 
 string ``None``. 
 If ``indent`` is a string, the output will be indented by that number of 
 spaces. 
 If ``separators`` is a string, it will be used to separate the keys and 
 values. 
 If ``encoding`` is a string, it will be used to encode the output. 
 If ``default`` is a string, it will be used as the default value for 
 missing values. 
 If ``use_decimal`` is ``True``, decimal values will be written as 
 strings. 
 The output will be written to the file in ch","'Serialize ``obj`` as a JSON formatted stream to ``fp`` (a 
 ``.write()``-supporting file-like object). 
 If ``skipkeys`` is true then ``dict`` keys that are not basic types 
 (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``) 
 will be skipped instead of raising a ``TypeError``. 
 If ``ensure_ascii`` is false, then the some chunks written to ``fp`` 
 may be ``unicode`` instances, subject to normal Python ``str`` to 
 ``unicode`` coercion rules. Unless ``fp.write()`` explicitly 
 understands ``unicode`` (as in ``codecs.getwriter()``) this is likely 
 to cause an error. 
 If ``check_circular`` is false, then the circular reference check 
 for container types will be skipped and a circular reference will 
 result in an ``OverflowError`` (or worse). 
 If ``allow_nan`` is false, then it will be a ``ValueError`` to 
 serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) 
 in strict compliance of the JSON specification, instead of using the 
 JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``). 
 If *indent* is a string, then JSON array elements and object members 
 will be pretty-printed with a newline followed by that string repeated 
 for each level of nesting. ``None`` (the default) selects the most compact 
 representation without any newlines. For backwards compatibility with 
 versions of simplejson earlier than 2.1.0, an integer is also accepted 
 and is converted to a string with that many spaces. 
 If ``separators`` is an ``(item_separator, dict_separator)`` tuple 
 then it will be used instead of the default ``(\', \', \': \')`` separators. 
 ``(\',\', \':\')`` is the most compact JSON representation. 
 ``encoding`` is the character encoding for str instances, default is UTF-8. 
 ``default(obj)`` is a function that should return a serializable version 
 of obj or raise TypeError. The default simply raises TypeError. 
 If *use_decimal* is true (default: ``True``) then decimal.Decimal 
 will be natively serialized to JSON with full precision. 
 To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the 
 ``.default()`` method to serialize additional types), specify it with 
 the ``cls`` kwarg.'"
"def require(source_module, target_module, all_macros=False, assignments={}, prefix=''): 
    seen_names = set() 
   if prefix: 
      prefix += '.' 
   for d in (_hy_macros, _hy_reader): 
      for (name, macro) in d[source_module].items(): 
         seen_names.add(name) 
         if all_macros: 
            d[target_module][(prefix + name)] = macro 
         elif (name in assignments): 
            d[target_module][(prefix + assignments[name])] = macro 
   if (not all_macros): 
      unseen = frozenset(assignments.keys()).difference(seen_names) 
      if unseen: 
         raise ImportError(('cannot   require   names:   ' + repr(list(unseen))))"," 'Require a module to have a set of macros. 
 This is a helper function for ``import`` and ``importlib.import_module`` 
 to require a module to have a set of macros. 
 The module to require is specified by the first argument, the module to 
 require to have the macros is the second argument. 
 The third argument is a dictionary of macros to assign to the module. 
 The fourth argument is a dictionary of names to assign to the module. 
 The fifth argument is the prefix to assign the macros to. 
 If ``all_macros`` is True, then all macros are required. 
 If ``all_macros`` is False, then only the macros that are not already 
 defined are required. 
 The module is not required to have the macros, it just has to define 
 them. 
 The macros are required to be defined as functions. 
 The macros are required to be defined with a single argument. 
 The macros are required to be defined with a single return value. 
 The macros are required to be defined with a single statement. 
 The macros","'Load macros from `source_module` in the namespace of 
 `target_module`. `assignments` maps old names to new names, but is 
 ignored if `all_macros` is true. If `prefix` is nonempty, it is 
 prepended to the name of each imported macro. (This means you get 
 macros named things like ""mymacromodule.mymacro"", which looks like 
 an attribute of a module, although it\'s actually just a symbol 
 with a period in its name.) 
 This function is called from the `require` special form in the compiler.'"
"def libvlc_media_list_player_get_state(p_mlp): 
    f = (_Cfunctions.get('libvlc_media_list_player_get_state', None) or _Cfunction('libvlc_media_list_player_get_state', ((1,),), None, State, MediaListPlayer)) 
   return f(p_mlp)"," 'Get the current state of the media list player. 
 @param p_mlp: the media list player. 
 @return: the current state of the media list player.'","'Get current libvlc_state of media list player. 
 @param p_mlp: media list player instance. 
 @return: libvlc_state_t for media list player.'"
"def get_c_init(r, name, sub): 
    pre = ('\n            py_%(name)s   =   Py_None;\n            {Py_XINCREF(py_%(name)s);}\n            ' % locals()) 
   return (pre + r.type.c_init(name, sub))"," 'Generate a C function to initialize a PyObject. 
 The function is called ``py_%(name)s`` and takes a single 
 argument ``sub``. 
 Parameters 
 r : rpython.rtyper.lltypes.r_class 
 The class to be initialized. 
 name : str 
 The name of the class to be initialized. 
 sub : rpython.rtyper.lltypes.r_subtype 
 The subtype to be initialized. 
 Returns 
 str 
 The generated C code.'",'Wrapper around c_init that initializes py_name to Py_None.'
"def exp(x): 
    np = import_module('numpy') 
   if isinstance(x, (int, float)): 
      return interval(np.exp(x), np.exp(x)) 
   elif isinstance(x, interval): 
      return interval(np.exp(x.start), np.exp(x.end), is_valid=x.is_valid) 
   else: 
      raise NotImplementedError"," 'Returns an interval containing the values of `x` that are in the 
 interval `[e^(-x), e^x]`. 
 Parameters 
 x : scalar 
 A real number. 
 Returns 
 interval 
 An interval containing the values of `x` that are in the 
 interval `[e^(-x), e^x]`.'",'evaluates the exponential of an interval'
"def _getinfos_http_client_authorization(spec): 
    infos = {} 
   fullinfos = {} 
   data = spec.get('fullvalue', spec['value']).split(None, 1) 
   if data[1:]: 
      if (data[0].lower() == 'basic'): 
         try: 
            (infos['username'], infos['password']) = ''.join(data[1].strip()).decode('base64').decode('latin-1').split(':', 1) 
            for field in ['username', 'password']: 
               if (len(infos[field]) > utils.MAXVALLEN): 
                  fullinfos[field] = infos[field] 
                  infos[field] = infos[field][:utils.MAXVALLEN] 
         except Exception: 
            pass 
      elif (data[0].lower() == 'digest'): 
         try: 
            infos = dict(((value.split('=', 1) if ('=' in value) else [value, None]) for value in _split_digest_auth(data[1].strip()))) 
            for (key, value) in infos.items(): 
               if (value.startswith('""') and value.endswith('""')): 
                  infos[key] = value[1:(-1)] 
         except Exception: 
            pass 
   res = {} 
   if infos: 
      res['infos'] = infos 
   if fullinfos: 
      res['fullinfos'] = fullinfos 
   return res"," 'Return a dictionary containing the HTTP client authorization information. 
 :param spec: the spec to extract the HTTP client authorization information from 
 :return: a dictionary containing the HTTP client authorization information'","'Extract (for now) the usernames and passwords from Basic 
 authorization headers'"
"def setup(base_path, root_module_name='autotest'): 
    if (root_module_name in sys.modules): 
      return 
   _create_module_and_parents(root_module_name) 
   imp.load_package(root_module_name, base_path) 
   sys.path.insert(0, os.path.join(base_path, 'site-packages'))", 'Load the module tree and all dependencies.',"'Setup a library namespace, with the appropriate top root module name. 
 Perform all the necessary setup so that all the packages at 
 \'base_path\' can be imported via ""import root_module_name.package"". 
 :param base_path: Base path for the module. 
 :param root_module_name: Top level name for the module.'"
"def get_messages(request): 
    return getattr(request, '_messages', [])", 'Returns a list of messages.',"'Returns the message storage on the request if it exists, otherwise returns 
 an empty list.'"
"def configure_paramiko_logging(): 
    l = logging.getLogger('paramiko') 
   l.setLevel(logging.DEBUG) 
   static.create_sc_config_dirs() 
   lh = logging.handlers.RotatingFileHandler(static.SSH_DEBUG_FILE, maxBytes=1048576, backupCount=2) 
   lh.setLevel(logging.DEBUG) 
   format = (('PID:   %s   ' % str(static.PID)) + '%(levelname)-.3s   [%(asctime)s.%(msecs)03d]   thr=%(_threadid)-3d   %(name)s:   %(message)s') 
   date_format = '%Y%m%d-%H:%M:%S' 
   lh.setFormatter(logging.Formatter(format, date_format)) 
   l.addHandler(lh)", 'Configure logging for paramiko','Configure ssh to log to a file for debug'
"def fixed_ip_get_by_host(context, host): 
    return IMPL.fixed_ip_get_by_host(context, host)", 'Get a fixed IP address for a host.','Get fixed IPs by compute host.'
"def check_non_negative(X, whom): 
    X = (X.data if sp.issparse(X) else X) 
   if (X < 0).any(): 
      raise ValueError(('Negative   values   in   data   passed   to   %s' % whom))", 'Check that the data passed to a function is non-negative.',"'Check if there is any negative value in an array. 
 Parameters 
 X : array-like or sparse matrix 
 Input data. 
 whom : string 
 Who passed X to this function.'"
"def main(): 
    initLogging(verbose=True) 
   initExperimentPrng() 
   @staticmethod 
   def _mockCreate(*args, **kwargs): 
      kwargs.pop('implementation', None) 
      return CLAClassifierDiff(*args, **kwargs) 
   CLAClassifierFactory.create = _mockCreate 
   runExperiment(sys.argv[1:])"," 'This is the main function of the script. It creates a new CLAClassifier 
 and runs the experiment. 
 :param args: 
 :param kwargs: 
 :returns: 
 :rtype: 
 :mnemonic: 
 :doc: 
 :author: 
 :date: 
 :license: 
 :copyright: 
 :credits: 
 :license_url: 
 :copyright_url: 
 :trademark_url: 
 :bugs: 
 :support: 
 :todo: 
 :ideas: 
 :version: 
 :history: 
 :contributors: 
 :contributor_since: 
 :contributor_email: 
 :status: 
 :notes: 
 :platform: 
 :requires: 
 :requires_dist: 
 :requires_setup: 
 :depends_on: 
 :depends_on_dist: 
 :depends_on_setup: 
 :provides: 
 :provides_dist: 
 :provides",'Run according to options in sys.argv and diff classifiers.'
"@pytest.mark.cmd 
 @pytest.mark.django_db 
 def test_refresh_scores_reset_user(capfd): 
    call_command('refresh_scores', '--reset', '--user=system') 
   (out, err) = capfd.readouterr() 
   assert ('Scores   for   specified   users   were   reset   to   0.' in out)", 'Test that the scores for a user are reset to zero when --reset is passed','Set scores to zero for given users.'
"def download_cover(log, title=None, authors=None, identifiers={}, timeout=30): 
    rq = Queue() 
   abort = Event() 
   run_download(log, rq, abort, title=title, authors=authors, identifiers=identifiers, timeout=timeout, get_best_cover=True) 
   results = [] 
   while True: 
      try: 
         results.append(rq.get_nowait()) 
      except Empty: 
         break 
   cp = msprefs[u'cover_priorities'] 
   def keygen(result): 
      (plugin, width, height, fmt, data) = result 
      return (cp.get(plugin.name, 1), (1 / (width * height))) 
   results.sort(key=keygen) 
   return (results[0] if results else None)"," 'Download a cover image from the given log. 
 :param log: The log to download the cover from. 
 :param title: The title of the work to be covered. 
 :param authors: A list of authors for the work. 
 :param identifiers: A dictionary of identifiers for the work. 
 :param timeout: The timeout in seconds for the download. 
 :returns: The cover image, or None if none was found. 
 :rtype: tuple(str, int, tuple(str, int, str, bytes))'","'Synchronous cover download. Returns the ""best"" cover as per user 
 prefs/cover resolution. 
 Returned cover is a tuple: (plugin, width, height, fmt, data) 
 Returns None if no cover is found.'"
"def _map_to_list(game_map): 
    list_map = game_map.split('\n') 
   return [(character.decode('UTF-8') if isinstance(character, basestring) else character) for character in list_map]"," 'Convert a game map string to a list of characters. 
 :param game_map: 
 A game map string. 
 :return: 
 A list of characters.'","'Splits multi line map string into list of rows, treats for UTF-8 encoding. 
 Args: 
 game_map (str): An ASCII map 
 Returns: 
 list (list): The map split into rows'"
"def _indent(elem, level=0): 
    i = ('\n' + (level * '      ')) 
   if len(elem): 
      if ((not elem.text) or (not elem.text.strip())): 
         elem.text = (i + '      ') 
      for e in elem: 
         _indent(e, (level + 1)) 
         if ((not e.tail) or (not e.tail.strip())): 
            e.tail = (i + '      ') 
      if ((not e.tail) or (not e.tail.strip())): 
         e.tail = i 
   elif (level and ((not elem.tail) or (not elem.tail.strip()))): 
      elem.tail = i", 'Indent an element and all its children.',"'Add line breaks and indentation to ElementTree in-place. 
 Sources: 
 - http://effbot.org/zone/element-lib.htm#prettyprint 
 - http://infix.se/2007/02/06/gentlemen-indent-your-xml'"
"def setvariable(cursor, mysqlvar, value): 
    query = ('SET   GLOBAL   %s   =   ' % mysql_quote_identifier(mysqlvar, 'vars')) 
   try: 
      cursor.execute((query + '%s'), (value,)) 
      cursor.fetchall() 
      result = True 
   except Exception: 
      e = get_exception() 
      result = str(e) 
   return result"," 'Set a variable in the mysql server. 
 :param cursor: A cursor object 
 :param mysqlvar: The name of the variable to set 
 :param value: The value to set the variable to'","'Set a global mysql variable to a given value 
 The DB driver will handle quoting of the given value based on its 
 type, thus numeric strings like \'3.0\' or \'8\' are illegal, they 
 should be passed as numeric literals.'"
"def group_membership(): 
    s3db.hrm_configure_pr_group_membership() 
   table = db.pr_group_membership 
   gtable = db.pr_group 
   htable = s3db.hrm_human_resource 
   s3.filter = ((((gtable.system == False) & (gtable.group_type == 3)) & (htable.type == 1)) & (htable.person_id == table.person_id)) 
   def prep(r): 
      if (r.method in ('create', 'create.popup', 'update', 'update.popup')): 
         person_id = get_vars.get('~.person_id', None) 
         if person_id: 
            field = table.person_id 
            field.default = person_id 
            field.readable = field.writable = False 
      return True 
   s3.prep = prep 
   output = s3_rest_controller('pr', 'group_membership', csv_stylesheet=('hrm', 'group_membership.xsl'), csv_template='group_membership') 
   return output", 'Group membership for human resources',"'Membership controller 
 - uses the group_membership table from PR'"
"def interface_is_portchannel(interface, module): 
    intf_type = get_interface_type(interface) 
   if (intf_type == 'ethernet'): 
      command = ('show   interface   ' + interface) 
      body = execute_show_command(command, module) 
      try: 
         interface_table = body[0]['TABLE_interface']['ROW_interface'] 
      except (KeyError, AttributeError, IndexError): 
         interface_table = None 
      if interface_table: 
         state = interface_table.get('eth_bundle') 
         if state: 
            return True 
         else: 
            return False 
   return False", 'Check if an interface is a portchannel.',"'Checks to see if an interface is part of portchannel bundle 
 Args: 
 interface (str): full name of interface, i.e. Ethernet1/1 
 Returns: 
 True/False based on if interface is a member of a portchannel bundle'"
"def wc_reducer(word, counts): 
    (yield (word, sum(counts)))", 'word count reducer','sum up the counts for a word'
"def serviceCommand(methodName, cmdClass=ServiceWrapperCommand): 
    def wrapper(obj, journal, *args, **kwargs): 
      return journal.executeCommand(cmdClass(methodName, args, kwargs)) 
   return wrapper", 'Decorator that wraps the given method with a command.',"'Wrap methods into commands for a journalled service. 
 The resulting callable will have signature identical to wrapped 
 function, except that it expects journal as first argument, and 
 returns a Deferred.'"
"def _native_package_type(): 
    distribution_name = CURRENT_DISTRIBUTION.name.lower() 
   for (package_type, distribution_names) in PACKAGE_TYPE_MAP.items(): 
      if (distribution_name.lower() in distribution_names): 
         return package_type 
   else: 
      raise ValueError('Unknown   distribution.', distribution_name)"," 'Returns the name of the native package type for the current distribution. 
 :rtype: str'",':return: The ``bytes`` name of the native package format for this platform.'
"@functools.lru_cache(maxsize=None) 
 def get_callable(lookup_view): 
    if callable(lookup_view): 
      return lookup_view 
   if (not isinstance(lookup_view, str)): 
      raise ViewDoesNotExist((""'%s'   is   not   a   callable   or   a   dot-notation   path"" % lookup_view)) 
   (mod_name, func_name) = get_mod_func(lookup_view) 
   if (not func_name): 
      raise ImportError((""Could   not   import   '%s'.   The   path   must   be   fully   qualified."" % lookup_view)) 
   try: 
      mod = import_module(mod_name) 
   except ImportError: 
      (parentmod, submod) = get_mod_func(mod_name) 
      if (submod and (not module_has_submodule(import_module(parentmod), submod))): 
         raise ViewDoesNotExist((""Could   not   import   '%s'.   Parent   module   %s   does   not   exist."" % (lookup_view, mod_name))) 
      else: 
         raise 
   else: 
      try: 
         view_func = getattr(mod, func_name) 
      except AttributeError: 
         raise ViewDoesNotExist((""Could   not   import   '%s'.   View   does   not   exist   in   module   %s."" % (lookup_view, mod_name))) 
      else: 
         if (not callable(view_func)): 
            raise ViewDoesNotExist((""Could   not   import   '%s.%s'.   View   is   not   callable."" % (mod_name, func_name))) 
         return view_func"," 'Returns a callable for a given view name. 
 :param lookup_view: 
 :type lookup_view: 
 :return: 
 :rtype:'","'Return a callable corresponding to lookup_view. 
 * If lookup_view is already a callable, return it. 
 * If lookup_view is a string import path that can be resolved to a callable, 
 import that callable and return it, otherwise raise an exception 
 (ImportError or ViewDoesNotExist).'"
"def process_validation(validation, is_compatibility=False, file_hash=None): 
    validation = fix_addons_linter_output(validation) 
   if is_compatibility: 
      mangle_compatibility_messages(validation) 
   validation.setdefault('ending_tier', 0) 
   if ((not validation['ending_tier']) and validation['messages']): 
      validation['ending_tier'] = max((msg.get('tier', (-1)) for msg in validation['messages'])) 
   if file_hash: 
      ValidationComparator(validation).annotate_results(file_hash) 
   limit_validation_results(validation) 
   htmlify_validation(validation) 
   return validation"," 'Processes validation results. 
 :param validation: 
 :param is_compatibility: 
 :param file_hash: 
 :return: 
 :rtype: 
 :raises: 
 :raises: ValueError 
 :raises: TypeError 
 :raises: AssertionError 
 :raises: LinterError 
 :raises: LinterError.ValidationError 
 :raises: LinterError.LinterError'","'Process validation results into the format expected by the web 
 frontend, including transforming certain fields into HTML,  mangling 
 compatibility messages, and limiting the number of messages displayed.'"
"@login_required 
 @enforce_shopping_cart_enabled 
 def show_cart(request): 
    cart = Order.get_cart_for_user(request.user) 
   (is_any_course_expired, expired_cart_items, expired_cart_item_names, valid_cart_item_tuples) = verify_for_closed_enrollment(request.user, cart) 
   site_name = configuration_helpers.get_value('SITE_NAME', settings.SITE_NAME) 
   if is_any_course_expired: 
      for expired_item in expired_cart_items: 
         Order.remove_cart_item_from_order(expired_item, request.user) 
      cart.update_order_type() 
   callback_url = request.build_absolute_uri(reverse('shoppingcart.views.postpay_callback')) 
   form_html = render_purchase_form_html(cart, callback_url=callback_url) 
   context = {'order': cart, 'shoppingcart_items': valid_cart_item_tuples, 'amount': cart.total_cost, 'is_course_enrollment_closed': is_any_course_expired, 'expired_course_names': expired_cart_item_names, 'site_name': site_name, 'form_html': form_html, 'currency_symbol': settings.PAID_COURSE_REGISTRATION_CURRENCY[1], 'currency': settings.PAID_COURSE_REGISTRATION_CURRENCY[0], 'enable_bulk_purchase': configuration_helpers.get_value('ENABLE_SHOPPING_CART_BULK_PURCHASE', True)} 
   return render_to_response('shoppingcart/shopping_cart.html', context)"," 'Show shopping cart for the current user. 
 This is the page that is displayed when the user clicks the shopping cart 
 icon in the top right corner of the page. 
 This page is only available when the shopping cart is enabled. 
 :param request: The current request. 
 :return: A response that renders the shopping cart page.'",'This view shows cart items.'
"def libvlc_video_set_deinterlace(p_mi, psz_mode): 
    f = (_Cfunctions.get('libvlc_video_set_deinterlace', None) or _Cfunction('libvlc_video_set_deinterlace', ((1,), (1,)), None, None, MediaPlayer, ctypes.c_char_p)) 
   return f(p_mi, psz_mode)"," 'Set deinterlacing mode. 
 @param p_mi: the media player. 
 @param psz_mode: the mode to set. 
 @return: 0 if OK, -1 on error.'","'Enable or disable deinterlace filter. 
 @param p_mi: libvlc media player. 
 @param psz_mode: type of deinterlace filter, NULL to disable.'"
"def PostVimMessage(message, warning=True, truncate=False): 
    echo_command = (u'echom' if warning else u'echo') 
   vim.command(u'redraw') 
   if warning: 
      vim.command(u'echohl   WarningMsg') 
   message = ToUnicode(message) 
   if truncate: 
      vim_width = GetIntValue(u'&columns') 
      message = message.replace(u'\n', u'   ') 
      if (len(message) > vim_width): 
         message = (message[:(vim_width - 4)] + u'...') 
      old_ruler = GetIntValue(u'&ruler') 
      old_showcmd = GetIntValue(u'&showcmd') 
      vim.command(u'set   noruler   noshowcmd') 
      vim.command(u""{0}   '{1}'"".format(echo_command, EscapeForVim(message))) 
      SetVariableValue(u'&ruler', old_ruler) 
      SetVariableValue(u'&showcmd', old_showcmd) 
   else: 
      for line in message.split(u'\n'): 
         vim.command(u""{0}   '{1}'"".format(echo_command, EscapeForVim(line))) 
   if warning: 
      vim.command(u'echohl   None')"," 'Post a message to Vim. 
 :param message: The message to post. 
 :param warning: If True, then a warning is printed to the console. 
 :param truncate: If True, then the message is truncated to fit in the 
 current window. 
 :return: None'","'Display a message on the Vim status line. By default, the message is 
 highlighted and logged to Vim command-line history (see :h history). 
 Unset the |warning| parameter to disable this behavior. Set the |truncate| 
 parameter to avoid hit-enter prompts (see :h hit-enter) when the message is 
 longer than the window width.'"
"def getRadiusByPrefix(prefix, sideLength, xmlElement): 
    radius = getFloatByPrefixSide((prefix + 'radius'), sideLength, xmlElement) 
   radius += (0.5 * getFloatByPrefixSide((prefix + 'diameter'), sideLength, xmlElement)) 
   return (radius + (0.5 * getFloatByPrefixSide((prefix + 'size'), sideLength, xmlElement)))"," 'Return the radius of a shape with the given prefix. 
 :param prefix: The prefix of the shape. 
 :param sideLength: The length of the side of the shape. 
 :param xmlElement: The xml element of the shape. 
 :return: The radius of the shape.'",'Get radius by prefix.'
"def addListsToRepository(fileNameHelp, getProfileDirectory, repository): 
    repository.displayEntities = [] 
   repository.executeTitle = None 
   repository.fileNameHelp = fileNameHelp 
   repository.fileNameInput = None 
   repository.lowerName = fileNameHelp.split('.')[(-2)] 
   repository.baseName = (repository.lowerName + '.csv') 
   repository.baseNameSynonym = None 
   repository.capitalizedName = getEachWordCapitalized(repository.lowerName) 
   repository.getProfileDirectory = getProfileDirectory 
   repository.openLocalHelpPage = HelpPage().getOpenFromDocumentationSubName(repository.fileNameHelp) 
   repository.openWikiManualHelpPage = None 
   repository.preferences = [] 
   repository.repositoryDialog = None 
   repository.saveListenerTable = {} 
   repository.title = (repository.capitalizedName + '   Settings') 
   repository.menuEntities = [] 
   repository.saveCloseTitle = 'Save   and   Close' 
   repository.windowPosition = WindowPosition().getFromValue(repository, '0+0') 
   for setting in repository.preferences: 
      setting.repository = repository"," 'Adds the necessary objects to the repository object. 
 :param fileNameHelp: The name of the help file to use for the repository 
 :param getProfileDirectory: A function that returns the directory where the 
 user\'s preferences are stored 
 :param repository: The repository object to add the objects to'",'Add the value to the lists.'
"def update(context, id, name, description, is_public=None): 
    if (id is None): 
      msg = _('id   cannot   be   None') 
      raise exception.InvalidGroupType(reason=msg) 
   elevated = (context if context.is_admin else context.elevated()) 
   try: 
      type_updated = db.group_type_update(elevated, id, dict(name=name, description=description, is_public=is_public)) 
   except db_exc.DBError: 
      LOG.exception(_LE('DB   error:')) 
      raise exception.GroupTypeUpdateFailed(id=id) 
   return type_updated"," 'Update group type. 
 :param context: Security context. 
 :param id: Group type id. 
 :param name: Group type name. 
 :param description: Group type description. 
 :param is_public: Group type is_public flag. 
 :returns: Group type updated.'",'Update group type by id.'
"def call_command(name, *args, **options): 
    try: 
      app_name = get_commands()[name] 
      if isinstance(app_name, BaseCommand): 
         klass = app_name 
      else: 
         klass = load_command_class(app_name, name) 
   except KeyError: 
      raise CommandError(('Unknown   command:   %r' % name)) 
   defaults = dict([(o.dest, o.default) for o in klass.option_list if (o.default is not NO_DEFAULT)]) 
   defaults.update(options) 
   return klass.execute(*args, **defaults)", 'Execute a command.',"'Calls the given command, with the given options and args/kwargs. 
 This is the primary API you should use for calling specific commands. 
 Some examples: 
 call_command(\'syncdb\') 
 call_command(\'shell\', plain=True) 
 call_command(\'sqlall\', \'myapp\')'"
"@csrf_protect 
 @permission_required('comments.can_moderate') 
 def delete(request, comment_id, next=None): 
    comment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID) 
   if (request.method == 'POST'): 
      perform_delete(request, comment) 
      return next_redirect(request, next, delete_done, c=comment.pk) 
   else: 
      return render_to_response('comments/delete.html', {'comment': comment, 'next': next}, template.RequestContext(request))", 'Delete a comment',"'Deletes a comment. Confirmation on GET, action on POST. Requires the ""can 
 moderate comments"" permission. 
 Templates: `comments/delete.html`, 
 Context: 
 comment 
 the flagged `comments.comment` object'"
"def _filter_doc_html(request, doc, doc_html, rendering_params): 
    if rendering_params['summary']: 
      return doc.get_summary_html() 
   if (not (rendering_params['section'] or rendering_params['raw'] or rendering_params['edit_links'] or rendering_params['include'])): 
      return doc_html 
   tool = kuma.wiki.content.parse(doc_html) 
   if rendering_params['raw']: 
      tool.injectSectionIDs() 
      tool.filterEditorSafety() 
   if rendering_params['section']: 
      tool.extractSection(rendering_params['section']) 
   if ((rendering_params['edit_links'] or (not rendering_params['raw'])) and request.user.is_authenticated() and doc.allows_revision_by(request.user)): 
      tool.injectSectionEditingLinks(doc.slug, doc.locale) 
   doc_html = tool.serialize() 
   if rendering_params['include']: 
      doc_html = kuma.wiki.content.filter_out_noinclude(doc_html) 
   return doc_html"," 'Filters the HTML output of a document. 
 This function is used to filter the HTML output of a document. 
 :param request: The request object. 
 :param doc: The document object. 
 :param doc_html: The HTML output of the document. 
 :param rendering_params: The rendering parameters. 
 :return: The HTML output of the document.'",'Apply needed filtering/annotating operations to a Document\'s HTML.'
"def create_minibatch_x(minibatches, minibatch_markers, epoch_axis): 
    if epoch_axis: 
      x = np.zeros((minibatches,)) 
      last_e = 0 
      for (e_idx, e) in enumerate(minibatch_markers): 
         e_minibatches = (e - last_e) 
         x[last_e:e] = (e_idx + (np.arange(float(e_minibatches)) / e_minibatches)) 
         last_e = e 
   else: 
      x = np.arange(minibatches) 
   return x"," 'Create a minibatch of data for the given epoch. 
 Parameters 
 minibatches : int 
 Number of minibatches. 
 minibatch_markers : int 
 Number of minibatches per epoch. 
 epoch_axis : int 
 If True, the minibatches are numbered from 0 to (minibatches - 1) 
 and the epoch is the number of minibatches that have been seen 
 (e.g., epoch=0 means that no minibatches have been seen yet). 
 If False, the minibatches are numbered from 1 to (minibatches - 1) 
 and the epoch is the number of epochs that have been seen 
 (e.g., epoch=1 means that one epoch has been seen already). 
 Returns 
 x : ndarray 
 A minibatch of data.'","'Helper function to build x axis for data captured per minibatch. 
 Arguments: 
 minibatches (int): how many total minibatches 
 minibatch_markers (int array): cumulative number of minibatches complete at a given epoch 
 epoch_axis (bool): whether to render epoch or minibatch as the integer step in the x axis'"
"def CDLMORNINGDOJISTAR(barDs, count, penetration=(-4e+37)): 
    return call_talib_with_ohlc(barDs, count, talib.CDLMORNINGDOJISTAR, penetration)"," 'CDLMORNINGDOJISTAR(barDs, count, penetration=(-4e+37)) 
 Calculate the CDLMORNINGDOJISTAR indicator for barDs. 
 barDs : BarData 
 The bar data. 
 count : int 
 The number of bars to calculate the indicator over. 
 penetration : float 
 The penetration value. 
 Returns 
 The CDLMORNINGDOJISTAR indicator. 
 Examples 
 >>> from ru.andal.ta.indicators import CDLMORNINGDOJISTAR 
 >>> barDs = ta.bar_data([""1994.01.01"", ""1994.01.02"", ""1994.01.03"", ""1994.01.04"", ""1994.01.05"", ""1994.01.06"", ""1994.01.07"", ""1994.01.08"", ""1994.01.",'Morning Doji Star'
"def gaussian_convolution(h, Xi, x): 
    return ((1.0 / np.sqrt((4 * np.pi))) * np.exp(((- ((Xi - x) ** 2)) / ((h ** 2) * 4.0))))"," 'Convolve the gaussian function with the given kernel. 
 Parameters 
 h : scalar 
 The standard deviation of the Gaussian. 
 Xi : ndarray 
 The kernel. 
 x : ndarray 
 The input array. 
 Returns 
 out : ndarray 
 The output array.'",'Calculates the Gaussian Convolution Kernel'
"def load_reg(): 
    reg_dir = _reg_dir() 
   regfile = os.path.join(reg_dir, 'register') 
   try: 
      with salt.utils.fopen(regfile, 'r') as fh_: 
         return msgpack.load(fh_) 
   except: 
      log.error('Could   not   write   to   msgpack   file   {0}'.format(__opts__['outdir'])) 
      raise", 'Load the register file','Load the register from msgpack files'
"def _map_plays_to_roles(graph, dirs, git_dir, key, type_1, type_2): 
    Node = namedtuple('Node', ['name', 'type']) 
   for d in dirs: 
      d = pathlib2.Path(git_dir, d) 
      for item in d.iterdir(): 
         if item.match('*.yml'): 
            yaml_file = _open_yaml_file(item) 
            if (yaml_file is not None): 
               for play in yaml_file: 
                  if (key in play): 
                     for role in play[key]: 
                        name = _get_role_name(role) 
                        node_1 = Node(item.stem, type_1) 
                        node_2 = Node(name, type_2) 
                        graph.add_edge(node_2, node_1)"," 'Map plays to roles in the graph. 
 :param graph: Graph to add edges to. 
 :param dirs: List of directories to search for yml files. 
 :param git_dir: Directory to search for yml files. 
 :param key: Key to look for in plays. 
 :param type_1: Type of the node to add for plays. 
 :param type_2: Type of the node to add for roles. 
 :return: None'","'Maps plays to the roles they use. 
 Input: 
 graph: A networkx digraph that is used to map Ansible dependencies. 
 dirs: A list of relative paths to directories in which Ansible playbooks reside. 
 git_dir: A path to the top-most directory in the local git repository tool is to be run in. 
 key: The key in a playbook yaml file in dirs that maps to relevant playbook data. In this case, key is 
 ""roles"", because the roles used by a playbook is of interest. 
 type_1: Given edges A-B, the type of node A. 
 type_2: Given edges A-B, the type of node B. 
 Since this function maps plays to the roles they use, both type_1 is a type of playbook and type_2 is ""role"".'"
"def _installHandlerUsingSetWakeup(fd): 
    if (fd == (-1)): 
      signal.signal(signal.SIGCHLD, signal.SIG_DFL) 
   else: 
      signal.signal(signal.SIGCHLD, _Handler(None)) 
      siginterrupt(signal.SIGCHLD, False) 
   return set_wakeup_fd(fd)"," 'Install handler using set_wakeup_fd() 
 :param fd: file descriptor to install handler on 
 :returns: file descriptor to use for wakeup 
 :rtype: int'","'Install a signal handler which will write a byte to C{fd} when 
 I{SIGCHLD} is received. 
 This is implemented by installing an instance of L{_Handler} wrapped 
 around C{None}, setting the I{SIGCHLD} handler as not allowed to 
 interrupt system calls, and using L{signal.set_wakeup_fd} to do the 
 actual writing. 
 @param fd: The file descriptor to which to write when I{SIGCHLD} is 
 received. 
 @type fd: C{int}'"
"def get_user_model(): 
    try: 
      model = get_model(AUTH_USER_APP_LABEL, AUTH_USER_MODEL_NAME) 
   except LookupError: 
      raise ImproperlyConfigured((""AUTH_USER_MODEL   refers   to   model   '%s'   that   has   not   been   installed"" % settings.AUTH_USER_MODEL)) 
   core_fields = set([f.name for f in User._meta.fields]) 
   model_fields = set([f.name for f in model._meta.fields]) 
   new_fields = model_fields.difference(core_fields) 
   model._meta.has_additional_fields = (len(new_fields) > 0) 
   model._meta.additional_fields = new_fields 
   return model"," 'Returns the model for the AUTH_USER_MODEL setting. 
 This is a convenience function that returns the model for 
 AUTH_USER_MODEL setting. 
 If the AUTH_USER_MODEL setting is not set, it raises an exception. 
 If the AUTH_USER_MODEL setting is set but the model is not installed, 
 it raises an exception. 
 If the AUTH_USER_MODEL setting is set and the model is installed, 
 but it has additional fields, it adds them to the model. 
 If the AUTH_USER_MODEL setting is set and the model is installed, 
 but it has no additional fields, it returns the model. 
 :return: The model for the AUTH_USER_MODEL setting.'","'Return the User model. Doesn\'t require the app cache to be fully 
 initialised. 
 This used to live in compat to support both Django 1.4\'s fixed User model 
 and custom user models introduced thereafter. 
 Support for Django 1.4 has since been dropped in Oscar, but our 
 get_user_model remains because code relies on us annotating the _meta class 
 with the additional fields, and other code might rely on it as well.'"
"def handler_url(block, handler_name, suffix='', query='', thirdparty=False): 
    view_name = 'xblock_handler' 
   if handler_name: 
      func = getattr(block.__class__, handler_name, None) 
      if (not func): 
         raise ValueError('{!r}   is   not   a   function   name'.format(handler_name)) 
   if thirdparty: 
      view_name = 'xblock_handler_noauth' 
   url = reverse(view_name, kwargs={'course_id': unicode(block.location.course_key), 'usage_id': quote_slashes(unicode(block.scope_ids.usage_id).encode('utf-8')), 'handler': handler_name, 'suffix': suffix}) 
   if (not suffix): 
      url = url.rstrip('/') 
   if query: 
      url += ('?' + query) 
   if thirdparty: 
      scheme = ('https' if (settings.HTTPS == 'on') else 'http') 
      url = '{scheme}://{host}{path}'.format(scheme=scheme, host=settings.SITE_NAME, path=url) 
   return url"," 'Returns a URL for the given handler function. 
 :param block: The XBlock to get the handler for. 
 :param handler_name: The name of the handler function to call. 
 :param suffix: The suffix to append to the URL. 
 :param query: The query string to append to the URL. 
 :param thirdparty: If True, return a URL for the handler without 
 authentication. 
 :returns: A URL for the given handler function.'","'This method matches the signature for `xblock.runtime:Runtime.handler_url()` 
 See :method:`xblock.runtime:Runtime.handler_url`'"
"def model_query(context, model, *args, **kwargs): 
    session = (kwargs.get('session') or get_session()) 
   read_deleted = (kwargs.get('read_deleted') or context.read_deleted) 
   project_only = kwargs.get('project_only', False) 
   def issubclassof_nova_base(obj): 
      return (isinstance(obj, type) and issubclass(obj, models.NovaBase)) 
   base_model = model 
   if (not issubclassof_nova_base(base_model)): 
      base_model = kwargs.get('base_model', None) 
      if (not issubclassof_nova_base(base_model)): 
         raise Exception(_('model   or   base_model   parameter   should   be   subclass   of   NovaBase')) 
   query = session.query(model, *args) 
   default_deleted_value = base_model.__mapper__.c.deleted.default.arg 
   if (read_deleted == 'no'): 
      query = query.filter((base_model.deleted == default_deleted_value)) 
   elif (read_deleted == 'yes'): 
      pass 
   elif (read_deleted == 'only'): 
      query = query.filter((base_model.deleted != default_deleted_value)) 
   else: 
      raise Exception((_(""Unrecognized   read_deleted   value   '%s'"") % read_deleted)) 
   if (nova.context.is_user_context(context) and project_only): 
      if (project_only == 'allow_none'): 
         query = query.filter(or_((base_model.project_id == context.project_id), (base_model.project_id == None))) 
      else: 
         query = query.filter_by(project_id=context.project_id) 
   return query"," 'Query for the model. 
 :param context: nova.context.RequestContext 
 :param model: model class 
 :param args: positional arguments 
 :param kwargs: keyword arguments 
 :return: query'","'Query helper that accounts for context\'s `read_deleted` field. 
 :param context: context to query under 
 :param session: if present, the session to use 
 :param read_deleted: if present, overrides context\'s read_deleted field. 
 :param project_only: if present and context is user-type, then restrict 
 query to match the context\'s project_id. If set to \'allow_none\', 
 restriction includes project_id = None. 
 :param base_model: Where model_query is passed a ""model"" parameter which is 
 not a subclass of NovaBase, we should pass an extra base_model 
 parameter that is a subclass of NovaBase and corresponds to the 
 model parameter.'"
"def attr_sparse_matrix(G, edge_attr=None, node_attr=None, normalized=False, rc_order=None, dtype=None): 
    try: 
      import numpy as np 
      from scipy import sparse 
   except ImportError: 
      raise ImportError('attr_sparse_matrix()   requires   scipy:   http://scipy.org/   ') 
   edge_value = _edge_value(G, edge_attr) 
   node_value = _node_value(G, node_attr) 
   if (rc_order is None): 
      ordering = list(set([node_value(n) for n in G])) 
   else: 
      ordering = rc_order 
   N = len(ordering) 
   undirected = (not G.is_directed()) 
   index = dict(zip(ordering, range(N))) 
   M = sparse.lil_matrix((N, N), dtype=dtype) 
   seen = set([]) 
   for (u, nbrdict) in G.adjacency(): 
      for v in nbrdict: 
         (i, j) = (index[node_value(u)], index[node_value(v)]) 
         if (v not in seen): 
            M[(i, j)] += edge_value(u, v) 
            if undirected: 
               M[(j, i)] = M[(i, j)] 
      if undirected: 
         seen.add(u) 
   if normalized: 
      norms = np.asarray(M.sum(axis=1)).ravel() 
      for (i, norm) in enumerate(norms): 
         M[i, :] /= norm 
   if (rc_order is None): 
      return (M, ordering) 
   else: 
      return M"," 'Returns a sparse matrix with node attributes on the rows and edges 
 on the columns. 
 Parameters 
 G : NetworkX DiGraph 
 The graph. 
 edge_attr : callable, optional 
 A function that takes a node and returns a number. 
 node_attr : callable, optional 
 A function that takes a node and returns a number. 
 normalized : bool, optional 
 Whether to normalize the matrix by summing the rows and dividing by 
 the sum of the row norms. 
 rc_order : list of ints, optional 
 An ordering of the nodes. 
 dtype : dtype, optional 
 The dtype of the matrix. 
 Returns 
 M : sparse matrix 
 A sparse matrix with node attributes on the rows and edges on the 
 columns. 
 order : list of ints 
 The ordering of the nodes. 
 Examples 
 >>> from networkx.algorithms.bipartite import bipartite_matching 
 >>> from networkx.algorithms.random_graphs import random_bipartite_graph 
 >>> G = random_bipartite_graph","'Returns a SciPy sparse matrix using attributes from G. 
 If only `G` is passed in, then the adjacency matrix is constructed. 
 Let A be a discrete set of values for the node attribute `node_attr`. Then 
 the elements of A represent the rows and columns of the constructed matrix. 
 Now, iterate through every edge e=(u,v) in `G` and consider the value 
 of the edge attribute `edge_attr`.  If ua and va are the values of the 
 node attribute `node_attr` for u and v, respectively, then the value of 
 the edge attribute is added to the matrix element at (ua, va). 
 Parameters 
 G : graph 
 The NetworkX graph used to construct the NumPy matrix. 
 edge_attr : str, optional 
 Each element of the matrix represents a running total of the 
 specified edge attribute for edges whose node attributes correspond 
 to the rows/cols of the matirx. The attribute must be present for 
 all edges in the graph. If no attribute is specified, then we 
 just count the number of edges whose node attributes correspond 
 to the matrix element. 
 node_attr : str, optional 
 Each row and column in the matrix represents a particular value 
 of the node attribute.  The attribute must be present for all nodes 
 in the graph. Note, the values of this attribute should be reliably 
 hashable. So, float values are not recommended. If no attribute is 
 specified, then the rows and columns will be the nodes of the graph. 
 normalized : bool, optional 
 If True, then each row is normalized by the summation of its values. 
 rc_order : list, optional 
 A list of the node attribute values. This list specifies the ordering 
 of rows and columns of the array. If no ordering is provided, then 
 the ordering will be random (and also, a return value). 
 Other Parameters 
 dtype : NumPy data-type, optional 
 A valid NumPy dtype used to initialize the array. Keep in mind certain 
 dtypes can yield unexpected results if the array is to be normalized. 
 The parameter is passed to numpy.zeros(). If unspecified, the NumPy 
 default is used. 
 Returns 
 M : SciPy sparse matrix 
 The attribute matrix. 
 ordering : list 
 If `rc_order` was specified, then only the matrix is returned. 
 However, if `rc_order` was None, then the ordering used to construct 
 the matrix is returned as well. 
 Examples 
 Construct an adjacency matrix: 
 >>> G = nx.Graph() 
 >>> G.add_edge(0,1,thickness=1,weight=3) 
 >>> G.add_edge(0,2,thickness=2) 
 >>> G.add_edge(1,2,thickness=3) 
 >>> M = nx.attr_sparse_matrix(G, rc_order=[0,1,2]) 
 >>> M.todense() 
 matrix([[ 0.,  1.,  1.], 
 [ 1.,  0.,  1.], 
 [ 1.,  1.,  0.]]) 
 Alternatively, we can obtain the matrix describing edge thickness. 
 >>> M = nx.attr_sparse_matrix(G, edge_attr=\'thickness\', rc_order=[0,1,2]) 
 >>> M.todense() 
 matrix([[ 0.,  1.,  2.], 
 [ 1.,  0.,  3.], 
 [ 2.,  3.,  0.]]) 
 We can also color the nodes and ask for the probability distribution over 
 all edges (u,v) describing: 
 Pr(v has color Y | u has color X) 
 >>> G.node[0][\'color\'] = \'red\' 
 >>> G.node[1][\'color\'] = \'red\' 
 >>> G.node[2][\'color\'] = \'blue\' 
 >>> rc = [\'red\', \'blue\'] 
 >>> M = nx.attr_sparse_matrix(G, node_attr=\'color\',                                   normalized=True, rc_order=rc) 
 >>> M.todense() 
 matrix([[ 0.33333333,  0.66666667], 
 [ 1.        ,  0.        ]]) 
 For example, the above tells us that for all edges (u,v): 
 Pr( v is red  | u is red)  = 1/3 
 Pr( v is blue | u is red)  = 2/3 
 Pr( v is red  | u is blue) = 1 
 Pr( v is blue | u is blue) = 0 
 Finally, we can obtain the total weights listed by the node colors. 
 >>> M = nx.attr_sparse_matrix(G, edge_attr=\'weight\',                                  node_attr=\'color\', rc_order=rc) 
 >>> M.todense() 
 matrix([[ 3.,  2.], 
 [ 2.,  0.]]) 
 Thus, the total weight over all edges (u,v) with u and v having colors: 
 (red, red)   is 3   # the sole contribution is from edge (0,1) 
 (red, blue)  is 2   # contributions from edges (0,2) and (1,2) 
 (blue, red)  is 2   # same as (red, blue) since graph is undirected 
 (blue, blue) is 0   # there are no edges with blue endpoints'"
"@protocol.commands.add(u'command_list_ok_begin', list_command=False) 
 def command_list_ok_begin(context): 
    context.dispatcher.command_list_receiving = True 
   context.dispatcher.command_list_ok = True 
   context.dispatcher.command_list = []", 'Begin receiving commands','See :meth:`command_list_begin()`.'
"def _ace_to_text(ace, objectType): 
    dc = daclConstants() 
   objectType = dc.getObjectTypeBit(objectType) 
   try: 
      userSid = win32security.LookupAccountSid('', ace[2]) 
      if userSid[1]: 
         userSid = '{1}\\{0}'.format(userSid[0], userSid[1]) 
      else: 
         userSid = '{0}'.format(userSid[0]) 
   except Exception: 
      userSid = win32security.ConvertSidToStringSid(ace[2]) 
   tPerm = ace[1] 
   tAceType = ace[0][0] 
   tProps = ace[0][1] 
   tInherited = '' 
   for x in dc.validAceTypes: 
      if (dc.validAceTypes[x]['BITS'] == tAceType): 
         tAceType = dc.validAceTypes[x]['TEXT'] 
         break 
   for x in dc.rights[objectType]: 
      if (dc.rights[objectType][x]['BITS'] == tPerm): 
         tPerm = dc.rights[objectType][x]['TEXT'] 
         break 
   if ((tProps & win32security.INHERITED_ACE) == win32security.INHERITED_ACE): 
      tInherited = '[Inherited]' 
      tProps = (tProps ^ win32security.INHERITED_ACE) 
   for x in dc.validPropagations[objectType]: 
      if (dc.validPropagations[objectType][x]['BITS'] == tProps): 
         tProps = dc.validPropagations[objectType][x]['TEXT'] 
         break 
   return '{0}   {1}   {2}   on   {3}   {4}'.format(userSid, tAceType, tPerm, tProps, tInherited)", 'Convert an ACE to a textual representation.','helper function to convert an ace to a textual representation'
"def _build_offset(offset, kwargs, default): 
    if (offset is None): 
      if (not kwargs): 
         return default 
      else: 
         return _td_check(datetime.timedelta(**kwargs)) 
   elif kwargs: 
      raise ValueError('Cannot   pass   kwargs   and   an   offset') 
   elif isinstance(offset, datetime.timedelta): 
      return _td_check(offset) 
   else: 
      raise TypeError(""Must   pass   'hours'   and/or   'minutes'   as   keywords"")"," 'Build a datetime.timedelta object from offset, kwargs, and default.'",'Builds the offset argument for event rules.'
"def dump(object_, file_, parameters=None, use_cpickle=False, protocol=DEFAULT_PROTOCOL, **kwargs): 
    if use_cpickle: 
      pickler = cPickle.Pickler 
   else: 
      pickler = _PicklerWithWarning 
   with closing(tarfile.TarFile(fileobj=file_, mode='w')) as tar_file: 
      external_objects = {} 
      def _save_parameters(f): 
         renamer = _Renamer() 
         named_parameters = {renamer(p): p for p in parameters} 
         numpy.savez(f, **{n: p.get_value() for (n, p) in named_parameters.items()}) 
         for (name, p) in named_parameters.items(): 
            array_ = p.container.storage[0] 
            external_objects[id(array_)] = _mangle_parameter_name(p, name) 
      if parameters: 
         _taradd(_save_parameters, tar_file, '_parameters') 
      if (object_ is not None): 
         save_object = _SaveObject(pickler, object_, external_objects, protocol, **kwargs) 
         _taradd(save_object, tar_file, '_pkl')"," 'Save an object to a tar file. 
 Parameters 
 object_: the object to save. 
 file_: the file to write to. 
 parameters: a dictionary of parameters to save. 
 use_cpickle: whether to use cPickle. 
 protocol: the protocol to use for pickling. 
 kwargs: additional keyword arguments to pass to cPickle. 
 Examples 
 >>> import numpy as np 
 >>> import matplotlib.mlab as mlab 
 >>> from mpl_toolkits.mplot3d import Axes3D 
 >>> from matplotlib.mlab import point_in_polygon 
 >>> import matplotlib.pyplot as plt 
 >>> x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 >>> y = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 >>> z = np.array([[1, 2, 3], [4, 5, 6], [","'Pickles an object, optionally saving its parameters separately. 
 Parameters 
 object_ : object 
 The object to pickle. If None, only the parameters passed to the 
 `parameters` argument will be saved. 
 file_ : file 
 The destination for saving. 
 parameters : list, optional 
 Shared variables whose internal numpy arrays should be saved 
 separately in the `_parameters` field of the tar file. 
 pickle_object : bool 
 If False, `object_` will not be serialized, only its parameters. 
 This flag can be used when `object_` is not serializable, but one 
 still want to save its parameters. Default: True 
 use_cpickle : bool 
 Use cPickle instead of pickle. Setting it to true will disable the 
 warning message if you try to pickle objects from the main module, 
 so be sure that there is no warning before turning this flag 
 on. Default: False. 
 protocol : int, optional 
 The pickling protocol to use. Unlike Python\'s built-in pickle, the 
 default is set to `2` instead of 0 for Python 2. The Python 3 
 default (level 3) is maintained. 
 \*\*kwargs 
 Keyword arguments to be passed to `pickle.Pickler`.'"
"def has_required_keys(xblock): 
    for key in ('discussion_id', 'discussion_category', 'discussion_target'): 
      if (getattr(xblock, key, None) is None): 
         log.debug(""Required   key   '%s'   not   in   discussion   %s,   leaving   out   of   category   map"", key, xblock.location) 
         return False 
   return True", 'Check if the discussion has required keys.',"'Returns True iff xblock has the proper attributes for generating metadata 
 with get_discussion_id_map_entry()'"
"def filter_labels(train, label, classes=None): 
    if isinstance(train, theano.tensor.sharedvar.SharedVariable): 
      train = train.get_value(borrow=True) 
   if isinstance(label, theano.tensor.sharedvar.SharedVariable): 
      label = label.get_value(borrow=True) 
   if (not (isinstance(train, numpy.ndarray) or scipy.sparse.issparse(train))): 
      raise TypeError('train   must   be   a   numpy   array,   a   scipy   sparse   matrix,   or   a   theano   shared   array') 
   if (classes is not None): 
      label = label[:, classes] 
   if scipy.sparse.issparse(train): 
      idx = label.sum(axis=1).nonzero()[0] 
      return (train[idx], label[idx]) 
   condition = label.any(axis=1) 
   return tuple((var.compress(condition, axis=0) for var in (train, label)))"," 'Filter the training set by label. 
 Parameters 
 train : Theano tensor or shared variable 
 The training set. 
 label : Theano tensor or shared variable 
 The labels. 
 classes : int 
 The class to filter on. 
 Returns 
 tuple(Theano tensor, Theano tensor) 
 The filtered training set and the labels of the filtered training set.'","'Filter examples of train for which we have labels 
 Parameters 
 train : WRITEME 
 label : WRITEME 
 classes : WRITEME 
 Returns 
 WRITEME'"
"def set_desktop_icons(visible_list, ignore_duplicate=True): 
    frappe.db.sql(u'delete   from   `tabDesktop   Icon`   where   standard=0') 
   frappe.db.sql(u'update   `tabDesktop   Icon`   set   blocked=0,   hidden=1') 
   for module_name in visible_list: 
      name = frappe.db.get_value(u'Desktop   Icon', {u'module_name': module_name}) 
      if name: 
         frappe.db.set_value(u'Desktop   Icon', name, u'hidden', 0) 
      elif frappe.db.exists(u'DocType', module_name): 
         try: 
            add_user_icon(module_name, standard=1) 
         except frappe.UniqueValidationError as e: 
            if (not ignore_duplicate): 
               raise e 
            else: 
               visible_list.remove(module_name) 
               if frappe.message_log: 
                  frappe.message_log.pop() 
   set_order(visible_list) 
   clear_desktop_icons_cache()"," 'Set desktop icons for modules. 
 The module name should be the name of the module and not the name of the 
 class. 
 :param visible_list: List of modules to set desktop icons for 
 :param ignore_duplicate: Ignore duplicate modules in the list 
 :returns: None'","'Resets all lists and makes only the given one standard, 
 if the desktop icon does not exist and the name is a DocType, then will create 
 an icon for the doctype'"
"def prime(nth): 
    n = as_int(nth) 
   if (n < 1): 
      raise ValueError('nth   must   be   a   positive   integer;   prime(1)   ==   2') 
   if (n <= len(sieve._list)): 
      return sieve[n] 
   from sympy.functions.special.error_functions import li 
   from sympy.functions.elementary.exponential import log 
   a = 2 
   b = int((n * (log(n) + log(log(n))))) 
   while (a < b): 
      mid = ((a + b) >> 1) 
      if (li(mid) > n): 
         b = mid 
      else: 
         a = (mid + 1) 
   n_primes = primepi((a - 1)) 
   while (n_primes < n): 
      if isprime(a): 
         n_primes += 1 
      a += 1 
   return (a - 1)"," 'Prime the nth prime. 
 prime(3) == 5 
 prime(1) == 2 
 prime(100) == 79'","'Return the nth prime, with the primes indexed as prime(1) = 2, 
 prime(2) = 3, etc.... The nth prime is approximately n*log(n). 
 Logarithmic integral of x is a pretty nice approximation for number of 
 primes <= x, i.e. 
 li(x) ~ pi(x) 
 In fact, for the numbers we are concerned about( x<1e11 ), 
 li(x) - pi(x) < 50000 
 Also, 
 li(x) > pi(x) can be safely assumed for the numbers which 
 can be evaluated by this function. 
 Here, we find the least integer m such that li(m) > n using binary search. 
 Now pi(m-1) < li(m-1) <= n, 
 We find pi(m - 1) using primepi function. 
 Starting from m, we have to find n - pi(m-1) more primes. 
 For the inputs this implementation can handle, we will have to test 
 primality for at max about 10**5 numbers, to get our answer. 
 References 
 - https://en.wikipedia.org/wiki/Prime_number_theorem#Table_of_.CF.80.28x.29.2C_x_.2F_log_x.2C_and_li.28x.29 
 - https://en.wikipedia.org/wiki/Prime_number_theorem#Approximations_for_the_nth_prime_number 
 - https://en.wikipedia.org/wiki/Skewes%27_number 
 Examples 
 >>> from sympy import prime 
 >>> prime(10) 
 29 
 >>> prime(1) 
 2 
 >>> prime(100000) 
 1299709 
 See Also 
 sympy.ntheory.primetest.isprime : Test if n is prime 
 primerange : Generate all primes in a given range 
 primepi : Return the number of primes less than or equal to n'"
"@register.function 
 @jinja2.contextfunction 
 def favorites_widget(context, addon, condensed=False): 
    c = dict(context.items()) 
   request = c['request'] 
   if request.user.is_authenticated(): 
      is_favorite = (addon.id in request.user.favorite_addons) 
      faved_class = ('faved' if is_favorite else '') 
      unfaved_text = ('' if condensed else _('Add   to   favorites')) 
      faved_text = (_('Favorite') if condensed else _('Remove   from   favorites')) 
      add_url = reverse('collections.alter', args=[request.user.username, 'favorites', 'add']) 
      remove_url = reverse('collections.alter', args=[request.user.username, 'favorites', 'remove']) 
      c.update(locals()) 
      t = get_env().get_template('bandwagon/favorites_widget.html').render(c) 
      return jinja2.Markup(t)"," 'Favorites widget. 
 :param context: 
 :param addon: 
 :param condensed: 
 :return: 
 :rtype: 
 :raises: 
 :param context: 
 :param addon: 
 :param condensed: 
 :return: 
 :rtype: 
 :raises: 
 :param context: 
 :param addon: 
 :param condensed: 
 :return: 
 :rtype: 
 :raises: 
 :param context: 
 :param addon: 
 :param condensed: 
 :return: 
 :rtype: 
 :raises: 
 :param context: 
 :param addon: 
 :param condensed: 
 :return: 
 :rtype: 
 :raises: 
 :param context: 
 :param addon: 
 :param condensed: 
 :return: 
 :rtype: 
 :raises: 
 :param context: 
 :param addon: 
 :param condensed: 
 :return: 
 :",'Displays \'Add to Favorites\' widget.'
"def names_from_file(filename): 
    with io.open(filename, 'rt', encoding='utf8') as names_file: 
      for name in filter_koan_names(names_file): 
         (yield name) 
   return", 'Read a file of names and return them as a generator.',"'Opens the given ``filename`` and yields the fully-qualified names 
 of TestCases found inside (one per line).'"
"def sanitize_separators(value): 
    if settings.USE_L10N: 
      decimal_separator = get_format('DECIMAL_SEPARATOR') 
      if isinstance(value, six.string_types): 
         parts = [] 
         if (decimal_separator in value): 
            (value, decimals) = value.split(decimal_separator, 1) 
            parts.append(decimals) 
         if settings.USE_THOUSAND_SEPARATOR: 
            parts.append(value.replace(get_format('THOUSAND_SEPARATOR'), '')) 
         else: 
            parts.append(value) 
         value = '.'.join(reversed(parts)) 
   return value"," 'Returns a sanitized version of a number, removing any separators. 
 This is to prevent the number from being interpreted as a currency.'","'Sanitizes a value according to the current decimal and 
 thousand separator setting. Used with form field input.'"
"def get_argnames(func): 
    if six.PY2: 
      if isinstance(func, functools.partial): 
         spec = inspect.getargspec(func.func) 
      elif inspect.isroutine(func): 
         spec = inspect.getargspec(func) 
      else: 
         spec = inspect.getargspec(func.__call__) 
      args = [arg for arg in spec.args if (arg != 'self')] 
   else: 
      sig = inspect.signature(func) 
      args = [param.name for param in sig.parameters.values() if (param.kind not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD))] 
   return args"," 'Returns a list of function arguments. 
 This function is only available in Python 3.'","'Introspecs the arguments of a callable. 
 Args: 
 func: The callable to introspect 
 Returns: 
 A list of argument names, excluding *arg and **kwargs 
 arguments.'"
"def get_indices(client): 
    try: 
      indices = list(client.indices.get_settings(index='_all', params={'expand_wildcards': 'open,closed'})) 
      version_number = get_version(client) 
      logger.debug('Detected   Elasticsearch   version   {0}'.format('.'.join(map(str, version_number)))) 
      if ((version_number >= (2, 4, 2)) and (version_number < (5, 0, 0))): 
         logger.debug('Using   Elasticsearch   >=   2.4.2   <   5.0.0') 
         if client.indices.exists(index='.security'): 
            logger.debug('Found   the   "".security""   index.      Adding   to   list   of   all   indices') 
            if (not ('.security' in indices)): 
               indices.append('.security') 
      logger.debug('All   indices:   {0}'.format(indices)) 
      return indices 
   except Exception as e: 
      raise FailedExecution('Failed   to   get   indices.   Error:   {0}'.format(e))"," 'Get all indices in Elasticsearch 
 :param client: Client to Elasticsearch 
 :return: List of indices 
 :rtype: list'","'Get the current list of indices from the cluster. 
 :arg client: An :class:`elasticsearch.Elasticsearch` client object 
 :rtype: list'"
"def test_message_hiding(qtbot, view): 
    with qtbot.waitSignal(view._clear_timer.timeout): 
      view.show_message(usertypes.MessageLevel.info, 'test') 
   assert (not view._messages)", 'Test that clearing the timer hides messages.','Messages should be hidden after the timer times out.'
"def update_dmg_makefile(): 
    print 'Updating   fancy-dmg   Makefile...' 
   url = 'https://raw.githubusercontent.com/remko/fancy-dmg/master/Makefile' 
   target_path = os.path.join('scripts', 'dev', 'Makefile-dmg') 
   urllib.request.urlretrieve(url, target_path) 
   urllib.request.urlcleanup()", 'Updates the Makefile for fancy-dmg.',"'Update fancy-dmg Makefile. 
 See https://el-tramo.be/blog/fancy-dmg/'"
"def _initialize_headers(headers): 
    return ({} if (headers is None) else dict(headers))"," 'Initialize a dictionary of HTTP headers. 
 :param headers: Dictionary of HTTP headers. 
 :return: Dictionary of HTTP headers.'","'Creates a copy of the headers. 
 Args: 
 headers: dict, request headers to copy. 
 Returns: 
 dict, the copied headers or a new dictionary if the headers 
 were None.'"
"@contextmanager 
 def secret_model(): 
    orig_model = None 
   try: 
      orig_model = loading.cache.app_models['tests']['secret'] 
      del loading.cache.app_models['tests']['secret'] 
   except KeyError: 
      pass 
   try: 
      attrs = {'name': EncryptedCharField('Name', max_length=Secret._meta.get_field('name').max_length), 'text': EncryptedTextField('Text'), '__module__': 'django_extensions.tests.models', 'Meta': type('Meta', (object,), {'managed': False, 'db_table': Secret._meta.db_table})} 
      (yield type('Secret', (models.Model,), attrs)) 
   except: 
      raise 
   finally: 
      try: 
         loading.cache.app_models['tests']['secret'] = orig_model 
      except KeyError: 
         pass"," 'A context manager that will create a secret model in the database. 
 This is a context manager, so it will be closed when the context is 
 exited. 
 :rtype: django.db.models.Model'","'A context manager that yields a Secret model defined at runtime. 
 All EncryptedField init logic occurs at model class definition time, not at 
 object instantiation time. This means that in order to test different keys 
 and modes, we must generate a new class definition at runtime, after 
 establishing the correct KeyCzar settings. This context manager handles 
 that process. 
 See http://dynamic-models.readthedocs.org/en/latest/ and 
 https://docs.djangoproject.com/en/dev/topics/db/models/ 
 #differences-between-proxy-inheritance-and-unmanaged-models'"
"def lsmod(): 
    ret = [] 
   for line in __salt__['cmd.run']('kldstat').splitlines(): 
      comps = line.split() 
      if (not (len(comps) > 2)): 
         continue 
      if (comps[0] == 'Id'): 
         continue 
      if (comps[4] == 'kernel'): 
         continue 
      ret.append({'module': comps[4][:(-3)], 'size': comps[3], 'depcount': comps[1]}) 
   return ret", 'Returns a list of loaded modules and their dependencies',"'Return a dict containing information about currently loaded modules 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' kmod.lsmod'"
"def mark_as_cover(container, name): 
    if (name not in container.mime_map): 
      raise ValueError((u'Cannot   mark   %s   as   cover   as   it   does   not   exist' % name)) 
   mt = container.mime_map[name] 
   if (not is_raster_image(mt)): 
      raise ValueError((u'Cannot   mark   %s   as   the   cover   image   as   it   is   not   a   raster   image' % name)) 
   if (container.book_type == u'azw3'): 
      mark_as_cover_azw3(container, name) 
   else: 
      mark_as_cover_epub(container, name)"," 'Marks a file as the cover image for a container. 
 :param container: The container to mark as the cover image. 
 :param name: The name of the file to mark as the cover image. 
 :raises ValueError: If the file does not exist or is not a raster image.'",'Mark the specified image as the cover image.'
"def after_VBD_create(vbd_ref, vbd_rec): 
    vbd_rec['currently_attached'] = False 
   vbd_rec['device'] = '' 
   vm_ref = vbd_rec['VM'] 
   vm_rec = _db_content['VM'][vm_ref] 
   vm_rec['VBDs'].append(vbd_ref) 
   vdi_ref = vbd_rec['VDI'] 
   vdi_rec = _db_content['VDI'][vdi_ref] 
   vdi_rec['VBDs'].append(vbd_ref) 
   vm_name_label = _db_content['VM'][vm_ref]['name_label'] 
   vbd_rec['vm_name_label'] = vm_name_label"," 'After creating a vbd, update the vm and vdi records.'","'Create read-only fields and backref from VM and VDI to VBD when VBD 
 is created.'"
"def is_unit(xblock, parent_xblock=None): 
    if (xblock.category == 'vertical'): 
      if (parent_xblock is None): 
         parent_xblock = get_parent_xblock(xblock) 
      parent_category = (parent_xblock.category if parent_xblock else None) 
      return (parent_category == 'sequential') 
   return False", 'Returns True if the xblock is a unit in a sequential category.',"'Returns true if the specified xblock is a vertical that is treated as a unit. 
 A unit is a vertical that is a direct child of a sequential (aka a subsection).'"
"def bus_routes(): 
    return wechat.response_news(app.config['BUS_ROUTES_NEWS'])", 'Returns a list of bus routes.',''
"def identify_format(origin, data_class_required, path, fileobj, args, kwargs): 
    valid_formats = [] 
   for (data_format, data_class) in _identifiers: 
      if _is_best_match(data_class_required, data_class, _identifiers): 
         if _identifiers[(data_format, data_class)](origin, path, fileobj, *args, **kwargs): 
            valid_formats.append(data_format) 
   return valid_formats"," 'Returns a list of data formats that match the data class specified in 
 ``data_class_required``. 
 :param origin: The origin of the file. 
 :type origin: str 
 :param data_class_required: The data class required for the file. 
 :type data_class_required: str 
 :param path: The path of the file. 
 :type path: str 
 :param fileobj: The file object. 
 :type fileobj: file 
 :param args: The arguments. 
 :type args: tuple 
 :param kwargs: The keyword arguments. 
 :type kwargs: dict 
 :return: A list of data formats. 
 :rtype: list(str)'","'Loop through identifiers to see which formats match. 
 Parameters 
 origin : str 
 A string ``""read`` or ``""write""`` identifying whether the file is to be 
 opened for reading or writing. 
 data_class_required : object 
 The specified class for the result of `read` or the class that is to be 
 written. 
 path : str, other path object or None 
 The path to the file or None. 
 fileobj : File object or None. 
 An open file object to read the file\'s contents, or ``None`` if the 
 file could not be opened. 
 args : sequence 
 Positional arguments for the `read` or `write` function. Note that 
 these must be provided as sequence. 
 kwargs : dict-like 
 Keyword arguments for the `read` or `write` function. Note that this 
 parameter must be `dict`-like. 
 Returns 
 valid_formats : list 
 List of matching formats.'"
"def get_messages_from_page(name): 
    return _get_messages_from_page_or_report(u'Page', name)"," 'Get messages from a page. 
 :param name: The name of the page. 
 :type name: str 
 :return: The messages from the page. 
 :rtype: list(Message)'",'Returns all translatable strings from a :class:`frappe.core.doctype.Page`'
"def render(template, **kwargs): 
    try: 
      return _env.get_template(template).render(**kwargs) 
   except jinja2.exceptions.UndefinedError: 
      log.misc.exception(('UndefinedError   while   rendering   ' + template)) 
      err_path = os.path.join('html', 'undef_error.html') 
      err_template = utils.read_file(err_path) 
      tb = traceback.format_exc() 
      return err_template.format(pagename=template, traceback=tb)", 'Render a template','Render the given template and pass the given arguments to it.'
"def assert_no_element_by_id(context, _id, wait_time=MAX_WAIT_FOR_UNEXPECTED_ELEMENT): 
    _assert_no_element_by(context, By.ID, _id, wait_time)", 'Asserts that no element with the given ID exists in the context.',"'Assert that no element is found. Use a wait in case the element currently exists 
 on the page, and we want to wait for it to disappear before doing the assert. 
 Finds the element using an id.'"
"def extra_padding_y_keep_ratio(original_size, padding): 
    return _resize(original_size, 1, padding=padding, keep_aspect_ratio=True)"," 'Resize an image with extra padding on the y axis, keeping the aspect ratio. 
 Args: 
 original_size: The original size of the image. 
 padding: The amount of padding to add to the image. 
 Returns: 
 A tuple of (new_size, new_padding). 
 New_size is the new size of the image. 
 New_padding is the new padding of the image.'","'Reduce the height of `original_size` by `padding`, maintaining the aspect 
 ratio.'"
"def GenerateEnvironmentFiles(toplevel_build_dir, generator_flags, system_includes, open_out): 
    archs = ('x86', 'x64') 
   if generator_flags.get('ninja_use_custom_environment_files', 0): 
      cl_paths = {} 
      for arch in archs: 
         cl_paths[arch] = 'cl.exe' 
      return cl_paths 
   vs = GetVSVersion(generator_flags) 
   cl_paths = {} 
   for arch in archs: 
      args = vs.SetupScript(arch) 
      args.extend(('&&', 'set')) 
      popen = subprocess.Popen(args, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) 
      (variables, _) = popen.communicate() 
      env = _ExtractImportantEnvironment(variables) 
      if system_includes: 
         system_includes = (system_includes | OrderedSet(env.get('INCLUDE', '').split(';'))) 
         env['INCLUDE'] = ';'.join(system_includes) 
      env_block = _FormatAsEnvironmentBlock(env) 
      f = open_out(os.path.join(toplevel_build_dir, ('environment.' + arch)), 'wb') 
      f.write(env_block) 
      f.close() 
      args = vs.SetupScript(arch) 
      args.extend(('&&', 'for', '%i', 'in', '(cl.exe)', 'do', '@echo', 'LOC:%~$PATH:i')) 
      popen = subprocess.Popen(args, shell=True, stdout=subprocess.PIPE) 
      (output, _) = popen.communicate() 
      cl_paths[arch] = _ExtractCLPath(output) 
   return cl_paths"," 'Generate environment files for the given generator flags. 
 :param toplevel_build_dir: The directory where the build files are located. 
 :param generator_flags: The generator flags. 
 :param system_includes: A list of include directories that should be 
 included in the environment. 
 :param open_out: A file object that will be used to write the environment 
 file. 
 :return: A dictionary containing the cl.exe path for each architecture.'","'It\'s not sufficient to have the absolute path to the compiler, linker, 
 etc. on Windows, as those tools rely on .dlls being in the PATH. We also 
 need to support both x86 and x64 compilers within the same build (to support 
 msvs_target_platform hackery). Different architectures require a different 
 compiler binary, and different supporting environment variables (INCLUDE, 
 LIB, LIBPATH). So, we extract the environment here, wrap all invocations 
 of compiler tools (cl, link, lib, rc, midl, etc.) via win_tool.py which 
 sets up the environment, and then we do not prefix the compiler with 
 an absolute path, instead preferring something like ""cl.exe"" in the rule 
 which will then run whichever the environment setup has put in the path. 
 When the following procedure to generate environment files does not 
 meet your requirement (e.g. for custom toolchains), you can pass 
 ""-G ninja_use_custom_environment_files"" to the gyp to suppress file 
 generation and use custom environment files prepared by yourself.'"
"def find_dest_path_comp_key(files, src_path=None): 
    src = files['src'] 
   dest = files['dest'] 
   src_type = src['type'] 
   dest_type = dest['type'] 
   if (src_path is None): 
      src_path = src['path'] 
   sep_table = {'s3': '/', 'local': os.sep} 
   if files['dir_op']: 
      rel_path = src_path[len(src['path']):] 
   else: 
      rel_path = src_path.split(sep_table[src_type])[(-1)] 
   compare_key = rel_path.replace(sep_table[src_type], '/') 
   if files['use_src_name']: 
      dest_path = dest['path'] 
      dest_path += rel_path.replace(sep_table[src_type], sep_table[dest_type]) 
   else: 
      dest_path = dest['path'] 
   return (dest_path, compare_key)"," 'Find the destination path and compare key. 
 :param files: 
 :param src_path: 
 :return: 
 :rtype: 
 :raises: 
 :param files: 
 :param src_path: 
 :return: 
 :rtype: 
 :raises: 
 :param files: 
 :param src_path: 
 :return: 
 :rtype: 
 :raises: 
 :param files: 
 :param src_path: 
 :return: 
 :rtype: 
 :raises: 
 :param files: 
 :param src_path: 
 :return: 
 :rtype: 
 :raises: 
 :param files: 
 :param src_path: 
 :return: 
 :rtype: 
 :raises: 
 :param files: 
 :param src_path: 
 :return: 
 :rtype: 
 :raises: 
 :param files: 
 :param src_path: 
 :return: 
 :rtype: 
 :raises","'This is a helper function that determines the destination path and compare 
 key given parameters received from the ``FileFormat`` class.'"
"def get_long_description(): 
    return 'Mangle   plugins   modify   requests   on   the   fly.\n            '", 'Get the long description of this module.',':return: The description for the plugin type.'
"@flake8ext 
 def use_jsonutils(logical_line, filename): 
    msg = 'N321:   jsonutils.%(fun)s   must   be   used   instead   of   json.%(fun)s' 
   json_check_skipped_patterns = ['neutron/plugins/ml2/drivers/openvswitch/agent/xenapi/etc/xapi.d/plugins/netwrap'] 
   for pattern in json_check_skipped_patterns: 
      if (pattern in filename): 
         return 
   if ('json.' in logical_line): 
      json_funcs = ['dumps(', 'dump(', 'loads(', 'load('] 
      for f in json_funcs: 
         pos = logical_line.find(('json.%s' % f)) 
         if (pos != (-1)): 
            (yield (pos, (msg % {'fun': f[:(-1)]})))"," 'Use jsonutils instead of json. 
 This is a flake8 extension that checks for the use of json. 
 Instead, it should be using jsonutils. 
 For example, instead of: 
 - json.dumps(my_dict) 
 use: 
 - jsonutils.dumps(my_dict) 
 - jsonutils.dump(my_dict) 
 - jsonutils.loads(my_json_str) 
 - jsonutils.load(my_json_str)'",'N321 - Use jsonutils instead of json.'
"def get_all_vlanids(): 
    LOG.debug(_('get_all_vlanids()   called')) 
   session = db.get_session() 
   try: 
      vlanids = session.query(network_models_v2.VlanID).all() 
      return vlanids 
   except exc.NoResultFound: 
      return []", 'Return all VLAN IDs.','Gets all the vlanids'
"def get_config(config_file, repo, ctx, dir): 
    config_file = basic_util.strip_path(config_file) 
   for changeset in reversed_upper_bounded_changelog(repo, ctx): 
      changeset_ctx = repo.changectx(changeset) 
      for ctx_file in changeset_ctx.files(): 
         ctx_file_name = basic_util.strip_path(ctx_file) 
         if (ctx_file_name == config_file): 
            return get_named_tmpfile_from_ctx(changeset_ctx, ctx_file, dir) 
   return None"," 'Get the file named by config_file in the given context. 
 If the file is found, return the path to the file.  If not, return None.'",'Return the latest version of config_filename from the repository manifest.'
"@masterserviceHandle 
 def serverStop(): 
    log.msg('stop') 
   if GlobalObject().stophandler: 
      GlobalObject().stophandler() 
   reactor.callLater(0.5, reactor.stop) 
   return True", 'stop the server','ä¾masterè°ç¨çæ¥å£ï¼å³é­æå¡å¨'
"def consume_length_prefix(rlp, start): 
    b0 = safe_ord(rlp[start]) 
   if (b0 < 128): 
      return (str, 1, start) 
   elif (b0 < (128 + 56)): 
      return (str, (b0 - 128), (start + 1)) 
   elif (b0 < 192): 
      ll = (((b0 - 128) - 56) + 1) 
      l = big_endian_to_int(rlp[(start + 1):((start + 1) + ll)]) 
      return (str, l, ((start + 1) + ll)) 
   elif (b0 < (192 + 56)): 
      return (list, (b0 - 192), (start + 1)) 
   else: 
      ll = (((b0 - 192) - 56) + 1) 
      l = big_endian_to_int(rlp[(start + 1):((start + 1) + ll)]) 
      return (list, l, ((start + 1) + ll))"," 'Consume a length prefix from the rlp stream. 
 :param rlp: the rlp stream to read from 
 :param start: the starting position in the rlp stream 
 :return: a tuple of the form (type, length, start_position) 
 :rtype: tuple'","'Read a length prefix from an RLP string. 
 :param rlp: the rlp string to read from 
 :param start: the position at which to start reading 
 :returns: a tuple ``(type, length, end)``, where ``type`` is either ``str`` 
 or ``list`` depending on the type of the following payload, 
 ``length`` is the length of the payload in bytes, and ``end`` is 
 the position of the first payload byte in the rlp string'"
"def addAssemblyCage(derivation, negatives, positives): 
    addCageGroove(derivation, negatives, positives) 
   for pegCenterX in derivation.pegCenterXs: 
      addPositivePeg(derivation, positives, pegCenterX, (- derivation.pegY)) 
      addPositivePeg(derivation, positives, pegCenterX, derivation.pegY) 
   translate.translateNegativesPositives(negatives, positives, Vector3(0.0, (- derivation.halfSeparationWidth))) 
   femaleNegatives = [] 
   femalePositives = [] 
   addCageGroove(derivation, femaleNegatives, femalePositives) 
   for pegCenterX in derivation.pegCenterXs: 
      addNegativePeg(derivation, femaleNegatives, pegCenterX, (- derivation.pegY)) 
      addNegativePeg(derivation, femaleNegatives, pegCenterX, derivation.pegY) 
   translate.translateNegativesPositives(femaleNegatives, femalePositives, Vector3(0.0, derivation.halfSeparationWidth)) 
   negatives += femaleNegatives 
   positives += femalePositives"," 'Adds the cage groove and pegs to the assembly. 
 :param derivation: The derivation to add the cage groove and pegs to. 
 :type derivation: :class:`~.derivation.Derivation` 
 :param negatives: A list of negative pegs. 
 :type negatives: list of :class:`~.peg.Peg` 
 :param positives: A list of positive pegs. 
 :type positives: list of :class:`~.peg.Peg` 
 :rtype: None'",'Add assembly linear bearing cage.'
"@treeio_login_required 
 @handle_response_format 
 def order_invoice_view(request, order_id, response_format='html'): 
    order = get_object_or_404(SaleOrder, pk=order_id) 
   if ((not request.user.profile.has_permission(order)) and (not request.user.profile.is_admin('treeio.sales'))): 
      return user_denied(request, message=""You   don't   have   access   to   this   Sale"") 
   ordered_products = order.orderedproduct_set.filter(trash=False) 
   try: 
      conf = ModuleSetting.get_for_module('treeio.finance', 'my_company')[0] 
      my_company = Contact.objects.get(pk=long(conf.value)) 
   except: 
      my_company = None 
   return render_to_response('sales/order_invoice_view', {'order': order, 'ordered_products': ordered_products, 'my_company': my_company}, context_instance=RequestContext(request), response_format=response_format)", 'View for a single order','Order view as Invoice'
"def getNewRepository(): 
    return ExportRepository()", 'Returns a new ExportRepository instance.','Get the repository constructor.'
"def json_underscore(body, charset='utf-8', **kwargs): 
    return _underscore_dict(json(body, charset=charset))", 'Convert JSON body to underscore-cased keys.',"'Converts JSON formatted date to native Python objects. 
 The keys in any JSON dict are transformed from camelcase to underscore separated words.'"
"def git_hook(): 
    (_, files_modified, _) = run('git   diff-index   --cached   --name-only   HEAD') 
   options = parse_options() 
   setup_logger(options) 
   candidates = list(map(str, files_modified)) 
   if candidates: 
      process_paths(options, candidates=candidates)"," 'Process git hooks 
 This is a wrapper around the git hooks code, which is a bit of a mess. 
 It is not run in a separate process, so the log file is written in the 
 same directory as the script. 
 This is run as a cron job. 
 :param options: 
 :type options: 
 :param candidates: 
 :type candidates: 
 :return: 
 :rtype: 
 :raises: 
 :param options: 
 :type options: 
 :param candidates: 
 :type candidates: 
 :return: 
 :rtype: 
 :raises: 
 :param options: 
 :type options: 
 :param candidates: 
 :type candidates: 
 :return: 
 :rtype: 
 :raises: 
 :param options: 
 :type options: 
 :param candidates: 
 :type candidates: 
 :return: 
 :rtype: 
 :raises: 
 :param options: 
 :type options: 
 :param candidates: 
 :type candidates: 
 :return:",'Run pylama after git commit.'
"def create_mgr(descr, item_shape=None): 
    if (item_shape is None): 
      item_shape = (N,) 
   offset = 0 
   mgr_items = [] 
   block_placements = OrderedDict() 
   for d in descr.split(';'): 
      d = d.strip() 
      if (not len(d)): 
         continue 
      (names, blockstr) = d.partition(':')[::2] 
      blockstr = blockstr.strip() 
      names = names.strip().split(',') 
      mgr_items.extend(names) 
      placement = list((np.arange(len(names)) + offset)) 
      try: 
         block_placements[blockstr].extend(placement) 
      except KeyError: 
         block_placements[blockstr] = placement 
      offset += len(names) 
   mgr_items = Index(mgr_items) 
   blocks = [] 
   num_offset = 0 
   for (blockstr, placement) in block_placements.items(): 
      typestr = blockstr.split('-')[0] 
      blocks.append(create_block(typestr, placement, item_shape=item_shape, num_offset=num_offset)) 
      num_offset += len(placement) 
   return BlockManager(sorted(blocks, key=(lambda b: b.mgr_locs[0])), ([mgr_items] + [np.arange(n) for n in item_shape]))"," 'Create a BlockManager from a string description of the manager. 
 The string must be of the form: 
 block1:1,2,3;block2:4,5,6;block3:7,8,9 
 where the first block is a 3x3 block, the second is a 2x2 block, 
 and the third is a 1x1 block. 
 Parameters 
 descr : str 
 The string description of the manager. 
 item_shape : tuple, optional 
 The shape of the items. 
 Returns 
 BlockManager 
 A BlockManager object. 
 Notes 
 The manager is created by concatenating the blocks, so the 
 blocks must be of the same shape. 
 Examples 
 >>> descr = \'block1:1,2,3;block2:4,5,6;block3:7,8,9\' 
 >>> mgr = create_mgr(descr) 
 >>> mgr.blocks[0].mgr_locs 
 (1, 2, 3) 
 >>> mgr.blocks[1].","'Construct BlockManager from string description. 
 String description syntax looks similar to np.matrix initializer.  It looks 
 like this:: 
 a,b,c: f8; d,e,f: i8 
 Rules are rather simple: 
 * see list of supported datatypes in `create_block` method 
 * components are semicolon-separated 
 * each component is `NAME,NAME,NAME: DTYPE_ID` 
 * whitespace around colons & semicolons are removed 
 * components with same DTYPE_ID are combined into single block 
 * to force multiple blocks with same dtype, use \'-SUFFIX\':: 
 \'a:f8-1; b:f8-2; c:f8-foobar\''"
"def role_delete(role_id=None, name=None, profile=None, **connection_args): 
    kstone = auth(profile, **connection_args) 
   if name: 
      for role in kstone.roles.list(): 
         if (role.name == name): 
            role_id = role.id 
            break 
   if (not role_id): 
      return {'Error': 'Unable   to   resolve   role   id'} 
   role = kstone.roles.get(role_id) 
   kstone.roles.delete(role) 
   ret = 'Role   ID   {0}   deleted'.format(role_id) 
   if name: 
      ret += '   ({0})'.format(name) 
   return ret", 'Delete a role.',"'Delete a role (keystone role-delete) 
 CLI Examples: 
 .. code-block:: bash 
 salt \'*\' keystone.role_delete c965f79c4f864eaaa9c3b41904e67082 
 salt \'*\' keystone.role_delete role_id=c965f79c4f864eaaa9c3b41904e67082 
 salt \'*\' keystone.role_delete name=admin'"
"def _needs_eeg_average_ref_proj(info): 
    eeg_sel = pick_types(info, meg=False, eeg=True, ref_meg=False, exclude='bads') 
   return ((len(eeg_sel) > 0) and (not info['custom_ref_applied']) and (not _has_eeg_average_ref_proj(info['projs'])))"," 'Check if the EEG average reference proj is needed. 
 This function checks if the EEG average reference proj is needed or not. 
 Parameters 
 info : dict 
 The info dictionary of the subject. 
 Returns 
 bool 
 Whether the EEG average reference proj is needed or not.'","'Determine if the EEG needs an averge EEG reference. 
 This returns True if no custom reference has been applied and no average 
 reference projection is present in the list of projections.'"
"@frappe.whitelist() 
 def add_tag(tag, dt, dn, color=None): 
    DocTags(dt).add(dn, tag) 
   return tag", 'Adds a tag to a document.',"'adds a new tag to a record, and creates the Tag master'"
"def CreateBudget(client): 
    budget_service = client.GetService('BudgetService', version='v201609') 
   budget = {'name': ('Interplanetary   Cruise   App   Budget   #%s' % uuid.uuid4()), 'amount': {'microAmount': '50000000'}, 'deliveryMethod': 'STANDARD', 'isExplicitlyShared': False} 
   budget_operations = [{'operator': 'ADD', 'operand': budget}] 
   budget_id = budget_service.mutate(budget_operations)['value'][0]['budgetId'] 
   return budget_id"," 'Create a budget for the interplanetary cruise app. 
 :param client: The client instance. 
 :return: The budget id.'","'Creates a budget and returns its budgetId. 
 Args: 
 client: An AdWordsClient instance. 
 Returns: 
 An int budgetId for the created Budget.'"
"def flatten(class_dict): 
    for (cls, fields_list) in class_dict.items(): 
      for fields in fields_list: 
         (yield (cls, fields))"," 'Flatten a dictionary of class/fields pairs into a list of 
 (class, fields) tuples. 
 Example: 
 >>> flatten({\'A\': [\'a1\', \'a2\'], \'B\': [\'b1\', \'b2\']}) 
 [(A, \'a1\'), (A, \'a2\'), (B, \'b1\'), (B, \'b2\')]'","'Flatten a dict from cls -> [fields, ...] and yields values of the form (cls, fields) 
 for each entry in the dictionary value.'"
"def _nova_to_osvif_network(network): 
    netobj = objects.network.Network(id=network['id'], bridge_interface=network.get_meta('bridge_interface'), subnets=_nova_to_osvif_subnets(network['subnets'])) 
   if (network['bridge'] is not None): 
      netobj.bridge = network['bridge'] 
   if (network['label'] is not None): 
      netobj.label = network['label'] 
   if (network.get_meta('mtu') is not None): 
      netobj.mtu = network.get_meta('mtu') 
   if (network.get_meta('multi_host') is not None): 
      netobj.multi_host = network.get_meta('multi_host') 
   if (network.get_meta('should_create_bridge') is not None): 
      netobj.should_provide_bridge = network.get_meta('should_create_bridge') 
   if (network.get_meta('should_create_vlan') is not None): 
      netobj.should_provide_vlan = network.get_meta('should_create_vlan') 
      if (network.get_meta('vlan') is None): 
         raise exception.NovaException((_('Missing   vlan   number   in   %s') % network)) 
      netobj.vlan = network.get_meta('vlan') 
   return netobj", 'Convert a nova.network.Network to an os-vif.network.Network.',"'Convert Nova network object into os_vif object 
 :param network: nova.network.model.Network instance 
 :returns: os_vif.objects.network.Network instance'"
"def norm_constraint(tensor_var, max_norm, norm_axes=None, epsilon=1e-07): 
    ndim = tensor_var.ndim 
   if (norm_axes is not None): 
      sum_over = tuple(norm_axes) 
   elif (ndim == 2): 
      sum_over = (0,) 
   elif (ndim in [3, 4, 5]): 
      sum_over = tuple(range(1, ndim)) 
   else: 
      raise ValueError('Unsupported   tensor   dimensionality   {}.Must   specify   `norm_axes`'.format(ndim)) 
   dtype = np.dtype(theano.config.floatX).type 
   norms = T.sqrt(T.sum(T.sqr(tensor_var), axis=sum_over, keepdims=True)) 
   target_norms = T.clip(norms, 0, dtype(max_norm)) 
   constrained_output = (tensor_var * (target_norms / (dtype(epsilon) + norms))) 
   return constrained_output"," 'Constrain tensor_var to have norm <= max_norm. 
 Parameters 
 tensor_var : tensor 
 Tensor to be constrained. 
 max_norm : float 
 Maximum norm for the constrained tensor. 
 norm_axes : tuple of ints, optional 
 Axis(s) to constrain the tensor to a norm of <= max_norm. 
 epsilon : float, optional 
 Small number to avoid division by zero. 
 Returns 
 constrained_output : tensor 
 Tensor with constrained norm. 
 Notes 
 This function is equivalent to the following: 
 ``` 
 max_norm = T.max(T.abs(tensor_var), axis=norm_axes) 
 constrained_output = (tensor_var * (max_norm / (T.abs(tensor_var) + epsilon))) 
 ``` 
 Examples 
 >>> x = T.tensor3(\'x\', \'y\', \'z\') 
 >>> y = norm_constraint(x, 10) 
 >>> y.eval() 
 array([[ 1.,","'Max weight norm constraints and gradient clipping 
 This takes a TensorVariable and rescales it so that incoming weight 
 norms are below a specified constraint value. Vectors violating the 
 constraint are rescaled so that they are within the allowed range. 
 Parameters 
 tensor_var : TensorVariable 
 Theano expression for update, gradient, or other quantity. 
 max_norm : scalar 
 This value sets the maximum allowed value of any norm in 
 `tensor_var`. 
 norm_axes : sequence (list or tuple) 
 The axes over which to compute the norm.  This overrides the 
 default norm axes defined for the number of dimensions 
 in `tensor_var`. When this is not specified and `tensor_var` is a 
 matrix (2D), this is set to `(0,)`. If `tensor_var` is a 3D, 4D or 
 5D tensor, it is set to a tuple listing all axes but axis 0. The 
 former default is useful for working with dense layers, the latter 
 is useful for 1D, 2D and 3D convolutional layers. 
 (Optional) 
 epsilon : scalar, optional 
 Value used to prevent numerical instability when dividing by 
 very small or zero norms. 
 Returns 
 TensorVariable 
 Input `tensor_var` with rescaling applied to weight vectors 
 that violate the specified constraints. 
 Examples 
 >>> param = theano.shared( 
 ...     np.random.randn(100, 200).astype(theano.config.floatX)) 
 >>> update = param + 100 
 >>> update = norm_constraint(update, 10) 
 >>> func = theano.function([], [], updates=[(param, update)]) 
 >>> # Apply constrained update 
 >>> _ = func() 
 >>> from lasagne.utils import compute_norms 
 >>> norms = compute_norms(param.get_value()) 
 >>> np.isclose(np.max(norms), 10) 
 True 
 Notes 
 When `norm_axes` is not specified, the axes over which the norm is 
 computed depend on the dimensionality of the input variable. If it is 
 2D, it is assumed to come from a dense layer, and the norm is computed 
 over axis 0. If it is 3D, 4D or 5D, it is assumed to come from a 
 convolutional layer and the norm is computed over all trailing axes 
 beyond axis 0. For other uses, you should explicitly specify the axes 
 over which to compute the norm using `norm_axes`.'"
"def delete_disk(kwargs=None, call=None): 
    if (call != 'function'): 
      raise SaltCloudSystemExit('The   delete_disk   function   must   be   called   with   -f   or   --function.') 
   if ((not kwargs) or ('disk_name' not in kwargs)): 
      log.error('A   disk_name   must   be   specified   when   deleting   a   disk.') 
      return False 
   conn = get_conn() 
   disk = conn.ex_get_volume(kwargs.get('disk_name')) 
   __utils__['cloud.fire_event']('event', 'delete   disk', 'salt/cloud/disk/deleting', args={'name': disk.name, 'location': disk.extra['zone'].name, 'size': disk.size}, sock_dir=__opts__['sock_dir'], transport=__opts__['transport']) 
   try: 
      result = conn.destroy_volume(disk) 
   except ResourceInUseError as exc: 
      log.error('Disk   {0}   is   in   use   and   must   be   detached   before   deleting.\nThe   following   exception   was   thrown   by   libcloud:\n{1}'.format(disk.name, exc), exc_info_on_loglevel=logging.DEBUG) 
      return False 
   __utils__['cloud.fire_event']('event', 'deleted   disk', 'salt/cloud/disk/deleted', args={'name': disk.name, 'location': disk.extra['zone'].name, 'size': disk.size}, sock_dir=__opts__['sock_dir'], transport=__opts__['transport']) 
   return result", 'Delete a disk.',"'Permanently delete a persistent disk. 
 CLI Example: 
 .. code-block:: bash 
 salt-cloud -f delete_disk gce disk_name=pd'"
"def _create_wx_app(): 
    wxapp = wx.GetApp() 
   if (wxapp is None): 
      wxapp = wx.PySimpleApp() 
      wxapp.SetExitOnFrameDelete(True) 
      _create_wx_app.theWxApp = wxapp"," 'Create the wx.App object. 
 :returns: The wx.App object.'",'Creates a wx.PySimpleApp instance if a wx.App has not been created.'
"def externals_finder(dirname, filename): 
    found = False 
   f = open(filename, 'rt') 
   for line in iter(f.readline, ''): 
      parts = line.split() 
      if (len(parts) == 2): 
         (kind, length) = parts 
         data = f.read(int(length)) 
         if ((kind == 'K') and (data == 'svn:externals')): 
            found = True 
         elif ((kind == 'V') and found): 
            f.close() 
            break 
   else: 
      f.close() 
      return 
   for line in data.splitlines(): 
      parts = line.split() 
      if parts: 
         (yield joinpath(dirname, parts[0]))"," 'Finds externals in a file. 
 :param dirname: The directory to search in 
 :param filename: The file to search 
 :rtype: iter(str)'",'Find any \'svn:externals\' directories'
"def quiet_close(closable): 
    try: 
      closable.close() 
   except Exception: 
      logger.debug(u'Exception   while   closing', exc_info=True)"," 'Close a file without printing a warning if it is not open. 
 :param closable: The file-like object to close. 
 :type closable: file-like object 
 :return: None'","'Quietly closes a closable object without throwing an exception. 
 :param closable: Object with a ``close`` method.'"
"def add_bbox_regression_targets(roidb): 
    assert (len(roidb) > 0) 
   assert ('max_classes' in roidb[0]), 'Did   you   call   prepare_roidb   first?' 
   num_images = len(roidb) 
   num_classes = roidb[0]['gt_overlaps'].shape[1] 
   for im_i in xrange(num_images): 
      rois = roidb[im_i]['boxes'] 
      max_overlaps = roidb[im_i]['max_overlaps'] 
      max_classes = roidb[im_i]['max_classes'] 
      roidb[im_i]['bbox_targets'] = _compute_targets(rois, max_overlaps, max_classes) 
   if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED: 
      means = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS), (num_classes, 1)) 
      stds = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS), (num_classes, 1)) 
   else: 
      class_counts = (np.zeros((num_classes, 1)) + cfg.EPS) 
      sums = np.zeros((num_classes, 4)) 
      squared_sums = np.zeros((num_classes, 4)) 
      for im_i in xrange(num_images): 
         targets = roidb[im_i]['bbox_targets'] 
         for cls in xrange(1, num_classes): 
            cls_inds = np.where((targets[:, 0] == cls))[0] 
            if (cls_inds.size > 0): 
               class_counts[cls] += cls_inds.size 
               sums[cls, :] += targets[cls_inds, 1:].sum(axis=0) 
               squared_sums[cls, :] += (targets[cls_inds, 1:] ** 2).sum(axis=0) 
      means = (sums / class_counts) 
      stds = np.sqrt(((squared_sums / class_counts) - (means ** 2))) 
   print 'bbox   target   means:' 
   print means 
   print means[1:, :].mean(axis=0) 
   print 'bbox   target   stdevs:' 
   print stds 
   print stds[1:, :].mean(axis=0) 
   if cfg.TRAIN.BBOX_NORMALIZE_TARGETS: 
      print 'Normalizing   targets' 
      for im_i in xrange(num_images): 
         targets = roidb[im_i]['bbox_targets'] 
         for cls in xrange(1, num_classes): 
            cls_inds = np.where((targets[:, 0] == cls))[0] 
            roidb[im_i]['bbox_targets'][cls_inds, 1:] -= means[cls, :] 
            roidb[im_i]['bbox_targets'][cls_inds, 1:] /= stds[cls, :] 
   else: 
      print 'NOT   normalizing   targets' 
   return (means.ravel(), stds.ravel())"," 'Computes the bbox regression targets for each image. 
 This function assumes that the roidb is already sorted by 
 image index. 
 Args: 
 roidb: a list of roidb objects. 
 Returns: 
 means, stds: a list of means and stds for each class. 
 Notes: 
 This function is used for training bbox regression and 
 for computing the bbox regression targets for testing.'",'Add information needed to train bounding-box regressors.'
"def is_installed(pkg_name): 
    with settings(warn_only=True): 
      res = run(('pkg_info   -e   %s' % pkg_name)) 
      return (res.succeeded is True)"," 'Check if a package is installed. 
 :param pkg_name: the package name 
 :return: True if the package is installed, False otherwise'",'Check if a package is installed.'
"def _cg(A, b, x0=None, tol=1e-10, maxiter=1000): 
    n = b.size 
   assert (A.n == n) 
   assert (A.m == n) 
   b_norm = np.linalg.norm(b) 
   kvec = A.diag 
   kvec = np.where((kvec > 1e-06), kvec, 1e-06) 
   if (x0 is None): 
      x = np.zeros(n) 
   else: 
      x = x0 
   r = (b - A.dot(x)) 
   w = (r / kvec) 
   p = np.zeros(n) 
   beta = 0.0 
   rho = np.dot(r, w) 
   k = 0 
   while ((np.sqrt(abs(rho)) > (tol * b_norm)) and (k < maxiter)): 
      p = (w + (beta * p)) 
      z = A.dot(p) 
      alpha = (rho / np.dot(p, z)) 
      r = (r - (alpha * z)) 
      w = (r / kvec) 
      rhoold = rho 
      rho = np.dot(r, w) 
      x = (x + (alpha * p)) 
      beta = (rho / rhoold) 
      k += 1 
   err = np.linalg.norm((A.dot(x) - b)) 
   return (x, err)"," 'CG algorithm for solving Ax = b. 
 Parameters 
 A : ndarray 
 The coefficient matrix. 
 b : ndarray 
 The right-hand side. 
 x0 : ndarray 
 The starting point. 
 tol : float 
 The tolerance. 
 maxiter : int 
 The maximum number of iterations. 
 Returns 
 x : ndarray 
 The solution. 
 err : float 
 The error. 
 Examples 
 >>> from sympy.linalg import _cg 
 >>> from sympy import Matrix 
 >>> from sympy.abc import x 
 >>> A = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 
 >>> b = Matrix([10, 11, 12]) 
 >>> _cg(A, b, x0=x) 
 (array([1., 2., 3.]), 0.0) 
 References 
 .. [1] http://en.wikipedia.org/wiki/Conjugate_gradient_method","'Use Preconditioned Conjugate Gradient iteration to solve A x = b 
 A simple Jacobi (diagonal) preconditionner is used. 
 Parameters 
 A: _Sparse_Matrix_coo 
 *A* must have been compressed before by compress_csc or 
 compress_csr method. 
 b: array 
 Right hand side of the linear system. 
 Returns 
 x: array. 
 The converged solution. 
 err: float 
 The absolute error np.linalg.norm(A.dot(x) - b) 
 Other parameters 
 x0: array. 
 Starting guess for the solution. 
 tol: float. 
 Tolerance to achieve. The algorithm terminates when the relative 
 residual is below tol. 
 maxiter: integer. 
 Maximum number of iterations. Iteration will stop 
 after maxiter steps even if the specified tolerance has not 
 been achieved.'"
"def contracted_edge(G, edge, self_loops=True): 
    if (not G.has_edge(*edge)): 
      raise ValueError('Edge   {0}   does   not   exist   in   graph   G;   cannot   contract   it'.format(edge)) 
   return contracted_nodes(G, self_loops=self_loops, *edge)"," 'Return a contracted version of the given edge. 
 Parameters 
 G : Graph 
 The graph to contract. 
 edge : tuple of vertices 
 The edge to contract. 
 self_loops : bool 
 Whether to contract self-loops. 
 Returns 
 A contracted version of the given edge. 
 Examples 
 >>> G = nx.Graph() 
 >>> G.add_edge(0, 1) 
 >>> G.add_edge(1, 2) 
 >>> G.add_edge(2, 0) 
 >>> G.add_edge(2, 3) 
 >>> G.add_edge(3, 2) 
 >>> G.add_edge(3, 0) 
 >>> G.add_edge(0, 3) 
 >>> G.add_edge(0, 2) 
 >>> G.add_edge(0, 1) 
 >>> nx.contracted_edge(G, (0, 1, 2, 3)) 
 (0, 1, 2, 3) 
 >>>","'Returns the graph that results from contracting the specified edge. 
 Edge contraction identifies the two endpoints of the edge as a single node 
 incident to any edge that was incident to the original two nodes. A graph 
 that results from edge contraction is called a *minor* of the original 
 graph. 
 Parameters 
 G : NetworkX graph 
 The graph whose edge will be contracted. 
 edge : tuple 
 Must be a pair of nodes in `G`. 
 self_loops : Boolean 
 If this is True, any edges (including `edge`) joining the 
 endpoints of `edge` in `G` become self-loops on the new node in the 
 returned graph. 
 Returns 
 Networkx graph 
 A new graph object of the same type as `G` (leaving `G` unmodified) 
 with endpoints of `edge` identified in a single node. The right node 
 of `edge` will be merged into the left one, so only the left one will 
 appear in the returned graph. 
 Raises 
 ValueError 
 If `edge` is not an edge in `G`. 
 Examples 
 Attempting to contract two nonadjacent nodes yields an error:: 
 >>> import networkx as nx 
 >>> G = nx.cycle_graph(4) 
 >>> nx.contracted_edge(G, (1, 3)) 
 Traceback (most recent call last): 
 ValueError: Edge (1, 3) does not exist in graph G; cannot contract it 
 Contracting two adjacent nodes in the cycle graph on *n* nodes yields the 
 cycle graph on *n - 1* nodes:: 
 >>> import networkx as nx 
 >>> C5 = nx.cycle_graph(5) 
 >>> C4 = nx.cycle_graph(4) 
 >>> M = nx.contracted_edge(C5, (0, 1), self_loops=False) 
 >>> nx.is_isomorphic(M, C4) 
 True 
 See also 
 contracted_nodes 
 quotient_graph'"
"def test_wheel_compiles_pyc(script, data): 
    script.pip('install', '--compile', 'simple.dist==0.1', '--no-index', ('--find-links=' + data.find_links)) 
   exists = [os.path.exists((script.site_packages_path / 'simpledist/__init__.pyc'))] 
   exists += glob.glob((script.site_packages_path / 'simpledist/__pycache__/__init__*.pyc')) 
   assert any(exists)", 'Verify that the wheel installs the pyc files.','Test installing from wheel with --compile on'
"def base_vectors(n): 
    n = (n / np.sqrt(np.square(n).sum(axis=(-1)))) 
   if (abs(n[0]) == 1): 
      l = np.r_[(n[2], 0, (- n[0]))] 
   else: 
      l = np.r_[(0, n[2], (- n[1]))] 
   l = (l / np.sqrt(np.square(l).sum(axis=(-1)))) 
   m = np.cross(n, l) 
   return (n, l, m)"," 'Returns a base vector of the form (n, l, m) where n is a normalized 
 vector, l is a vector in the same direction but with length 1, and m 
 is a vector orthogonal to n and l. 
 Parameters 
 n : array-like 
 A normalized vector. 
 Returns 
 n, l, m : array-like 
 The base vectors.'","'Returns 3 orthognal base vectors, the first one colinear to n.'"
"def DateFromTicks(ticks): 
    return Date(*time.localtime(ticks)[:3])", 'Convert a time ticks to a datetime.date object.',"'Construct an object holding a date value from the given ticks value 
 (number of seconds since the epoch). 
 This function is part of the `DBAPI 2.0 specification 
 <http://www.python.org/dev/peps/pep-0249/>`_. 
 :rtype: :class:`datetime.date`'"
"def instance_tag_delete_all(context, instance_uuid): 
    return IMPL.instance_tag_delete_all(context, instance_uuid)", 'Delete all tags from an instance.','Delete all tags from the instance.'
"def invalidate_star_import_cache(path): 
    try: 
      parser_cache_item = parser_cache[path] 
   except KeyError: 
      pass 
   else: 
      _invalidate_star_import_cache_module(parser_cache_item.parser.module)"," 'Invalidate the import cache for a given path. 
 This is used to invalidate the import cache when the module is 
 reloaded, so that we can detect if a module has been renamed. 
 :param path: The path to the module to invalidate the import cache for. 
 :type path: str'",'On success returns True.'
"def coverage(fn): 
    fp = TraceFuncCoverage(fn) 
   def new_fn(*args, **kw): 
      return fp(*args, **kw) 
   new_fn.__doc__ = fn.__doc__ 
   new_fn.__name__ = fn.__name__ 
   new_fn.__dict__ = fn.__dict__ 
   new_fn.__module__ = fn.__module__ 
   return new_fn"," 'Returns a function that calls the given function with a trace 
 function that records the line numbers where the function was called. 
 The trace function is a callable that takes the function name as 
 argument and returns a callable that takes the function name and 
 the line number where the function was called as arguments. 
 The trace function is called by the function with the given name. 
 The trace function is called with the line number where the function 
 was called as argument. 
 The trace function returns a callable that is called by the function 
 with the given name. The callable takes the function name and the 
 line number where the function was called as arguments. 
 Example: 
 >>> def foo(x): 
 ...     if x == 1: 
 ...         return 1 
 ...     return 2 
 >>> def trace(fn): 
 ...     def trace_fn(name, line): 
 ...         print \'Called   function   %s   at   line   %d\' % (name, line) 
 ...         return trace_fn 
 ...     return trace_fn 
 >>> def bar(x","'Mark `fn` for line coverage analysis. 
 Results will be printed to sys.stdout on program termination. 
 Usage:: 
 def fn(...): 
 fn = coverage(fn) 
 If you are using Python 2.4, you should be able to use the decorator 
 syntax:: 
 @coverage 
 def fn(...):'"
"def _downgrade_sqlite(t_images, t_image_members, t_image_properties): 
    sql_commands = ['CREATE   TABLE   images_backup   (\n                                 id   INTEGER   NOT   NULL,\n                                 name   VARCHAR(255),\n                                 size   INTEGER,\n                                 status   VARCHAR(30)   NOT   NULL,\n                                 is_public   BOOLEAN   NOT   NULL,\n                                 location   TEXT,\n                                 created_at   DATETIME   NOT   NULL,\n                                 updated_at   DATETIME,\n                                 deleted_at   DATETIME,\n                                 deleted   BOOLEAN   NOT   NULL,\n                                 disk_format   VARCHAR(20),\n                                 container_format   VARCHAR(20),\n                                 checksum   VARCHAR(32),\n                                 owner   VARCHAR(255),\n                                 min_disk   INTEGER   NOT   NULL,\n                                 min_ram   INTEGER   NOT   NULL,\n                                 PRIMARY   KEY   (id),\n                                 CHECK   (is_public   IN   (0,   1)),\n                                 CHECK   (deleted   IN   (0,   1))\n                        );', 'INSERT   INTO   images_backup\n                                 SELECT   *   FROM   images;', 'CREATE   TABLE   image_members_backup   (\n                                    id   INTEGER   NOT   NULL,\n                                    image_id   INTEGER   NOT   NULL,\n                                    member   VARCHAR(255)   NOT   NULL,\n                                    can_share   BOOLEAN   NOT   NULL,\n                                    created_at   DATETIME   NOT   NULL,\n                                    updated_at   DATETIME,\n                                    deleted_at   DATETIME,\n                                    deleted   BOOLEAN   NOT   NULL,\n                                    PRIMARY   KEY   (id),\n                                    UNIQUE   (image_id,   member),\n                                    CHECK   (can_share   IN   (0,   1)),\n                                    CHECK   (deleted   IN   (0,   1)),\n                                    FOREIGN   KEY(image_id)   REFERENCES   images   (id)\n                        );', 'INSERT   INTO   image_members_backup\n                                    SELECT   *   FROM   image_members;', 'CREATE   TABLE   image_properties_backup   (\n                                    id   INTEGER   NOT   NULL,\n                                    image_id   INTEGER      NOT   NULL,\n                                    name   VARCHAR(255)   NOT   NULL,\n                                    value   TEXT,\n                                    created_at   DATETIME   NOT   NULL,\n                                    updated_at   DATETIME,\n                                    deleted_at   DATETIME,\n                                    deleted   BOOLEAN   NOT   NULL,\n                                    PRIMARY   KEY   (id),\n                                    CHECK   (deleted   IN   (0,   1)),\n                                    UNIQUE   (image_id,   name),\n                                    FOREIGN   KEY(image_id)   REFERENCES   images   (id)\n                        );', 'INSERT   INTO   image_properties_backup\n                                    SELECT   *   FROM   image_properties;'] 
   for command in sql_commands: 
      meta.bind.execute(command) 
   _sqlite_table_swap(t_image_members, t_image_properties, t_images)", 'Downgrades the database from SQLite to PostgreSQL','Downgrade 012 -> 011 with special SQLite-compatible logic.'
"def test_system_dynamic(): 
    print 'TODO'", 'Test that system.dynamic() returns the correct result',"'http://msdn.microsoft.com/en-us/library/system.dynamic(VS.100).aspx 
 Only a sanity check here.  Exhaustive testing of \'dynamic\' will exist 
 elsewhere.  Piggyback off of CSharp/VB\'s work.'"
"def naturaldate(date, include_seconds=False): 
    if (not date): 
      return u'' 
   right_now = now() 
   today = datetime(right_now.year, right_now.month, right_now.day, tzinfo=right_now.tzinfo) 
   delta = (right_now - date) 
   delta_midnight = (today - date) 
   days = delta.days 
   hours = (delta.seconds // 3600) 
   minutes = (delta.seconds // 60) 
   seconds = delta.seconds 
   if (days < 0): 
      return _(u'just   now') 
   if (days == 0): 
      if (hours == 0): 
         if (minutes > 0): 
            return ungettext(_(u'{minutes}   minute   ago'), _(u'{minutes}   minutes   ago'), minutes).format(minutes=minutes) 
         else: 
            if (include_seconds and seconds): 
               return ungettext(_(u'{seconds}   second   ago'), _(u'{seconds}   seconds   ago'), seconds).format(seconds=seconds) 
            return _(u'just   now') 
      else: 
         return ungettext(_(u'{hours}   hour   ago'), _(u'{hours}   hours   ago'), hours).format(hours=hours) 
   if (delta_midnight.days == 0): 
      return _(u'yesterday   at   {time}').format(time=date.strftime(u'%H:%M')) 
   count = 0 
   for (chunk, pluralizefun) in OLDER_CHUNKS: 
      if (days >= chunk): 
         count = int(round(((delta_midnight.days + 1) / chunk), 0)) 
         fmt = pluralizefun(count) 
         return fmt.format(num=count)"," 'Convert a date to a human-readable string. 
 :param date: The date to convert. 
 :param include_seconds: If True, include seconds in the output. 
 :return: A string representing the date in a human-readable format. 
 :rtype: str'",'Convert datetime into a human natural date string.'
"def SetupSharedModules(module_dict): 
    output_dict = {} 
   for (module_name, module) in module_dict.iteritems(): 
      if (module is None): 
         continue 
      if IsEncodingsModule(module_name): 
         output_dict[module_name] = module 
         continue 
      shared_prefix = ModuleNameHasPrefix(module_name, SHARED_MODULE_PREFIXES) 
      banned_prefix = ModuleNameHasPrefix(module_name, NOT_SHARED_MODULE_PREFIXES) 
      if (shared_prefix and (not banned_prefix)): 
         output_dict[module_name] = module 
   return output_dict"," 'Returns a dictionary of modules that should be shared between the 
 interpreter and the shared library.'","'Creates a module dictionary for the hardened part of the process. 
 Module dictionary will contain modules that should be shared between the 
 hardened and unhardened parts of the process. 
 Args: 
 module_dict: Module dictionary from which existing modules should be 
 pulled (usually sys.modules). 
 Returns: 
 A new module dictionary.'"
"@task 
 def clean(): 
    path('build').rmtree() 
   path('dist').rmtree()", 'Clean the build and dist directories.','Cleans build directories'
"def test_ada_fit_invalid_ratio(): 
    ratio = (1.0 / 10000.0) 
   ada = ADASYN(ratio=ratio, random_state=RND_SEED) 
   assert_raises(RuntimeError, ada.fit, X, Y)", 'Test that an error is raised if the ratio is invalid.',"'Test either if an error is raised when the balancing ratio to fit is 
 smaller than the one of the data'"
"def test_custom_model_subclass(): 
    @custom_model 
   def model_a(x, a=1): 
      return (x * a) 
   class model_b(model_a, ): 
      @classmethod 
      def evaluate(cls, x, a): 
         return (- super(model_b, cls).evaluate(x, a)) 
   b = model_b() 
   assert (b.param_names == (u'a',)) 
   assert (b.a == 1) 
   assert (b(1) == (-1)) 
   sig = signature(model_b.__init__) 
   assert (list(sig.parameters.keys()) == [u'self', u'a', u'kwargs']) 
   sig = signature(model_b.__call__) 
   assert (list(sig.parameters.keys()) == [u'self', u'x', u'model_set_axis'])", 'Test custom model subclass.','Test that custom models can be subclassed.'
"def countNonPrintableChars(string): 
    counter = 0 
   for i in range(len(string)): 
      if ((ord(string[i]) <= 31) or (ord(string[i]) > 127)): 
         counter += 1 
   return counter", 'Returns the number of non-printable characters in the string.',"'Simple method to return the non printable characters found in an string 
 @param string: A string 
 @return: Number of non printable characters in the string'"
"def get_scores(video): 
    if isinstance(video, Episode): 
      return episode_scores 
   elif isinstance(video, Movie): 
      return movie_scores 
   raise ValueError('video   must   be   an   instance   of   Episode   or   Movie')"," 'Return a list of scores for the given video. 
 :param video: 
 :type video: 
 :return: 
 :rtype: 
 :raise ValueError: 
 :param video: 
 :type video: 
 :param video: 
 :type video: 
 :return: 
 :rtype: 
 :raise ValueError: 
 :param video: 
 :type video: 
 :param video: 
 :type video: 
 :return: 
 :rtype: 
 :raise ValueError: 
 :param video: 
 :type video: 
 :param video: 
 :type video: 
 :return: 
 :rtype: 
 :raise ValueError: 
 :param video: 
 :type video: 
 :param video: 
 :type video: 
 :return: 
 :rtype: 
 :raise ValueError: 
 :param video: 
 :type video: 
 :param video: 
 :type video: 
 :return: 
 :rtype: 
 :raise ValueError: 
 :param video","'Get the scores dict for the given `video`. 
 This will return either :data:`episode_scores` or :data:`movie_scores` based on the type of the `video`. 
 :param video: the video to compute the score against. 
 :type video: :class:`~subliminal.video.Video` 
 :return: the scores dict. 
 :rtype: dict'"
"def kill_pid(pid, signal=15): 
    try: 
      psutil.Process(pid).send_signal(signal) 
      return True 
   except psutil.NoSuchProcess: 
      return False"," 'Kill a process with the given pid. 
 :param pid: the process id to kill 
 :param signal: the signal to send 
 :return: True if the process was killed, False otherwise'","'Kill a process by PID. 
 .. code-block:: bash 
 salt \'minion\' ps.kill_pid pid [signal=signal_number] 
 pid 
 PID of process to kill. 
 signal 
 Signal to send to the process. See manpage entry for kill 
 for possible values. Default: 15 (SIGTERM). 
 **Example:** 
 Send SIGKILL to process with PID 2000: 
 .. code-block:: bash 
 salt \'minion\' ps.kill_pid 2000 signal=9'"
"def escape(text): 
    text = text.replace('\\', '\\\\') 
   text = text.replace('""""""', '""""\\""') 
   text = text.replace('   \n', '   \\n\\\n') 
   return text"," 'Escape a string for use in a template. 
 This is a copy of the default Django template engine escape method, 
 but it escapes quotes and newlines, since those are important in 
 the context of a template.'",'Return `text` in triple-double-quoted Python string form.'
"def int_to_str(value, length=2): 
    try: 
      int(value) 
   except: 
      raise ValueError('expected   an   integer   value') 
   content = str(value) 
   while (len(content) < length): 
      content = ('0' + content) 
   return content"," 'Convert integer to string. 
 :param value: integer 
 :type value: int 
 :param length: length of the string 
 :type length: int 
 :return: string 
 :rtype: str'","'Converts integer to string eg 3 to ""03""'"
"def validate_payload(payload, api_model, check_required=True): 
    if check_required: 
      for key in api_model: 
         if (api_model[key].required and (key not in payload)): 
            raise ValidationError(field=key, message=""Required   field   '{}'   missing"".format(key)) 
   for key in payload: 
      field = api_model[key] 
      if isinstance(field, fields.List): 
         field = field.container 
         data = payload[key] 
      elif isinstance(field, fields.Nested): 
         if payload[key]: 
            validate_payload(payload[key], field.model) 
      else: 
         data = [payload[key]] 
      if (isinstance(field, CustomField) and hasattr(field, 'validate')): 
         field.payload = payload 
         for i in data: 
            if (not field.validate(i)): 
               raise ValidationError(field=key, message=(field.validation_error % (""'%s'"" % key)))"," 'Validate payload against api_model. 
 :param payload: dict 
 :param api_model: dict 
 :param check_required: bool 
 :return: None 
 :raises ValidationError: 
 :raises ValueError: 
 :raises TypeError: 
 :raises AttributeError: 
 :raises KeyError: 
 :raises NotImplementedError: 
 :raises InvalidPayloadError: 
 :raises FieldError: 
 :raises ValidationError: 
 :raises MissingFieldError: 
 :raises EmptyFieldError: 
 :raises NotEmptyFieldError: 
 :raises InvalidFieldError: 
 :raises UnknownFieldError: 
 :raises InvalidValueError: 
 :raises ValueError: 
 :raises KeyError: 
 :raises TypeError: 
 :raises InvalidPayloadError: 
 :raises ValueError: 
 :raises FieldError: 
 :raises ValidationError: 
 :raises MissingFieldError: 
 :raises EmptyFieldError: 
","'Validate payload against an api_model. Aborts in case of failure 
 - This function is for custom fields as they can\'t be validated by 
 flask restplus automatically. 
 - This is to be called at the start of a post or put method'"
"def serialize_item(collection, item): 
    if ((item.name is None) or (item.name == '')): 
      raise exceptions.RuntimeError('name   unset   for   item!') 
   if (collection.collection_type() in ['mgmtclass']): 
      filename = ('/var/lib/cobbler/collections/%ses/%s' % (collection.collection_type(), item.name)) 
   else: 
      filename = ('/var/lib/cobbler/collections/%ss/%s' % (collection.collection_type(), item.name)) 
   _dict = item.to_dict() 
   if capi.CobblerAPI().settings().serializer_pretty_json: 
      sort_keys = True 
      indent = 4 
   else: 
      sort_keys = False 
      indent = None 
   filename += '.json' 
   _dict = item.to_dict() 
   fd = open(filename, 'w+') 
   data = simplejson.dumps(_dict, encoding='utf-8', sort_keys=sort_keys, indent=indent) 
   fd.write(data) 
   fd.close()"," 'Serialize a single item to a file. 
 :param collection: The collection to serialize. 
 :param item: The item to serialize. 
 :return: None'","'Save a collection item to file system 
 @param Collection collection collection 
 @param Item item collection item'"
"def get_raising_file_and_line(tb=None): 
    if (not tb): 
      tb = sys.exc_info()[2] 
   (filename, lineno, _context, _line) = traceback.extract_tb(tb)[(-1)] 
   return (filename, lineno)", 'Returns the filename and line number of the raising exception.',"'Return the file and line number of the statement that raised the tb. 
 Returns: (filename, lineno) tuple'"
"def _minimize_cg(fun, x0, args=(), jac=None, callback=None, gtol=1e-05, norm=Inf, eps=_epsilon, maxiter=None, disp=False, return_all=False, **unknown_options): 
    _check_unknown_options(unknown_options) 
   f = fun 
   fprime = jac 
   epsilon = eps 
   retall = return_all 
   x0 = asarray(x0).flatten() 
   if (maxiter is None): 
      maxiter = (len(x0) * 200) 
   (func_calls, f) = wrap_function(f, args) 
   if (fprime is None): 
      (grad_calls, myfprime) = wrap_function(approx_fprime, (f, epsilon)) 
   else: 
      (grad_calls, myfprime) = wrap_function(fprime, args) 
   gfk = myfprime(x0) 
   k = 0 
   xk = x0 
   old_fval = f(xk) 
   old_old_fval = (old_fval + (np.linalg.norm(gfk) / 2)) 
   if retall: 
      allvecs = [xk] 
   warnflag = 0 
   pk = (- gfk) 
   gnorm = vecnorm(gfk, ord=norm) 
   while ((gnorm > gtol) and (k < maxiter)): 
      deltak = numpy.dot(gfk, gfk) 
      try: 
         (alpha_k, fc, gc, old_fval, old_old_fval, gfkp1) = _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval, old_old_fval, c2=0.4, amin=1e-100, amax=1e+100) 
      except _LineSearchError: 
         warnflag = 2 
         break 
      xk = (xk + (alpha_k * pk)) 
      if retall: 
         allvecs.append(xk) 
      if (gfkp1 is None): 
         gfkp1 = myfprime(xk) 
      yk = (gfkp1 - gfk) 
      beta_k = max(0, (numpy.dot(yk, gfkp1) / deltak)) 
      pk = ((- gfkp1) + (beta_k * pk)) 
      gfk = gfkp1 
      gnorm = vecnorm(gfk, ord=norm) 
      if (callback is not None): 
         callback(xk) 
      k += 1 
   fval = old_fval 
   if (warnflag == 2): 
      msg = _status_message['pr_loss'] 
      if disp: 
         print(('Warning:   ' + msg)) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   elif (k >= maxiter): 
      warnflag = 1 
      msg = _status_message['maxiter'] 
      if disp: 
         print(('Warning:   ' + msg)) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   else: 
      msg = _status_message['success'] 
      if disp: 
         print(msg) 
         print(('                           Current   function   value:   %f' % fval)) 
         print(('                           Iterations:   %d' % k)) 
         print(('                           Function   evaluations:   %d' % func_calls[0])) 
         print(('                           Gradient   evaluations:   %d' % grad_calls[0])) 
   result = OptimizeResult(fun=fval, jac=gfk, nfev=func_calls[0], njev=grad_calls[0], status=warnflag, success=(warnflag == 0), message=msg, x=xk, nit=k) 
   if retall: 
      result['allvecs'] = allvecs 
   return result"," 'Minimize a function using the conjugate gradient method. 
 The conjugate gradient method is a simple and efficient method for 
 minimizing a function of several variables.  It is a nonlinear method, 
 which means that the function is not differentiable in general. 
 The method is based on the following steps: 
 1. Starting with an initial guess, the gradient of the function is 
 evaluated at the initial guess. 
 2. The gradient is used to compute a direction in which the function 
 is expected to decrease. 
 3. The direction is used to update the initial guess. 
 4. The steps are repeated until the direction is sufficiently small. 
 The method is very simple to implement, but it is not always guaranteed 
 to converge to a local minimum. 
 Parameters 
 fun : callable 
 The objective function. 
 x0 : array_like 
 Initial guess for the variables. 
 args : tuple of array_like 
 Optional arguments for the objective function. 
 jac : callable 
 Optional function to compute the Jacobian of the objective function. 
 If jac is None, the","'Minimization of scalar function of one or more variables using the 
 conjugate gradient algorithm. 
 Options 
 disp : bool 
 Set to True to print convergence messages. 
 maxiter : int 
 Maximum number of iterations to perform. 
 gtol : float 
 Gradient norm must be less than `gtol` before successful 
 termination. 
 norm : float 
 Order of norm (Inf is max, -Inf is min). 
 eps : float or ndarray 
 If `jac` is approximated, use this value for the step size.'"
"def get_recurring(**filter_data): 
    return rpc_utils.prepare_rows_as_nested_dicts(models.RecurringRun.query_objects(filter_data), ('job', 'owner'))"," 'Returns a list of recurring run objects. 
 :param filter_data: A dict containing the filter data. 
 :returns: A list of recurring run objects. 
 :raises: :class:`~sqlalchemy.exc.SQLAlchemyError` if no recurring runs are found.'","'Return recurring jobs. 
 :param filter_data: Filters out which recurring jobs to get. 
 :return: Sequence of recurring jobs.'"
"def absent(name): 
    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''} 
   grp_info = __salt__['group.info'](name) 
   if grp_info: 
      if __opts__['test']: 
         ret['result'] = None 
         ret['comment'] = 'Group   {0}   is   set   for   removal'.format(name) 
         return ret 
      ret['result'] = __salt__['group.delete'](name) 
      if ret['result']: 
         ret['changes'] = {name: ''} 
         ret['comment'] = 'Removed   group   {0}'.format(name) 
         return ret 
      else: 
         ret['comment'] = 'Failed   to   remove   group   {0}'.format(name) 
         return ret 
   else: 
      ret['comment'] = 'Group   not   present' 
      return ret"," 'Remove a group from the system. 
 This function will only remove the group if it is not already 
 present. 
 This function will return a dictionary with the following keys: 
 name 
 The name of the group to be removed. 
 changes 
 A dictionary of changes made to the system. 
 result 
 True if the group was removed, False otherwise. 
 comment 
 A comment describing what was done.'","'Ensure that the named group is absent 
 name 
 The name of the group to remove'"
"def quitWindows(event=None): 
    global globalRepositoryDialogListTable 
   globalRepositoryDialogValues = euclidean.getListTableElements(globalRepositoryDialogListTable) 
   for globalRepositoryDialogValue in globalRepositoryDialogValues: 
      quitWindow(globalRepositoryDialogValue.root)", 'Close all windows opened by this dialog.','Quit all windows.'
"def get(key, profile=None): 
    data = _get_values(profile) 
   return salt.utils.traverse_dict_and_list(data, key, None)"," 'Get a value from the config file. 
 If the value is not in the config file, return None. 
 :param key: The key to search for. 
 :param profile: The profile to use. 
 :returns: The value.'",'Get a value from the REST interface'
"def for_all_dtypes(name='dtype', no_float16=False, no_bool=False): 
    return for_dtypes(_make_all_dtypes(no_float16, no_bool), name=name)"," 'A generator that yields all types for which the given function is 
 True.'","'Decorator that checks the fixture with all dtypes. 
 Args: 
 name(str): Argument name to which specified dtypes are passed. 
 no_float16(bool): If, True, ``numpy.float16`` is 
 omitted from candidate dtypes. 
 no_bool(bool): If, True, ``numpy.bool_`` is 
 omitted from candidate dtypes. 
 dtypes to be tested: ``numpy.float16`` (optional), ``numpy.float32``, 
 ``numpy.float64``, ``numpy.dtype(\'b\')``, ``numpy.dtype(\'h\')``, 
 ``numpy.dtype(\'i\')``, ``numpy.dtype(\'l\')``, ``numpy.dtype(\'q\')``, 
 ``numpy.dtype(\'B\')``, ``numpy.dtype(\'H\')``, ``numpy.dtype(\'I\')``, 
 ``numpy.dtype(\'L\')``, ``numpy.dtype(\'Q\')``, and ``numpy.bool_`` (optional). 
 The usage is as follows. 
 This test fixture checks if ``cPickle`` successfully reconstructs 
 :class:`cupy.ndarray` for various dtypes. 
 ``dtype`` is an argument inserted by the decorator. 
 >>> import unittest 
 >>> from cupy import testing 
 >>> @testing.gpu 
 ... class TestNpz(unittest.TestCase): 
 ...     @testing.for_all_dtypes() 
 ...     def test_pickle(self, dtype): 
 ...         a = testing.shaped_arange((2, 3, 4), dtype=dtype) 
 ...         s = six.moves.cPickle.dumps(a) 
 ...         b = six.moves.cPickle.loads(s) 
 ...         testing.assert_array_equal(a, b) 
 Typically, we use this decorator in combination with 
 decorators that check consistency between NumPy and CuPy like 
 :func:`cupy.testing.numpy_cupy_allclose`. 
 The following is such an example. 
 >>> import unittest 
 >>> from cupy import testing 
 >>> @testing.gpu 
 ... class TestMean(unittest.TestCase): 
 ...     @testing.for_all_dtypes() 
 ...     @testing.numpy_cupy_allclose() 
 ...     def test_mean_all(self, xp, dtype): 
 ...         a = testing.shaped_arange((2, 3), xp, dtype) 
 ...         return a.mean() 
 .. seealso:: :func:`cupy.testing.for_dtypes`'"
"def mapping_file_to_dict(mapping_data, header): 
    map_dict = {} 
   for i in range(len(mapping_data)): 
      sam = mapping_data[i] 
      map_dict[sam[0]] = {} 
      for j in range(len(header)): 
         if (j == 0): 
            continue 
         map_dict[sam[0]][header[j]] = sam[j] 
   return map_dict"," 'Convert a mapping file to a dictionary. 
 :param mapping_data: The mapping file. 
 :param header: The header of the mapping file. 
 :return: A dictionary mapping the sample names to the mapping 
 data.'",'processes mapping data in list of lists format into a 2 deep dict'
"def firstof(parser, token): 
    bits = token.split_contents()[1:] 
   if (len(bits) < 1): 
      raise TemplateSyntaxError(""'firstof'   statement   requires   at   least   one   argument"") 
   return FirstOfNode([parser.compile_filter(bit) for bit in bits])"," 'Parse the first of statement. 
 :param parser: The parser instance. 
 :param token: The token. 
 :type token: :class:`argparse.Token` 
 :rtype: :class:`jinja2.nodes.NodeList` 
 :return: A list of nodes. 
 :raise TemplateSyntaxError: If the firstof statement is not valid.'","'Outputs the first variable passed that is not False, without escaping. 
 Outputs nothing if all the passed variables are False. 
 Sample usage:: 
 {% firstof var1 var2 var3 %} 
 This is equivalent to:: 
 {% if var1 %} 
 {{ var1|safe }} 
 {% else %}{% if var2 %} 
 {{ var2|safe }} 
 {% else %}{% if var3 %} 
 {{ var3|safe }} 
 {% endif %}{% endif %}{% endif %} 
 but obviously much cleaner! 
 You can also use a literal string as a fallback value in case all 
 passed variables are False:: 
 {% firstof var1 var2 var3 ""fallback value"" %} 
 If you want to escape the output, use a filter tag:: 
 {% filter force_escape %} 
 {% firstof var1 var2 var3 ""fallback value"" %} 
 {% endfilter %}'"
"@app.route('/scans/<int:scan_id>/exceptions/', methods=['GET']) 
 @requires_auth 
 def list_exceptions(scan_id): 
    scan_info = get_scan_info_from_id(scan_id) 
   if (scan_info is None): 
      abort(404, 'Scan   not   found') 
   data = [] 
   all_exceptions = scan_info.w3af_core.exception_handler.get_all_exceptions() 
   for (exception_id, exception_data) in enumerate(all_exceptions): 
      data.append(exception_to_json(exception_data, scan_id, exception_id)) 
   return jsonify({'items': data})"," 'List all exceptions raised by the scan. 
 :param scan_id: The id of the scan. 
 :returns: JSON list of all exceptions raised by the scan.'","'List all exceptions found during a scan 
 :return: A JSON containing a list of: 
 - Exception resource URL (eg. /scans/0/exceptions/3) 
 - The exceptions id (eg. 3) 
 - Exception string 
 - Exception file name 
 - Exception line number'"
"def _check_even_rewrite(func, arg): 
    return (func(arg).args[0] == (- arg))"," 'Check if the function is even rewritable. 
 If the function is even rewritable, then the first argument 
 is even, otherwise it is odd.'","'Checks that the expr has been rewritten using f(-x) -> f(x) 
 arg : -x'"
"def is_jid(jid): 
    if (not isinstance(jid, six.string_types)): 
      return False 
   if (len(jid) != 20): 
      return False 
   try: 
      int(jid) 
      return True 
   except ValueError: 
      return False"," 'Check if the given jid is a valid jid. 
 :param jid: JID to check 
 :type jid: string 
 :return: True if the jid is valid, False otherwise 
 :rtype: bool'",'Returns True if the passed in value is a job id'
"def encode_single(typ, arg): 
    (base, sub, _) = typ 
   if (base == 'uint'): 
      sub = int(sub) 
      if (not ((0 < sub <= 256) and ((sub % 8) == 0))): 
         raise ValueError('invalid   unsigned   integer   bit   length   {}'.format(sub)) 
      try: 
         i = decint(arg, signed=False) 
      except EncodingError: 
         raise ValueOutOfBounds(repr(arg)) 
      if (not (0 <= i < (2 ** sub))): 
         raise ValueOutOfBounds(repr(arg)) 
      value_encoded = int_to_big_endian(i) 
      return zpad(value_encoded, 32) 
   if (base == 'int'): 
      sub = int(sub) 
      bits = (sub - 1) 
      if (not ((0 < sub <= 256) and ((sub % 8) == 0))): 
         raise ValueError('invalid   integer   bit   length   {}'.format(sub)) 
      try: 
         i = decint(arg, signed=True) 
      except EncodingError: 
         raise ValueOutOfBounds(repr(arg)) 
      if (not ((- (2 ** bits)) <= i < (2 ** bits))): 
         raise ValueOutOfBounds(repr(arg)) 
      value = (i % (2 ** sub)) 
      value_encoded = int_to_big_endian(value) 
      return zpad(value_encoded, 32) 
   if (base == 'bool'): 
      if (arg is True): 
         value_encoded = int_to_big_endian(1) 
      elif (arg is False): 
         value_encoded = int_to_big_endian(0) 
      else: 
         raise ValueError(('%r   is   not   bool' % arg)) 
      return zpad(value_encoded, 32) 
   if (base == 'ufixed'): 
      sub = str(sub) 
      (high_str, low_str) = sub.split('x') 
      high = int(high_str) 
      low = int(low_str) 
      if (not ((0 < (high + low) <= 256) and ((high % 8) == 0) and ((low % 8) == 0))): 
         raise ValueError('invalid   unsigned   fixed   length   {}'.format(sub)) 
      if (not (0 <= arg < (2 ** high))): 
         raise ValueOutOfBounds(repr(arg)) 
      float_point = (arg * (2 ** low)) 
      fixed_point = int(float_point) 
      return zpad(int_to_big_endian(fixed_point), 32) 
   if (base == 'fixed'): 
      sub = str(sub) 
      (high_str, low_str) = sub.split('x') 
      high = int(high_str) 
      low = int(low_str) 
      bits = (high - 1) 
      if (not ((0 < (high + low) <= 256) and ((high % 8) == 0) and ((low % 8) == 0))): 
         raise ValueError('invalid   unsigned   fixed   length   {}'.format(sub)) 
      if (not ((- (2 ** bits)) <= arg < (2 ** bits))): 
         raise ValueOutOfBounds(repr(arg)) 
      float_point = (arg * (2 ** low)) 
      fixed_point = int(float_point) 
      value = (fixed_point % (2 ** 256)) 
      return zpad(int_to_big_endian(value), 32) 
   if (base == 'string'): 
      if isinstance(arg, utils.unicode): 
         arg = arg.encode('utf8') 
      else: 
         try: 
            arg.decode('utf8') 
         except UnicodeDecodeError: 
            raise ValueError('string   must   be   utf8   encoded') 
      if len(sub): 
         if (not (0 <= len(arg) <= int(sub))): 
            raise ValueError('invalid   string   length   {}'.format(sub)) 
         if (not (0 <= int(sub) <= 32)): 
            raise ValueError('invalid   string   length   {}'.format(sub)) 
         return rzpad(arg, 32) 
      if (not (0 <= len(arg) < TT256)): 
         raise Exception(('Integer   invalid   or   out   of   range:   %r' % arg)) 
      length_encoded = zpad(int_to_big_endian(len(arg)), 32) 
      value_encoded = rzpad(arg, utils.ceil32(len(arg))) 
      return (length_encoded + value_encoded) 
   if (base == 'bytes'): 
      if (not is_string(arg)): 
         raise EncodingError(('Expecting   string:   %r' % arg)) 
      arg = utils.to_string(arg) 
      if len(sub): 
         if (not (0 <= len(arg) <= int(sub))): 
            raise ValueError('string   must   be   utf8   encoded') 
         if (not (0 <= int(sub) <= 32)): 
            raise ValueError('string   must   be   utf8   encoded') 
         return rzpad(arg, 32) 
      if (not (0 <= len(arg) < TT256)): 
         raise Exception(('Integer   invalid   or   out   of   range:   %r' % arg)) 
      length_encoded = zpad(int_to_big_endian(len(arg)), 32) 
      value_encoded = rzpad(arg, utils.ceil32(len(arg))) 
      return (length_encoded + value_encoded) 
   if (base == 'hash'): 
      if (not (int(sub) and (int(sub) <= 32))): 
         raise EncodingError(('too   long:   %r' % arg)) 
      if isnumeric(arg): 
         return zpad(encode_int(arg), 32) 
      if (len(arg) == int(sub)): 
         return zpad(arg, 32) 
      if (len(arg) == (int(sub) * 2)): 
         return zpad(decode_hex(arg), 32) 
      raise EncodingError(('Could   not   parse   hash:   %r' % arg)) 
   if (base == 'address'): 
      assert (sub == '') 
      if isnumeric(arg): 
         return zpad(encode_int(arg), 32) 
      if (len(arg) == 20): 
         return zpad(arg, 32) 
      if (len(arg) == 40): 
         return zpad(decode_hex(arg), 32) 
      if ((len(arg) == 42) and (arg[:2] == '0x')): 
         return zpad(decode_hex(arg[2:]), 32) 
      raise EncodingError(('Could   not   parse   address:   %r' % arg)) 
   raise EncodingError(('Unhandled   type:   %r   %r' % (base, sub)))"," 'Encode a value into a string. 
 Parameters 
 arg : any 
 The value to encode. 
 typ : string 
 The type of the value to encode. 
 sub : int 
 The size of the value to encode. 
 Returns 
 value_encoded : string 
 The encoded value. 
 Raises 
 ValueError 
 ValueOutOfBounds 
 EncodingError 
 Notes 
 The encoding is done by using the type of the value as the base 
 and then using the sub field as the size. 
 The type of the value is determined by the following rules: 
 * If the value is a number, then the type is determined by the 
 base type of the number. 
 * If the value is a string, then the type is determined by the 
 length of the string. 
 * If the value is a bytes object, then the type is determined by 
 the length of the bytes object. 
 * If the value is a hash, then the type is determined by the 
 length of the hash. 
 * If the value is an address, then the type is determined by the 
 length of","'Encode `arg` as `typ`. 
 `arg` will be encoded in a best effort manner, were necessary the function 
 will try to correctly define the underlying binary representation (ie. 
 decoding a hex-encoded address/hash). 
 Args: 
 typ (Tuple[(str, int, list)]): A 3-tuple defining the `arg` type. 
 The first element defines the type name. 
 The second element defines the type length in bits. 
 The third element defines if it\'s an array type. 
 Together the first and second defines the elementary type, the third 
 element must be present but is ignored. 
 Valid type names are: 
 - uint 
 - int 
 - bool 
 - ufixed 
 - fixed 
 - string 
 - bytes 
 - hash 
 - address 
 arg (object): The object to be encoded, it must be a python object 
 compatible with the `typ`. 
 Raises: 
 ValueError: when an invalid `typ` is supplied. 
 ValueOutOfBounds: when `arg` cannot be encoded as `typ` because of the 
 binary contraints. 
 Note: 
 This function don\'t work with array types, for that use the `enc` 
 function.'"
"def init_runspace(): 
    global shell 
   cmds = {} 
   for cmdlet in InvokeCommand('get-command'): 
      cmds[translate(cmdlet.Name)] = ShellCommand(cmdlet.Name) 
   for alias in InvokeCommand('get-alias'): 
      cmdName = translate(alias.ReferencedCommand.Name) 
      if (cmdName in cmds): 
         cmds[translate(alias.Name)] = cmds[cmdName] 
   shell = Shell(cmds) 
   for key in cmds.keys(): 
      setattr(ShellOutput, key, cmds[key])", 'Initialize a Runspace.',''
"def test_hsl_to_rgb_part_3(): 
    assert (hsl_to_rgb(6120, 100, 50) == (255, 0, 0)) 
   assert (hsl_to_rgb((-9660), 100, 50) == (255, 255, 0)) 
   assert (hsl_to_rgb(99840, 100, 50) == (0, 255, 0)) 
   assert (hsl_to_rgb((-900), 100, 50) == (0, 255, 255)) 
   assert (hsl_to_rgb((-104880), 100, 50) == (0, 0, 255)) 
   assert (hsl_to_rgb(2820, 100, 50) == (255, 0, 255))", 'Test HSL to RGB conversion with hue of 60 degrees and saturation of 50%.','Test hsl to rgb color function'
"def test_cx_Oracle(): 
    if (('ORACLE_HOME' not in os.environ) and ('ORACLE_INSTANTCLIENT_HOME' not in os.environ)): 
      raise SkipTest 
   try: 
      import cx_Oracle 
      return 
   except ImportError as ex: 
      if ('No   module   named' in ex.message): 
         assert_true(False, 'cx_Oracle   skipped   its   build.   This   happens   if   env   var   ORACLE_HOME   or   ORACLE_INSTANTCLIENT_HOME   is   not   defined.   So   ignore   this   test   failure   if   your   build   does   not   need   to   work   with   an   oracle   backend.')", 'Test cx_Oracle','Tests that cx_Oracle (external dependency) is built correctly.'
"def default_channel(): 
    try: 
      chan = _open_session() 
   except ssh.SSHException as err: 
      if (str(err) == 'SSH   session   not   active'): 
         connections[env.host_string].close() 
         del connections[env.host_string] 
         chan = _open_session() 
      else: 
         raise 
   chan.settimeout(0.1) 
   chan.input_enabled = True 
   return chan"," 'Returns an SSH connection to the default channel. 
 :returns: An SSH connection to the default channel. 
 :rtype: :class:`ssh.Channel` 
 :raises: :class:`ssh.SSHException` if the default channel is not 
 available.'",'Return a channel object based on ``env.host_string``.'
"def pixels_to_EMU(value): 
    return int((value * 9525))", 'Convert from pixels to EMU','1 pixel = 9525 EMUs'
"def global_fixes(): 
    for function in list(globals().values()): 
      if inspect.isfunction(function): 
         arguments = inspect.getargspec(function)[0] 
         if (arguments[:1] != [u'source']): 
            continue 
         code = extract_code_from_function(function) 
         if code: 
            (yield (code, function))", 'Extracts the source code from a function and returns it.',"'Yield multiple (code, function) tuples.'"
"def get_metadata(stream): 
    lrf = (stream if isinstance(stream, LRFMetaFile) else LRFMetaFile(stream)) 
   authors = string_to_authors(lrf.author) 
   mi = MetaInformation(lrf.title.strip(), authors) 
   mi.author = lrf.author.strip() 
   mi.comments = lrf.free_text.strip() 
   mi.category = ((lrf.category.strip() + ',   ') + lrf.classification.strip()) 
   tags = [x.strip() for x in mi.category.split(',') if x.strip()] 
   if tags: 
      mi.tags = tags 
   if (mi.category.strip() == ','): 
      mi.category = None 
   mi.publisher = lrf.publisher.strip() 
   mi.cover_data = lrf.get_cover() 
   try: 
      mi.title_sort = lrf.title_reading.strip() 
      if (not mi.title_sort): 
         mi.title_sort = None 
   except: 
      pass 
   try: 
      mi.author_sort = lrf.author_reading.strip() 
      if (not mi.author_sort): 
         mi.author_sort = None 
   except: 
      pass 
   if ((not mi.title) or ('unknown' in mi.title.lower())): 
      mi.title = None 
   if (not mi.authors): 
      mi.authors = None 
   if ((not mi.author) or ('unknown' in mi.author.lower())): 
      mi.author = None 
   if ((not mi.category) or ('unknown' in mi.category.lower())): 
      mi.category = None 
   if ((not mi.publisher) or ('unknown' in mi.publisher.lower()) or ('some   publisher' in mi.publisher.lower())): 
      mi.publisher = None 
   return mi", 'Reads metadata from a stream and returns a MetaInformation object.',"'Return basic meta-data about the LRF file in C{stream} as a 
 L{MetaInformation} object. 
 @param stream: A file like object or an instance of L{LRFMetaFile}'"
"def _make_triplets(seq, phase=0): 
    pre = seq[:phase] 
   np_seq = seq[phase:] 
   non_triplets = (len(np_seq) % 3) 
   post = ('' if (not non_triplets) else np_seq[((-1) * non_triplets):]) 
   intacts = [np_seq[(3 * i):(3 * (i + 1))] for i in range((len(np_seq) // 3))] 
   return (pre, intacts, post)"," 'Make a list of triplets from a sequence. 
 This is a helper function for the ``_make_triplets_from_seq`` function. 
 Parameters 
 seq : list 
 The sequence to split into triplets. 
 phase : int 
 The phase of the sequence to split into triplets. 
 Returns 
 tuple 
 A tuple containing the pre-triplets, the triplets, and the post-triplets. 
 See Also 
 _make_triplets_from_seq'","'Selects a valid amino acid sequence given a 3-letter code input. 
 This function takes a single three-letter amino acid sequence and the phase 
 of the sequence to return the longest intact amino acid sequence possible. 
 Parts of the input sequence before and after the selected sequence are also 
 returned. 
 This is an internal private function and is meant for parsing Exonerate\'s 
 three-letter amino acid output. 
 >>> from Bio.SearchIO.ExonerateIO._base import _make_triplets 
 >>> _make_triplets(\'GlyThrSerAlaPro\') 
 (\'\', [\'Gly\', \'Thr\', \'Ser\', \'Ala\', \'Pro\'], \'\') 
 >>> _make_triplets(\'yThSerAla\', phase=1) 
 (\'y\', [\'Thr\', \'Ser\', \'Ala\'], \'\') 
 >>> _make_triplets(\'yThSerAlaPr\', phase=1) 
 (\'y\', [\'Thr\', \'Ser\', \'Ala\'], \'Pr\')'"
"def loadClassPath(classpath): 
    if (':' in classpath): 
      (modname, objname) = classpath.split(':', 1) 
      try: 
         __import__(modname) 
         module = modules[modname] 
         _class = eval(objname, module.__dict__) 
         if (_class is None): 
            raise Exception(('eval(%(objname)s)   in   %(modname)s   came   up   None' % locals())) 
      except Exception as e: 
         raise KnownUnknown(('Tried   to   import   %s,   but:   %s' % (classpath, e))) 
   else: 
      classpath = classpath.split('.') 
      try: 
         module = __import__('.'.join(classpath[:(-1)]), fromlist=str(classpath[(-1)])) 
      except ImportError as e: 
         raise KnownUnknown(('Tried   to   import   %s,   but:   %s' % ('.'.join(classpath), e))) 
      try: 
         _class = getattr(module, classpath[(-1)]) 
      except AttributeError as e: 
         raise KnownUnknown(('Tried   to   import   %s,   but:   %s' % ('.'.join(classpath), e))) 
   return _class"," 'Loads a class from a classpath. 
 :param classpath: 
 :type classpath: 
 :return: 
 :rtype: 
 :raises: 
 :class:`KnownUnknown` 
 :class:`ImportError`'","'Load external class based on a path. 
 Example classpath: ""Module.Submodule:Classname"". 
 Equivalent soon-to-be-deprecated classpath: ""Module.Submodule.Classname"".'"
"def _memoize(func, *args, **opts): 
    if opts: 
      key = (args, frozenset(opts.items())) 
   else: 
      key = args 
   cache = func.cache 
   try: 
      result = cache[key] 
   except KeyError: 
      result = cache[key] = func(*args, **opts) 
   return result"," 'Memoize a function, caching the result for each set of arguments and 
 options. 
 If the function has a `cache` attribute, it is used. Otherwise, a new 
 `cache` attribute is created. 
 :param func: The function to memoize. 
 :param args: The arguments to pass to the function. 
 :param opts: The options to pass to the function. 
 :return: The memoized function. 
 :rtype: function'",'Implements memoized cache lookups'
"def git_pull(): 
    app = get_app() 
   if (not have_git): 
      session.flash = GIT_MISSING 
      redirect(URL('site')) 
   dialog = FORM.confirm(T('Pull'), {T('Cancel'): URL('site')}) 
   if dialog.accepted: 
      try: 
         repo = git.Repo(os.path.join(apath(r=request), app)) 
         origin = repo.remotes.origin 
         origin.fetch() 
         origin.pull() 
         session.flash = T('Application   updated   via   git   pull') 
         redirect(URL('site')) 
      except git.CheckoutError: 
         session.flash = T('Pull   failed,   certain   files   could   not   be   checked   out.   Check   logs   for   details.') 
         redirect(URL('site')) 
      except git.UnmergedEntriesError: 
         session.flash = T('Pull   is   not   possible   because   you   have   unmerged   files.   Fix   them   up   in   the   work   tree,   and   then   try   again.') 
         redirect(URL('site')) 
      except git.GitCommandError: 
         session.flash = T('Pull   failed,   git   exited   abnormally.   See   logs   for   details.') 
         redirect(URL('site')) 
      except AssertionError: 
         session.flash = T('Pull   is   not   possible   because   you   have   unmerged   files.   Fix   them   up   in   the   work   tree,   and   then   try   again.') 
         redirect(URL('site')) 
   elif ('cancel' in request.vars): 
      redirect(URL('site')) 
   return dict(app=app, dialog=dialog)", 'Pull from remote repository','Git Pull handler'
"def _get_css_imports_cssutils(data, inline=False): 
    try: 
      import cssutils 
   except (ImportError, re.error): 
      return None 
   parser = cssutils.CSSParser(loglevel=100, fetcher=(lambda url: (None, '')), validate=False) 
   if (not inline): 
      sheet = parser.parseString(data) 
      return list(cssutils.getUrls(sheet)) 
   else: 
      urls = [] 
      declaration = parser.parseStyle(data) 
      for prop in declaration: 
         for value in prop.propertyValue: 
            if isinstance(value, cssutils.css.URIValue): 
               if value.uri: 
                  urls.append(value.uri) 
      return urls"," 'Returns a list of urls from the css data. 
 :param data: the css data 
 :param inline: whether to parse inline css or not 
 :return: a list of urls'","'Return all assets that are referenced in the given CSS document. 
 The returned URLs are relative to the stylesheet\'s URL. 
 Args: 
 data: The content of the stylesheet to scan as string. 
 inline: True if the argument is an inline HTML style attribute.'"
"def make_script_path(script): 
    s_path = None 
   path = cfg.script_dir.get_path() 
   if (path and script): 
      if (script.lower() not in ('none', 'default')): 
         s_path = os.path.join(path, script) 
         if (not os.path.exists(s_path)): 
            s_path = None 
   return s_path"," 'Make a script path from a script name. 
 :param script: script name to make a path for 
 :return: script path or None'","'Return full script path, if any valid script exists, else None'"
"def follow(): 
    t = Twitter(auth=authen()) 
   screen_name = g['stuff'].split()[0] 
   if screen_name.startswith('@'): 
      t.friendships.create(screen_name=screen_name[1:], follow=True) 
      printNicely(green((('You   are   following   ' + screen_name) + '   now!'))) 
   else: 
      printNicely(red(""A   name   should   begin   with   a   '@'""))", 'Follow a user on Twitter','Follow a user'
"def select_template(template_name_list): 
    not_found = [] 
   for template_name in template_name_list: 
      try: 
         return get_template(template_name) 
      except TemplateDoesNotExist as e: 
         if (e.args[0] not in not_found): 
            not_found.append(e.args[0]) 
         continue 
   raise TemplateDoesNotExist(',   '.join(not_found))"," 'Select a template from a list of template names. 
 :param template_name_list: 
 :type template_name_list: 
 :return: 
 :rtype: 
 :raise: 
 :raises: 
 :seealso: 
 :requires: 
 :todo: 
 :notes: 
 :author: 
 :copyright: 
 :license: 
 :version: 
 :maintainer: 
 :email: 
 :status: 
 :stability: 
 :release: 
 :url: 
 :class: 
 :class_var: 
 :data: 
 :data_var: 
 :method: 
 :method_var: 
 :method_class: 
 :method_class_var: 
 :method_data: 
 :method_data_var: 
 :method_data_class: 
 :method_data_class_var: 
 :method_data_data: 
 :method_data_data_var: 
 :method_data_data_class: 
 :method_","'Given a list of template names, returns the first that can be loaded.'"
"def _update_data(entity_id, public_key, sso_url, expires_at): 
    data_obj = SAMLProviderData.current(entity_id) 
   fetched_at = datetime.datetime.now() 
   if (data_obj and ((data_obj.public_key == public_key) and (data_obj.sso_url == sso_url))): 
      data_obj.expires_at = expires_at 
      data_obj.fetched_at = fetched_at 
      data_obj.save() 
      return False 
   else: 
      SAMLProviderData.objects.create(entity_id=entity_id, fetched_at=fetched_at, expires_at=expires_at, sso_url=sso_url, public_key=public_key) 
      return True"," 'Update SAML provider data 
 :param entity_id: 
 :param public_key: 
 :param sso_url: 
 :param expires_at: 
 :return: 
 True if update was successful, False otherwise'","'Update/Create the SAMLProviderData for the given entity ID. 
 Return value: 
 False if nothing has changed and existing data\'s ""fetched at"" timestamp is just updated. 
 True if a new record was created. (Either this is a new provider or something changed.)'"
"def InstallDriver(kext_path): 
    km = objc.KextManager() 
   cf_kext_path = km.PyStringToCFString(kext_path) 
   kext_url = km.dll.CFURLCreateWithFileSystemPath(objc.CF_DEFAULT_ALLOCATOR, cf_kext_path, objc.POSIX_PATH_STYLE, True) 
   status = km.iokit.KextManagerLoadKextWithURL(kext_url, None) 
   km.dll.CFRelease(kext_url) 
   km.dll.CFRelease(cf_kext_path) 
   if (status is not objc.OS_SUCCESS): 
      raise OSError('Failed   to   load   kext   at   {0}:   {1}'.format(kext_path, status))"," 'Install a kext. 
 :param kext_path: Path to the kext. 
 :raises OSError: If the kext could not be loaded.'","'Calls into the IOKit to load a kext by file-system path. 
 Apple kext API doco here: 
 http://developer.apple.com/library/mac/#documentation/IOKit/Reference/ 
 KextManager_header_reference/Reference/reference.html 
 Args: 
 kext_path: Absolute or relative POSIX path to the kext. 
 Raises: 
 OSError: On failure to load the kext.'"
"@depends(HAS_PYVMOMI) 
 def list_hosts(host, username, password, protocol=None, port=None): 
    service_instance = salt.utils.vmware.get_service_instance(host=host, username=username, password=password, protocol=protocol, port=port) 
   return salt.utils.vmware.list_hosts(service_instance)"," 'List hosts on a vCenter server. 
 This function will return a list of hosts on a vCenter server. 
 :param host: The hostname of the vCenter server. 
 :param username: The username to authenticate with. 
 :param password: The password to authenticate with. 
 :param protocol: The protocol to use (default: \'https\'). 
 :param port: The port to use (default: \'443\'). 
 :returns: A list of hosts on the vCenter server. 
 :rtype: list'","'Returns a list of hosts for the the specified VMware environment. 
 host 
 The location of the host. 
 username 
 The username used to login to the host, such as ``root``. 
 password 
 The password used to login to the host. 
 protocol 
 Optionally set to alternate protocol if the host is not using the default 
 protocol. Default protocol is ``https``. 
 port 
 Optionally set to alternate port if the host is not using the default 
 port. Default port is ``443``. 
 .. code-block:: bash 
 salt \'*\' vsphere.list_hosts 1.2.3.4 root bad-password'"
"def get_cluster(options, env): 
    cluster_option = options['cluster'] 
   if cluster_option: 
      try: 
         cluster = BenchmarkCluster.from_cluster_yaml(FilePath(cluster_option)) 
      except IOError as e: 
         usage(options, 'Cluster   file   {!r}   not   found.'.format(e.filename)) 
   else: 
      try: 
         cluster = BenchmarkCluster.from_acceptance_test_env(env) 
      except KeyError as e: 
         usage(options, 'Environment   variable   {!r}   not   set.'.format(e.args[0])) 
      except ValueError as e: 
         usage(options, e.args[0]) 
      except ValidationError as e: 
         usage(options, e.message) 
   return cluster", 'Returns the cluster to use for the benchmark.',"'Obtain a cluster from the command line options and environment. 
 :param BenchmarkOption options: Parsed command line options. 
 :param dict env: Dictionary of environment variables. 
 :return BenchmarkCluster: Cluster to benchmark.'"
"def runsimple(func, port=8080): 
    import SimpleHTTPServer, SocketServer, BaseHTTPServer, urlparse 
   import socket, errno 
   import traceback 
   class WSGIHandler(SimpleHTTPServer.SimpleHTTPRequestHandler, ): 
      def run_wsgi_app(self): 
         (protocol, host, path, parameters, query, fragment) = urlparse.urlparse(('http://dummyhost%s' % self.path)) 
         env = {'wsgi.version': (1, 0), 'wsgi.url_scheme': 'http', 'wsgi.input': self.rfile, 'wsgi.errors': sys.stderr, 'wsgi.multithread': 1, 'wsgi.multiprocess': 0, 'wsgi.run_once': 0, 'REQUEST_METHOD': self.command, 'REQUEST_URI': self.path, 'PATH_INFO': path, 'QUERY_STRING': query, 'CONTENT_TYPE': self.headers.get('Content-Type', ''), 'CONTENT_LENGTH': self.headers.get('Content-Length', ''), 'REMOTE_ADDR': self.client_address[0], 'SERVER_NAME': self.server.server_address[0], 'SERVER_PORT': str(self.server.server_address[1]), 'SERVER_PROTOCOL': self.request_version} 
         for (http_header, http_value) in self.headers.items(): 
            env[('HTTP_%s' % http_header.replace('-', '_').upper())] = http_value 
         self.wsgi_sent_headers = 0 
         self.wsgi_headers = [] 
         try: 
            result = self.server.app(env, self.wsgi_start_response) 
            try: 
               try: 
                  for data in result: 
                     if data: 
                        self.wsgi_write_data(data) 
               finally: 
                  if hasattr(result, 'close'): 
                     result.close() 
            except socket.error as socket_err: 
               if (socket_err.args[0] in (errno.ECONNABORTED, errno.EPIPE)): 
                  return 
            except socket.timeout as socket_timeout: 
               return 
         except: 
            print >>debug, traceback.format_exc(), 
            internalerror() 
            if (not self.wsgi_sent_headers): 
               self.wsgi_start_response(ctx.status, ctx.headers) 
            self.wsgi_write_data(ctx.output) 
         if (not self.wsgi_sent_headers): 
            self.wsgi_write_data('   ') 
         return 
      do_POST = run_wsgi_app 
      def do_GET(self): 
         if self.path.startswith('/static/'): 
            SimpleHTTPServer.SimpleHTTPRequestHandler.do_GET(self) 
         else: 
            self.run_wsgi_app() 
      def wsgi_start_response(self, response_status, response_headers, exc_info=None): 
         if self.wsgi_sent_headers: 
            raise Exception('Headers   already   sent   and   start_response   called   again!') 
         self.wsgi_headers = (response_status, response_headers) 
         return self.wsgi_write_data 
      def wsgi_write_data(self, data): 
         if (not self.wsgi_sent_headers): 
            (status, headers) = self.wsgi_headers 
            status_code = status[:status.find('   ')] 
            status_msg = status[(status.find('   ') + 1):] 
            self.send_response(int(status_code), status_msg) 
            for (header, value) in headers: 
               self.send_header(header, value) 
            self.end_headers() 
            self.wsgi_sent_headers = 1 
         self.wfile.write(data) 
   class WSGIServer(SocketServer.ThreadingMixIn, BaseHTTPServer.HTTPServer, ): 
      def __init__(self, func): 
         BaseHTTPServer.HTTPServer.__init__(self, ('0.0.0.0', int(port)), WSGIHandler) 
         self.app = func 
         self.serverShuttingDown = 0 
   print (('Launching   server:   http://0.0.0.0:' + str(port)) + '/') 
   WSGIServer(func).serve_forever()", 'Simple HTTP Server',"'Runs a simple HTTP server hosting WSGI app `func`. The directory `static/` 
 is hosted statically. 
 Based on [WsgiServer](http://www.owlfish.com/software/wsgiutils/documentation/wsgi-server-api.html) 
 from [Colin Stewart](http://www.owlfish.com/).'"
"def setup_form_view(view, request, form, *args, **kwargs): 
    view.request = request 
   try: 
      view.request.user = request.user 
   except AttributeError: 
      view.request.user = UserFactory() 
   view.args = args 
   view.kwargs = kwargs 
   view.form = form 
   return view"," 'Setup a form view. 
 The view is passed to the setup_form_view function, which sets the 
 request and user attributes. 
 The view is then passed to the view factory, which is responsible for 
 creating the view. 
 The view factory can be overridden by setting the ``view_factory`` 
 attribute on the view. 
 Parameters 
 view : django.views.generic.base.View 
 The view to be set up. 
 request : django.http.HttpRequest 
 The current request. 
 form : django.forms.Form 
 The form to be used by the view. 
 args : tuple 
 Additional arguments to be passed to the view. 
 kwargs : dict 
 Additional keyword arguments to be passed to the view. 
 Returns 
 django.views.generic.base.View 
 The view, with the request and user attributes set. 
 Examples 
 >>> class MyView(FormView): 
 ...     form_class = MyForm 
 ...     template_name = \'my_form.html\' 
 ...     success_url = \'/thanks/\'",'Mimic as_view and with forms to skip some of the context'
"def safeCSValue(value): 
    retVal = value 
   if (retVal and isinstance(retVal, basestring)): 
      if (not (retVal[0] == retVal[(-1)] == '""')): 
         if any(((_ in retVal) for _ in (conf.get('csvDel', defaults.csvDel), '""', '\n'))): 
            retVal = ('""%s""' % retVal.replace('""', '""""')) 
   return retVal"," 'Safely convert a value to a string. 
 :param value: 
 :return: 
 :rtype: 
 :type value: 
 :param value: 
 :return: 
 :rtype: 
 :type value: 
 :param value: 
 :return: 
 :rtype: 
 :type value: 
 :param value: 
 :return: 
 :rtype: 
 :type value: 
 :param value: 
 :return: 
 :rtype: 
 :type value: 
 :param value: 
 :return: 
 :rtype: 
 :type value: 
 :param value: 
 :return: 
 :rtype: 
 :type value: 
 :param value: 
 :return: 
 :rtype: 
 :type value: 
 :param value: 
 :return: 
 :rtype: 
 :type value: 
 :param value: 
 :return: 
 :rtype: 
 :type value: 
 :param value: 
 :return: 
 :r","'Returns value safe for CSV dumping 
 Reference: http://tools.ietf.org/html/rfc4180 
 >>> safeCSValue(u\'foo, bar\') 
 u\'""foo, bar""\' 
 >>> safeCSValue(u\'foobar\') 
 u\'foobar\''"
"def cmp(field, value, comparison='=', escape=(lambda v: _escape(v)), table=''): 
    if table: 
      field = abs(table, field) 
   if (isinstance(value, basestring) and (value.startswith(('*', '%')) or value.endswith(('*', '%')))): 
      if (comparison in ('=', 'i=', '==', LIKE)): 
         return ('%s   like   %s' % (field, escape(value.replace('*', '%')))) 
      if (comparison in ('!=', '<>')): 
         return ('%s   not   like   %s' % (field, escape(value.replace('*', '%')))) 
   if isinstance(value, basestring): 
      if (comparison == 'i='): 
         return ('%s   like   %s' % (field, escape(value))) 
   if isinstance(value, (list, tuple)): 
      if find((lambda v: (isinstance(v, basestring) and (v.startswith('*') or v.endswith('*')))), value): 
         return ('(%s)' % any(*[(field, v) for v in value]).sql(escape=escape)) 
      if (comparison in ('=', '==', IN)): 
         return ('%s   in   (%s)' % (field, ','.join((escape(v) for v in value)))) 
      if (comparison in ('!=', '<>')): 
         return ('%s   not   in   (%s)' % (field, ','.join((escape(v) for v in value)))) 
      if (comparison in (':', BETWEEN)): 
         return ('%s   between   %s   and   %s' % (field, escape(value[0]), escape(value[1]))) 
   if isinstance(value, type(None)): 
      if (comparison in ('=', '==')): 
         return ('%s   is   null' % field) 
      if (comparison in ('!=', '<>')): 
         return ('%s   is   not   null' % field) 
   if isinstance(value, Query): 
      if (comparison in ('=', '==', IN)): 
         return ('%s   in   %s' % (field, escape(value))) 
      if (comparison in ('!=', '<>')): 
         return ('%s   not   in   %s' % (field, escape(value))) 
   return ('%s%s%s' % (field, comparison, escape(value)))"," 'Compares the value of a field to the given value. 
 :param field: the name of the field to compare 
 :param value: the value to compare to 
 :param comparison: the comparison operator (e.g. \'=\' or \'!=\') 
 :param escape: a function to escape the value (e.g. \'%s\') 
 :param table: the table name to use for the field name 
 :return: the SQL statement to use 
 :rtype: str'","'Returns an SQL WHERE comparison string using =, i=, !=, >, <, >=, <= or BETWEEN. 
 Strings may contain wildcards (*) at the start or at the end. 
 A list or tuple of values can be given when using =, != or BETWEEN.'"
"def parse_bdist_wininst(name): 
    lower = name.lower() 
   (base, py_ver) = (None, None) 
   if lower.endswith('.exe'): 
      if lower.endswith('.win32.exe'): 
         base = name[:(-10)] 
      elif lower.startswith('.win32-py', (-16)): 
         py_ver = name[(-7):(-4)] 
         base = name[:(-16)] 
   return (base, py_ver)", 'Parse the bdist_wininst name into base and py_ver.',"'Return (base,pyversion) or (None,None) for possible .exe name'"
"def run_pylint_gui(): 
    try: 
      from pylint.gui import Run 
      Run(sys.argv[1:]) 
   except ImportError: 
      sys.exit('tkinter   is   not   available')", 'Run pylint in a gui.','run pylint-gui'
"def _process_caption(caption): 
    tokenized_caption = [FLAGS.start_word] 
   tokenized_caption.extend(nltk.tokenize.word_tokenize(caption.lower())) 
   tokenized_caption.append(FLAGS.end_word) 
   return tokenized_caption"," 'Process a caption, tokenizing it and adding the start and end words.'","'Processes a caption string into a list of tonenized words. 
 Args: 
 caption: A string caption. 
 Returns: 
 A list of strings; the tokenized caption.'"
"@pytest.fixture 
 def Worksheet(): 
    from openpyxl.worksheet import Worksheet 
   return Worksheet", 'Returns a Worksheet fixture.','Worksheet Class'
"def allclose(x, y, rtol=1e-05, atol=1e-08): 
    for (xi, yi) in zip(x, y): 
      if (not (abs((xi - yi)) <= (atol + (rtol * abs(yi))))): 
         return False 
   return True", 'Check whether all elements of x are within rtol of y.',"'Returns True if x and y are sufficiently close, elementwise. 
 Parameters 
 rtol : float 
 The relative error tolerance. 
 atol : float 
 The absolute error tolerance.'"
"def abort_merge(): 
    title = N_(u'Abort   Merge...') 
   txt = N_(u'Aborting   the   current   merge   will   cause   *ALL*   uncommitted   changes   to   be   lost.\nRecovering   uncommitted   changes   is   not   possible.') 
   info_txt = N_(u'Aborting   the   current   merge?') 
   ok_txt = N_(u'Abort   Merge') 
   if qtutils.confirm(title, txt, info_txt, ok_txt, default=False, icon=icons.undo()): 
      gitcmds.abort_merge()", 'Abort the current merge','Prompts before aborting a merge in progress'
"def _patched_list_steps(emr_conn, *args, **kwargs): 
    try: 
      boto.emr.emrobject.ClusterTimeline = _PatchedClusterTimeline 
      return emr_conn.list_steps(*args, **kwargs) 
   finally: 
      boto.emr.emrobject.ClusterTimeline = ClusterTimeline"," 'Patches the boto.emr.emrobject.ClusterTimeline class to use 
 _PatchedClusterTimeline instead. 
 :param emr_conn: The connection to the EMR service. 
 :param *args: Additional arguments to pass to the list_steps method. 
 :param **kwargs: Additional keyword arguments to pass to the list_steps method. 
 :return: A list of steps. 
 :rtype: list[Step]'","'Wrapper for :py:meth:`boto.emr.EmrConnection.list_steps()` 
 that works around around `boto\'s startdatetime bug 
 <https://github.com/boto/boto/issues/3268>`__.'"
"@register.filter 
 def has_unrendered_errors(bound_field): 
    return (bound_field.errors and (not hasattr(bound_field.field.widget, u'render_with_errors')))", 'Check if the bound field has unrendered errors.',"'Return true if this field has errors that were not accounted for by render_with_errors, because 
 the widget does not support the render_with_errors method'"
"def scrub_text(text): 
    scrubbed_text = text.rstrip().replace('\n', '\\n').replace(' DCTB ', ('   ' * 4)) 
   return scrubbed_text"," 'Scrub text to remove trailing whitespace and newlines. 
 :param text: 
 :return: 
 text with trailing whitespace and newlines removed.'","'Cleans up text. 
 Escapes newlines and tabs. 
 Parameters 
 text : str 
 Text to clean up.'"
"def formatStatResponse(msgs): 
    i = 0 
   bytes = 0 
   for size in msgs: 
      i += 1 
      bytes += size 
      (yield None) 
   (yield successResponse(('%d   %d' % (i, bytes))))", 'Format a stat response.',"'Format a list of message sizes into a STAT response. 
 This generator function is intended to be used with 
 L{Cooperator <twisted.internet.task.Cooperator>}. 
 @type msgs: L{list} of L{int} 
 @param msgs: A list of message sizes. 
 @rtype: L{None} or L{bytes} 
 @return: Yields none until a result is available, then a string that is 
 suitable for use in a STAT response. The string consists of the number 
 of messages and the total size of the messages in octets.'"
"def find_exact(tracks, query=None, limit=100, offset=0, uris=None): 
    if (query is None): 
      query = {} 
   _validate_query(query) 
   for (field, values) in query.items(): 
      for value in values: 
         if (field == u'track_no'): 
            q = _convert_to_int(value) 
         else: 
            q = value.strip() 
         def uri_filter(t): 
            return (q == t.uri) 
         def track_name_filter(t): 
            return (q == t.name) 
         def album_filter(t): 
            return (q == getattr(getattr(t, u'album', None), u'name', None)) 
         def artist_filter(t): 
            return filter((lambda a: (q == a.name)), t.artists) 
         def albumartist_filter(t): 
            return any([(q == a.name) for a in getattr(t.album, u'artists', [])]) 
         def composer_filter(t): 
            return any([(q == a.name) for a in getattr(t, u'composers', [])]) 
         def performer_filter(t): 
            return any([(q == a.name) for a in getattr(t, u'performers', [])]) 
         def track_no_filter(t): 
            return (q == t.track_no) 
         def genre_filter(t): 
            return (t.genre and (q == t.genre)) 
         def date_filter(t): 
            return (q == t.date) 
         def comment_filter(t): 
            return (q == t.comment) 
         def any_filter(t): 
            return (uri_filter(t) or track_name_filter(t) or album_filter(t) or artist_filter(t) or albumartist_filter(t) or composer_filter(t) or performer_filter(t) or track_no_filter(t) or genre_filter(t) or date_filter(t) or comment_filter(t)) 
         if (field == u'uri'): 
            tracks = filter(uri_filter, tracks) 
         elif (field == u'track_name'): 
            tracks = filter(track_name_filter, tracks) 
         elif (field == u'album'): 
            tracks = filter(album_filter, tracks) 
         elif (field == u'artist'): 
            tracks = filter(artist_filter, tracks) 
         elif (field == u'albumartist'): 
            tracks = filter(albumartist_filter, tracks) 
         elif (field == u'composer'): 
            tracks = filter(composer_filter, tracks) 
         elif (field == u'performer'): 
            tracks = filter(performer_filter, tracks) 
         elif (field == u'track_no'): 
            tracks = filter(track_no_filter, tracks) 
         elif (field == u'genre'): 
            tracks = filter(genre_filter, tracks) 
         elif (field == u'date'): 
            tracks = filter(date_filter, tracks) 
         elif (field == u'comment'): 
            tracks = filter(comment_filter, tracks) 
         elif (field == u'any'): 
            tracks = filter(any_filter, tracks) 
         else: 
            raise LookupError((u'Invalid   lookup   field:   %s' % field)) 
   if (limit is None): 
      tracks = tracks[offset:] 
   else: 
      tracks = tracks[offset:(offset + limit)] 
   return SearchResult(uri=u'local:search', tracks=tracks)"," 'Finds tracks that match the given query. 
 :param query: dict of query terms 
 :param limit: maximum number of results to return 
 :param offset: starting index of results to return 
 :param uris: tracks to search for by uri 
 :return: SearchResult'","'Filter a list of tracks where ``field`` is ``values``. 
 :param list tracks: a list of :class:`~mopidy.models.Track` 
 :param dict query: one or more field/value pairs to search for 
 :param int limit: maximum number of results to return 
 :param int offset: offset into result set to use. 
 :param uris: zero or more URI roots to limit the search to 
 :type uris: list of strings or :class:`None` 
 :rtype: :class:`~mopidy.models.SearchResult`'"
"@transaction.non_atomic_requests 
 @cache_control(no_cache=True, no_store=True, must_revalidate=True) 
 @coach_dashboard 
 def ccx_grades_csv(request, course, ccx=None): 
    if (not ccx): 
      raise Http404 
   ccx_key = CCXLocator.from_course_locator(course.id, unicode(ccx.id)) 
   with ccx_course(ccx_key) as course: 
      prep_course_for_grading(course, request) 
      enrolled_students = User.objects.filter(courseenrollment__course_id=ccx_key, courseenrollment__is_active=1).order_by('username').select_related('profile') 
      grades = CourseGradeFactory().iter(course, enrolled_students) 
      header = None 
      rows = [] 
      for (student, course_grade, __) in grades: 
         if course_grade: 
            if (not header): 
               header = [section['label'].encode('utf-8') for section in course_grade.summary[u'section_breakdown']] 
               rows.append((['id', 'email', 'username', 'grade'] + header)) 
            percents = {section['label']: section.get('percent', 0.0) for section in course_grade.summary[u'section_breakdown'] if ('label' in section)} 
            row_percents = [percents.get(label, 0.0) for label in header] 
            rows.append(([student.id, student.email, student.username, course_grade.percent] + row_percents)) 
      buf = StringIO() 
      writer = csv.writer(buf) 
      for row in rows: 
         writer.writerow(row) 
      response = HttpResponse(buf.getvalue(), content_type='text/csv') 
      response['Content-Disposition'] = 'attachment' 
      return response"," 'Returns a CSV file of grades for a course with CCX enabled. 
 :param request: The request. 
 :param course: The course. 
 :param ccx: The CCX to use. 
 :returns: The CSV response. 
 :rtype: HttpResponse'",'Download grades as CSV.'
"@register_canonicalize 
 @register_uncanonicalize 
 @register_specialize 
 @gof.local_optimizer(ALL_REDUCE) 
 def local_reduce_broadcastable(node): 
    if isinstance(node.op, T.CAReduce): 
      (reduced,) = node.inputs 
      odtype = node.outputs[0].dtype 
      if (node.op.axis is None): 
         if all(reduced.broadcastable): 
            return [reduced.dimshuffle().astype(odtype)] 
      else: 
         axis = list(node.op.axis) 
         cuttable = [a for a in axis if reduced.broadcastable[a]] 
         if cuttable: 
            new_axis = [] 
            pattern = [] 
            ii = 0 
            for p in xrange(reduced.ndim): 
               if (p not in cuttable): 
                  if (p in axis): 
                     new_axis.append(ii) 
                  pattern.append(p) 
                  ii += 1 
            new_reduced = reduced.dimshuffle(*pattern) 
            if new_axis: 
               if (type(node.op) == theano.tensor.elemwise.CAReduce): 
                  new_op = node.op.__class__(node.op.scalar_op, axis=new_axis) 
               else: 
                  new_op = node.op.__class__(axis=new_axis) 
               return [new_op(new_reduced)] 
            else: 
               return [new_reduced.astype(odtype)]", 'Reduce broadcastable tensors','Remove reduction over broadcastable dimensions.'
"def _get_immediate_cls_attr(cls, attrname, strict=False): 
    if (not issubclass(cls, object)): 
      return None 
   for base in cls.__mro__: 
      _is_declarative_inherits = hasattr(base, '_decl_class_registry') 
      if ((attrname in base.__dict__) and ((base is cls) or (((base in cls.__bases__) if strict else True) and (not _is_declarative_inherits)))): 
         return getattr(base, attrname) 
   else: 
      return None"," 'Returns the immediate class attribute of the class, or None if it doesn\'t 
 exist. 
 :param cls: the class to look for the attribute in 
 :param attrname: the name of the attribute to look for 
 :param strict: if True, only look for attributes defined on the class, not 
 on its base classes 
 :returns: the immediate class attribute, or None if it doesn\'t exist'","'return an attribute of the class that is either present directly 
 on the class, e.g. not on a superclass, or is from a superclass but 
 this superclass is a mixin, that is, not a descendant of 
 the declarative base. 
 This is used to detect attributes that indicate something about 
 a mapped class independently from any mapped classes that it may 
 inherit from.'"
"def wiki_escape(s): 
    ret = [] 
   for word in s.split(): 
      if re.match('[A-Z]+[a-z]+[A-Z]', word): 
         word = ('!%s' % word) 
      ret.append(word) 
   return '   '.join(ret)", 'Escape words that contain uppercase letters.',"'Detect WikiSyntax (i.e. InterCaps, a.k.a. CamelCase) and escape it.'"
"def getaddresses(fieldvalues): 
    all = COMMASPACE.join(fieldvalues) 
   a = _AddressList(all) 
   return a.addresslist", 'Return a list of addresses from the field values.',"'Return a list of (REALNAME, EMAIL) for each fieldvalue.'"
"def main(): 
    errors = 0 
   fits_files = handle_options(sys.argv[1:]) 
   setup_logging() 
   for filename in fits_files: 
      errors += process_file(filename) 
   if errors: 
      log.warning('{}   errors'.format(errors)) 
   return int(bool(errors))", 'Runs the main program.',"'Processes command line parameters into options and files,  then checks 
 or update FITS DATASUM and CHECKSUM keywords for the specified files.'"
"def _extend_external_network_default(core_plugin, net_res, net_db): 
    if (net_db.external is not None): 
      net_res[IS_DEFAULT] = net_db.external.is_default 
   return net_res"," 'Extends the net_res dictionary with the default value 
 of the external network. 
 :param core_plugin: 
 :param net_res: 
 :param net_db: 
 :return: 
 net_res'",'Add is_default field to \'show\' response.'
"def cg_optimization_mnist(n_epochs=50, mnist_pkl_gz='mnist.pkl.gz'): 
    datasets = load_data(mnist_pkl_gz) 
   (train_set_x, train_set_y) = datasets[0] 
   (valid_set_x, valid_set_y) = datasets[1] 
   (test_set_x, test_set_y) = datasets[2] 
   batch_size = 600 
   n_train_batches = (train_set_x.get_value(borrow=True).shape[0] // batch_size) 
   n_valid_batches = (valid_set_x.get_value(borrow=True).shape[0] // batch_size) 
   n_test_batches = (test_set_x.get_value(borrow=True).shape[0] // batch_size) 
   n_in = (28 * 28) 
   n_out = 10 
   print('...   building   the   model') 
   minibatch_offset = T.lscalar() 
   x = T.matrix() 
   y = T.ivector() 
   classifier = LogisticRegression(input=x, n_in=(28 * 28), n_out=10) 
   cost = classifier.negative_log_likelihood(y).mean() 
   test_model = theano.function([minibatch_offset], classifier.errors(y), givens={x: test_set_x[minibatch_offset:(minibatch_offset + batch_size)], y: test_set_y[minibatch_offset:(minibatch_offset + batch_size)]}, name='test') 
   validate_model = theano.function([minibatch_offset], classifier.errors(y), givens={x: valid_set_x[minibatch_offset:(minibatch_offset + batch_size)], y: valid_set_y[minibatch_offset:(minibatch_offset + batch_size)]}, name='validate') 
   batch_cost = theano.function([minibatch_offset], cost, givens={x: train_set_x[minibatch_offset:(minibatch_offset + batch_size)], y: train_set_y[minibatch_offset:(minibatch_offset + batch_size)]}, name='batch_cost') 
   batch_grad = theano.function([minibatch_offset], T.grad(cost, classifier.theta), givens={x: train_set_x[minibatch_offset:(minibatch_offset + batch_size)], y: train_set_y[minibatch_offset:(minibatch_offset + batch_size)]}, name='batch_grad') 
   def train_fn(theta_value): 
      classifier.theta.set_value(theta_value, borrow=True) 
      train_losses = [batch_cost((i * batch_size)) for i in range(n_train_batches)] 
      return numpy.mean(train_losses) 
   def train_fn_grad(theta_value): 
      classifier.theta.set_value(theta_value, borrow=True) 
      grad = batch_grad(0) 
      for i in range(1, n_train_batches): 
         grad += batch_grad((i * batch_size)) 
      return (grad / n_train_batches) 
   validation_scores = [numpy.inf, 0] 
   def callback(theta_value): 
      classifier.theta.set_value(theta_value, borrow=True) 
      validation_losses = [validate_model((i * batch_size)) for i in range(n_valid_batches)] 
      this_validation_loss = numpy.mean(validation_losses) 
      print(('validation   error   %f   %%' % ((this_validation_loss * 100.0),))) 
      if (this_validation_loss < validation_scores[0]): 
         validation_scores[0] = this_validation_loss 
         test_losses = [test_model((i * batch_size)) for i in range(n_test_batches)] 
         validation_scores[1] = numpy.mean(test_losses) 
   import scipy.optimize 
   print('Optimizing   using   scipy.optimize.fmin_cg...') 
   start_time = timeit.default_timer() 
   best_w_b = scipy.optimize.fmin_cg(f=train_fn, x0=numpy.zeros(((n_in + 1) * n_out), dtype=x.dtype), fprime=train_fn_grad, callback=callback, disp=0, maxiter=n_epochs) 
   end_time = timeit.default_timer() 
   print(('Optimization   complete   with   best   validation   score   of   %f   %%,   with   test   performance   %f   %%' % ((validation_scores[0] * 100.0), (validation_scores[1] * 100.0)))) 
   print((('The   code   for   file   ' + os.path.split(__file__)[1]) + ('   ran   for   %.1fs' % (end_time - start_time))), file=sys.stderr)"," 'Train a logistic regression model on the MNIST dataset. 
 This function is a demonstration of how to use Theano's gradient 
 descent optimizer to train a model. 
 Parameters 
 n_epochs : int 
 Number of training epochs to run. 
 mnist_pkl_gz : str 
 Path to a pickle file containing the MNIST dataset. 
 Returns 
 best_w_b : numpy.ndarray 
 The best weights obtained during training. 
 Notes 
 This function is a demonstration of how to use Theano's gradient 
 descent optimizer to train a model. 
 It uses the logistic regression model from the MNIST dataset. 
 Theano's gradient descent optimizer is a simple implementation of 
 the conjugate gradient (CG) algorithm. 
 The CG algorithm is a non-linear optimization method. 
 It iteratively finds a local minimum of a function by finding the 
 gradient of the function and using it to adjust the parameters of 
 the model. 
 The CG algorithm is a simple method that is very easy to implement. 
 However, it is","'Demonstrate conjugate gradient optimization of a log-linear model 
 This is demonstrated on MNIST. 
 :type n_epochs: int 
 :param n_epochs: number of epochs to run the optimizer 
 :type mnist_pkl_gz: string 
 :param mnist_pkl_gz: the path of the mnist training file from 
 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'"
"def modify_node(hostname, username, password, name, connection_limit=None, description=None, dynamic_ratio=None, logging=None, monitor=None, rate_limit=None, ratio=None, session=None, state=None, trans_label=None): 
    params = {'connection-limit': connection_limit, 'description': description, 'dynamic-ratio': dynamic_ratio, 'logging': logging, 'monitor': monitor, 'rate-limit': rate_limit, 'ratio': ratio, 'session': session, 'state': state} 
   bigip_session = _build_session(username, password, trans_label) 
   payload = _loop_payload(params) 
   payload['name'] = name 
   try: 
      response = bigip_session.put((BIG_IP_URL_BASE.format(host=hostname) + '/ltm/node/{name}'.format(name=name)), data=json.dumps(payload)) 
   except requests.exceptions.ConnectionError as e: 
      return _load_connection_error(hostname, e) 
   return _load_response(response)"," 'Modify a node. 
 :param hostname: The hostname of the BIG-IP 
 :param username: The username for basic authentication 
 :param password: The password for basic authentication 
 :param name: The name of the node to modify 
 :param connection_limit: The maximum number of connections allowed for the node 
 :param description: A description of the node 
 :param dynamic_ratio: The dynamic ratio of the node 
 :param logging: The logging level for the node 
 :param monitor: The monitoring level for the node 
 :param rate_limit: The rate limit of the node 
 :param ratio: The ratio of the node 
 :param session: The session for the BIG-IP 
 :param state: The state of the node 
 :param trans_label: The label of the transaction 
 :returns: A dictionary of the node's attributes'","'A function to connect to a bigip device and modify an existing node. 
 hostname 
 The host/address of the bigip device 
 username 
 The iControl REST username 
 password 
 The iControl REST password 
 name 
 The name of the node to modify 
 connection_limit 
 [integer] 
 description 
 [string] 
 dynamic_ratio 
 [integer] 
 logging 
 [enabled | disabled] 
 monitor 
 [[name] | none | default] 
 rate_limit 
 [integer] 
 ratio 
 [integer] 
 session 
 [user-enabled | user-disabled] 
 state 
 [user-down | user-up ] 
 trans_label 
 The label of the transaction stored within the grain: 
 ``bigip_f5_trans:<label>`` 
 CLI Example:: 
 salt \'*\' bigip.modify_node bigip admin admin 10.1.1.2 ratio=2 logging=enabled'"
"def get_manager(cls): 
    db_user = boto.config.get('DB', 'db_user', None) 
   db_passwd = boto.config.get('DB', 'db_passwd', None) 
   db_type = boto.config.get('DB', 'db_type', 'SimpleDB') 
   db_name = boto.config.get('DB', 'db_name', None) 
   db_table = boto.config.get('DB', 'db_table', None) 
   db_host = boto.config.get('DB', 'db_host', 'sdb.amazonaws.com') 
   db_port = boto.config.getint('DB', 'db_port', 443) 
   enable_ssl = boto.config.getbool('DB', 'enable_ssl', True) 
   sql_dir = boto.config.get('DB', 'sql_dir', None) 
   debug = boto.config.getint('DB', 'debug', 0) 
   module_name = cls.__module__.replace('.', '_') 
   db_section = ((('DB_' + module_name) + '_') + cls.__name__) 
   if (not boto.config.has_section(db_section)): 
      db_section = ('DB_' + cls.__name__) 
   if boto.config.has_section(db_section): 
      db_user = boto.config.get(db_section, 'db_user', db_user) 
      db_passwd = boto.config.get(db_section, 'db_passwd', db_passwd) 
      db_type = boto.config.get(db_section, 'db_type', db_type) 
      db_name = boto.config.get(db_section, 'db_name', db_name) 
      db_table = boto.config.get(db_section, 'db_table', db_table) 
      db_host = boto.config.get(db_section, 'db_host', db_host) 
      db_port = boto.config.getint(db_section, 'db_port', db_port) 
      enable_ssl = boto.config.getint(db_section, 'enable_ssl', enable_ssl) 
      debug = boto.config.getint(db_section, 'debug', debug) 
   elif (hasattr(cls, '_db_name') and (cls._db_name is not None)): 
      db_name = cls._db_name 
   elif hasattr(cls.__bases__[0], '_manager'): 
      return cls.__bases__[0]._manager 
   if (db_type == 'SimpleDB'): 
      from boto.sdb.db.manager.sdbmanager import SDBManager 
      return SDBManager(cls, db_name, db_user, db_passwd, db_host, db_port, db_table, sql_dir, enable_ssl) 
   elif (db_type == 'XML'): 
      from boto.sdb.db.manager.xmlmanager import XMLManager 
      return XMLManager(cls, db_name, db_user, db_passwd, db_host, db_port, db_table, sql_dir, enable_ssl) 
   else: 
      raise ValueError(('Unknown   db_type:   %s' % db_type))"," 'Returns a manager for this class. 
 This method is used to obtain a manager for a class. 
 :param cls: The class for which to get a manager. 
 :returns: A manager for the class. 
 :rtype: boto.sdb.db.manager.Manager'","'Returns the appropriate Manager class for a given Model class.  It 
 does this by looking in the boto config for a section like this:: 
 [DB] 
 db_type = SimpleDB 
 db_user = <aws access key id> 
 db_passwd = <aws secret access key> 
 db_name = my_domain 
 [DB_TestBasic] 
 db_type = SimpleDB 
 db_user = <another aws access key id> 
 db_passwd = <another aws secret access key> 
 db_name = basic_domain 
 db_port = 1111 
 The values in the DB section are ""generic values"" that will be used 
 if nothing more specific is found.  You can also create a section for 
 a specific Model class that gives the db info for that class. 
 In the example above, TestBasic is a Model subclass.'"
"def I(attr): 
    return range_property(attr, 0, 4294967295)", 'A range property that returns a random integer between 0 and 4294967295.','Unsigned Long'
"def set_edit_mode(request, flag): 
    if (flag and could_edit(request)): 
      request.session[EDIT_FLAG_NAME] = True 
   else: 
      request.session.pop(EDIT_FLAG_NAME, None)"," 'Set the edit mode flag. 
 If the flag is True, the user is allowed to edit the page. 
 Otherwise, the user is not allowed to edit the page. 
 :param request: The request object. 
 :param flag: True if the user is allowed to edit the page, False if not. 
 :return: None'","'Enable or disable edit mode for the request. 
 :param request: HTTP request 
 :type request: django.http.HttpRequest 
 :param flag: Enable flag 
 :type flag: bool'"
"def register(func, msg_type=None, dispatcher_name=None, active_by_default=True): 
    if (msg_type and (msg_type not in MSG_TYPE_MAP)): 
      raise InvalidHandlerType(('Invalid   message   type   [%s]:   type   should   be   in   %s' % (msg_type, str(MSG_TYPES)))) 
   handler = Handler(func=func, name=dispatcher_name) 
   if (msg_type is None): 
      _registry[RAW_TYPE].append(handler) 
   else: 
      _registry[msg_type].append(handler) 
   if active_by_default: 
      _active.add(dispatcher_name)"," 'Register a new handler with the dispatcher. 
 :param func: the handler function. 
 :param msg_type: the message type. 
 :param dispatcher_name: the name of the dispatcher. 
 :param active_by_default: whether to activate the handler by default.'","'Register handler to RAW if msg_type not given. 
 :type func: callable 
 :type msg_type: str or unicode'"
"def query(params=None): 
    path = 'https://api.qingcloud.com/iaas/' 
   access_key_id = config.get_cloud_config_value('access_key_id', get_configured_provider(), __opts__, search_global=False) 
   access_key_secret = config.get_cloud_config_value('secret_access_key', get_configured_provider(), __opts__, search_global=False) 
   real_parameters = {'access_key_id': access_key_id, 'signature_version': DEFAULT_QINGCLOUD_SIGNATURE_VERSION, 'time_stamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()), 'version': DEFAULT_QINGCLOUD_API_VERSION} 
   if params: 
      for (key, value) in params.items(): 
         if isinstance(value, list): 
            for i in range(1, (len(value) + 1)): 
               if isinstance(value[(i - 1)], dict): 
                  for (sk, sv) in value[(i - 1)].items(): 
                     if (isinstance(sv, dict) or isinstance(sv, list)): 
                        sv = json.dumps(sv, separators=(',', ':')) 
                     real_parameters['{0}.{1}.{2}'.format(key, i, sk)] = sv 
               else: 
                  real_parameters['{0}.{1}'.format(key, i)] = value[(i - 1)] 
         else: 
            real_parameters[key] = value 
   signature = _compute_signature(real_parameters, access_key_secret, 'GET', '/iaas/') 
   real_parameters['signature'] = signature 
   request = requests.get(path, params=real_parameters, verify=False) 
   if (request.status_code != 200): 
      raise SaltCloudSystemExit(""An   error   occurred   while   querying   QingCloud.   HTTP   Code:   {0}      Error:   '{1}'"".format(request.status_code, request.text)) 
   log.debug(request.url) 
   content = request.text 
   result = json.loads(content, object_hook=salt.utils.decode_dict) 
   if (result['ret_code'] != 0): 
      raise SaltCloudSystemExit(pprint.pformat(result.get('message', {}))) 
   return result"," 'Query QingCloud API. 
 :param params: A dictionary of parameters to be sent to the API. 
 :returns: A dictionary of the API response.'",'Make a web call to QingCloud IaaS API.'
"def is_list_of_ints(intlist): 
    if (not isinstance(intlist, list)): 
      return False 
   for i in intlist: 
      if (not isinstance(i, int)): 
         return False 
   return True", 'Check if intlist is a list of ints.','Return True if list is a list of ints.'
"def pade(an, m): 
    an = asarray(an) 
   N = (len(an) - 1) 
   n = (N - m) 
   if (n < 0): 
      raise ValueError('Order   of   q   <m>   must   be   smaller   than   len(an)-1.') 
   Akj = eye((N + 1), (n + 1)) 
   Bkj = zeros(((N + 1), m), 'd') 
   for row in range(1, (m + 1)): 
      Bkj[row, :row] = (- an[:row][::(-1)]) 
   for row in range((m + 1), (N + 1)): 
      Bkj[row, :] = (- an[(row - m):row][::(-1)]) 
   C = hstack((Akj, Bkj)) 
   pq = linalg.solve(C, an) 
   p = pq[:(n + 1)] 
   q = r_[(1.0, pq[(n + 1):])] 
   return (poly1d(p[::(-1)]), poly1d(q[::(-1)]))"," 'Solve linear system of equations for Pade approximation 
 an = [a0, a1, ..., an] 
 m = order of Pade approximation 
 pade(an, m) = (p, q) 
 p = [p0, p1, ..., pn] 
 q = [q0, q1, ..., qn] 
 p = poly1d(p) 
 q = poly1d(q) 
 p = p[::(-1)] 
 q = q[::(-1)] 
 Examples 
 >>> from sympy.polys.polytools import pade 
 >>> from sympy.abc import x, y 
 >>> pade(x**2 + x*y + y**2, 3) 
 (x**2 - 3*x*y + y**2, x**2 + 3*x*y + y**2) 
 >>> pade(x**2 + x*y + y**2, 2) 
 (x**2 - x*y + y**2, x**2 + x*y + y**2) ","'Return Pade approximation to a polynomial as the ratio of two polynomials. 
 Parameters 
 an : (N,) array_like 
 Taylor series coefficients. 
 m : int 
 The order of the returned approximating polynomials. 
 Returns 
 p, q : Polynomial class 
 The Pade approximation of the polynomial defined by `an` is 
 ``p(x)/q(x)``. 
 Examples 
 >>> from scipy.interpolate import pade 
 >>> e_exp = [1.0, 1.0, 1.0/2.0, 1.0/6.0, 1.0/24.0, 1.0/120.0] 
 >>> p, q = pade(e_exp, 2) 
 >>> e_exp.reverse() 
 >>> e_poly = np.poly1d(e_exp) 
 Compare ``e_poly(x)`` and the Pade approximation ``p(x)/q(x)`` 
 >>> e_poly(1) 
 2.7166666666666668 
 >>> p(1)/q(1) 
 2.7179487179487181'"
"def count(session, query): 
    counts = query.selectable.with_only_columns([func.count()]) 
   num_results = session.execute(counts.order_by(None)).scalar() 
   if ((num_results is None) or (query._limit is not None)): 
      return query.order_by(None).count() 
   return num_results"," 'Returns the number of rows in the result set. 
 :param session: An active :class:`~sqlalchemy.orm.Session` 
 :param query: A :class:`~sqlalchemy.orm.query.Query` instance 
 :return: The number of rows in the result set, or None if there is no 
 limit.'","'Returns the count of the specified `query`. 
 This function employs an optimization that bypasses the 
 :meth:`sqlalchemy.orm.Query.count` method, which can be very slow 
 for large queries.'"
"def test_create_angles(): 
    u'   The   ""angle""   is   a   fundamental   object.   The   internal\n            representation   is   stored   in   radians,   but   this   is   transparent   to   the   user.\n            Units   *must*   be   specified   rather   than   a   default   value   be   assumed.   This   is\n            as   much   for   self-documenting   code   as   anything   else.\n\n            Angle   objects   simply   represent   a   single   angular   coordinate.   More   specific\n            angular   coordinates   (e.g.   Longitude,   Latitude)   are   subclasses   of   Angle.' 
   a1 = Angle(54.12412, unit=u.degree) 
   a2 = Angle(u'54.12412', unit=u.degree) 
   a3 = Angle(u'54:07:26.832', unit=u.degree) 
   a4 = Angle(u'54.12412   deg') 
   a5 = Angle(u'54.12412   degrees') 
   a6 = Angle(u'54.12412\xb0') 
   a7 = Angle((54, 7, 26.832), unit=u.degree) 
   a8 = Angle(u'54\xb007\'26.832""') 
   a9 = Angle([54, 7, 26.832], unit=u.degree) 
   assert_allclose(a9.value, [54, 7, 26.832]) 
   assert (a9.unit is u.degree) 
   a10 = Angle(3.60827466667, unit=u.hour) 
   a11 = Angle(u'3:36:29.7888000120', unit=u.hour) 
   a12 = Angle((3, 36, 29.788800012), unit=u.hour) 
   a13 = Angle((3, 36, 29.788800012), unit=u'hour') 
   Angle(0.944644098745, unit=u.radian) 
   with pytest.raises(u.UnitsError): 
      Angle(54.12412) 
   with pytest.raises(u.UnitsError): 
      Angle(54.12412, unit=u.m) 
   with pytest.raises(ValueError): 
      Angle(12.34, unit=u'not   a   unit') 
   a14 = Angle(u'03h36m29.7888000120') 
   a15 = Angle(u'5h4m3s') 
   assert (a15.unit == u.hourangle) 
   a16 = Angle(u'1   d') 
   a17 = Angle(u'1   degree') 
   assert (a16.degree == 1) 
   assert (a17.degree == 1) 
   a18 = Angle(u'54   07.4472', unit=u.degree) 
   a19 = Angle(u'54:07.4472', unit=u.degree) 
   a20 = Angle(u'54d07.4472m', unit=u.degree) 
   a21 = Angle(u'3h36m', unit=u.hour) 
   a22 = Angle(u'3.6h', unit=u.hour) 
   a23 = Angle(u'-   3h', unit=u.hour) 
   a24 = Angle(u'+   3h', unit=u.hour) 
   assert (a1 == a2 == a3 == a4 == a5 == a6 == a7 == a8 == a18 == a19 == a20) 
   assert_allclose(a1.radian, a2.radian) 
   assert_allclose(a2.degree, a3.degree) 
   assert_allclose(a3.radian, a4.radian) 
   assert_allclose(a4.radian, a5.radian) 
   assert_allclose(a5.radian, a6.radian) 
   assert_allclose(a6.radian, a7.radian) 
   assert_allclose(a10.degree, a11.degree) 
   assert (a11 == a12 == a13 == a14) 
   assert (a21 == a22) 
   assert (a23 == (- a24)) 
   with pytest.raises(IllegalSecondError): 
      a = Angle(u'12   32   99', unit=u.degree) 
   with pytest.raises(IllegalMinuteError): 
      a = Angle(u'12   99   23', unit=u.degree) 
   with pytest.raises(IllegalSecondError): 
      a = Angle(u'12   32   99', unit=u.hour) 
   with pytest.raises(IllegalMinuteError): 
      a = Angle(u'12   99   23', unit=u.hour) 
   with pytest.raises(IllegalHourError): 
      a = Angle(u'99   25   51.0', unit=u.hour) 
   with pytest.raises(ValueError): 
      a = Angle(u'12   25   51.0xxx', unit=u.hour) 
   with pytest.raises(ValueError): 
      a = Angle(u'12h34321m32.2s') 
   assert (a1 is not None)", 'Test the Angle class.','Tests creating and accessing Angle objects'
"def getVector3RemoveByPrefix(prefix, vector3, xmlElement): 
    vector3RemoveByPrefix = getVector3ByPrefix(vector3, prefix, xmlElement) 
   euclidean.removePrefixFromDictionary(xmlElement.attributeDictionary, prefix) 
   return vector3RemoveByPrefix"," 'Remove the prefix from a vector3 object. 
 :param prefix: The prefix to remove. 
 :type prefix: str 
 :param vector3: The vector3 object to remove the prefix from. 
 :type vector3: :class:`~.Vector3` 
 :param xmlElement: The xml element to remove the prefix from. 
 :type xmlElement: :class:`~.XMLElement` 
 :return: The vector3 object after removing the prefix. 
 :rtype: :class:`~.Vector3`'","'Get vector3 from prefix and xml element, then remove prefix attributes from dictionary.'"
"def _escape_jid(jid): 
    jid = str(jid) 
   jid = re.sub(""'*"", '', jid) 
   return jid", 'Escape jid characters that are not allowed in a JID.','Do proper formatting of the jid'
"def parse_time(value): 
    match = time_re.match(value) 
   if match: 
      kw = match.groupdict() 
      if kw['microsecond']: 
         kw['microsecond'] = kw['microsecond'].ljust(6, '0') 
      kw = dict(((k, int(v)) for (k, v) in six.iteritems(kw) if (v is not None))) 
      return datetime.time(**kw)"," 'Parse a time string. 
 :param value: 
 A time string. 
 :return: 
 A datetime.time object.'","'Parses a string and return a datetime.time. 
 This function doesn\'t support time zone offsets. 
 Raises ValueError if the input is well formatted but not a valid time. 
 Returns None if the input isn\'t well formatted, in particular if it 
 contains an offset.'"
"def unparse_vs(tup): 
    return '.'.join(map(str, tup))", 'Unparse a tuple of VS.','version list to string'
"def _convert(expected_type, value): 
    if (not isinstance(value, expected_type)): 
      try: 
         value = expected_type(value) 
      except: 
         raise TypeError(('expected   ' + str(expected_type))) 
   return value"," 'Convert a value to the expected type. 
 :param expected_type: 
 :param value: 
 :return: 
 :raises: 
 :rtype: 
 :type expected_type: 
 :type value: 
 :return: 
 :rtype: 
 :type expected_type: 
 :type value: 
 :return: 
 :rtype: 
 :type expected_type: 
 :type value: 
 :return: 
 :rtype: 
 :type expected_type: 
 :type value: 
 :return: 
 :rtype: 
 :type expected_type: 
 :type value: 
 :return: 
 :rtype: 
 :type expected_type: 
 :type value: 
 :return: 
 :rtype: 
 :type expected_type: 
 :type value: 
 :return: 
 :rtype: 
 :type expected_type: 
 :type value: 
 :return: 
 :rtype: 
 :type expected_type: 
 :type value: 
",'Check value is of or can be converted to expected type.'
"@apply_to_binary_file 
 def xmlminify(data): 
    parser = lxml.etree.XMLParser(remove_blank_text=True) 
   newdata = lxml.etree.XML(data, parser=parser) 
   return lxml.etree.tostring(newdata, encoding='utf-8', method='xml', xml_declaration=True)"," 'Minifies an XML document by removing whitespace and comments. 
 :param data: A string containing the XML document to minify. 
 :returns: A string containing the minified XML document.'",'Minify XML files (strip whitespace and use minimal separators).'
"def qd(A, B, output='real', lwork=None, sort=None, overwrite_a=False, overwrite_b=False, check_finite=True): 
    (result, _) = _qd(A, B, output=output, lwork=lwork, sort=sort, overwrite_a=overwrite_a, overwrite_b=overwrite_b, check_finite=check_finite) 
   return (result[0], result[1], result[(-4)], result[(-3)])"," 'QR decomposition of A and B. 
 Parameters 
 A, B : (M, N) array_like 
 Matrix to be factorized. 
 output : {'real', 'complex', 'conjugate'} 
 Type of output. 
 lwork : {None, int} 
 Workspace length. 
 sort : {'forward', 'backward'} 
 Sorting type. 
 overwrite_a : bool 
 Whether to overwrite A. 
 overwrite_b : bool 
 Whether to overwrite B. 
 check_finite : bool 
 Whether to check for NaN and Inf. 
 Returns 
 A : (M, N) array 
 Q. 
 B : (M, N) array 
 R. 
 Rcond : float 
 Condition number of the factorization. 
 Infnorm : float 
 The infinity norm of the error. 
 Norm : float 
 The 2-norm of the error. 
 Examples 
 >>> from scipy.sparse import csr_matrix 
 >>> from scipy.sparse.linalg import qd 
 >>> A = csr_matrix","'QZ decomposition for generalized eigenvalues of a pair of matrices. 
 The QZ, or generalized Schur, decomposition for a pair of N x N 
 nonsymmetric matrices (A,B) is:: 
 (A,B) = (Q*AA*Z\', Q*BB*Z\') 
 where AA, BB is in generalized Schur form if BB is upper-triangular 
 with non-negative diagonal and AA is upper-triangular, or for real QZ 
 decomposition (``output=\'real\'``) block upper triangular with 1x1 
 and 2x2 blocks.  In this case, the 1x1 blocks correspond to real 
 generalized eigenvalues and 2x2 blocks are \'standardized\' by making 
 the corresponding elements of BB have the form:: 
 [ a 0 ] 
 [ 0 b ] 
 and the pair of corresponding 2x2 blocks in AA and BB will have a complex 
 conjugate pair of generalized eigenvalues.  If (``output=\'complex\'``) or 
 A and B are complex matrices, Z\' denotes the conjugate-transpose of Z. 
 Q and Z are unitary matrices. 
 Parameters 
 A : (N, N) array_like 
 2d array to decompose 
 B : (N, N) array_like 
 2d array to decompose 
 output : {\'real\', \'complex\'}, optional 
 Construct the real or complex QZ decomposition for real matrices. 
 Default is \'real\'. 
 lwork : int, optional 
 Work array size.  If None or -1, it is automatically computed. 
 sort : {None, callable, \'lhp\', \'rhp\', \'iuc\', \'ouc\'}, optional 
 NOTE: THIS INPUT IS DISABLED FOR NOW. Use ordqd instead. 
 Specifies whether the upper eigenvalues should be sorted.  A callable 
 may be passed that, given a eigenvalue, returns a boolean denoting 
 whether the eigenvalue should be sorted to the top-left (True). For 
 real matrix pairs, the sort function takes three real arguments 
 (alphar, alphai, beta). The eigenvalue 
 ``x = (alphar + alphai*1j)/beta``.  For complex matrix pairs or 
 output=\'complex\', the sort function takes two complex arguments 
 (alpha, beta). The eigenvalue ``x = (alpha/beta)``.  Alternatively, 
 string parameters may be used: 
 - \'lhp\'   Left-hand plane (x.real < 0.0) 
 - \'rhp\'   Right-hand plane (x.real > 0.0) 
 - \'iuc\'   Inside the unit circle (x*x.conjugate() < 1.0) 
 - \'ouc\'   Outside the unit circle (x*x.conjugate() > 1.0) 
 Defaults to None (no sorting). 
 overwrite_a : bool, optional 
 Whether to overwrite data in a (may improve performance) 
 overwrite_b : bool, optional 
 Whether to overwrite data in b (may improve performance) 
 check_finite : bool, optional 
 If true checks the elements of `A` and `B` are finite numbers. If 
 false does no checking and passes matrix through to 
 underlying algorithm. 
 Returns 
 AA : (N, N) ndarray 
 Generalized Schur form of A. 
 BB : (N, N) ndarray 
 Generalized Schur form of B. 
 Q : (N, N) ndarray 
 The left Schur vectors. 
 Z : (N, N) ndarray 
 The right Schur vectors. 
 Notes 
 Q is transposed versus the equivalent function in Matlab. 
 .. versionadded:: 0.11.0 
 Examples 
 >>> from scipy import linalg 
 >>> np.random.seed(1234) 
 >>> A = np.arange(9).reshape((3, 3)) 
 >>> B = np.random.randn(3, 3) 
 >>> AA, BB, Q, Z = linalg.qd(A, B) 
 >>> AA 
 array([[-13.40928183,  -4.62471562,   1.09215523], 
 [  0.        ,   0.        ,   1.22805978], 
 [  0.        ,   0.        ,   0.31973817]]) 
 >>> BB 
 array([[ 0.33362547, -1.37393632,  0.02179805], 
 [ 0.        ,  1.68144922,  0.74683866], 
 [ 0.        ,  0.        ,  0.9258294 ]]) 
 >>> Q 
 array([[ 0.14134727, -0.97562773,  0.16784365], 
 [ 0.49835904, -0.07636948, -0.86360059], 
 [ 0.85537081,  0.20571399,  0.47541828]]) 
 >>> Z 
 array([[-0.24900855, -0.51772687,  0.81850696], 
 [-0.79813178,  0.58842606,  0.12938478], 
 [-0.54861681, -0.6210585 , -0.55973739]]) 
 See also 
 ordqd'"
"def GetMostRecentClient(client_list, token=None): 
    last = rdfvalue.RDFDatetime(0) 
   client_urn = None 
   for client in aff4.FACTORY.MultiOpen(client_list, token=token): 
      client_last = client.Get(client.Schema.LAST) 
      if (client_last > last): 
         last = client_last 
         client_urn = client.urn 
   return client_urn"," 'Get the most recent client in a list of clients. 
 :param client_list: 
 :param token: 
 :return: 
 :rtype: 
 :returns: 
 :returns: client_urn: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_last: 
 :returns: client_",'Return most recent client from list of clients.'
"def _fwd_eeg_fit_berg_scherg(m, nterms, nfit): 
    from scipy.optimize import fmin_cobyla 
   assert (nfit >= 2) 
   u = dict(y=np.zeros((nterms - 1)), resi=np.zeros((nterms - 1)), nfit=nfit, nterms=nterms, M=np.zeros(((nterms - 1), (nfit - 1)))) 
   u['fn'] = _fwd_eeg_get_multi_sphere_model_coeffs(m, (nterms + 1)) 
   f = (min([layer['rad'] for layer in m['layers']]) / max([layer['rad'] for layer in m['layers']])) 
   k = np.arange(1, (nterms + 1)) 
   u['w'] = (np.sqrt(((((2.0 * k) + 1) * ((3.0 * k) + 1.0)) / k)) * np.power(f, (k - 1.0))) 
   u['w'][(-1)] = 0 
   mu_0 = (np.random.RandomState(0).rand(nfit) * f) 
   fun = partial(_one_step, u=u) 
   max_ = (1.0 - 0.0002) 
   cons = [(lambda x: (max_ - np.abs(x[ii]))) for ii in range(nfit)] 
   mu = fmin_cobyla(fun, mu_0, cons, rhobeg=0.5, rhoend=0.005, disp=0) 
   (rv, lambda_) = _compute_linear_parameters(mu, u) 
   order = np.argsort(mu)[::(-1)] 
   (mu, lambda_) = (mu[order], lambda_[order]) 
   m['mu'] = mu 
   m['lambda'] = (lambda_ / m['layers'][(-1)]['sigma']) 
   m['nfit'] = nfit 
   return rv"," 'Fit the Berg-Scherg model to the data. 
 This is a modification of the original implementation of the 
 Berg-Scherg model (see the `Berg-Scherg model <http://www.biorxiv.org/content/early/2014/08/14/005992>`_ 
 paper) to allow for multiple layers of spherical sources. 
 Parameters 
 m : dict 
 A dictionary of the form 
 `{layer_name: {...}, ...}` 
 where each layer is a dictionary of the form 
 `{layer_name: {...}, ...}` 
 where each layer is a dictionary of the form 
 `{name: {...}, ...}` 
 where each name is a string and each value is a dictionary of the form 
 `{name: {...}, ...}` 
 where each name is a string and each value is a dictionary of the form 
 `{name: {...}, ...}` 
 where each name is a string and each value is a dictionary of the form 
 `{name: {...}, ...}` 
 where each name is a string and each value is",'Fit the Berg-Scherg equivalent spherical model dipole parameters.'
"@pytest.mark.parametrize('stream', ['stdout', 'stderr']) 
 def test_exit_unsuccessful_output(qtbot, proc, caplog, py_proc, stream): 
    with caplog.at_level(logging.ERROR): 
      with qtbot.waitSignal(proc.finished, timeout=10000): 
         proc.start(*py_proc('\n                                                import   sys\n                                                print(""test"",   file=sys.{})\n                                                sys.exit(1)\n                                    '.format(stream))) 
   assert (len(caplog.records) == 2) 
   assert (caplog.records[1].msg == 'Process   {}:\ntest'.format(stream))"," 'Test that an unsuccessful process exits with a non-zero exit code and 
 emits a log message with the process output'","'When a process fails, its output should be logged.'"
"def create_move(project, resource, offset=None): 
    if (offset is None): 
      return MoveModule(project, resource) 
   this_pymodule = project.get_pymodule(resource) 
   pyname = evaluate.eval_location(this_pymodule, offset) 
   if (pyname is not None): 
      pyobject = pyname.get_object() 
      if (isinstance(pyobject, pyobjects.PyModule) or isinstance(pyobject, pyobjects.PyPackage)): 
         return MoveModule(project, pyobject.get_resource()) 
      if (isinstance(pyobject, pyobjects.PyFunction) and isinstance(pyobject.parent, pyobjects.PyClass)): 
         return MoveMethod(project, resource, offset) 
      if ((isinstance(pyobject, pyobjects.PyDefinedObject) and isinstance(pyobject.parent, pyobjects.PyModule)) or isinstance(pyname, pynames.AssignedName)): 
         return MoveGlobal(project, resource, offset) 
   raise exceptions.RefactoringError('Move   only   works   on   global   classes/functions/variables,   modules   and   methods.')", 'Creates a Move object from an offset.',"'A factory for creating Move objects 
 Based on `resource` and `offset`, return one of `MoveModule`, 
 `MoveGlobal` or `MoveMethod` for performing move refactoring.'"
"def in6_isaddr6to4(x): 
    x = inet_pton(socket.AF_INET6, x) 
   return (x[:2] == '   \x02')"," 'Returns True if the address is an IPv6 address in IPv4-mapped IPv6 
 format, False otherwise.'","'Return True if provided address (in printable format) is a 6to4 
 address (being in 2002::/16).'"
"def __virtual__(): 
    if (HAS_NAPALM and ('proxy' in __opts__)): 
      return __virtualname__ 
   else: 
      return (False, 'The   network   NTP   state   (netntp)   cannot   be   loaded:                                                   NAPALM   or   proxy   could   not   be   loaded.')"," 'Load the network NTP state (netntp) plugin. 
 This plugin is only loaded if HAS_NAPALM is True or proxy is set in 
 the configuration. 
 :return: The virtual name of the module'","'NAPALM library must be installed for this module to work. 
 Also, the key proxymodule must be set in the __opts___ dictionary.'"
"def extract_python(fileobj, keywords, comment_tags, options): 
    funcname = lineno = message_lineno = None 
   call_stack = (-1) 
   buf = [] 
   messages = [] 
   translator_comments = [] 
   in_def = in_translator_comments = False 
   comment_tag = None 
   encoding = (parse_encoding(fileobj) or options.get('encoding', 'iso-8859-1')) 
   tokens = generate_tokens(fileobj.readline) 
   for (tok, value, (lineno, _), _, _) in tokens: 
      if ((call_stack == (-1)) and (tok == NAME) and (value in ('def', 'class'))): 
         in_def = True 
      elif ((tok == OP) and (value == '(')): 
         if in_def: 
            in_def = False 
            continue 
         if funcname: 
            message_lineno = lineno 
            call_stack += 1 
      elif (in_def and (tok == OP) and (value == ':')): 
         in_def = False 
         continue 
      elif ((call_stack == (-1)) and (tok == COMMENT)): 
         value = value.decode(encoding)[1:].strip() 
         if (in_translator_comments and (translator_comments[(-1)][0] == (lineno - 1))): 
            translator_comments.append((lineno, value)) 
            continue 
         for comment_tag in comment_tags: 
            if value.startswith(comment_tag): 
               in_translator_comments = True 
               translator_comments.append((lineno, value)) 
               break 
      elif (funcname and (call_stack == 0)): 
         if ((tok == OP) and (value == ')')): 
            if buf: 
               messages.append(''.join(buf)) 
               del buf[:] 
            else: 
               messages.append(None) 
            if (len(messages) > 1): 
               messages = tuple(messages) 
            else: 
               messages = messages[0] 
            if (translator_comments and (translator_comments[(-1)][0] < (message_lineno - 1))): 
               translator_comments = [] 
            (yield (message_lineno, funcname, messages, [comment[1] for comment in translator_comments])) 
            funcname = lineno = message_lineno = None 
            call_stack = (-1) 
            messages = [] 
            translator_comments = [] 
            in_translator_comments = False 
         elif (tok == STRING): 
            value = eval(('#   coding=%s\n%s' % (encoding, value)), {'__builtins__': {}}, {}) 
            if isinstance(value, str): 
               value = value.decode(encoding) 
            buf.append(value) 
         elif ((tok == OP) and (value == ',')): 
            if buf: 
               messages.append(''.join(buf)) 
               del buf[:] 
            else: 
               messages.append(None) 
            if translator_comments: 
               (old_lineno, old_comment) = translator_comments.pop() 
               translator_comments.append(((old_lineno + 1), old_comment)) 
      elif ((call_stack > 0) and (tok == OP) and (value == ')')): 
         call_stack -= 1 
      elif (funcname and (call_stack == (-1))): 
         funcname = None 
      elif ((tok == NAME) and (value in keywords)): 
         funcname = value"," 'Extracts Python code from a file. 
 This function is used to extract Python code from a file. 
 The file is read line by line and each line is parsed to extract 
 the Python code. 
 The function returns a list of tuples containing the following 
 information about each line: 
 (lineno, funcname, messages, translator_comments) 
 lineno: The line number of the line. 
 funcname: The function name of the line. 
 messages: A list of messages that are generated by the Python code. 
 translator_comments: A list of translator comments that are generated 
 by the Python code. 
 The messages are a list of strings that are generated by the Python 
 code. 
 The translator comments are a list of tuples that are generated by 
 the Python code.  The first element of the tuple is the line number 
 of the line and the second element is the comment that is generated 
 by the Python code. 
 The function also returns a list of tuples containing the following 
 information about the file: 
 (lineno, funcname, messages, translator_comments)","'Extract messages from Python source code. 
 :param fileobj: the seekable, file-like object the messages should be 
 extracted from 
 :param keywords: a list of keywords (i.e. function names) that should be 
 recognized as translation functions 
 :param comment_tags: a list of translator tags to search for and include 
 in the results 
 :param options: a dictionary of additional options (optional) 
 :return: an iterator over ``(lineno, funcname, message, comments)`` tuples 
 :rtype: ``iterator``'"
"def generate_replace_result_xml(result_sourcedid, score): 
    elem = ElementMaker(nsmap={None: 'http://www.imsglobal.org/services/ltiv1p1/xsd/imsoms_v1p0'}) 
   xml = elem.imsx_POXEnvelopeRequest(elem.imsx_POXHeader(elem.imsx_POXRequestHeaderInfo(elem.imsx_version('V1.0'), elem.imsx_messageIdentifier(str(uuid.uuid4())))), elem.imsx_POXBody(elem.replaceResultRequest(elem.resultRecord(elem.sourcedGUID(elem.sourcedId(result_sourcedid)), elem.result(elem.resultScore(elem.language('en'), elem.textString(str(score)))))))) 
   return etree.tostring(xml, xml_declaration=True, encoding='UTF-8')"," 'Generate a POX request to replace a result. 
 :param result_sourcedid: The SourcedID of the result to replace 
 :type result_sourcedid: str 
 :param score: The score to replace the result with 
 :type score: int'","'Create the XML document that contains the new score to be sent to the LTI 
 consumer. The format of this message is defined in the LTI 1.1 spec.'"
"def _get_params(mapper_spec, allowed_keys=None): 
    if ('input_reader' not in mapper_spec.params): 
      message = ""Input   reader's   parameters   should   be   specified   in   input_reader   subdictionary."" 
      if allowed_keys: 
         raise errors.BadReaderParamsError(message) 
      params = mapper_spec.params 
      params = dict(((str(n), v) for (n, v) in params.iteritems())) 
   else: 
      if (not isinstance(mapper_spec.params.get('input_reader'), dict)): 
         raise errors.BadReaderParamsError('Input   reader   parameters   should   be   a   dictionary') 
      params = mapper_spec.params.get('input_reader') 
      params = dict(((str(n), v) for (n, v) in params.iteritems())) 
      if allowed_keys: 
         params_diff = (set(params.keys()) - allowed_keys) 
         if params_diff: 
            raise errors.BadReaderParamsError(('Invalid   input_reader   parameters:   %s' % ','.join(params_diff))) 
   return params", 'Get parameters for input reader from mapper spec.',"'Obtain input reader parameters. 
 Utility function for input readers implementation. Fetches parameters 
 from mapreduce specification giving appropriate usage warnings. 
 Args: 
 mapper_spec: The MapperSpec for the job 
 allowed_keys: set of all allowed keys in parameters as strings. If it is not 
 None, then parameters are expected to be in a separate ""input_reader"" 
 subdictionary of mapper_spec parameters. 
 Returns: 
 mapper parameters as dict 
 Raises: 
 BadReaderParamsError: if parameters are invalid/missing or not allowed.'"
"def dir_list(load): 
    gitfs = salt.utils.gitfs.GitFS(__opts__) 
   gitfs.init_remotes(__opts__['gitfs_remotes'], PER_REMOTE_OVERRIDES, PER_REMOTE_ONLY) 
   return gitfs.dir_list(load)"," 'List a directory tree 
 :param load: Load a file from the remote 
 :return: A list of the files in the directory'",'Return a list of all directories on the master'
"def find_duplicative_certs(config, domains): 
    def update_certs_for_domain_matches(candidate_lineage, rv): 
      'Return   cert   as   identical_names_cert   if   it   matches,\n                                 or   subset_names_cert   if   it   matches   as   subset\n                        ' 
      (identical_names_cert, subset_names_cert) = rv 
      candidate_names = set(candidate_lineage.names()) 
      if (candidate_names == set(domains)): 
         identical_names_cert = candidate_lineage 
      elif candidate_names.issubset(set(domains)): 
         if (subset_names_cert is None): 
            subset_names_cert = candidate_lineage 
         elif (len(candidate_names) > len(subset_names_cert.names())): 
            subset_names_cert = candidate_lineage 
      return (identical_names_cert, subset_names_cert) 
   return _search_lineages(config, update_certs_for_domain_matches, (None, None))"," 'Finds the lineage with the most certificates in the domains list. 
 :param config: Configuration 
 :param domains: List of domains to check 
 :returns: Lineage with the most certificates in the domains list, None if 
 no lineage has all the domains'",'Find existing certs that duplicate the request.'
"def assemble(block): 
    return [('\n'.join(sub_block) + '\n') for sub_block in block]", 'Assemble a block of code into a single line.','Assemble a block into multi-line sub-blocks.'
"def getNewDerivation(elementNode): 
    return evaluate.EmptyObject(elementNode)"," 'Returns a new derivation for the given elementNode. 
 :param elementNode: The element node. 
 :type elementNode: :py:class:`~sympy.core.expr.Expr` 
 :return: A new derivation for the element node. 
 :rtype: :py:class:`~sympy.core.expr.Derivative`'",'Get new derivation.'
"def make_colorizer(color): 
    def inner(text): 
      return colorizer.colorize(color, text) 
   return inner", 'Make a colorizer function that uses the given color.',"'Creates a function that colorizes text with the given color. 
 For example: 
 green = make_colorizer(\'darkgreen\') 
 red = make_colorizer(\'red\') 
 Then, you can use: 
 print ""It\'s either "" + green(\'OK\') + \' or \' + red(\'Oops\')'"
"def is_var(exp): 
    return isinstance(exp, Var)"," 'Check if the expression is a variable. 
 Parameters 
 exp : expression 
 The expression to check. 
 Returns 
 True if the expression is a variable, False otherwise.'",'Is `exp` a `Var` node.'
"def make_middleware(app, global_conf=None, warning_level=None): 
    if warning_level: 
      warning_level = int(warning_level) 
   return HTTPExceptionHandler(app, warning_level=warning_level)"," 'Make a middleware that catches HTTP exceptions and logs them. 
 :param app: The app to wrap. 
 :param global_conf: The global configuration. 
 :param warning_level: The warning level. 
 :return: The middleware. 
 :rtype: :class:`Middleware` 
 :raises ValueError: If warning_level is not an integer.'","'``httpexceptions`` middleware; this catches any 
 ``paste.httpexceptions.HTTPException`` exceptions (exceptions like 
 ``HTTPNotFound``, ``HTTPMovedPermanently``, etc) and turns them 
 into proper HTTP responses. 
 ``warning_level`` can be an integer corresponding to an HTTP code. 
 Any code over that value will be passed \'up\' the chain, potentially 
 reported on by another piece of middleware.'"
"def fromRoman(s): 
    if (not s): 
      raise InvalidRomanNumeralError, 'Input   can   not   be   blank' 
   if (not romanNumeralPattern.search(s)): 
      raise InvalidRomanNumeralError, ('Invalid   Roman   numeral:   %s' % s) 
   result = 0 
   index = 0 
   for (numeral, integer) in romanNumeralMap: 
      while (s[index:(index + len(numeral))] == numeral): 
         result += integer 
         index += len(numeral) 
   return result"," 'Converts a roman numeral to an integer. 
 >>> fromRoman(\'IV\') 
 4'",'convert Roman numeral to integer'
"def new_figure_manager_given_figure(num, figure): 
    canvas = FigureCanvasGDK(figure) 
   manager = FigureManagerBase(canvas, num) 
   return manager"," 'Create a new figure manager for a given figure. 
 This is a helper function for :meth:`FigureManagerBase.new_figure_manager`. 
 Parameters 
 figure : Figure 
 The figure to manage. 
 Returns 
 FigureManager 
 The new figure manager.'",'Create a new figure manager instance for the given figure.'
"def _py_convert_agg_to_wx_bitmap(agg, bbox): 
    if (bbox is None): 
      return wx.BitmapFromImage(_py_convert_agg_to_wx_image(agg, None)) 
   else: 
      return _clipped_image_as_bitmap(_py_convert_agg_to_wx_image(agg, None), bbox)", 'Convert a PyQt4.QtGui.QPixmap to a wx.Bitmap.',"'Convert the region of the agg buffer bounded by bbox to a wx.Bitmap.  If 
 bbox is None, the entire buffer is converted. 
 Note: agg must be a backend_agg.RendererAgg instance.'"
"def generate_presigned_url(self, ClientMethod, Params=None, ExpiresIn=3600, HttpMethod=None): 
    client_method = ClientMethod 
   params = Params 
   expires_in = ExpiresIn 
   http_method = HttpMethod 
   request_signer = self._request_signer 
   serializer = self._serializer 
   try: 
      operation_name = self._PY_TO_OP_NAME[client_method] 
   except KeyError: 
      raise UnknownClientMethodError(method_name=client_method) 
   operation_model = self.meta.service_model.operation_model(operation_name) 
   request_dict = serializer.serialize_to_request(params, operation_model) 
   if (http_method is not None): 
      request_dict['method'] = http_method 
   prepare_request_dict(request_dict, endpoint_url=self.meta.endpoint_url) 
   return request_signer.generate_presigned_url(request_dict=request_dict, expires_in=expires_in, operation_name=operation_name)"," 'Generate a presigned URL for the given operation. 
 :param client_method: The name of the client method. 
 :param Params: The parameters for the request. 
 :param ExpiresIn: The number of seconds the presigned URL is valid for. 
 :param HttpMethod: The HTTP method to use for the request. 
 :return: A presigned URL.'","'Generate a presigned url given a client, its method, and arguments 
 :type ClientMethod: string 
 :param ClientMethod: The client method to presign for 
 :type Params: dict 
 :param Params: The parameters normally passed to 
 ``ClientMethod``. 
 :type ExpiresIn: int 
 :param ExpiresIn: The number of seconds the presigned url is valid 
 for. By default it expires in an hour (3600 seconds) 
 :type HttpMethod: string 
 :param HttpMethod: The http method to use on the generated url. By 
 default, the http method is whatever is used in the method\'s model. 
 :returns: The presigned url'"
"def get_scene_numbering_for_show(indexer_id, indexer): 
    if (indexer_id is None): 
      return {} 
   indexer_id = int(indexer_id) 
   indexer = int(indexer) 
   result = {} 
   for dbData in [x[u'doc'] for x in sickrage.srCore.mainDB.db.get_many(u'scene_numbering', indexer_id, with_doc=True)]: 
      season = int((dbData[u'season'] or 0)) 
      episode = int((dbData[u'episode'] or 0)) 
      scene_season = int((dbData[u'scene_season'] or 0)) 
      scene_episode = int((dbData[u'scene_episode'] or 0)) 
      if ((int(dbData[u'indexer']) != indexer) or ((scene_season or scene_episode) == 0)): 
         continue 
      result[(season, episode)] = (scene_season, scene_episode) 
   return result"," 'Get scene numbering for a show. 
 :param int indexer_id: 
 :param int indexer: 
 :return: dict of season, episode -> scene season, scene episode'","'Returns a dict of (season, episode) : (sceneSeason, sceneEpisode) mappings 
 for an entire show.  Both the keys and values of the dict are tuples. 
 Will be empty if there are no scene numbers set'"
"def oo_random_word(length, source='abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'): 
    return ''.join((random.choice(source) for i in range(length)))"," 'Return a random word of the given length. 
 :param length: The length of the word. 
 :param source: The source of the letters. 
 :type source: str'","'Generates a random string of given length from a set of alphanumeric characters. 
 The default source uses [a-z][A-Z][0-9] 
 Ex: 
 - oo_random_word(3)                => aB9 
 - oo_random_word(4, source=\'012\')  => 0123'"
"def removed(name, requirements=None, bin_env=None, log=None, proxy=None, timeout=None, user=None, cwd=None, use_vt=False): 
    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}} 
   try: 
      pip_list = __salt__['pip.list'](bin_env=bin_env, user=user, cwd=cwd) 
   except (CommandExecutionError, CommandNotFoundError) as err: 
      ret['result'] = False 
      ret['comment'] = ""Error   uninstalling   '{0}':   {1}"".format(name, err) 
      return ret 
   if (name not in pip_list): 
      ret['result'] = True 
      ret['comment'] = 'Package   is   not   installed.' 
      return ret 
   if __opts__['test']: 
      ret['result'] = None 
      ret['comment'] = 'Package   {0}   is   set   to   be   removed'.format(name) 
      return ret 
   if __salt__['pip.uninstall'](pkgs=name, requirements=requirements, bin_env=bin_env, log=log, proxy=proxy, timeout=timeout, user=user, cwd=cwd, use_vt=use_vt): 
      ret['result'] = True 
      ret['changes'][name] = 'Removed' 
      ret['comment'] = 'Package   was   successfully   removed.' 
   else: 
      ret['result'] = False 
      ret['comment'] = 'Could   not   remove   package.' 
   return ret"," 'Uninstall a package from pip. 
 :param name: 
 :param requirements: 
 :param bin_env: 
 :param log: 
 :param proxy: 
 :param timeout: 
 :param user: 
 :param cwd: 
 :param use_vt: 
 :return: 
 :rtype: dict'","'Make sure that a package is not installed. 
 name 
 The name of the package to uninstall 
 user 
 The user under which to run pip 
 bin_env : None 
 the pip executable or virtualenenv to use 
 use_vt 
 Use VT terminal emulation (see output while installing)'"
"@frappe.whitelist() 
 def take_backup(): 
    enqueue(u'frappe.integrations.doctype.dropbox_settings.dropbox_settings.take_backup_to_dropbox', queue=u'long') 
   frappe.msgprint(_(u'Queued   for   backup.   It   may   take   a   few   minutes   to   an   hour.'))", 'Take a backup to Dropbox','Enqueue longjob for taking backup to dropbox'
"@task 
 @needs('pavelib.prereqs.install_prereqs') 
 @cmdopts([('settings=', 's', 'Django   settings   for   both   LMS   and   Studio'), ('asset-settings=', 'a', 'Django   settings   for   updating   assets   for   both   LMS   and   Studio   (defaults   to   settings)'), ('worker-settings=', 'w', 'Celery   worker   Django   settings'), ('fast', 'f', 'Skip   updating   assets'), ('optimized', 'o', 'Run   with   optimized   assets'), ('settings-lms=', 'l', 'Set   LMS   only,   overriding   the   value   from   --settings   (if   provided)'), ('asset-settings-lms=', None, 'Set   LMS   only,   overriding   the   value   from   --asset-settings   (if   provided)'), ('settings-cms=', 'c', 'Set   Studio   only,   overriding   the   value   from   --settings   (if   provided)'), ('asset-settings-cms=', None, 'Set   Studio   only,   overriding   the   value   from   --asset-settings   (if   provided)'), ('asset_settings=', None, 'deprecated   in   favor   of   asset-settings'), ('asset_settings_cms=', None, 'deprecated   in   favor   of   asset-settings-cms'), ('asset_settings_lms=', None, 'deprecated   in   favor   of   asset-settings-lms'), ('settings_cms=', None, 'deprecated   in   favor   of   settings-cms'), ('settings_lms=', None, 'deprecated   in   favor   of   settings-lms'), ('worker_settings=', None, 'deprecated   in   favor   of   worker-settings')]) 
 def run_all_servers(options): 
    settings = getattr(options, 'settings', DEFAULT_SETTINGS) 
   asset_settings = getattr(options, 'asset_settings', settings) 
   worker_settings = getattr(options, 'worker_settings', 'dev_with_worker') 
   fast = getattr(options, 'fast', False) 
   optimized = getattr(options, 'optimized', False) 
   if optimized: 
      settings = OPTIMIZED_SETTINGS 
      asset_settings = OPTIMIZED_ASSETS_SETTINGS 
   settings_lms = getattr(options, 'settings_lms', settings) 
   settings_cms = getattr(options, 'settings_cms', settings) 
   asset_settings_lms = getattr(options, 'asset_settings_lms', asset_settings) 
   asset_settings_cms = getattr(options, 'asset_settings_cms', asset_settings) 
   if (not fast): 
      args = ['lms', 'studio', '--settings={}'.format(asset_settings), '--skip-collect'] 
      call_task('pavelib.assets.update_assets', args=args) 
      if (settings != DEFAULT_SETTINGS): 
         collect_assets(['lms'], asset_settings_lms) 
         collect_assets(['studio'], asset_settings_cms) 
      call_task('pavelib.assets.watch_assets', options={'background': True}) 
   lms_port = DEFAULT_PORT['lms'] 
   cms_port = DEFAULT_PORT['studio'] 
   lms_runserver_args = ['0.0.0.0:{}'.format(lms_port)] 
   cms_runserver_args = ['0.0.0.0:{}'.format(cms_port)] 
   run_multi_processes([django_cmd('lms', settings_lms, 'runserver', '--traceback', '--pythonpath=.', *lms_runserver_args), django_cmd('studio', settings_cms, 'runserver', '--traceback', '--pythonpath=.', *cms_runserver_args), django_cmd('lms', worker_settings, 'celery', 'worker', '--beat', '--loglevel=INFO', '--pythonpath=.')])"," 'Run the LMS and Studio servers and Celery workers. 
 :param options: command line options 
 :type options: :class:`argparse.Namespace`'","'Runs Celery workers, Studio, and LMS.'"
"def _get_info_slice(obj, indexer): 
    if (not hasattr(obj, '_info_axis_number')): 
      raise TypeError(('object   of   type   %r   has   no   info   axis' % type(obj).__name__)) 
   slices = ([slice(None)] * obj.ndim) 
   slices[obj._info_axis_number] = indexer 
   return tuple(slices)"," 'Returns a slice of the given object, with the given indexer. 
 The slice is returned in the form of a tuple of slices. 
 Parameters 
 obj : array-like 
 The object to slice. 
 indexer : int 
 The indexer to slice with. 
 Returns 
 tuple 
 The slice to be applied to the object. 
 Raises 
 TypeError 
 If the object has no info axis.'",'Slice the info axis of `obj` with `indexer`.'
"def device_memory_size(devmem): 
    sz = getattr(devmem, '_cuda_memsize_', None) 
   if (sz is None): 
      (s, e) = device_extents(devmem) 
      sz = (e - s) 
      devmem._cuda_memsize_ = sz 
   assert (sz > 0), 'zero   length   array' 
   return sz"," 'Returns the size of the device memory used by the given CUDA device. 
 This is the size of the array, not the size of the data stored in the 
 array. 
 Parameters 
 devmem : cuda.Device 
 The CUDA device. 
 Returns 
 size : int 
 The size of the device memory used by the given CUDA device.'","'Check the memory size of the device memory. 
 The result is cached in the device memory object. 
 It may query the driver for the memory size of the device memory allocation.'"
"@register.tag('filter') 
 def do_filter(parser, token): 
    (_, rest) = token.contents.split(None, 1) 
   filter_expr = parser.compile_filter(('var|%s' % rest)) 
   for (func, unused) in filter_expr.filters: 
      filter_name = getattr(func, '_filter_name', None) 
      if (filter_name in ('escape', 'safe')): 
         raise TemplateSyntaxError(('""filter   %s""   is   not   permitted.      Use   the   ""autoescape""   tag   instead.' % filter_name)) 
   nodelist = parser.parse(('endfilter',)) 
   parser.delete_first_token() 
   return FilterNode(filter_expr, nodelist)"," 'Render a filter expression. 
 The filter expression is a string that contains one or more 
 filters, separated by a vertical bar. 
 Each filter is a call to a filter function. 
 The function can be a Python function or a callable object 
 that takes a single argument and returns a boolean value. 
 The filter function can also be a callable object that takes 
 two arguments and returns a boolean value. 
 The first argument is the value to filter, and the second 
 argument is a dictionary of keyword arguments to pass to the 
 function. 
 The filter function must return a boolean value. 
 The value of the filter expression is a list of nodes. 
 The first node is the filter expression itself. 
 The second node is a list of nodes that are rendered if the 
 filter expression returns true. 
 The third node is a list of nodes that are rendered if the 
 filter expression returns false. 
 The fourth node is a list of nodes that are rendered if the 
 filter expression returns a non-boolean value. 
 The fifth node is a list of nodes that are rendered if the 
 filter expression returns a boolean value. 
 The","'Filters the contents of the block through variable filters. 
 Filters can also be piped through each other, and they can have 
 arguments -- just like in variable syntax. 
 Sample usage:: 
 {% filter force_escape|lower %} 
 This text will be HTML-escaped, and will appear in lowercase. 
 {% endfilter %} 
 Note that the ``escape`` and ``safe`` filters are not acceptable arguments. 
 Instead, use the ``autoescape`` tag to manage autoescaping for blocks of 
 template code.'"
"def get_queue_names(app_id=None, max_rows=100): 
    rpc = apiproxy_stub_map.UserRPC('taskqueue') 
   request = taskqueue_service_pb.TaskQueueFetchQueuesRequest() 
   response = taskqueue_service_pb.TaskQueueFetchQueuesResponse() 
   if app_id: 
      request.set_app_id(app_id) 
   request.set_max_rows(max_rows) 
   queues = ['default'] 
   try: 
      rpc.make_call('FetchQueues', request, response) 
      rpc.check_success() 
      for queue in response.queue_list(): 
         if ((queue.mode() == taskqueue_service_pb.TaskQueueMode.PUSH) and (not queue.queue_name().startswith('__')) and (queue.queue_name() != 'default')): 
            queues.append(queue.queue_name()) 
   except Exception: 
      logging.exception('Failed   to   get   queue   names.') 
   return queues"," 'Get all the queue names for the given app_id. 
 :param app_id: The app_id to get the queues for. 
 :param max_rows: The maximum number of rows to return. 
 :returns: A list of queue names.'",'Returns a list with all non-special queue names for app_id.'
"def add_key_to_url(url, scheme, key): 
    query = request.args.to_dict() 
   query['view_only'] = key 
   replacements = {'query': urllib.urlencode(query)} 
   if scheme: 
      replacements['scheme'] = scheme 
   parsed_url = urlparse.urlparse(url) 
   if parsed_url.fragment: 
      replacements['path'] = '{}%23{}'.format(parsed_url.path, parsed_url.fragment) 
      replacements['fragment'] = '' 
   parsed_redirect_url = parsed_url._replace(**replacements) 
   return urlparse.urlunparse(parsed_redirect_url)"," 'Adds a key to the url. 
 :param url: The url to modify. 
 :param scheme: The scheme to use for the url. 
 :param key: The key to add to the url. 
 :return: The modified url.'",'Redirects the user to the requests URL with the given key appended to the query parameters.'
"def route_exists(destination_cidr_block, route_table_name=None, route_table_id=None, gateway_id=None, instance_id=None, interface_id=None, tags=None, region=None, key=None, keyid=None, profile=None, vpc_peering_connection_id=None): 
    if (not any((route_table_name, route_table_id))): 
      raise SaltInvocationError('At   least   one   of   the   following   must   be   specified:   route   table   name   or   route   table   id.') 
   if (not any((gateway_id, instance_id, interface_id, vpc_peering_connection_id))): 
      raise SaltInvocationError('At   least   one   of   the   following   must   be   specified:   gateway   id,   instance   id,   interface   id   or   VPC   peering   connection   id.') 
   try: 
      conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile) 
      filter_parameters = {'filters': {}} 
      if route_table_id: 
         filter_parameters['route_table_ids'] = [route_table_id] 
      if route_table_name: 
         filter_parameters['filters']['tag:Name'] = route_table_name 
      if tags: 
         for (tag_name, tag_value) in six.iteritems(tags): 
            filter_parameters['filters']['tag:{0}'.format(tag_name)] = tag_value 
      route_tables = conn.get_all_route_tables(**filter_parameters) 
      if (len(route_tables) != 1): 
         raise SaltInvocationError('Found   more   than   one   route   table.') 
      route_check = {'destination_cidr_block': destination_cidr_block, 'gateway_id': gateway_id, 'instance_id': instance_id, 'interface_id': interface_id, 'vpc_peering_connection_id': vpc_peering_connection_id} 
      for route_match in route_tables[0].routes: 
         route_dict = {'destination_cidr_block': route_match.destination_cidr_block, 'gateway_id': route_match.gateway_id, 'instance_id': route_match.instance_id, 'interface_id': route_match.interface_id, 'vpc_peering_connection_id': vpc_peering_connection_id} 
         route_comp = (set(route_dict.items()) ^ set(route_check.items())) 
         if (len(route_comp) == 0): 
            log.info('Route   {0}   exists.'.format(destination_cidr_block)) 
            return {'exists': True} 
      log.warning('Route   {0}   does   not   exist.'.format(destination_cidr_block)) 
      return {'exists': False} 
   except BotoServerError as e: 
      return {'error': salt.utils.boto.get_error(e)}"," 'Checks if the given route exists in the specified route table. 
 :param destination_cidr_block: The destination CIDR block. 
 :param route_table_name: The name of the route table. 
 :param route_table_id: The ID of the route table. 
 :param gateway_id: The ID of the gateway. 
 :param instance_id: The ID of the instance. 
 :param interface_id: The ID of the interface. 
 :param tags: A dictionary of tags to match. 
 :param region: The region. 
 :param key: The access key. 
 :param keyid: The access key ID. 
 :param profile: The profile to use. 
 :param vpc_peering_connection_id: The ID of the VPC peering connection. 
 :returns: True if the route exists, False otherwise.'","'Checks if a route exists. 
 .. versionadded:: 2015.8.0 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_vpc.route_exists destination_cidr_block=\'10.0.0.0/20\' gateway_id=\'local\' route_table_name=\'test\''"
"def protected(callback=None): 
    def wrapper(f): 
      @functools.wraps(f) 
      def inner(self, request, *args, **kwargs): 
         request.assert_authenticated() 
         if request.context.is_admin: 
            LOG.warning(_LW('RBAC:   Bypassing   authorization')) 
         elif (callback is not None): 
            prep_info = {'f_name': f.__name__, 'input_attr': kwargs} 
            callback(self, request, prep_info, *args, **kwargs) 
         else: 
            action = ('identity:%s' % f.__name__) 
            creds = _build_policy_check_credentials(self, action, request.context_dict, kwargs) 
            policy_dict = {} 
            if (hasattr(self, 'get_member_from_driver') and (self.get_member_from_driver is not None)): 
               key = ('%s_id' % self.member_name) 
               if (key in kwargs): 
                  ref = self.get_member_from_driver(kwargs[key]) 
                  policy_dict['target'] = {self.member_name: ref} 
            if (request.context_dict.get('subject_token_id') is not None): 
               window_seconds = self._token_validation_window(request) 
               token_ref = token_model.KeystoneToken(token_id=request.context_dict['subject_token_id'], token_data=self.token_provider_api.validate_token(request.context_dict['subject_token_id'], window_seconds=window_seconds)) 
               policy_dict.setdefault('target', {}) 
               policy_dict['target'].setdefault(self.member_name, {}) 
               policy_dict['target'][self.member_name]['user_id'] = token_ref.user_id 
               try: 
                  user_domain_id = token_ref.user_domain_id 
               except exception.UnexpectedError: 
                  user_domain_id = None 
               if user_domain_id: 
                  policy_dict['target'][self.member_name].setdefault('user', {}) 
                  policy_dict['target'][self.member_name]['user'].setdefault('domain', {}) 
                  policy_dict['target'][self.member_name]['user']['domain']['id'] = user_domain_id 
            policy_dict.update(kwargs) 
            self.policy_api.enforce(creds, action, utils.flatten_dict(policy_dict)) 
            LOG.debug('RBAC:   Authorization   granted') 
         return f(self, request, *args, **kwargs) 
      return inner 
   return wrapper"," 'Decorator to check that the request is authenticated and 
 that the user has the required role to access the resource. 
 :param callback: Function to call if the user is authorized. 
 :param request: Request to check authorization for. 
 :param args: Arguments to pass to the callback. 
 :param kwargs: Keyword arguments to pass to the callback. 
 :return: Wrapped function. 
 :rtype: function'","'Wrap API calls with role based access controls (RBAC). 
 This handles both the protection of the API parameters as well as any 
 target entities for single-entity API calls. 
 More complex API calls (for example that deal with several different 
 entities) should pass in a callback function, that will be subsequently 
 called to check protection for these multiple entities. This callback 
 function should gather the appropriate entities needed and then call 
 check_protection() in the V3Controller class.'"
"def _domain_variants(domain): 
    parts = domain.split('.') 
   for i in range(len(parts), 1, (-1)): 
      (yield '.'.join(parts[(- i):]))", 'Returns all the possible domain variants of a domain.',"'>>> list(_domain_variants(""foo.bar.example.com"")) 
 [\'foo.bar.example.com\', \'bar.example.com\', \'example.com\']'"
"def calculateDeltaSeconds(start): 
    return (time.time() - start)"," 'Calculate the difference between the start and end time of the 
 function.'",'Returns elapsed time from start till now'
"def _collect_dirs(start_dir, blacklist=set(['conftest.py', 'nox.py']), suffix='_test.py'): 
    for (parent, subdirs, files) in os.walk(start_dir): 
      if any((f for f in files if (f.endswith(suffix) and (f not in blacklist)))): 
         del subdirs[:] 
         (yield parent) 
      else: 
         subdirs[:] = [s for s in subdirs if (s[0].isalpha() and (os.path.join(parent, s) not in blacklist))]"," 'Collect all test directories that match the pattern. 
 :param start_dir: the directory to start from 
 :param blacklist: a list of directories to exclude 
 :param suffix: the suffix to match 
 :return: a generator of directories'","'Recursively collects a list of dirs that contain a file matching the 
 given suffix. 
 This works by listing the contents of directories and finding 
 directories that have `*_test.py` files.'"
"def expand_default(self, option): 
    if ((self.parser is None) or (not self.default_tag)): 
      return option.help 
   optname = option._long_opts[0][2:] 
   try: 
      provider = self.parser.options_manager._all_options[optname] 
   except KeyError: 
      value = None 
   else: 
      optdict = provider.get_option_def(optname) 
      optname = provider.option_attrname(optname, optdict) 
      value = getattr(provider.config, optname, optdict) 
      value = format_option_value(optdict, value) 
   if ((value is optik_ext.NO_DEFAULT) or (not value)): 
      value = self.NO_DEFAULT_VALUE 
   return option.help.replace(self.default_tag, str(value))"," 'Expands the default value of a option. 
 :param option: the option to expand 
 :type option: optik.Option 
 :return: the expanded default value of the option 
 :rtype: str'","'monkey patch OptionParser.expand_default since we have a particular 
 way to handle defaults to avoid overriding values in the configuration 
 file'"
"def password_option(*param_decls, **attrs): 
    def decorator(f): 
      attrs.setdefault('prompt', True) 
      attrs.setdefault('confirmation_prompt', True) 
      attrs.setdefault('hide_input', True) 
      return option(*(param_decls or ('--password',)), **attrs)(f) 
   return decorator", 'Decorator to add password option to a command.',"'Shortcut for password prompts. 
 This is equivalent to decorating a function with :func:`option` with 
 the following parameters:: 
 @click.command() 
 @click.option(\'--password\', prompt=True, confirmation_prompt=True, 
 hide_input=True) 
 def changeadmin(password): 
 pass'"
"def stop(name): 
    cmd = '/etc/rc.d/{0}   -f   stop'.format(name) 
   return (not __salt__['cmd.retcode'](cmd))", 'Stop a service',"'Stop the specified service 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' service.stop <service name>'"
"@click.command() 
 @click.option('--count', default=2, callback=validate_count, help='A   positive   even   number.') 
 @click.option('--foo', help='A   mysterious   parameter.') 
 @click.option('--url', help='A   URL', type=URL()) 
 @click.version_option() 
 def cli(count, foo, url): 
    if ((foo is not None) and (foo != 'wat')): 
      raise click.BadParameter('If   a   value   is   provided   it   needs   to   be   the   value   ""wat"".', param_hint=['--foo']) 
   click.echo(('count:   %s' % count)) 
   click.echo(('foo:   %s' % foo)) 
   click.echo(('url:   %s' % repr(url)))"," 'This is a simple example of how to use the click library to generate 
 documentation.'","'Validation. 
 This example validates parameters in different ways.  It does it 
 through callbacks, through a custom type as well as by validating 
 manually in the function.'"
"def _collectWarnings(observeWarning, f, *args, **kwargs): 
    def showWarning(message, category, filename, lineno, file=None, line=None): 
      assert isinstance(message, Warning) 
      observeWarning(_Warning(message.args[0], category, filename, lineno)) 
   for v in sys.modules.itervalues(): 
      if (v is not None): 
         try: 
            v.__warningregistry__ = None 
         except: 
            pass 
   origFilters = warnings.filters[:] 
   origShow = warnings.showwarning 
   warnings.simplefilter('always') 
   try: 
      warnings.showwarning = showWarning 
      result = f(*args, **kwargs) 
   finally: 
      warnings.filters[:] = origFilters 
      warnings.showwarning = origShow 
   return result"," 'Collects warnings, and then calls f(). 
 :param observeWarning: A function that takes a warning, and does something with it. 
 :param f: A function to call. 
 :param args: The arguments to pass to f. 
 :param kwargs: The keyword arguments to pass to f. 
 :return: The result of f()'","'Call C{f} with C{args} positional arguments and C{kwargs} keyword arguments 
 and collect all warnings which are emitted as a result in a list. 
 @param observeWarning: A callable which will be invoked with a L{_Warning} 
 instance each time a warning is emitted. 
 @return: The return value of C{f(*args, **kwargs)}.'"
"def comment_counts(user, all_comments, filediff, interfilediff=None): 
    comment_dict = {} 
   if interfilediff: 
      key = (filediff.pk, interfilediff.pk) 
   else: 
      key = (filediff.pk, None) 
   comments = all_comments.get(key, []) 
   for comment in comments: 
      review = comment.get_review() 
      if (review and (review.public or (review.user == user))): 
         key = (comment.first_line, comment.num_lines) 
         comment_dict.setdefault(key, []).append({u'comment_id': comment.id, u'text': normalize_text_for_edit(user, comment.text, comment.rich_text), u'html': markdown_render_conditional(comment.text, comment.rich_text), u'rich_text': comment.rich_text, u'line': comment.first_line, u'num_lines': comment.num_lines, u'user': {u'username': review.user.username, u'name': (review.user.get_full_name() or review.user.username)}, u'url': comment.get_review_url(), u'localdraft': ((review.user == user) and (not review.public)), u'review_id': review.id, u'issue_opened': comment.issue_opened, u'issue_status': BaseComment.issue_status_to_string(comment.issue_status), u'reply_to_id': comment.reply_to_id}) 
   comments_array = [] 
   for (key, value) in six.iteritems(comment_dict): 
      comments_array.append({u'linenum': key[0], u'num_lines': key[1], u'comments': value}) 
   comments_array.sort(cmp=(lambda x, y: cmp(x[u'linenum'], (y[u'linenum'] or cmp(x[u'num_lines'], y[u'num_lines']))))) 
   return comments_array"," 'Returns a list of comments with the following fields: 
 linenum: the line number of the comment 
 num_lines: the number of lines in the comment 
 comments: a list of comments 
 linenum: the line number of the comment 
 num_lines: the number of lines in the comment 
 comments: a list of comments 
 linenum: the line number of the comment 
 num_lines: the number of lines in the comment 
 comments: a list of comments 
 linenum: the line number of the comment 
 num_lines: the number of lines in the comment 
 comments: a list of comments 
 linenum: the line number of the comment 
 num_lines: the number of lines in the comment 
 comments: a list of comments 
 linenum: the line number of the comment 
 num_lines: the number of lines in the comment 
 comments: a list of comments 
 linenum: the line number of the comment 
 num_lines: the number of lines in the comment 
 comments: a list of comments 
 linenum: the line number of the comment 
 num_lines: the number of","'Returns an array of current comments for a filediff, sorted by line number. 
 Each entry in the array has a dictionary containing the following keys: 
 Key                Description 
 comment_id         The ID of the comment 
 text               The plain or rich text of the comment 
 rich_text          The rich text flag for the comment 
 line               The first line number 
 num_lines          The number of lines this comment spans 
 user               A dictionary containing ""username"" and ""name"" keys 
 for the user 
 url                The URL to the comment 
 localdraft         True if this is the current user\'s draft comment 
 review_id          The ID of the review this comment is associated with'"
"def req_match(): 
    return s3db.req_match()"," 'Returns the current request match. 
 This is the current request match, or None if no request match is 
 set. 
 .. versionadded:: 0.8.3'",'Match Requests for Sites'
"def _get_TV(codon_lst1, codon_lst2, codon_table=default_codon_table): 
    purine = ('A', 'G') 
   pyrimidine = ('C', 'T') 
   TV = [0, 0] 
   sites = 0 
   for (codon1, codon2) in zip(codon_lst1, codon_lst2): 
      if ('---' not in (codon1, codon2)): 
         for (i, j) in zip(codon1, codon2): 
            if (i == j): 
               pass 
            elif ((i in purine) and (j in purine)): 
               TV[0] += 1 
            elif ((i in pyrimidine) and (j in pyrimidine)): 
               TV[0] += 1 
            else: 
               TV[1] += 1 
            sites += 1 
   return ((TV[0] / sites), (TV[1] / sites))"," 'Calculates the TV score of two codon lists. 
 Parameters 
 codon_lst1 : list of strings 
 codon list 1 
 codon_lst2 : list of strings 
 codon list 2 
 codon_table : codon table 
 codon table to use for the calculation. 
 Returns 
 tuple 
 TV score of codon lists. 
 Examples 
 >>> from Bio.Seq import Seq 
 >>> from Bio.SeqFeature import SeqFeature 
 >>> from Bio.SeqUtils.CodonTable import default_codon_table 
 >>> from Bio.SeqUtils.CodonTable import _get_TV 
 >>> seq1 = Seq(""AGA"") 
 >>> seq2 = Seq(""TCC"") 
 >>> seq1.id = ""seq1"" 
 >>> seq2.id = ""seq2"" 
 >>> seq1.features = [SeqFeature(Seq(""A""), ""A"", ""a"", ""A"", ""A"")] 
 >>> seq2.features = [SeqFeature(Seq(""T""), ""T"", ""t"", ""T"", ""T"")] 
 >>> _get_","'Get TV (PRIVATE). 
 Arguments: 
 - T - proportions of transitional differences 
 - V - proportions of transversional differences'"
"def failing_checks(node_address, timeout=30): 
    ctx = ssl.create_default_context() 
   ctx.check_hostname = False 
   ctx.load_verify_locations(cafile='ssl/cacert.pem') 
   ctx.verify_mode = ssl.CERT_REQUIRED 
   url = 'https://{}:8500/v1/health/state/any'.format(node_address) 
   request = urllib2.Request(url) 
   auth = ('Basic   ' + base64.b64encode(get_credentials())) 
   request.add_header('Authorization', auth) 
   f = urllib2.urlopen(request, None, timeout, context=ctx) 
   checks = json.loads(f.read().decode('utf8')) 
   return [c for c in checks if (c.get('Status', '').lower() != 'passing')]", 'Checks for failing checks on a node','Returns a list of failing checks.'
"def batch_indices(batch_nb, data_length, batch_size): 
    start = int((batch_nb * batch_size)) 
   end = int(((batch_nb + 1) * batch_size)) 
   if (end > data_length): 
      shift = (end - data_length) 
      start -= shift 
      end -= shift 
   return (start, end)"," 'Return indices of the batch in the dataset. 
 Parameters 
 batch_nb : int 
 The number of batches in the dataset. 
 data_length : int 
 The length of the dataset. 
 batch_size : int 
 The batch size. 
 Returns 
 start, end : int 
 The start and end indices of the batch in the dataset. 
 Examples 
 >>> from sklearn.utils import batch_indices 
 >>> batch_indices(10, 100, 10) 
 (0, 9) 
 >>> batch_indices(10, 100, 100) 
 (0, 99) 
 >>> batch_indices(10, 100, 1000) 
 (0, 999)'","'This helper function computes a batch start and end index 
 :param batch_nb: the batch number 
 :param data_length: the total length of the data being parsed by batches 
 :param batch_size: the number of inputs in each batch 
 :return: pair of (start, end) indices'"
"def import_doc(d, doctype, overwrite, row_idx, submit=False, ignore_links=False): 
    if (d.get(u'name') and frappe.db.exists(doctype, d[u'name'])): 
      if overwrite: 
         doc = frappe.get_doc(doctype, d[u'name']) 
         doc.flags.ignore_links = ignore_links 
         doc.update(d) 
         if (d.get(u'docstatus') == 1): 
            doc.update_after_submit() 
         else: 
            doc.save() 
         return (u'Updated   row   (#%d)   %s' % ((row_idx + 1), getlink(doctype, d[u'name']))) 
      else: 
         return (u'Ignored   row   (#%d)   %s   (exists)' % ((row_idx + 1), getlink(doctype, d[u'name']))) 
   else: 
      doc = frappe.get_doc(d) 
      doc.flags.ignore_links = ignore_links 
      doc.insert() 
      if submit: 
         doc.submit() 
      return (u'Inserted   row   (#%d)   %s' % ((row_idx + 1), getlink(doctype, doc.get(u'name'))))"," 'Import a row into the specified doc type. 
 :param d: dict with the row to import 
 :param doctype: doc type to import the row into 
 :param overwrite: True if the row should overwrite an existing row 
 :param row_idx: row index to import (starting from 0) 
 :param submit: if True, submit the row after inserting 
 :param ignore_links: if True, ignore links in the row 
 :return: a string describing what happened'",'import main (non child) document'
"def fmtstr_payload(offset, writes, numbwritten=0, write_size='byte'): 
    config = {32: {'byte': (4, 1, 255, 'hh', 8), 'short': (2, 2, 65535, 'h', 16), 'int': (1, 4, 4294967295, '', 32)}, 64: {'byte': (8, 1, 255, 'hh', 8), 'short': (4, 2, 65535, 'h', 16), 'int': (2, 4, 4294967295, '', 32)}} 
   if (write_size not in ['byte', 'short', 'int']): 
      log.error(""write_size   must   be   'byte',   'short'   or   'int'"") 
   (number, step, mask, formatz, decalage) = config[context.bits][write_size] 
   payload = '' 
   for (where, what) in writes.items(): 
      for i in range(0, (number * step), step): 
         payload += pack((where + i)) 
   numbwritten += len(payload) 
   fmtCount = 0 
   for (where, what) in writes.items(): 
      for i in range(0, number): 
         current = (what & mask) 
         if ((numbwritten & mask) <= current): 
            to_add = (current - (numbwritten & mask)) 
         else: 
            to_add = ((current | (mask + 1)) - (numbwritten & mask)) 
         if (to_add != 0): 
            payload += '%{}c'.format(to_add) 
         payload += '%{}${}n'.format((offset + fmtCount), formatz) 
         numbwritten += to_add 
         what >>= decalage 
         fmtCount += 1 
   return payload"," 'Generate a payload for a write operation. 
 :param offset: the offset of the write operation 
 :param writes: a dict of (write_type, value) 
 :param numbwritten: the number of bytes written 
 :param write_size: the size of the write, can be byte, short or int'","'fmtstr_payload(offset, writes, numbwritten=0, write_size=\'byte\') -> str 
 Makes payload with given parameter. 
 It can generate payload for 32 or 64 bits architectures. 
 The size of the addr is taken from ``context.bits`` 
 Arguments: 
 offset(int): the first formatter\'s offset you control 
 writes(dict): dict with addr, value ``{addr: value, addr2: value2}`` 
 numbwritten(int): number of byte already written by the printf function 
 write_size(str): must be ``byte``, ``short`` or ``int``. Tells if you want to write byte by byte, short by short or int by int (hhn, hn or n) 
 Returns: 
 The payload in order to do needed writes 
 Examples: 
 >>> context.clear(arch = \'amd64\') 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'int\')) 
 \'\x00\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00%322419374c%1$n%3972547906c%2$n\' 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'short\')) 
 \'\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00%47774c%1$hn%22649c%2$hn%60617c%3$hn%4$hn\' 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'byte\')) 
 \'\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00\x00\x00\x00\x00\x00%126c%1$hhn%252c%2$hhn%125c%3$hhn%220c%4$hhn%237c%5$hhn%6$hhn%7$hhn%8$hhn\' 
 >>> context.clear(arch = \'i386\') 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'int\')) 
 \'\x00\x00\x00\x00%322419386c%1$n\' 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'short\')) 
 \'\x00\x00\x00\x00\x02\x00\x00\x00%47798c%1$hn%22649c%2$hn\' 
 >>> print repr(fmtstr_payload(1, {0x0: 0x1337babe}, write_size=\'byte\')) 
 \'\x00\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00%174c%1$hhn%252c%2$hhn%125c%3$hhn%220c%4$hhn\''"
"def makeTerms(nodes, title='Node', term='xterm'): 
    terms = [] 
   for node in nodes: 
      terms += makeTerm(node, title, term) 
   return terms"," 'Generate a list of terms for the given nodes. 
 The terms are generated by calling the term generator 
 for each node, then concatenating the results. 
 Parameters 
 nodes : list of nodes 
 title : str 
 title of the terms 
 term : str 
 term generator 
 Returns 
 list of terms 
 Examples 
 >>> from sympy.utilities.term import makeTerms 
 >>> from sympy import symbols, Add, sin, cos 
 >>> from sympy.abc import x 
 >>> terms = makeTerms([x, sin(x), cos(x)]) 
 >>> terms 
 [x, sin(x), cos(x)] 
 See Also 
 makeTerm'","'Create terminals. 
 nodes: list of Node objects 
 title: base title for each 
 returns: list of created tunnel/terminal processes'"
"def set_using_network_time(enable): 
    state = salt.utils.mac_utils.validate_enabled(enable) 
   cmd = 'systemsetup   -setusingnetworktime   {0}'.format(state) 
   salt.utils.mac_utils.execute_return_success(cmd) 
   return (state == salt.utils.mac_utils.validate_enabled(get_using_network_time()))"," 'Set the Mac OS X system clock to use the network time server. 
 This will only work on Mac OS X 10.5 and newer. 
 This function is only available on Mac OS X 10.6 and newer. 
 .. versionadded:: 0.17.0'","'Set whether network time is on or off. 
 :param enable: True to enable, False to disable. Can also use \'on\' or \'off\' 
 :type: str bool 
 :return: True if successful, False if not 
 :rtype: bool 
 :raises: CommandExecutionError on failure 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' timezone.set_using_network_time True'"
"def read_cache(stream): 
    (version, num_entries) = read_header(stream) 
   count = 0 
   entries = dict() 
   read = stream.read 
   tell = stream.tell 
   while (count < num_entries): 
      beginoffset = tell() 
      ctime = unpack('>8s', read(8))[0] 
      mtime = unpack('>8s', read(8))[0] 
      (dev, ino, mode, uid, gid, size, sha, flags) = unpack('>LLLLLL20sH', read(((20 + (4 * 6)) + 2))) 
      path_size = (flags & CE_NAMEMASK) 
      path = read(path_size).decode(defenc) 
      real_size = (((tell() - beginoffset) + 8) & (~ 7)) 
      read(((beginoffset + real_size) - tell())) 
      entry = IndexEntry((mode, sha, flags, path, ctime, mtime, dev, ino, uid, gid, size)) 
      entries[(path, entry.stage)] = entry 
      count += 1 
   extension_data = stream.read((~ 0)) 
   assert (len(extension_data) > 19), ('Index   Footer   was   not   at   least   a   sha   on   content   as   it   was   only   %i   bytes   in   size' % len(extension_data)) 
   content_sha = extension_data[(-20):] 
   extension_data = extension_data[:(-20)] 
   return (version, entries, extension_data, content_sha)"," 'Reads the index. 
 :param stream: The file stream to read the index from. 
 :return: A tuple containing the version number, a dictionary of 
 IndexEntry objects, the extension data, and the content sha.'","'Read a cache file from the given stream 
 :return: tuple(version, entries_dict, extension_data, content_sha) 
 * version is the integer version number 
 * entries dict is a dictionary which maps IndexEntry instances to a path at a stage 
 * extension_data is \'\' or 4 bytes of type + 4 bytes of size + size bytes 
 * content_sha is a 20 byte sha on all cache file contents'"
"def test_construction(): 
    s3_deleter.Deleter()", 'Test construction','The constructor basically works.'
"def getGeometryOutput(elementNode): 
    derivation = HeightmapDerivation(elementNode) 
   heightGrid = derivation.heightGrid 
   if (derivation.fileName != ''): 
      heightGrid = getHeightGrid(archive.getAbsoluteFolderPath(elementNode.getOwnerDocument().fileName, derivation.fileName)) 
   return getGeometryOutputByHeightGrid(derivation, elementNode, heightGrid)"," 'Returns the geometry output of the given element node. 
 :param elementNode: the element node 
 :return: the geometry output'",'Get vector3 vertexes from attribute dictionary.'
"def maybe_download(filename, work_directory): 
    if (not os.path.exists(work_directory)): 
      os.mkdir(work_directory) 
   filepath = os.path.join(work_directory, filename) 
   if (not os.path.exists(filepath)): 
      (filepath, _) = urllib.urlretrieve((SOURCE_URL + filename), filepath) 
      statinfo = os.stat(filepath) 
      print('Succesfully   downloaded', filename, statinfo.st_size, 'bytes.') 
   return filepath"," 'Download a file and return the path to it. 
 :param filename: The name of the file to download. 
 :param work_directory: The directory to put the downloaded file. 
 :return: The path to the downloaded file.'","'Download the data from Yann\'s website, unless it\'s already here.'"
"def _set_rpm_probes(probes): 
    return __salt__['probes.set_probes'](_ordered_dict_to_dict(probes), commit=False)"," 'Set the rpm probes. 
 :param probes: 
 The probes to set. 
 :return: 
 The rpm probes as a dictionary.'","'Calls the Salt module ""probes"" to configure the probes on the device.'"
"def find_subsections(section): 
    result = [] 
   for child in section.children: 
      if isinstance(child, nodes.section): 
         result.append(child) 
         continue 
      result.extend(find_subsections(child)) 
   return result", 'Return a list of all the subsections of the given section.','Return a list of subsections for the given ``section``.'
"def recreate_tags_from_list(list_of_tags): 
    tags = list() 
   i = 0 
   list_of_tags = list_of_tags 
   for i in range(len(list_of_tags)): 
      key_name = list_of_tags[i][0] 
      key_val = list_of_tags[i][1] 
      tags.append({'Key': key_name, 'Value': key_val}) 
   return tags", 'Recreate tags from list',"'Recreate tags from a list of tuples into the Amazon Tag format. 
 Args: 
 list_of_tags (list): List of tuples. 
 Basic Usage: 
 >>> list_of_tags = [(\'Env\', \'Development\')] 
 >>> recreate_tags_from_list(list_of_tags) 
 ""Value"": ""Development"", 
 ""Key"": ""Env"" 
 Returns: 
 List'"
"def parse_sphinx_searchindex(searchindex): 
    def _select_block(str_in, start_tag, end_tag): 
      'Select   first   block   delimited   by   start_tag   and   end_tag' 
      start_pos = str_in.find(start_tag) 
      if (start_pos < 0): 
         raise ValueError('start_tag   not   found') 
      depth = 0 
      for pos in range(start_pos, len(str_in)): 
         if (str_in[pos] == start_tag): 
            depth += 1 
         elif (str_in[pos] == end_tag): 
            depth -= 1 
         if (depth == 0): 
            break 
      sel = str_in[(start_pos + 1):pos] 
      return sel 
   def _parse_dict_recursive(dict_str): 
      'Parse   a   dictionary   from   the   search   index' 
      dict_out = dict() 
      pos_last = 0 
      pos = dict_str.find(':') 
      while (pos >= 0): 
         key = dict_str[pos_last:pos] 
         if (dict_str[(pos + 1)] == '['): 
            pos_tmp = dict_str.find(']', (pos + 1)) 
            if (pos_tmp < 0): 
               raise RuntimeError('error   when   parsing   dict') 
            value = dict_str[(pos + 2):pos_tmp].split(',') 
            for i in range(len(value)): 
               try: 
                  value[i] = int(value[i]) 
               except ValueError: 
                  pass 
         elif (dict_str[(pos + 1)] == '{'): 
            subdict_str = _select_block(dict_str[pos:], '{', '}') 
            value = _parse_dict_recursive(subdict_str) 
            pos_tmp = (pos + len(subdict_str)) 
         else: 
            raise ValueError('error   when   parsing   dict:   unknown   elem') 
         key = key.strip('""') 
         if (len(key) > 0): 
            dict_out[key] = value 
         pos_last = dict_str.find(',', pos_tmp) 
         if (pos_last < 0): 
            break 
         pos_last += 1 
         pos = dict_str.find(':', pos_last) 
      return dict_out 
   query = 'objects:' 
   pos = searchindex.find(query) 
   if (pos < 0): 
      raise ValueError('""objects:""   not   found   in   search   index') 
   sel = _select_block(searchindex[pos:], '{', '}') 
   objects = _parse_dict_recursive(sel) 
   query = 'filenames:' 
   pos = searchindex.find(query) 
   if (pos < 0): 
      raise ValueError('""filenames:""   not   found   in   search   index') 
   filenames = searchindex[((pos + len(query)) + 1):] 
   filenames = filenames[:filenames.find(']')] 
   filenames = [f.strip('""') for f in filenames.split(',')] 
   return (filenames, objects)"," 'Parses the searchindex from sphinx. 
 :param searchindex: The searchindex from sphinx. 
 :return: A tuple of (filenames, objects). 
 :rtype: tuple'","'Parse a Sphinx search index 
 Parameters 
 searchindex : str 
 The Sphinx search index (contents of searchindex.js) 
 Returns 
 filenames : list of str 
 The file names parsed from the search index. 
 objects : dict 
 The objects parsed from the search index.'"
"def _get_deep(data_structure, dot_path_or_list, default_value=None): 
    search_path = None 
   param_type = type(dot_path_or_list) 
   if (param_type in (tuple, list)): 
      search_path = dot_path_or_list 
   elif (param_type == str): 
      search_path = dot_path_or_list.split('.') 
   assert (len(search_path) > 0), 'Missing   valid   search   path' 
   try: 
      current_item = data_structure 
      for search_key in search_path: 
         current_item = current_item[search_key] 
   except (KeyError, IndexError, TypeError): 
      return default_value 
   return current_item"," 'Returns the value of a dot path or list of dot paths. 
 If the path is a list, it will be searched in order. 
 If the path is a string, it will be split into a list and searched 
 in order. 
 :param data_structure: The data structure to search in. 
 :param dot_path_or_list: A dot path or list of dot paths to search. 
 :param default_value: The default value to return if the path does 
 not exist. 
 :return: The value of the dot path.'","'Attempts access nested data structures and not blow up on a gross key 
 error 
 ""hello"": { 
 ""hi"": 5'"
"def strip_unneeded(bkts, sufficient_funds): 
    bkts = sorted(bkts, key=(lambda bkt: bkt.value)) 
   for i in range(len(bkts)): 
      if (not sufficient_funds(bkts[(i + 1):])): 
         return bkts[i:] 
   return bkts"," 'Return a list of bkts that are sufficient for the given funds. 
 :param bkts: List of bkts. 
 :type bkts: list(Bucket) 
 :param sufficient_funds: Function that takes a list of bkts and returns 
 True if the given bkts are sufficient for the given funds. 
 :type sufficient_funds: callable 
 :return: List of bkts that are sufficient for the given funds. 
 :rtype: list(Bucket)'",'Remove buckets that are unnecessary in achieving the spend amount'
"def run(cmd, **kwargs): 
    log('-', cmd) 
   cmd = cmd.split() 
   arg0 = cmd[0] 
   if (not find_executable(arg0)): 
      raise Exception((('Cannot   find   executable   ""%s"";' % arg0) + ('you   might   try   %s   --depend' % argv[0]))) 
   return check_output(cmd, **kwargs)", 'Run a command with the given arguments.','Convenient interface to check_output'
"@treeio_login_required 
 def ajax_object_lookup(request, response_format='html'): 
    objects = [] 
   if (request.GET and ('term' in request.GET)): 
      objects = Object.filter_permitted(request.user.profile, Object.objects.filter(object_name__icontains=request.GET['term']), mode='x')[:10] 
   return render_to_response('core/ajax_object_lookup', {'objects': objects}, context_instance=RequestContext(request), response_format=response_format)"," 'AJAX-friendly object lookup. 
 :param response_format: The format of the response. 
 :type response_format: str 
 :return: The response.'",'Returns a list of matching objects'
"def scope2index(scope, descr, where=None): 
    try: 
      return scopes.index(scope) 
   except ValueError: 
      raise ValueError(""{0}   {1}has   an   unsupported   scope   value   '{2}'"".format(descr, ('from   {0}   '.format(where) if where else ''), scope))"," 'Returns the index of the given scope in the scopes dictionary. 
 :param scope: 
 :type scope: 
 :param descr: 
 :type descr: 
 :param where: 
 :type where: 
 :return: 
 :rtype: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises: 
 :type: 
 :raises","'Look up the index of ``scope`` and raise a descriptive value error 
 if not defined.'"
"def get_context(context): 
    host = get_request_site_address() 
   blog_list = frappe.db.sql(u' DCTB  DCTB select   route   as   name,   published_on,   modified,   title,   content   from   `tabBlog   Post`\n DCTB  DCTB where   ifnull(published,0)=1\n DCTB  DCTB order   by   published_on   desc   limit   20', as_dict=1) 
   for blog in blog_list: 
      blog_page = cstr(urllib.quote(blog.route.encode(u'utf-8'))) 
      blog.link = urllib.basejoin(host, blog_page) 
      blog.content = escape_html((blog.content or u'')) 
   if blog_list: 
      modified = max((blog[u'modified'] for blog in blog_list)) 
   else: 
      modified = now() 
   blog_settings = frappe.get_doc(u'Blog   Settings', u'Blog   Settings') 
   context = {u'title': (blog_settings.blog_title or u'Blog'), u'description': (blog_settings.blog_introduction or u''), u'modified': modified, u'items': blog_list, u'link': (host + u'/blog')} 
   return context", 'Returns a dictionary containing the blog page context.','generate rss feed'
"def publish_cmdline(reader=None, reader_name='standalone', parser=None, parser_name='restructuredtext', writer=None, writer_name='pseudoxml', settings=None, settings_spec=None, settings_overrides=None, config_section=None, enable_exit_status=True, argv=None, usage=default_usage, description=default_description): 
    pub = Publisher(reader, parser, writer, settings=settings) 
   pub.set_components(reader_name, parser_name, writer_name) 
   output = pub.publish(argv, usage, description, settings_spec, settings_overrides, config_section=config_section, enable_exit_status=enable_exit_status) 
   return output"," 'Publish a command line interface. 
 :param reader: The reader to use. 
 :type reader: :class:`Reader` 
 :param reader_name: The name of the reader. 
 :type reader_name: str 
 :param parser: The parser to use. 
 :type parser: :class:`Parser` 
 :param parser_name: The name of the parser. 
 :type parser_name: str 
 :param writer: The writer to use. 
 :type writer: :class:`Writer` 
 :param writer_name: The name of the writer. 
 :type writer_name: str 
 :param settings: The settings to use. 
 :type settings: :class:`Settings` 
 :param settings_spec: The settings specification. 
 :type settings_spec: :class:`SettingsSpec` 
 :param settings_overrides: The settings overrides. 
 :type settings_overrides: :class:`SettingsOverrides` 
 :param config_section: The config section. 
 :type config_section: str 
 :param enable_exit_status: Whether to enable exit","'Set up & run a `Publisher` for command-line-based file I/O (input and 
 output file paths taken automatically from the command line).  Return the 
 encoded string output also. 
 Parameters: see `publish_programmatically` for the remainder. 
 - `argv`: Command-line argument list to use instead of ``sys.argv[1:]``. 
 - `usage`: Usage string, output if there\'s a problem parsing the command 
 line. 
 - `description`: Program description, output for the ""--help"" option 
 (along with command-line option descriptions).'"
"def user_remove(name, database=None, user=None, password=None, host=None, port=None): 
    if (not user_exists(name, database, user, password, host, port)): 
      if database: 
         log.info(""User   '{0}'   does   not   exist   for   DB   '{1}'"".format(name, database)) 
      else: 
         log.info(""Cluster   admin   '{0}'   does   not   exist"".format(name)) 
      return False 
   client = _client(user=user, password=password, host=host, port=port) 
   if (not database): 
      return client.delete_cluster_admin(name) 
   client.switch_database(database) 
   return client.delete_database_user(name)"," 'Remove a user from the cluster. 
 :param name: The name of the user to remove. 
 :param database: The database to remove the user from. 
 :param user: The username of the user to remove. 
 :param password: The password of the user to remove. 
 :param host: The hostname of the server to connect to. 
 :param port: The port to connect to. 
 :return: True if the user was removed, False otherwise.'","'Remove a cluster admin or a database user. 
 If a database is specified: it will remove the database user. 
 If a database is not specified: it will remove the cluster admin. 
 name 
 User name to remove 
 database 
 The database to remove the user from 
 user 
 User name for the new user to delete 
 user 
 The user to connect as 
 password 
 The password of the user 
 host 
 The host to connect to 
 port 
 The port to connect to 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' influxdb08.user_remove <name> 
 salt \'*\' influxdb08.user_remove <name> <database> 
 salt \'*\' influxdb08.user_remove <name> <database> <user> <password> <host> <port>'"
"def get_base_domain(url): 
    domain = get_domain(url) 
   tld = '.'.join(domain.split('.')[(-2):]) 
   if (tld in ['co.uk', 'com.au', 'au.com']): 
      end_chunks = domain.split('.')[(-3):] 
   else: 
      end_chunks = domain.split('.')[(-2):] 
   base_domain = '.'.join(end_chunks) 
   return base_domain"," 'Get the base domain from a URL. 
 :param url: The URL to parse 
 :return: The base domain'","'For example, the base url of uk.reuters.com => reuters.com'"
"def relativize_classpath(classpath, root_dir, followlinks=True): 
    def relativize_url(url, root_dir): 
      url = (os.path.realpath(url) if followlinks else url) 
      root_dir = (os.path.realpath(root_dir) if followlinks else root_dir) 
      url_in_bundle = os.path.relpath(url, root_dir) 
      if os.path.isdir(url): 
         url_in_bundle += u'/' 
      return url_in_bundle 
   return [relativize_url(url, root_dir) for url in classpath]"," 'Return a list of classpaths, relative to the root_dir.'","'Convert into classpath relative to a directory. 
 This is eventually used by a jar file located in this directory as its manifest 
 attribute Class-Path. See 
 https://docs.oracle.com/javase/7/docs/technotes/guides/extensions/spec.html#bundled 
 :param list classpath: Classpath to be relativized. 
 :param string root_dir: directory to relativize urls in the classpath, does not 
 have to exist yet. 
 :param bool followlinks: whether to follow symlinks to calculate relative path. 
 :returns: Converted classpath of the same size as input classpath. 
 :rtype: list of strings'"
"def byte_compile(py_files, optimize=0, force=0, prefix=None, base_dir=None, verbose=1, dry_run=0, direct=None): 
    if sys.dont_write_bytecode: 
      raise DistutilsByteCompileError('byte-compiling   is   disabled.') 
   if (direct is None): 
      direct = (__debug__ and (optimize == 0)) 
   if (not direct): 
      try: 
         from tempfile import mkstemp 
         (script_fd, script_name) = mkstemp('.py') 
      except ImportError: 
         from tempfile import mktemp 
         (script_fd, script_name) = (None, mktemp('.py')) 
      log.info(""writing   byte-compilation   script   '%s'"", script_name) 
      if (not dry_run): 
         if (script_fd is not None): 
            script = os.fdopen(script_fd, 'w') 
         else: 
            script = open(script_name, 'w') 
         script.write('from   distutils.util   import   byte_compile\nfiles   =   [\n') 
         script.write((string.join(map(repr, py_files), ',\n') + ']\n')) 
         script.write(('\nbyte_compile(files,   optimize=%r,   force=%r,\n                                       prefix=%r,   base_dir=%r,\n                                       verbose=%r,   dry_run=0,\n                                       direct=1)\n' % (optimize, force, prefix, base_dir, verbose))) 
         script.close() 
      cmd = [sys.executable, script_name] 
      if (optimize == 1): 
         cmd.insert(1, '-O') 
      elif (optimize == 2): 
         cmd.insert(1, '-OO') 
      spawn(cmd, dry_run=dry_run) 
      execute(os.remove, (script_name,), ('removing   %s' % script_name), dry_run=dry_run) 
   else: 
      from py_compile import compile 
      for file in py_files: 
         if (file[(-3):] != '.py'): 
            continue 
         cfile = (file + ((__debug__ and 'c') or 'o')) 
         dfile = file 
         if prefix: 
            if (file[:len(prefix)] != prefix): 
               raise ValueError, (""invalid   prefix:   filename   %r   doesn't   start   with   %r"" % (file, prefix)) 
            dfile = dfile[len(prefix):] 
         if base_dir: 
            dfile = os.path.join(base_dir, dfile) 
         cfile_base = os.path.basename(cfile) 
         if direct: 
            if (force or newer(file, cfile)): 
               log.info('byte-compiling   %s   to   %s', file, cfile_base) 
               if (not dry_run): 
                  compile(file, cfile, dfile) 
            else: 
               log.debug('skipping   byte-compilation   of   %s   to   %s', file, cfile_base)"," 'Byte-compile a list of Python files. 
 The files in py_files are compiled into bytecode and written to 
 the files in the base_dir. 
 The files in py_files are compiled into bytecode and written to 
 the files in the base_dir. 
 If force is true, then the bytecode is written even if the files 
 have not changed since the last time they were compiled. 
 If verbose is true, then the bytecode is written to stdout. 
 If dry_run is true, then the bytecode is not written to files. 
 If direct is true, then the bytecode is written to the same file 
 as the source. 
 If prefix is specified, then the base_dir is prepended to the 
 source file names. 
 If base_dir is specified, then the bytecode is written to the 
 files in base_dir. 
 The bytecode is written to files with names of the form 
 <prefix>.<file>.<optimization>.<extension>.o. 
 The <file> portion of the name is the original source file name, 
 with any extension removed. 
 The","'Byte-compile a collection of Python source files to either .pyc 
 or .pyo files in the same directory.  \'py_files\' is a list of files 
 to compile; any files that don\'t end in "".py"" are silently skipped. 
 \'optimize\' must be one of the following: 
 0 - don\'t optimize (generate .pyc) 
 1 - normal optimization (like ""python -O"") 
 2 - extra optimization (like ""python -OO"") 
 If \'force\' is true, all files are recompiled regardless of 
 timestamps. 
 The source filename encoded in each bytecode file defaults to the 
 filenames listed in \'py_files\'; you can modify these with \'prefix\' and 
 \'basedir\'.  \'prefix\' is a string that will be stripped off of each 
 source filename, and \'base_dir\' is a directory name that will be 
 prepended (after \'prefix\' is stripped).  You can supply either or both 
 (or neither) of \'prefix\' and \'base_dir\', as you wish. 
 If \'dry_run\' is true, doesn\'t actually do anything that would 
 affect the filesystem. 
 Byte-compilation is either done directly in this interpreter process 
 with the standard py_compile module, or indirectly by writing a 
 temporary script and executing it.  Normally, you should let 
 \'byte_compile()\' figure out to use direct compilation or not (see 
 the source for details).  The \'direct\' flag is used by the script 
 generated in indirect mode; unless you know what you\'re doing, leave 
 it set to None.'"
"def HoursSince(timestamp): 
    return (SecondsSince(timestamp) / 3600.0)", 'Return the number of hours since the given timestamp.','Hours since a given timestamp. Floating point.'
"def volume_present(name, volume_size, sparse=False, create_parent=False, properties=None, cloned_from=None): 
    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''} 
   if (not properties): 
      properties = {} 
   log.debug('zfs.volume_present::{0}::config::volume_size   =   {1}'.format(name, volume_size)) 
   log.debug('zfs.volume_present::{0}::config::sparse   =   {1}'.format(name, sparse)) 
   log.debug('zfs.volume_present::{0}::config::create_parent   =   {1}'.format(name, create_parent)) 
   log.debug('zfs.volume_present::{0}::config::cloned_from   =   {1}'.format(name, cloned_from)) 
   log.debug('zfs.volume_present::{0}::config::properties   =   {1}'.format(name, properties)) 
   for prop in properties.keys(): 
      if isinstance(properties[prop], bool): 
         properties[prop] = ('on' if properties[prop] else 'off') 
   if (('@' in name) or ('#' in name)): 
      ret['result'] = False 
      ret['comment'] = 'invalid   filesystem   or   volume   name:   {0}'.format(name) 
   if cloned_from: 
      cloned_parent = cloned_from[:cloned_from.index('@')] 
      if ('@' not in cloned_from): 
         ret['result'] = False 
         ret['comment'] = '{0}   is   not   a   snapshot'.format(cloned_from) 
      elif (cloned_from not in __salt__['zfs.list'](cloned_from, **{'type': 'snapshot'})): 
         ret['result'] = False 
         ret['comment'] = 'snapshot   {0}   does   not   exist'.format(cloned_from) 
      elif (cloned_parent not in __salt__['zfs.list'](cloned_parent, **{'type': 'volume'})): 
         ret['result'] = False 
         ret['comment'] = 'snapshot   {0}   is   not   from   a   volume'.format(cloned_from) 
   if ret['result']: 
      if (name in __salt__['zfs.list'](name, **{'type': 'volume'})): 
         properties['volsize'] = volume_size 
         result = __salt__['zfs.get'](name, **{'properties': ','.join(properties.keys()), 'fields': 'value', 'depth': 1}) 
         for prop in properties.keys(): 
            if (properties[prop] != result[name][prop]['value']): 
               if (name not in ret['changes']): 
                  ret['changes'][name] = {} 
               ret['changes'][name][prop] = properties[prop] 
         if (len(ret['changes']) > 0): 
            if (not __opts__['test']): 
               result = __salt__['zfs.set'](name, **ret['changes'][name]) 
               if (name not in result): 
                  ret['result'] = False 
               else: 
                  for prop in result[name].keys(): 
                     if (result[name][prop] != 'set'): 
                        ret['result'] = False 
            if ret['result']: 
               ret['comment'] = 'volume   {0}   was   updated'.format(name) 
            else: 
               ret['changes'] = {} 
               ret['comment'] = 'volume   {0}   failed   to   be   updated'.format(name) 
         else: 
            ret['comment'] = 'volume   {0}   is   up   to   date'.format(name) 
      else: 
         result = {name: 'created'} 
         if (not __opts__['test']): 
            if (not cloned_from): 
               result = __salt__['zfs.create'](name, **{'volume_size': volume_size, 'sparse': sparse, 'create_parent': create_parent, 'properties': properties}) 
            else: 
               result = __salt__['zfs.clone'](cloned_from, name, **{'create_parent': create_parent, 'properties': properties}) 
         ret['result'] = (name in result) 
         if ret['result']: 
            ret['result'] = ((result[name] == 'created') or result[name].startswith('cloned')) 
         if ret['result']: 
            ret['changes'][name] = (properties if (len(properties) > 0) else result[name]) 
            ret['comment'] = 'volume   {0}   was   created'.format(name) 
         else: 
            ret['comment'] = 'failed   to   create   volume   {0}'.format(name) 
            if (name in result): 
               ret['comment'] = result[name] 
   return ret"," 'Check if a volume exists. 
 If a volume does not exist, it will be created. 
 If a volume exists, it will be updated. 
 The volume will be created or updated with the given properties. 
 :param name: Name of the volume to check for 
 :param volume_size: Size of the volume to create or update 
 :param sparse: Set the volume to sparse 
 :param create_parent: Create a parent volume if it does not exist 
 :param properties: Properties to set on the volume 
 :param cloned_from: Clone a volume from another volume 
 :return: Dictionary with the following structure: 
 { 
 \'name\': Name of the volume, 
 \'changes\': Dictionary of properties that were changed 
 \'result\': True or False 
 \'comment\': Comment explaining what happened 
 }'","'ensure volume exists and has properties set 
 name : string 
 name of volume 
 volume_size : string 
 size of volume 
 sparse : boolean 
 create sparse volume 
 create_parent : boolean 
 creates all the non-existing parent datasets. 
 any property specified on the command line using the -o option is ignored. 
 cloned_from : string 
 name of snapshot to clone 
 properties : dict 
 additional zfs properties (-o) 
 .. note:: 
 ``cloned_from`` is only use if the volume does not exist yet, 
 when ``cloned_from`` is set after the volume exists it will be ignored. 
 .. note:: 
 Properties do not get cloned, if you specify the properties in the state file 
 they will be applied on a subsequent run. 
 ``volume_size`` is considered a property, so the volume\'s size will be 
 corrected when the properties get updated if it differs from the 
 original volume. 
 The sparse parameter is ignored when using ``cloned_from``.'"
"def prepare_grant_uri(uri, client_id, response_type, redirect_uri=None, scope=None, state=None, **kwargs): 
    if (not is_secure_transport(uri)): 
      raise InsecureTransportError() 
   params = [(u'response_type', response_type), (u'client_id', client_id)] 
   if redirect_uri: 
      params.append((u'redirect_uri', redirect_uri)) 
   if scope: 
      params.append((u'scope', list_to_scope(scope))) 
   if state: 
      params.append((u'state', state)) 
   for k in kwargs: 
      if kwargs[k]: 
         params.append((unicode_type(k), kwargs[k])) 
   return add_params_to_uri(uri, params)"," 'Prepare a grant URI for authorization. 
 :param uri: The URI to which the authorization request will be sent. 
 :param client_id: The client ID. 
 :param response_type: The response type. 
 :param redirect_uri: The redirect URI. 
 :param scope: The scope. 
 :param state: The state. 
 :param kwargs: Keyword arguments to be added to the URI. 
 :returns: The URI with the prepared parameters.'","'Prepare the authorization grant request URI. 
 The client constructs the request URI by adding the following 
 parameters to the query component of the authorization endpoint URI 
 using the ``application/x-www-form-urlencoded`` format as defined by 
 [`W3C.REC-html401-19991224`_]: 
 :param response_type: To indicate which OAuth 2 grant/flow is required, 
 ""code"" and ""token"". 
 :param client_id: The client identifier as described in `Section 2.2`_. 
 :param redirect_uri: The client provided URI to redirect back to after 
 authorization as described in `Section 3.1.2`_. 
 :param scope: The scope of the access request as described by 
 `Section 3.3`_. 
 :param state: An opaque value used by the client to maintain 
 state between the request and callback.  The authorization 
 server includes this value when redirecting the user-agent 
 back to the client.  The parameter SHOULD be used for 
 preventing cross-site request forgery as described in 
 `Section 10.12`_. 
 :param kwargs: Extra arguments to embed in the grant/authorization URL. 
 An example of an authorization code grant authorization URL: 
 .. code-block:: http 
 GET /authorize?response_type=code&client_id=s6BhdRkqt3&state=xyz 
 &redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb HTTP/1.1 
 Host: server.example.com 
 .. _`W3C.REC-html401-19991224`: http://tools.ietf.org/html/rfc6749#ref-W3C.REC-html401-19991224 
 .. _`Section 2.2`: http://tools.ietf.org/html/rfc6749#section-2.2 
 .. _`Section 3.1.2`: http://tools.ietf.org/html/rfc6749#section-3.1.2 
 .. _`Section 3.3`: http://tools.ietf.org/html/rfc6749#section-3.3 
 .. _`section 10.12`: http://tools.ietf.org/html/rfc6749#section-10.12'"
"def ctcpStringify(messages): 
    coded_messages = [] 
   for (tag, data) in messages: 
      if data: 
         if (not isinstance(data, str)): 
            try: 
               data = '   '.join(map(str, data)) 
            except TypeError: 
               pass 
         m = ('%s   %s' % (tag, data)) 
      else: 
         m = str(tag) 
      m = ctcpQuote(m) 
      m = ('%s%s%s' % (X_DELIM, m, X_DELIM)) 
      coded_messages.append(m) 
   line = ''.join(coded_messages) 
   return line", 'Returns a string that can be sent to a CCTP server.',"'@type messages: a list of extended messages.  An extended 
 message is a (tag, data) tuple, where \'data\' may be L{None}, a 
 string, or a list of strings to be joined with whitespace. 
 @returns: String'"
"def beacon(config): 
    ret = [] 
   changes = {} 
   txt = {} 
   global LAST_GRAINS 
   _validate = __validate__(config) 
   if (not _validate[0]): 
      log.warning('Beacon   {0}   configuration   invalid,   not   adding.   {1}'.format(__virtualname__, _validate[1])) 
      return ret 
   if ('servicename' in config): 
      servicename = config['servicename'] 
   else: 
      servicename = __grains__['host'] 
   for item in config['txt']: 
      if config['txt'][item].startswith('grains.'): 
         grain = config['txt'][item][7:] 
         grain_index = None 
         square_bracket = grain.find('[') 
         if ((square_bracket != (-1)) and (grain[(-1)] == ']')): 
            grain_index = int(grain[(square_bracket + 1):(-1)]) 
            grain = grain[:square_bracket] 
         grain_value = __grains__.get(grain, '') 
         if isinstance(grain_value, list): 
            if (grain_index is not None): 
               grain_value = grain_value[grain_index] 
            else: 
               grain_value = ','.join(grain_value) 
         txt[item] = grain_value 
         if (LAST_GRAINS and (LAST_GRAINS.get(grain, '') != __grains__.get(grain, ''))): 
            changes[str(('txt.' + item))] = txt[item] 
      else: 
         txt[item] = config['txt'][item] 
      if (not LAST_GRAINS): 
         changes[str(('txt.' + item))] = txt[item] 
   if changes: 
      if (not LAST_GRAINS): 
         changes['servicename'] = servicename 
         changes['servicetype'] = config['servicetype'] 
         changes['port'] = config['port'] 
         GROUP.AddService(avahi.IF_UNSPEC, avahi.PROTO_UNSPEC, dbus.UInt32(0), servicename, config['servicetype'], '', '', dbus.UInt16(config['port']), avahi.dict_to_txt_array(txt)) 
         GROUP.Commit() 
      elif config.get('reset_on_change', False): 
         GROUP.Reset() 
         reset_wait = config.get('reset_wait', 0) 
         if (reset_wait > 0): 
            time.sleep(reset_wait) 
         GROUP.AddService(avahi.IF_UNSPEC, avahi.PROTO_UNSPEC, dbus.UInt32(0), servicename, config['servicetype'], '', '', dbus.UInt16(config['port']), avahi.dict_to_txt_array(txt)) 
         GROUP.Commit() 
      else: 
         GROUP.UpdateServiceTxt(avahi.IF_UNSPEC, avahi.PROTO_UNSPEC, dbus.UInt32(0), servicename, config['servicetype'], '', avahi.dict_to_txt_array(txt)) 
      ret.append({'tag': 'result', 'changes': changes}) 
   if config.get('copy_grains', False): 
      LAST_GRAINS = __grains__.copy() 
   else: 
      LAST_GRAINS = __grains__ 
   return ret"," 'Adds the beacon service to avahi. 
 :param config: The configuration dictionary. 
 :returns: A list of results.'","'Broadcast values via zeroconf 
 If the announced values are static, it is adviced to set run_once: True 
 (do not poll) on the beacon configuration. 
 The following are required configuration settings: 
 \'servicetype\': The service type to announce. 
 \'port\': The port of the service to announce. 
 \'txt\': The TXT record of the service being announced as a dict. 
 Grains can be used to define TXT values using the syntax: 
 grains.<grain_name> 
 or: 
 grains.<grain_name>[i] 
 where i is an integer representing the index of the grain to 
 use. If the grain is not a list, the index is ignored. 
 The following are optional configuration settings: 
 \'servicename\': Set the name of the service. Will use the hostname from 
 __grains__[\'host\'] if not set. 
 \'reset_on_change\': If true and there is a change in TXT records 
 detected, it will stop announcing the service and 
 then restart announcing the service. This 
 interruption in service announcement may be 
 desirable if the client relies on changes in the 
 browse records to update its cache of the TXT 
 records. 
 Defaults to False. 
 \'reset_wait\': The number of seconds to wait after announcement stops 
 announcing and before it restarts announcing in the 
 case where there is a change in TXT records detected 
 and \'reset_on_change\' is True. 
 Defaults to 0. 
 \'copy_grains\': If set to True, it will copy the grains passed into 
 the beacon when it backs them up to check for changes 
 on the next iteration. Normally, instead of copy, it 
 would use straight value assignment. This will allow 
 detection of changes to grains where the grains are 
 modified in-place instead of completely replaced. 
 In-place grains changes are not currently done in the 
 main Salt code but may be done due to a custom 
 plug-in. 
 Defaults to False. 
 Example Config 
 .. code-block:: yaml 
 beacons: 
 avahi_announce: 
 run_once: True 
 servicetype: _demo._tcp 
 port: 1234 
 txt: 
 ProdName: grains.productname 
 SerialNo: grains.serialnumber 
 Comments: \'this is a test\''"
"@loader_option() 
 def noload(loadopt, attr): 
    return loadopt.set_relationship_strategy(attr, {'lazy': 'noload'})"," 'Set the relationship strategy for an attribute to ""noload"". 
 This will prevent the attribute from being loaded until it is 
 requested. 
 :param loadopt: The loader to set the relationship strategy for. 
 :param attr: The attribute to set the relationship strategy for. 
 :type loadopt: :class:`~sqlalchemy.ext.declarative.Loader` 
 :type attr: :class:`~sqlalchemy.orm.attributes.Attribute` 
 :return: The :class:`~sqlalchemy.ext.declarative.Loader` instance.'","'Indicate that the given relationship attribute should remain unloaded. 
 This function is part of the :class:`.Load` interface and supports 
 both method-chained and standalone operation. 
 :func:`.orm.noload` applies to :func:`.relationship` attributes; for 
 column-based attributes, see :func:`.orm.defer`.'"
"def masterserviceHandle(target): 
    GlobalObject().masterremote._reference._service.mapTarget(target)", 'Set the master remote service to be a target.',''
"def fetch_species_distributions(data_home=None, download_if_missing=True): 
    data_home = get_data_home(data_home) 
   if (not exists(data_home)): 
      makedirs(data_home) 
   extra_params = dict(x_left_lower_corner=(-94.8), Nx=1212, y_left_lower_corner=(-56.05), Ny=1592, grid_size=0.05) 
   dtype = np.int16 
   archive_path = _pkl_filepath(data_home, DATA_ARCHIVE_NAME) 
   if (not exists(archive_path)): 
      if (not download_if_missing): 
         raise IOError('Data   not   found   and   `download_if_missing`   is   False') 
      print ('Downloading   species   data   from   %s   to   %s' % (SAMPLES_URL, data_home)) 
      X = np.load(BytesIO(urlopen(SAMPLES_URL).read())) 
      for f in X.files: 
         fhandle = BytesIO(X[f]) 
         if ('train' in f): 
            train = _load_csv(fhandle) 
         if ('test' in f): 
            test = _load_csv(fhandle) 
      print ('Downloading   coverage   data   from   %s   to   %s' % (COVERAGES_URL, data_home)) 
      X = np.load(BytesIO(urlopen(COVERAGES_URL).read())) 
      coverages = [] 
      for f in X.files: 
         fhandle = BytesIO(X[f]) 
         print ('   -   converting', f) 
         coverages.append(_load_coverage(fhandle)) 
      coverages = np.asarray(coverages, dtype=dtype) 
      bunch = Bunch(coverages=coverages, test=test, train=train, **extra_params) 
      joblib.dump(bunch, archive_path, compress=9) 
   else: 
      bunch = joblib.load(archive_path) 
   return bunch"," 'Fetch the distribution of species on the planet. 
 Parameters 
 data_home : str 
 Path to the directory where the data should be stored. 
 download_if_missing : bool 
 If ``True``, download the data if it is not present in the given 
 directory. 
 Returns 
 bunch : Bunch 
 A Bunch containing the following fields: 
 coverages : numpy array 
 Coverage values of the species on the planet. 
 train : numpy array 
 Species distribution on the training set. 
 test : numpy array 
 Species distribution on the test set. 
 See Also 
 :func:`~.load_species_distributions` 
 Notes 
 This function downloads the data from the internet. 
 The data is stored in the ``data_home`` directory. 
 References 
 .. [1] http://www.cs.toronto.edu/~fritz/data/species.zip'","'Loader for species distribution dataset from Phillips et. al. (2006) 
 Read more in the :ref:`User Guide <datasets>`. 
 Parameters 
 data_home : optional, default: None 
 Specify another download and cache folder for the datasets. By default 
 all scikit learn data is stored in \'~/scikit_learn_data\' subfolders. 
 download_if_missing : optional, True by default 
 If False, raise a IOError if the data is not locally available 
 instead of trying to download the data from the source site. 
 Returns 
 The data is returned as a Bunch object with the following attributes: 
 coverages : array, shape = [14, 1592, 1212] 
 These represent the 14 features measured at each point of the map grid. 
 The latitude/longitude values for the grid are discussed below. 
 Missing data is represented by the value -9999. 
 train : record array, shape = (1623,) 
 The training points for the data.  Each point has three fields: 
 - train[\'species\'] is the species name 
 - train[\'dd long\'] is the longitude, in degrees 
 - train[\'dd lat\'] is the latitude, in degrees 
 test : record array, shape = (619,) 
 The test points for the data.  Same format as the training data. 
 Nx, Ny : integers 
 The number of longitudes (x) and latitudes (y) in the grid 
 x_left_lower_corner, y_left_lower_corner : floats 
 The (x,y) position of the lower-left corner, in degrees 
 grid_size : float 
 The spacing between points of the grid, in degrees 
 Notes 
 This dataset represents the geographic distribution of species. 
 The dataset is provided by Phillips et. al. (2006). 
 The two species are: 
 - `""Bradypus variegatus"" 
 <http://www.iucnredlist.org/details/3038/0>`_ , 
 the Brown-throated Sloth. 
 - `""Microryzomys minutus"" 
 <http://www.iucnredlist.org/details/13408/0>`_ , 
 also known as the Forest Small Rice Rat, a rodent that lives in Peru, 
 Colombia, Ecuador, Peru, and Venezuela. 
 References 
 * `""Maximum entropy modeling of species geographic distributions"" 
 <http://www.cs.princeton.edu/~schapire/papers/ecolmod.pdf>`_ 
 S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling, 
 190:231-259, 2006. 
 Notes 
 * See examples/applications/plot_species_distribution_modeling.py 
 for an example of using this dataset with scikit-learn'"
"def automaster(config='/etc/auto_salt'): 
    ret = {} 
   if (not os.path.isfile(config)): 
      return ret 
   with salt.utils.fopen(config) as ifile: 
      for line in ifile: 
         if line.startswith('#'): 
            continue 
         if (not line.strip()): 
            continue 
         comps = line.split() 
         if (len(comps) != 3): 
            continue 
         prefix = '/..' 
         name = comps[0].replace(prefix, '') 
         device_fmt = comps[2].split(':') 
         opts = comps[1].split(',') 
         ret[name] = {'device': device_fmt[1], 'fstype': opts[0], 'opts': opts[1:]} 
   return ret", 'Get auto_salt config',"'List the contents of the auto master 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' mount.automaster'"
"@requires_sklearn 
 def test_ica_reject_buffer(): 
    raw = read_raw_fif(raw_fname).crop(1.5, stop).load_data() 
   picks = pick_types(raw.info, meg=True, stim=False, ecg=False, eog=False, exclude='bads') 
   ica = ICA(n_components=3, max_pca_components=4, n_pca_components=4) 
   raw._data[2, 1000:1005] = 5e-12 
   with catch_logging() as drop_log: 
      with warnings.catch_warnings(record=True): 
         ica.fit(raw, picks[:5], reject=dict(mag=2.5e-12), decim=2, tstep=0.01, verbose=True) 
      assert_true(((raw._data[:5, ::2].shape[1] - 4) == ica.n_samples_)) 
   log = [l for l in drop_log.getvalue().split('\n') if ('detected' in l)] 
   assert_equal(len(log), 1)", 'Test that the ICA rejects the bad channel.','Test ICA data raw buffer rejection.'
"def check_valid_naming(pattern=None, multi=None, anime_type=None): 
    if (pattern is None): 
      pattern = sickbeard.NAMING_PATTERN 
   if (anime_type is None): 
      anime_type = sickbeard.NAMING_ANIME 
   logger.log(((u'Checking   whether   the   pattern   ' + pattern) + u'   is   valid   for   a   single   episode'), logger.DEBUG) 
   valid = validate_name(pattern, None, anime_type) 
   if (multi is not None): 
      logger.log(((u'Checking   whether   the   pattern   ' + pattern) + u'   is   valid   for   a   multi   episode'), logger.DEBUG) 
      valid = (valid and validate_name(pattern, multi, anime_type)) 
   return valid"," 'Checks if the given naming pattern is valid for a single episode or 
 a multi episode. 
 :param pattern: The naming pattern to check 
 :type pattern: str 
 :param multi: The multi episode name to check 
 :type multi: str 
 :param anime_type: The anime type to check 
 :type anime_type: str 
 :return: True if the pattern is valid, False otherwise'","'Checks if the name is can be parsed back to its original form for both single and multi episodes. 
 :return: true if the naming is valid, false if not.'"
"def constrain_rgb(r, g, b): 
    w = (- min([0, r, g, b])) 
   if (w > 0): 
      r += w 
      g += w 
      b += w 
   return (r, g, b)"," 'Constrain `r`, `g`, and `b` to be in the range [0, 255]. 
 If `r`, `g`, or `b` is outside the range, it is clamped to 0. 
 If any of the values is 0, the values are set to 255. 
 :param r: The red value. 
 :param g: The green value. 
 :param b: The blue value. 
 :return: The constrained red, green, and blue values.'","'If the requested RGB shade contains a negative weight for 
 one of the primaries, it lies outside the colour gamut 
 accessible from the given triple of primaries.  Desaturate 
 it by adding white, equal quantities of R, G, and B, enough 
 to make RGB all positive.  The function returns 1 if the 
 components were modified, zero otherwise.'"
"def _parse_split_test_data_str(): 
    tuple_class = collections.namedtuple('TestCase', 'input,   keep,   no_keep') 
   for line in test_data_str.splitlines(): 
      if (not line): 
         continue 
      data = line.split('/') 
      item = tuple_class(input=data[0], keep=data[1].split('|'), no_keep=data[2].split('|')) 
      (yield item) 
   (yield tuple_class(input='', keep=[], no_keep=[]))"," 'Parse the test data string into a list of tuples 
 :param test_data_str: 
 :return: 
 :rtype: 
 :class: 
 :type: 
 :example: 
 >>> _parse_split_test_data_str() 
 (TestCase(input=u\'foo\', keep=[u\'foo\'], no_keep=[u\'foo\']), 
 TestCase(input=u\'foo|bar\', keep=[u\'foo|bar\'], no_keep=[u\'foo|bar\']), 
 TestCase(input=u\'foo|bar|baz\', keep=[u\'foo|bar|baz\'], 
 no_keep=[u\'foo|bar|baz\']), 
 TestCase(input=u\'foo\', keep=[u\'foo\'], no_keep=[u\'foo\']), 
 TestCase(input=u\'foo|bar\', keep=[u\'foo|bar\'], no_keep=[u\'foo|bar\']), 
 TestCase(input=u\'foo|bar|baz\', keep=[u\'foo|","'Parse the test data set into a namedtuple to use in tests. 
 Returns: 
 A list of namedtuples with str attributes: input, keep, no_keep'"
"def parse_network(rule): 
    parser = argparse.ArgumentParser() 
   rules = shlex.split(rule) 
   rules.pop(0) 
   parser.add_argument('--bootproto', dest='bootproto', action='store', choices=['dhcp', 'bootp', 'static', 'ibft']) 
   parser.add_argument('--device', dest='device', action='store') 
   parser.add_argument('--ip', dest='ip', action='store') 
   parser.add_argument('--ipv6', dest='ipv6', action='store') 
   parser.add_argument('--gateway', dest='gateway', action='store') 
   parser.add_argument('--nodefroute', dest='nodefroute', action='store_true') 
   parser.add_argument('--nameserver', dest='nameserver', action='store') 
   parser.add_argument('--nodns', dest='nodns', action='store_true') 
   parser.add_argument('--netmask', dest='netmask', action='store') 
   parser.add_argument('--hostname', dest='hostname', action='store') 
   parser.add_argument('--ethtool', dest='ethtool', action='store') 
   parser.add_argument('--essid', dest='essid', action='store') 
   parser.add_argument('--wepkey', dest='wepkey', action='store') 
   parser.add_argument('--wpakey', dest='wpakey', action='store') 
   parser.add_argument('--onboot', dest='onboot', action='store') 
   parser.add_argument('--dhcpclass', dest='dhcpclass', action='store') 
   parser.add_argument('--mtu', dest='mtu', action='store') 
   parser.add_argument('--noipv4', dest='noipv4', action='store_true') 
   parser.add_argument('--noipv6', dest='noipv6', action='store_true') 
   parser.add_argument('--activate', dest='activate', action='store_true') 
   args = clean_args(vars(parser.parse_args(rules))) 
   parser = None 
   return args"," 'Parse a network rule string into a dict. 
 :param rule: A string representing a network rule. 
 :return: A dict representing the parsed rule. 
 :rtype: dict'",'Parse the network line'
"def is_field_shared_factory(field_name): 
    class IsFieldShared(permissions.BasePermission, ): 
      '\n                        Grants   access   if   a   particular   profile   field   is   shared   with   the   requesting   user.\n                        ' 
      def has_permission(self, request, view): 
         url_username = request.parser_context.get('kwargs', {}).get('username', '') 
         if (request.user.username.lower() == url_username.lower()): 
            return True 
         if request.user.is_staff: 
            return True 
         user = get_object_or_404(User, username__iexact=url_username) 
         if (field_name in visible_fields(user.profile, user)): 
            return True 
         raise Http404() 
   return IsFieldShared"," 'Returns a permission class for checking if a field is shared with the user. 
 This permission is only available for UserProfile fields. 
 :param field_name: The name of the field to check. 
 :type field_name: str 
 :return: A permission class. 
 :rtype: :class:`django.contrib.auth.models.BasePermission` 
 :raises Http404: If the field is not shared with the user.'","'Generates a permission class that grants access if a particular profile field is 
 shared with the requesting user.'"
"def fix_epoch(epoch): 
    epoch = int(epoch) 
   if (len(str(epoch)) <= 10): 
      return epoch 
   elif (len(str(epoch)) == 13): 
      return int((epoch / 1000)) 
   elif ((len(str(epoch)) > 10) and (len(str(epoch)) < 13)): 
      raise ValueError('Unusually   formatted   epoch   timestamp.      Should   be   10,   13,   or   more   digits') 
   else: 
      orders_of_magnitude = (len(str(epoch)) - 10) 
      powers_of_ten = (10 ** orders_of_magnitude) 
      epoch = int((epoch / powers_of_ten)) 
   return epoch"," 'Convert a string representation of an epoch timestamp to a 
 integer. 
 The epoch timestamp is a 10-digit integer that is the number of 
 seconds since the Unix epoch. 
 The returned value is an integer that is the epoch timestamp 
 divided by 1000. 
 If the epoch timestamp is not a 10-digit integer, it is truncated 
 to the nearest 10-digit integer. 
 If the epoch timestamp is not a 10-digit integer, it is raised as 
 an error. 
 Parameters 
 epoch : string 
 The string representation of an epoch timestamp. 
 Returns 
 epoch : int 
 The epoch timestamp divided by 1000. 
 Raises 
 ValueError 
 If the epoch timestamp is not a 10-digit integer.'","'Fix value of `epoch` to be epoch, which should be 10 or fewer digits long. 
 :arg epoch: An epoch timestamp, in epoch + milliseconds, or microsecond, or 
 even nanoseconds. 
 :rtype: int'"
"def uninstall(pecls): 
    if isinstance(pecls, six.string_types): 
      pecls = [pecls] 
   return _pecl('uninstall   {0}'.format(_cmd_quote('   '.join(pecls))))"," 'Uninstalls a package. 
 :param pecls: 
 :type pecls: 
 :return: 
 :rtype:'","'Uninstall one or several pecl extensions. 
 pecls 
 The pecl extensions to uninstall. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' pecl.uninstall fuse'"
"def make_user_coach(user, master_course_key): 
    coach_role_on_master_course = CourseCcxCoachRole(master_course_key) 
   coach_role_on_master_course.add_users(user)"," 'Adds the user to the coach role on the master course. 
 This is necessary for the user to be able to add coaches to their course.'","'Makes an user coach on the master course. 
 This function is needed because an user cannot become a coach of the CCX if s/he is not 
 coach on the master course. 
 Args: 
 user (User): User object 
 master_course_key (CourseKey): Key locator object for the course'"
"def weights_uniform(nlags): 
    return np.ones((nlags + 1))", 'Returns a uniform distribution over the lags.',"'uniform weights for HAC 
 this will be moved to another module 
 Parameters 
 nlags : int 
 highest lag in the kernel window, this does not include the zero lag 
 Returns 
 kernel : ndarray, (nlags+1,) 
 weights for uniform kernel'"
"def literalquery(statement, dialect=None): 
    if isinstance(statement, sqlalchemy.orm.Query): 
      if (dialect is None): 
         dialect = statement.session.get_bind(statement._mapper_zero_or_none()).dialect 
      statement = statement.statement 
   if (dialect is None): 
      dialect = getattr(statement.bind, 'dialect', None) 
   if (dialect is None): 
      from sqlalchemy.dialects import mysql 
      dialect = mysql.dialect() 
   Compiler = type(statement._compiler(dialect)) 
   class LiteralCompiler(Compiler, ): 
      visit_bindparam = Compiler.render_literal_bindparam 
      def render_literal_value(self, value, type_): 
         if isinstance(value, (Decimal, long)): 
            return str(value) 
         elif isinstance(value, datetime): 
            return repr(str(value)) 
         else: 
            value = super(LiteralCompiler, self).render_literal_value(value, type_) 
            if isinstance(value, unicode): 
               return value.encode('UTF-8') 
            else: 
               return value 
   return LiteralCompiler(dialect, statement)", 'Returns a compiler that will render all bind parameters as literals.',"'Generate an SQL expression string with bound parameters rendered inline 
 for the given SQLAlchemy statement. 
 WARNING: This method of escaping is insecure, incomplete, and for debugging 
 purposes only. Executing SQL statements with inline-rendered user values is 
 extremely insecure.'"
"def available(): 
    ret = [] 
   mod_dir = os.path.join('/lib/modules/', os.uname()[2]) 
   for (root, dirs, files) in os.walk(mod_dir): 
      for fn_ in files: 
         if ('.ko' in fn_): 
            ret.append(fn_[:fn_.index('.ko')].replace('-', '_')) 
   if ('Arch' in __grains__['os_family']): 
      mod_dir_arch = (('/lib/modules/extramodules-' + os.uname()[2][0:3]) + '-ARCH') 
      for (root, dirs, files) in os.walk(mod_dir_arch): 
         for fn_ in files: 
            if ('.ko' in fn_): 
               ret.append(fn_[:fn_.index('.ko')].replace('-', '_')) 
   return sorted(list(ret))"," 'Returns a list of kernel module names. 
 :return: a list of kernel module names'","'Return a list of all available kernel modules 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' kmod.available'"
"def get_clonespec_for_valid_snapshot(config_spec, object_ref, reloc_spec, template, vm_): 
    moving = True 
   if (QUICK_LINKED_CLONE == vm_['snapshot']['disk_move_type']): 
      reloc_spec.diskMoveType = QUICK_LINKED_CLONE 
   elif (CURRENT_STATE_LINKED_CLONE == vm_['snapshot']['disk_move_type']): 
      reloc_spec.diskMoveType = CURRENT_STATE_LINKED_CLONE 
   elif (COPY_ALL_DISKS_FULL_CLONE == vm_['snapshot']['disk_move_type']): 
      reloc_spec.diskMoveType = COPY_ALL_DISKS_FULL_CLONE 
   elif (FLATTEN_DISK_FULL_CLONE == vm_['snapshot']['disk_move_type']): 
      reloc_spec.diskMoveType = FLATTEN_DISK_FULL_CLONE 
   else: 
      moving = False 
   if moving: 
      return build_clonespec(config_spec, object_ref, reloc_spec, template) 
   else: 
      return None"," 'Return the clonespec to use for a snapshot. 
 :param config_spec: The config spec to use to build the clonespec. 
 :param object_ref: The object ref to use to build the clonespec. 
 :param reloc_spec: The reloc spec to use to build the clonespec. 
 :param template: The template to use to build the clonespec. 
 :param vm_: The vm to use to build the clonespec. 
 :return: The clonespec to use for a snapshot.'",'return clonespec only if values are valid'
"def test_email(): 
    os.environ['PYTHONPATH'] = current_directory 
   os.environ['DJANGO_SETTINGS_MODULE'] = 'djangoapp' 
   (status, out) = commands.getstatusoutput('django-admin.py   harvest   email.feature   --verbosity=2') 
   assert_not_equals(status, 0)", 'Test email harvesting.','lettuce should be able to receive emails sent from django server'
"@decorators.which('chef-client') 
 def client(whyrun=False, localmode=False, logfile=None, **kwargs): 
    if (logfile is None): 
      logfile = _default_logfile('chef-client') 
   args = ['chef-client', '--no-color', '--once', '--logfile   ""{0}""'.format(logfile), '--format   doc'] 
   if whyrun: 
      args.append('--why-run') 
   if localmode: 
      args.append('--local-mode') 
   return _exec_cmd(*args, **kwargs)"," 'Run chef-client. 
 :param whyrun: If true, this will be passed to chef-client. 
 :param localmode: If true, this will be passed to chef-client. 
 :param logfile: If set, this will be passed to chef-client. 
 :param kwargs: Any additional arguments to pass to chef-client. 
 :return: The exit status of chef-client.'","'Execute a chef client run and return a dict with the stderr, stdout, 
 return code, and pid. 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' chef.client server=https://localhost 
 server 
 The chef server URL 
 client_key 
 Set the client key file location 
 config 
 The configuration file to use 
 config-file-jail 
 Directory under which config files are allowed to be loaded 
 (no client.rb or knife.rb outside this path will be loaded). 
 environment 
 Set the Chef Environment on the node 
 group 
 Group to set privilege to 
 json-attributes 
 Load attributes from a JSON file or URL 
 localmode 
 Point chef-client at local repository if True 
 log_level 
 Set the log level (debug, info, warn, error, fatal) 
 logfile 
 Set the log file location 
 node-name 
 The node name for this client 
 override-runlist 
 Replace current run list with specified items for a single run 
 pid 
 Set the PID file location, defaults to /tmp/chef-client.pid 
 run-lock-timeout 
 Set maximum duration to wait for another client run to finish, 
 default is indefinitely. 
 runlist 
 Permanently replace current run list with specified items 
 user 
 User to set privilege to 
 validation_key 
 Set the validation key file location, used for registering new clients 
 whyrun 
 Enable whyrun mode when set to True'"
"def forwards_move_repo_source(apps, schema_editor): 
    RemoteRepository = apps.get_model(u'oauth', u'RemoteRepository') 
   SocialAccount = apps.get_model(u'socialaccount', u'SocialAccount') 
   for account in SocialAccount.objects.all(): 
      rows = RemoteRepository.objects.filter(users=account.user, source=account.provider).update(account=account)", 'Forwards move of a repository from one provider to another.','Use source field to set repository account'
"def set_wake_on_network(enabled): 
    state = salt.utils.mac_utils.validate_enabled(enabled) 
   cmd = 'systemsetup   -setwakeonnetworkaccess   {0}'.format(state) 
   salt.utils.mac_utils.execute_return_success(cmd) 
   return salt.utils.mac_utils.confirm_updated(state, get_wake_on_network)"," 'Set the Wake on Network access for the Mac. 
 :param bool enabled: 
 :return: 
 :rtype: bool'","'Set whether or not the computer will wake from sleep when network activity 
 is detected. 
 :param bool enabled: True to enable, False to disable. ""On"" and ""Off"" are 
 also acceptable values. Additionally you can pass 1 and 0 to represent True 
 and False respectively 
 :return: True if successful, False if not 
 :rtype: bool 
 CLI Example: 
 .. code-block:: bash 
 salt \'*\' power.set_wake_on_network True'"
"def check_files(files, file_check, exclusions=set(), pattern=None): 
    if (not files): 
      return 
   for fname in files: 
      if ((not exists(fname)) or (not isfile(fname))): 
         continue 
      if any(((ex in fname) for ex in exclusions)): 
         continue 
      if ((pattern is None) or re.match(pattern, fname)): 
         file_check(fname)"," 'Check files for integrity. 
 :param files: list of files to check 
 :param file_check: function to call for each file 
 :param exclusions: set of file names to exclude 
 :param pattern: regular expression to match against file names'","'Checks all files with the file_check function provided, skipping files 
 that contain any of the strings in the set provided by exclusions.'"
"def determine_format(request, serializer, default_format=u'application/json'): 
    format = request.GET.get(u'format') 
   if format: 
      if (format in serializer.formats): 
         return serializer.get_mime_for_format(format) 
   if ((u'callback' in request.GET) and (u'jsonp' in serializer.formats)): 
      return serializer.get_mime_for_format(u'jsonp') 
   accept = request.META.get(u'HTTP_ACCEPT', u'*/*') 
   if (accept != u'*/*'): 
      try: 
         best_format = mimeparse.best_match(serializer.supported_formats_reversed, accept) 
      except ValueError: 
         raise BadRequest(u'Invalid   Accept   header') 
      if best_format: 
         return best_format 
   return default_format"," 'Determine the format for the request. 
 :param request: The request. 
 :param serializer: The serializer. 
 :param default_format: The default format. 
 :return: The format for the request. 
 :rtype: str'","'Tries to ""smartly"" determine which output format is desired. 
 First attempts to find a ``format`` override from the request and supplies 
 that if found. 
 If no request format was demanded, it falls back to ``mimeparse`` and the 
 ``Accepts`` header, allowing specification that way. 
 If still no format is found, returns the ``default_format`` (which defaults 
 to ``application/json`` if not provided). 
 NOTE: callers *must* be prepared to handle BadRequest exceptions due to 
 malformed HTTP request headers!'"
"def remove_packages(module, port_path, packages): 
    remove_c = 0 
   for package in packages: 
      if (not query_package(module, port_path, package)): 
         continue 
      (rc, out, err) = module.run_command(('%s   uninstall   %s' % (port_path, package))) 
      if query_package(module, port_path, package): 
         module.fail_json(msg=('failed   to   remove   %s:   %s' % (package, out))) 
      remove_c += 1 
   if (remove_c > 0): 
      module.exit_json(changed=True, msg=('removed   %s   package(s)' % remove_c)) 
   module.exit_json(changed=False, msg='package(s)   already   absent')", 'Removes packages from a port','Uninstalls one or more packages if installed.'
"def _get_options(ret=None): 
    defaults = {'level': 'LOG_INFO', 'facility': 'LOG_USER', 'options': []} 
   attrs = {'level': 'level', 'facility': 'facility', 'tag': 'tag', 'options': 'options'} 
   _options = salt.returners.get_returner_options(__virtualname__, ret, attrs, __salt__=__salt__, __opts__=__opts__, defaults=defaults) 
   return _options", 'Return options for the log module.','Get the returner options from salt.'
"def listdir(path): 
    if (not hasattr(sys, 'frozen')): 
      return os.listdir(path) 
   (zipPath, archivePath) = splitZip(path) 
   if (archivePath is None): 
      return os.listdir(path) 
   with zipfile.ZipFile(zipPath, 'r') as zipobj: 
      contents = zipobj.namelist() 
   results = set() 
   for name in contents: 
      if (name.startswith(archivePath) and (len(name) > len(archivePath))): 
         name = name[len(archivePath):].split('/')[0] 
         results.add(name) 
   return list(results)"," 'Return a list of the contents of a zip file. 
 This is a thin wrapper around os.listdir() that works around the 
 issue that zipfile.ZipFile.namelist() does not work on frozen Python 
 executables. 
 :param path: path to the zip file 
 :type path: str 
 :return: a list of the contents of the zip file 
 :rtype: list'",'Replacement for os.listdir that works in frozen environments.'
"def object_id(value): 
    _object_id = '{}_{}_{}'.format(slugify(_value_name(value)), value.node.node_id, value.index) 
   if (value.instance > 1): 
      return '{}_{}'.format(_object_id, value.instance) 
   return _object_id"," 'Returns a unique ID for an object. 
 :param value: The object to get the ID for. 
 :type value: :py:class:`~pyspark.sql.Column` 
 :return: The unique ID for the object. 
 :rtype: str'","'Return the object_id of the device value. 
 The object_id contains node_id and value instance id 
 to not collide with other entity_ids.'"
"def mquantiles_cimj(data, prob=[0.25, 0.5, 0.75], alpha=0.05, axis=None): 
    alpha = min(alpha, (1 - alpha)) 
   z = norm.ppf((1 - (alpha / 2.0))) 
   xq = mstats.mquantiles(data, prob, alphap=0, betap=0, axis=axis) 
   smj = mjci(data, prob, axis=axis) 
   return ((xq - (z * smj)), (xq + (z * smj)))"," 'Calculate the confidence intervals for the quantiles of a distribution. 
 Parameters 
 data : array_like 
 Data. 
 prob : array_like, optional 
 Probabilities of the quantiles. 
 alpha : float, optional 
 Confidence level. 
 axis : {0, 1}, optional 
 Axis along which to compute the confidence interval. 
 Returns 
 result : 2-element array 
 The confidence intervals for the quantiles. 
 Examples 
 >>> from scipy.stats import norm, mquantiles_cimj 
 >>> x = norm.rvs(5, scale=2, size=1000) 
 >>> mquantiles_cimj(x, 0.25, 0.5, 0.75) 
 (array([0.25, 0.5, 0.75]), array([0.25, 0.5, 0.75])) 
 >>> mquantiles_cimj(x, 0.25, 0.5, 0.75, axis=1) 
","'Computes the alpha confidence interval for the selected quantiles of the 
 data, with Maritz-Jarrett estimators. 
 Parameters 
 data : ndarray 
 Data array. 
 prob : sequence, optional 
 Sequence of quantiles to compute. 
 alpha : float, optional 
 Confidence level of the intervals. 
 axis : int or None, optional 
 Axis along which to compute the quantiles. 
 If None, use a flattened array.'"
"def get_pools(client): 
    return [server.pool for server in client._get_topology().select_servers(any_server_selector)]", 'Returns a list of pools.','Get all pools.'
"def get_cpu_list(ip, user, passwd): 
    cmd = 'statcpu   -iter   1   -t' 
   showcpu_list = run_ssh_thread(ip, user, passwd, cmd) 
   cpu_list = [] 
   line_num = 0 
   for line in showcpu_list: 
      line_num += 1 
      if (line_num >= 3): 
         cpu_stats = line.split() 
         if (len(cpu_stats) > 2): 
            cpu_list.append(cpu_stats[0].split(',')[0]) 
   return cpu_list", 'Return a list of all the CPUs on the server.','Get a list of cpus to build metric definitions with'
"def test_append(hist, config_stub): 
    config_stub.data = CONFIG_NOT_PRIVATE 
   hist.append('new   item') 
   assert ('new   item' in hist.history) 
   hist.history.remove('new   item') 
   assert (hist.history == HISTORY)", 'Test that the history is appended correctly.','Test append outside private mode.'
"def new(rsa_key): 
    return PKCS115_SigScheme(rsa_key)"," 'Create a new SignatureScheme instance. 
 :param rsa_key: A RSA key. 
 :type rsa_key: Cryptodome.RSA.RSAPublicKey'","'Return a signature scheme object `PKCS115_SigScheme` that 
 can create or verify PKCS#1 v1.5 signatures. 
 :Parameters: 
 rsa_key : RSA key object 
 The RSA key to use to sign or verify the message. 
 This is a `Cryptodome.PublicKey.RSA` object. 
 Signing is only possible if ``rsa_key`` is a private RSA key.'"
"def get_connection(backend=None, fail_silently=False, **kwds): 
    path = (backend or settings.EMAIL_BACKEND) 
   try: 
      (mod_name, klass_name) = path.rsplit('.', 1) 
      mod = import_module(mod_name) 
   except ImportError as e: 
      raise ImproperlyConfigured(('Error   importing   email   backend   module   %s:   ""%s""' % (mod_name, e))) 
   try: 
      klass = getattr(mod, klass_name) 
   except AttributeError: 
      raise ImproperlyConfigured(('Module   ""%s""   does   not   define   a   ""%s""   class' % (mod_name, klass_name))) 
   return klass(fail_silently=fail_silently, **kwds)"," 'Return an instance of the given backend. 
 If backend is not specified, then the value of EMAIL_BACKEND is used. 
 If the backend is not available, an exception is raised. 
 :param backend: the name of the backend to use. 
 :param fail_silently: if True, do not raise an exception if the backend is not available. 
 :param kwds: keyword arguments to be passed to the backend.'","'Load an email backend and return an instance of it. 
 If backend is None (default) settings.EMAIL_BACKEND is used. 
 Both fail_silently and other keyword arguments are used in the 
 constructor of the backend.'"
"def detect(): 
    devlist = list_devices() 
   print(('Found   %d   CUDA   devices' % len(devlist))) 
   supported_count = 0 
   for dev in devlist: 
      attrs = [] 
      cc = dev.compute_capability 
      attrs += [('compute   capability', ('%d.%d' % cc))] 
      attrs += [('pci   device   id', dev.PCI_DEVICE_ID)] 
      attrs += [('pci   bus   id', dev.PCI_BUS_ID)] 
      if (cc < (2, 0)): 
         support = '[NOT   SUPPORTED:   CC   <   2.0]' 
      else: 
         support = '[SUPPORTED]' 
         supported_count += 1 
      print(('id   %d            %20s   %40s' % (dev.id, dev.name, support))) 
      for (key, val) in attrs: 
         print(('%40s:   %s' % (key, val))) 
   print('Summary:') 
   print((' DCTB %d/%d   devices   are   supported' % (supported_count, len(devlist)))) 
   return (supported_count > 0)", 'Detects CUDA devices and returns True if at least one is supported',"'Detect supported CUDA hardware and print a summary of the detected hardware. 
 Returns a boolean indicating whether any supported devices were detected.'"
"def _collect_filetree_revs(obj_store, tree_sha, kset): 
    filetree = obj_store[tree_sha] 
   for (name, mode, sha) in filetree.iteritems(): 
      if ((not S_ISGITLINK(mode)) and (sha not in kset)): 
         kset.add(sha) 
         if stat.S_ISDIR(mode): 
            _collect_filetree_revs(obj_store, sha, kset)"," 'Collect all the SHA-1s of all the files in a filetree. 
 :param obj_store: The object store. 
 :param tree_sha: The SHA-1 of the filetree. 
 :param kset: A set of SHA-1s. 
 :return: None. 
 :rtype: None'","'Collect SHA1s of files and directories for specified tree. 
 :param obj_store: Object store to get objects by SHA from 
 :param tree_sha: tree reference to walk 
 :param kset: set to fill with references to files and directories'"
"def destroy_vm(session, instance, vm_ref=None): 
    try: 
      if (not vm_ref): 
         vm_ref = get_vm_ref(session, instance) 
      LOG.debug('Destroying   the   VM', instance=instance) 
      destroy_task = session._call_method(session.vim, 'Destroy_Task', vm_ref) 
      session._wait_for_task(destroy_task) 
      LOG.info(_LI('Destroyed   the   VM'), instance=instance) 
   except Exception: 
      LOG.exception(_LE('Destroy   VM   failed'), instance=instance)", 'Destroy a VM.','Destroy a VM instance. Assumes VM is powered off.'
"def dirichlet_likelihood(weights, alpha=None): 
    if (type(weights) is Variable): 
      n_topics = weights.data.shape[1] 
   else: 
      n_topics = weights.W.data.shape[1] 
   if (alpha is None): 
      alpha = (1.0 / n_topics) 
   if (type(weights) is Variable): 
      log_proportions = F.log_softmax(weights) 
   else: 
      log_proportions = F.log_softmax(weights.W) 
   loss = ((alpha - 1.0) * log_proportions) 
   return (- F.sum(loss))"," 'Computes the Dirichlet log-likelihood for a set of weights. 
 Parameters 
 weights : Variable or array-like 
 A Variable or array-like of shape `(n_samples, n_topics)` 
 alpha : float, optional 
 The alpha parameter for the Dirichlet distribution. 
 Returns 
 float 
 The log-likelihood of the given weights.'","'Calculate the log likelihood of the observed topic proportions. 
 A negative likelihood is more likely than a negative likelihood. 
 Args: 
 weights (chainer.Variable): Unnormalized weight vector. The vector 
 will be passed through a softmax function that will map the input 
 onto a probability simplex. 
 alpha (float): The Dirichlet concentration parameter. Alpha 
 greater than 1.0 results in very dense topic weights such 
 that each document belongs to many topics. Alpha < 1.0 results 
 in sparser topic weights. The default is to set alpha to 
 1.0 / n_topics, effectively enforcing the prior belief that a 
 document belong to very topics at once. 
 Returns: 
 ~chainer.Variable: Output loss variable.'"
"def _urlopen_cached(url, cache): 
    from_cache = False 
   if (cache is not None): 
      cache_path = join(cache, (url.split('://')[(-1)].replace('/', ',') + '.zip')) 
      try: 
         data = _open_cache(cache_path) 
         from_cache = True 
      except: 
         pass 
   if (not from_cache): 
      data = urlopen(url).read() 
      if (cache is not None): 
         _cache_it(data, cache_path) 
   return (data, from_cache)"," 'Returns the content of the given url, optionally from cache. 
 :param url: The url to fetch. 
 :param cache: The cache to use. 
 :return: (content, from_cache) 
 :rtype: (str, bool)'","'Tries to load data from cache location otherwise downloads it. If it 
 downloads the data and cache is not None then it will put the downloaded 
 data in the cache path.'"
"def create_network_acl_entry(network_acl_id=None, rule_number=None, protocol=None, rule_action=None, cidr_block=None, egress=None, network_acl_name=None, icmp_code=None, icmp_type=None, port_range_from=None, port_range_to=None, region=None, key=None, keyid=None, profile=None): 
    kwargs = locals() 
   return _create_network_acl_entry(**kwargs)"," 'Create a network ACL rule. 
 :param network_acl_id: The ID of the network ACL. 
 :param rule_number: The number of the rule. 
 :param protocol: The protocol of the rule. 
 :param rule_action: The action of the rule. 
 :param cidr_block: The CIDR block of the rule. 
 :param egress: Whether the rule is for egress traffic. 
 :param network_acl_name: The name of the network ACL. 
 :param icmp_code: The ICMP code of the rule. 
 :param icmp_type: The ICMP type of the rule. 
 :param port_range_from: The port range from of the rule. 
 :param port_range_to: The port range to of the rule. 
 :param region: The region of the network ACL. 
 :param key: The key of the network ACL. 
 :param keyid: The key ID of the network ACL. 
 :param profile: The profile of the network ACL. 
 :param network_ac","'Creates a network acl entry. 
 CLI Example: 
 .. code-block:: bash 
 salt myminion boto_vpc.create_network_acl_entry \'acl-5fb85d36\' \'32767\' \ 
 \'all\' \'deny\' \'0.0.0.0/0\' egress=true'"
"def interface_field(interfaces, **field_kwargs): 
    if (not isinstance(interfaces, tuple)): 
      raise TypeError('The   ``interfaces``   argument   must   be   a   tuple.   Got:   {!r}'.format(interfaces)) 
   original_invariant = field_kwargs.pop('invariant', None) 
   def invariant(value): 
      error_messages = [] 
      if (original_invariant is not None): 
         (original_invariant_result, _original_invariant_message) = original_invariant(value) 
         if original_invariant_result: 
            error_messages.append(original_invariant_result) 
      missing_interfaces = [] 
      for interface in interfaces: 
         if (not interface.providedBy(value)): 
            missing_interfaces.append(interface.getName()) 
      if missing_interfaces: 
         error_messages.append('The   value   {!r}   did   not   provide   these   required   interfaces:   {}'.format(value, ',   '.join(missing_interfaces))) 
      if error_messages: 
         return (False, '\n'.join(error_messages)) 
      else: 
         return (True, '') 
   field_kwargs['invariant'] = invariant 
   return field(**field_kwargs)", 'Create a field with the given interfaces.',"'A ``PClass`` field which checks that the assigned value provides all the 
 ``interfaces``. 
 :param tuple interfaces: The ``Interface`` that a value must provide.'"
"def resource_delete(context, data_dict): 
    model = context['model'] 
   id = _get_or_bust(data_dict, 'id') 
   entity = model.Resource.get(id) 
   if (entity is None): 
      raise NotFound 
   _check_access('resource_delete', context, data_dict) 
   package_id = entity.get_package_id() 
   pkg_dict = _get_action('package_show')(context, {'id': package_id}) 
   for plugin in plugins.PluginImplementations(plugins.IResourceController): 
      plugin.before_delete(context, data_dict, pkg_dict.get('resources', [])) 
   if pkg_dict.get('resources'): 
      pkg_dict['resources'] = [r for r in pkg_dict['resources'] if (not (r['id'] == id))] 
   try: 
      pkg_dict = _get_action('package_update')(context, pkg_dict) 
   except ValidationError as e: 
      errors = e.error_dict['resources'][(-1)] 
      raise ValidationError(errors) 
   for plugin in plugins.PluginImplementations(plugins.IResourceController): 
      plugin.after_delete(context, pkg_dict.get('resources', [])) 
   model.repo.commit()"," 'Delete a resource. 
 :param context: security context 
 :param data_dict: dictionary of data to save 
 :returns: None'","'Delete a resource from a dataset. 
 You must be a sysadmin or the owner of the resource to delete it. 
 :param id: the id of the resource 
 :type id: string'"
"def multisplit(container, name, xpath, before=True): 
    root = container.parsed(name) 
   nodes = root.xpath(xpath, namespaces=XPNSMAP) 
   if (not nodes): 
      raise AbortError((_(u'The   expression   %s   did   not   match   any   nodes') % xpath)) 
   for split_point in nodes: 
      if in_table(split_point): 
         raise AbortError(u'Cannot   split   inside   tables') 
      if split_point.tag.endswith(u'}body'): 
         raise AbortError(u'Cannot   split   on   the   <body>   tag') 
   for (i, tag) in enumerate(nodes): 
      tag.set(u'calibre-split-point', str(i)) 
   current = name 
   all_names = [name] 
   for i in xrange(len(nodes)): 
      current = split(container, current, (u'//*[@calibre-split-point=""%d""]' % i), before=before) 
      all_names.append(current) 
   for x in all_names: 
      for tag in container.parsed(x).xpath(u'//*[@calibre-split-point]'): 
         tag.attrib.pop(u'calibre-split-point') 
      container.dirty(x) 
   return all_names[1:]"," 'Split the container into several containers. 
 :param container: The container to split. 
 :param name: The name of the container to split. 
 :param xpath: The xpath to use to split the container. 
 :param before: Whether to split before or after the nodes matched by 
 :param xpath. 
 :return: A list of the new containers.'","'Split the specified file at multiple locations (all tags that match the specified XPath expression. See also: :func:`split`. 
 Splitting automatically migrates all links and references to the affected 
 files. 
 :param before: If True the splits occur before the identified element otherwise after it.'"
"def create_pool(lb, name, method='ROUND_ROBIN'): 
    if __opts__['load_balancers'].get(lb, None): 
      (username, password) = list(__opts__['load_balancers'][lb].values()) 
   else: 
      raise Exception('Unable   to   find   `{0}`   load   balancer'.format(lb)) 
   F5 = F5Mgmt(lb, username, password) 
   F5.create_pool(name, method) 
   return True", 'Create a new pool.',"'Create a pool on the F5 load balancer 
 CLI Examples: 
 .. code-block:: bash 
 salt-run f5.create_pool load_balancer pool_name loadbalance_method 
 salt-run f5.create_pool load_balancer my_pool ROUND_ROBIN'"
"def threads(request, document_slug): 
    doc = get_document(document_slug, request) 
   try: 
      sort = int(request.GET.get('sort', 0)) 
   except ValueError: 
      sort = 0 
   try: 
      desc = int(request.GET.get('desc', 0)) 
   except ValueError: 
      desc = 0 
   desc_toggle = (0 if desc else 1) 
   threads_ = sort_threads(doc.thread_set, sort, desc) 
   threads_ = paginate(request, threads_, per_page=kbforums.THREADS_PER_PAGE) 
   feed_urls = ((reverse('wiki.discuss.threads.feed', args=[document_slug]), ThreadsFeed().title(doc)),) 
   is_watching_forum = (request.user.is_authenticated() and NewThreadEvent.is_notifying(request.user, doc)) 
   return render(request, 'kbforums/threads.html', {'document': doc, 'threads': threads_, 'is_watching_forum': is_watching_forum, 'sort': sort, 'desc_toggle': desc_toggle, 'feeds': feed_urls})"," 'Displays a list of threads for a document. 
 :param request: The current request. 
 :param document_slug: The document slug. 
 :return: A rendered template. 
 :rtype: :class:`django.template.TemplateResponse`'",'View all the threads in a discussion forum.'
"def _read_uint64(f): 
    return np.uint64(struct.unpack('>Q', f.read(8))[0])", 'Read a uint64 from a file.','Read an unsigned 64-bit integer'
"def get(name, default=_UNSET, scope='global', window=None, tab=None): 
    reg = _get_registry(scope, window, tab) 
   try: 
      return reg[name] 
   except KeyError: 
      if (default is not _UNSET): 
         return default 
      else: 
         raise"," 'Get the value of a variable. 
 If the variable is not set, return the default value. 
 :param name: Name of the variable 
 :param default: Default value if the variable is not set 
 :param scope: Scope to use. 
 :param window: Window to use. 
 :param tab: Tab to use. 
 :return: Value of the variable'","'Helper function to get an object. 
 Args: 
 default: A default to return if the object does not exist.'"
"@command(usage='parse   links') 
 def extend_links(args): 
    args = parse_command_line(args, [], ['name']) 
   import lixian_tasks_extended 
   for x in (lixian_tasks_extended.extend_links if (not args.name) else lixian_tasks_extended.extend_links_name)(args): 
      print x", 'Extend links to the task\'s name.',"'usage: lx extend-links http://kuai.xunlei.com/d/... http://www.verycd.com/topics/... 
 parse and print links from pages 
 lx extend-links urls... 
 lx extend-links --name urls...'"
"def send(text, connections, **kwargs): 
    if (not isinstance(connections, collections.Iterable)): 
      connections = [connections] 
   router = get_router() 
   message = router.new_outgoing_message(text=text, connections=connections, **kwargs) 
   router.send_outgoing(message) 
   return message"," 'Send a message to the router. 
 :param text: The message to send. 
 :param connections: A list of connections to send the message to. 
 :param kwargs: Keyword arguments that will be passed to the router. 
 :return: The message that was sent. 
 :rtype: :class:`router.Message`'","'Creates an outgoing message and passes it to the router to be processed 
 and sent via the respective backend. 
 Arbitrary arguments are passed along to 
 :py:meth:`~rapidsms.router.blocking.BlockingRouter.new_outgoing_message`. 
 :param text: text message 
 :param connections: list or QuerySet of RapidSMS 
 :py:class:`~rapidsms.models.Connection` objects 
 :param kwargs: Extra kwargs to pass to 
 :py:class:`~rapidsms.messages.outgoing.OutgoingMessage` constructor 
 :returns: message constructed by router. A returned 
 message object does not indicate that router processing has 
 finished or even started, as this depends on the router defined 
 in :setting:`RAPIDSMS_ROUTER`. 
 :rtype: :py:class:`~rapidsms.messages.outgoing.OutgoingMessage`'"
"def load_template(template_name, template_source=None, template_path=None, template_hash=None, template_hash_name=None, template_user='root', template_group='root', template_mode='755', saltenv=None, template_engine='jinja', skip_verify=True, defaults=None, test=False, commit=True, debug=False, replace=False, **template_vars): 
    _rendered = '' 
   _loaded = {'result': True, 'comment': '', 'out': None} 
   loaded_config = None 
   if (template_engine not in salt.utils.templates.TEMPLATE_REGISTRY): 
      _loaded.update({'result': False, 'comment': 'Invalid   templating   engine!   Choose   between:   {tpl_eng_opts}'.format(tpl_eng_opts=',   '.join(list(salt.utils.templates.TEMPLATE_REGISTRY.keys())))}) 
      return _loaded 
   salt_render_prefixes = ('salt://', 'http://', 'https://', 'ftp://') 
   salt_render = False 
   for salt_render_prefix in salt_render_prefixes: 
      if (not salt_render): 
         salt_render = (salt_render or template_name.startswith(salt_render_prefix) or (template_path and template_path.startswith(salt_render_prefix))) 
   file_exists = __salt__['file.file_exists'](template_name) 
   if (template_source or template_path or file_exists or salt_render): 
      if template_source: 
         if (not saltenv): 
            saltenv = (template_path if template_path else 'base') 
         _rendered = __salt__['file.apply_template_on_contents'](contents=template_source, template=template_engine, context=template_vars, defaults=defaults, saltenv=saltenv) 
         if (not isinstance(_rendered, six.string_types)): 
            if ('result' in _rendered): 
               _loaded['result'] = _rendered['result'] 
            else: 
               _loaded['result'] = False 
            if ('comment' in _rendered): 
               _loaded['comment'] = _rendered['comment'] 
            else: 
               _loaded['comment'] = 'Error   while   rendering   the   template.' 
            return _loaded 
      else: 
         if (template_path and (not file_exists)): 
            template_name = __salt__['file.join'](template_path, template_name) 
            if (not saltenv): 
               saltenv = (template_path if (not salt_render) else 'base') 
         elif (salt_render and (not saltenv)): 
            saltenv = (template_path if template_path else 'base') 
         if (not saltenv): 
            saltenv = 'base' 
         _rand_filename = __salt__['random.hash'](template_name, 'md5') 
         _temp_file = __salt__['file.join']('/tmp', _rand_filename) 
         _managed = __salt__['file.get_managed'](name=_temp_file, source=template_name, source_hash=template_hash, source_hash_name=template_hash_name, user=template_user, group=template_group, mode=template_mode, template=template_engine, context=template_vars, defaults=defaults, saltenv=saltenv, skip_verify=skip_verify) 
         if ((not isinstance(_managed, (list, tuple))) and isinstance(_managed, six.string_types)): 
            _loaded['comment'] = _managed 
            _loaded['result'] = False 
         elif (isinstance(_managed, (list, tuple)) and (not (len(_managed) > 0))): 
            _loaded['result'] = False 
            _loaded['comment'] = 'Error   while   rendering   the   template.' 
         elif (isinstance(_managed, (list, tuple)) and (not (len(_managed[0]) > 0))): 
            _loaded['result'] = False 
            _loaded['comment'] = _managed[(-1)] 
         if _loaded['result']: 
            _temp_tpl_file = _managed[0] 
            _temp_tpl_file_exists = __salt__['file.file_exists'](_temp_tpl_file) 
            if (not _temp_tpl_file_exists): 
               _loaded['result'] = False 
               _loaded['comment'] = 'Error   while   rendering   the   template.' 
               return _loaded 
            _rendered = open(_temp_tpl_file).read() 
         else: 
            return _loaded 
      if debug: 
         loaded_config = _rendered 
      if _loaded['result']: 
         fun = 'load_merge_candidate' 
         if replace: 
            fun = 'load_replace_candidate' 
         _loaded = __proxy__['napalm.call'](fun, **{'config': _rendered}) 
   else: 
      load_templates_params = (defaults if defaults else {}) 
      load_templates_params.update(template_vars) 
      load_templates_params.update({'template_name': template_name, 'template_source': template_source, 'template_path': template_path, 'pillar': __pillar__, 'grains': __grains__, 'opts': __opts__}) 
      _loaded = __proxy__['napalm.call']('load_template', **load_templates_params) 
   return _config_logic(_loaded, test=test, commit_config=commit, loaded_config=loaded_config)"," 'Load a template from a source or path. 
 If a template source is provided, the template is rendered on the 
 fly and the rendered content is returned. 
 If a template path is provided, the template is loaded from disk and 
 the content is returned. 
 If a template name is provided, the template is loaded from disk and 
 the content is returned. 
 If a template name and a template source are provided, the template is 
 loaded from disk and the rendered content is returned. 
 If a template name, a template source and a template path are provided, 
 the template is loaded from disk and the rendered content is returned. 
 If a template name, a template source and a template path are provided, 
 the template is loaded from disk and the rendered content is returned. 
 If a template name, a template source and a template path are provided, 
 the template is loaded from disk and the rendered content is returned. 
 If a template name, a template source and a template path are provided, 
 the template is loaded from disk and the rendered content is returned. 
 If a template name, a template source and a template path are provided, 
","'Renders a configuration template (default: Jinja) and loads the result on the device. 
 By default this function will commit the changes. If there are no changes, 
 it does not commit, discards he config and the flag ``already_configured`` 
 will be set as ``True`` to point this out. 
 To avoid committing the configuration, set the argument ``test`` to ``True`` 
 and will discard (dry run). 
 To preserve the chnages, set ``commit`` to ``False``. 
 However, this is recommended to be used only in exceptional cases 
 when there are applied few consecutive states 
 and/or configuration changes. 
 Otherwise the user might forget that the config DB is locked 
 and the candidate config buffer is not cleared/merged in the running config. 
 To replace the config, set ``replace`` to ``True``. 
 template_name 
 Identifies path to the template source. 
 The template can be either stored on the local machine, either remotely. 
 The recommended location is under the ``file_roots`` 
 as specified in the master config file. 
 For example, let\'s suppose the ``file_roots`` is configured as: 
 .. code-block:: yaml 
 file_roots: 
 base: 
 - /etc/salt/states 
 Placing the template under ``/etc/salt/states/templates/example.jinja``, 
 it can be used as ``salt://templates/example.jinja``. 
 Alternatively, for local files, the user can specify the abolute path. 
 If remotely, the source can be retrieved via ``http``, ``https`` or ``ftp``. 
 Examples: 
 - ``salt://my_template.jinja`` 
 - ``/absolute/path/to/my_template.jinja`` 
 - ``http://example.com/template.cheetah`` 
 - ``https:/example.com/template.mako`` 
 - ``ftp://example.com/template.py`` 
 template_source: None 
 Inline config template to be rendered and loaded on the device. 
 template_path: None 
 Required only in case the argument ``template_name`` provides only the file basename 
 when referencing a local template using the absolute path. 
 E.g.: if ``template_name`` is specified as ``my_template.jinja``, 
 in order to find the template, this argument must be provided: 
 ``template_path: /absolute/path/to/``. 
 template_hash: None 
 Hash of the template file. Format: ``{hash_type: \'md5\', \'hsum\': <md5sum>}`` 
 .. versionadded:: 2016.11.2 
 template_hash_name: None 
 When ``template_hash`` refers to a remote file, 
 this specifies the filename to look for in that file. 
 .. versionadded:: 2016.11.2 
 template_group: root 
 Owner of file. 
 .. versionadded:: 2016.11.2 
 template_user: root 
 Group owner of file. 
 .. versionadded:: 2016.11.2 
 template_user: 755 
 Permissions of file. 
 .. versionadded:: 2016.11.2 
 saltenv: base 
 Specifies the template environment. 
 This will influence the relative imports inside the templates. 
 .. versionadded:: 2016.11.2 
 template_engine: jinja 
 The following templates engines are supported: 
 - :mod:`cheetah<salt.renderers.cheetah>` 
 - :mod:`genshi<salt.renderers.genshi>` 
 - :mod:`jinja<salt.renderers.jinja>` 
 - :mod:`mako<salt.renderers.mako>` 
 - :mod:`py<salt.renderers.py>` 
 - :mod:`wempy<salt.renderers.wempy>` 
 .. versionadded:: 2016.11.2 
 skip_verify: True 
 If ``True``, hash verification of remote file sources 
 (``http://``, ``https://``, ``ftp://``) will be skipped, 
 and the ``source_hash`` argument will be ignored. 
 .. versionadded:: 2016.11.2 
 test: False 
 Dry run? If set to ``True``, will apply the config, 
 discard and return the changes. 
 Default: ``False`` and will commit the changes on the device. 
 commit: True 
 Commit? (default: ``True``) 
 debug: False 
 Debug mode. Will insert a new key under the output dictionary, 
 as ``loaded_config`` contaning the raw result after the template was rendered. 
 .. versionadded:: 2016.11.2 
 replace: False 
 Load and replace the configuration. 
 .. versionadded:: 2016.11.2 
 defaults: None 
 Default variables/context passed to the template. 
 .. versionadded:: 2016.11.2 
 **template_vars 
 Dictionary with the arguments/context to be used when the template is rendered. 
 .. note:: 
 Do not explicitely specify this argument. 
 This represents any other variable that will be sent 
 to the template rendering system. 
 Please see the examples below! 
 :return: a dictionary having the following keys: 
 * result (bool): if the config was applied successfully. It is ``False`` only in case of failure. In case     there are no changes to be applied and successfully performs all operations it is still ``True`` and so will be     the ``already_configured`` flag (example below) 
 * comment (str): a message for the user 
 * already_configured (bool): flag to check if there were no changes applied 
 * loaded_config (str): the configuration loaded on the device, after rendering the template. Requires ``debug``     to be set as ``True`` 
 * diff (str): returns the config changes applied 
 The template can use variables from the ``grains``, ``pillar`` or ``opts``, for example: 
 .. code-block:: jinja 
 {% set router_model = grains.get(\'model\') -%} 
 {% set router_vendor = grains.get(\'vendor\') -%} 
 {% set os_version = grains.get(\'version\') -%} 
 {% set hostname = pillar.get(\'proxy\', {}).get(\'host\') -%} 
 {% if router_vendor|lower == \'juniper\' %} 
 system { 
 host-name {{hostname}}; 
 {% elif router_vendor|lower == \'cisco\' %} 
 hostname {{hostname}} 
 {% endif %} 
 CLI Examples: 
 .. code-block:: bash 
 salt \'*\' net.load_template set_ntp_peers peers=[192.168.0.1]  # uses NAPALM default templates 
 # inline template: 
 salt -G \'os:junos\' net.load_template set_hostname template_source=\'system { host-name {{host_name}}; }\'         host_name=\'MX480.lab\' 
 # inline template using grains info: 
 salt -G \'os:junos\' net.load_template set_hostname         template_source=\'system { host-name {{grains.model}}.lab; }\' 
 # if the device is a MX480, the command above will set the hostname as: MX480.lab 
 # inline template using pillar data: 
 salt -G \'os:junos\' net.load_template set_hostname template_source=\'system { host-name {{pillar.proxy.host}}; }\' 
 salt \'*\' net.load_template my_template template_path=\'/tmp/tpl/\' my_param=\'aaa\'  # will commit 
 salt \'*\' net.load_template my_template template_path=\'/tmp/tpl/\' my_param=\'aaa\' test=True  # dry run 
 salt \'*\' net.load_template salt://templates/my_stuff.jinja debug=True  # equivalent of the next command 
 salt \'*\' net.load_template my_stuff.jinja template_path=salt://templates/ debug=True 
 # in case the template needs to include files that are not under the same path (e.g. http://), 
 # to help the templating engine find it, you will need to specify the `saltenv` argument: 
 salt \'*\' net.load_template my_stuff.jinja template_path=salt://templates saltenv=/path/to/includes debug=True 
 # render a mako template: 
 salt \'*\' net.load_template salt://templates/my_stuff.mako template_engine=mako debug=True 
 # render remote template 
 salt -G \'os:junos\' net.load_template http://bit.ly/2fReJg7 test=True debug=True peers=[\'192.168.0.1\'] 
 salt -G \'os:ios\' net.load_template http://bit.ly/2gKOj20 test=True debug=True peers=[\'192.168.0.1\'] 
 Example output: 
 .. code-block:: python 
 \'comment\': \'\', 
 \'already_configured\': False, 
 \'result\': True, 
 \'diff\': \'[edit system]+  host-name edge01.bjm01\', 
 \'loaded_config\': \'system { host-name edge01.bjm01; }\'\''"
"def _array_to_datum(image, label, encoding): 
    if (not encoding): 
      if (image.ndim == 3): 
         image = image.transpose((2, 0, 1)) 
         if (image.shape[0] == 3): 
            image = image[[2, 1, 0], ...] 
      elif (image.ndim == 2): 
         image = image[np.newaxis, :, :] 
      else: 
         raise Exception(('Image   has   unrecognized   shape:   ""%s""' % image.shape)) 
      datum = caffe.io.array_to_datum(image, label) 
   else: 
      datum = caffe_pb2.Datum() 
      if (image.ndim == 3): 
         datum.channels = image.shape[2] 
      else: 
         datum.channels = 1 
      datum.height = image.shape[0] 
      datum.width = image.shape[1] 
      datum.label = label 
      s = StringIO() 
      if (encoding == 'png'): 
         PIL.Image.fromarray(image).save(s, format='PNG') 
      elif (encoding == 'jpg'): 
         PIL.Image.fromarray(image).save(s, format='JPEG', quality=90) 
      else: 
         raise ValueError('Invalid   encoding   type') 
      datum.data = s.getvalue() 
      datum.encoded = True 
   return datum"," 'Convert an image to a caffe datum. 
 Parameters 
 image : ndarray 
 Image to convert. 
 label : int 
 Label to associate with the image. 
 encoding : str, optional 
 Image encoding to use. 
 Examples 
 >>> _array_to_datum(np.zeros((10, 10, 3)), 0) 
 Datum(channels=3, height=10, width=10, label=0, data=b\'', 
 b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x",'Create a caffe Datum from a numpy.ndarray'
"def self_test(): 
    with tf.Session() as sess: 
      print('Self-test   for   neural   translation   model.') 
      model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2, 5.0, 32, 0.3, 0.99, num_samples=8) 
      sess.run(tf.global_variables_initializer()) 
      data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])], [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])]) 
      for _ in xrange(5): 
         bucket_id = random.choice([0, 1]) 
         (encoder_inputs, decoder_inputs, target_weights) = model.get_batch(data_set, bucket_id) 
         model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)", 'Run self-test for neural translation model.','Test the translation model.'
"def _calibrate_comp(comp, chs, row_names, col_names, mult_keys=('range', 'cal'), flip=False): 
    ch_names = [c['ch_name'] for c in chs] 
   row_cals = np.zeros(len(row_names)) 
   col_cals = np.zeros(len(col_names)) 
   for (names, cals, inv) in zip((row_names, col_names), (row_cals, col_cals), (False, True)): 
      for ii in range(len(cals)): 
         p = ch_names.count(names[ii]) 
         if (p != 1): 
            raise RuntimeError(('Channel   %s   does   not   appear   exactly   once   in   data' % names[ii])) 
         idx = ch_names.index(names[ii]) 
         val = (chs[idx][mult_keys[0]] * chs[idx][mult_keys[1]]) 
         val = (float((1.0 / val)) if inv else float(val)) 
         val = ((1.0 / val) if flip else val) 
         cals[ii] = val 
   comp['rowcals'] = row_cals 
   comp['colcals'] = col_cals 
   comp['data']['data'] = ((row_cals[:, None] * comp['data']['data']) * col_cals[None, :])"," 'Calibrate the data in a channel to the data in the other channels. 
 :param comp: A :class:`~pybamm.core.Channel` object. 
 :param chs: A list of :class:`~pybamm.core.Channel` objects. 
 :param row_names: A list of strings containing the names of the rows in 
 the calibration data. 
 :param col_names: A list of strings containing the names of the columns 
 in the calibration data. 
 :param mult_keys: A tuple containing the keys of the mults in the 
 calibration data. 
 :param flip: A boolean indicating whether the calibration data should 
 be flipped. 
 :return: A :class:`~pybamm.core.Channel` object.'",'Get row and column cals.'
"@treeio_login_required 
 @handle_response_format 
 def opportunity_view(request, opportunity_id, response_format='html'): 
    profile = request.user.profile 
   opportunity = get_object_or_404(Opportunity, pk=opportunity_id) 
   if ((not profile.has_permission(opportunity)) and (not profile.is_admin('treeio.sales'))): 
      return user_denied(request, message=""You   don't   have   access   to   this   Opportunity"") 
   form = _do_update_record(profile, request, opportunity) 
   return render_to_response('sales/opportunity_view', {'opportunity': opportunity, 'record_form': form}, context_instance=RequestContext(request), response_format=response_format)", 'View a single opportunity record.','Opportunity view'
"def start(name, call=None): 
    if (call != 'action'): 
      raise SaltCloudSystemExit('The   start   action   must   be   called   with   -a   or   --action.') 
   data = show_instance(name, call='action') 
   if (data.get('status') == 'active'): 
      return {'success': True, 'action': 'start', 'status': 'active', 'msg': 'Machine   is   already   running.'} 
   ret = query(droplet_id=data['id'], command='actions', args={'type': 'power_on'}, http_method='post') 
   return {'success': True, 'action': ret['action']['type'], 'state': ret['action']['status']}", 'Starts a droplet',"'Start a droplet in DigitalOcean. 
 .. versionadded:: 2015.8.8 
 name 
 The name of the droplet to start. 
 CLI Example: 
 .. code-block:: bash 
 salt-cloud -a start droplet_name'"
"def inpaint_biharmonic(img, mask, multichannel=False): 
    if (img.ndim < 1): 
      raise ValueError('Input   array   has   to   be   at   least   1D') 
   img_baseshape = (img.shape[:(-1)] if multichannel else img.shape) 
   if (img_baseshape != mask.shape): 
      raise ValueError('Input   arrays   have   to   be   the   same   shape') 
   if np.ma.isMaskedArray(img): 
      raise TypeError('Masked   arrays   are   not   supported') 
   img = skimage.img_as_float(img) 
   mask = mask.astype(np.bool) 
   kernel = ndi.morphology.generate_binary_structure(mask.ndim, 1) 
   mask_dilated = ndi.morphology.binary_dilation(mask, structure=kernel) 
   (mask_labeled, num_labels) = label(mask_dilated, return_num=True) 
   mask_labeled *= mask 
   if (not multichannel): 
      img = img[..., np.newaxis] 
   out = np.copy(img) 
   for idx_channel in range(img.shape[(-1)]): 
      known_points = img[..., idx_channel][(~ mask)] 
      limits = (np.min(known_points), np.max(known_points)) 
      for idx_region in range(1, (num_labels + 1)): 
         mask_region = (mask_labeled == idx_region) 
         _inpaint_biharmonic_single_channel(img[..., idx_channel], mask_region, out[..., idx_channel], limits) 
   if (not multichannel): 
      out = out[..., 0] 
   return out"," 'Inpaints a 2D image with the biharmonic interpolation. 
 Parameters 
 img : 2D array 
 The input image. 
 mask : 2D array 
 The mask array. 
 multichannel : bool, optional 
 If True, the function inpaints all channels of the input image. 
 Returns 
 out : 2D array 
 The inpainted image.'","'Inpaint masked points in image with biharmonic equations. 
 Parameters 
 img : (M[, N[, ..., P]][, C]) ndarray 
 Input image. 
 mask : (M[, N[, ..., P]]) ndarray 
 Array of pixels to be inpainted. Have to be the same shape as one 
 of the \'img\' channels. Unknown pixels have to be represented with 1, 
 known pixels - with 0. 
 multichannel : boolean, optional 
 If True, the last `img` dimension is considered as a color channel, 
 otherwise as spatial. 
 Returns 
 out : (M[, N[, ..., P]][, C]) ndarray 
 Input image with masked pixels inpainted. 
 References 
 .. [1]  N.S.Hoang, S.B.Damelin, ""On surface completion and image inpainting 
 by biharmonic functions: numerical aspects"", 
 http://www.ima.umn.edu/~damelin/biharmonic 
 Examples 
 >>> img = np.tile(np.square(np.linspace(0, 1, 5)), (5, 1)) 
 >>> mask = np.zeros_like(img) 
 >>> mask[2, 2:] = 1 
 >>> mask[1, 3:] = 1 
 >>> mask[0, 4:] = 1 
 >>> out = inpaint_biharmonic(img, mask)'"
"def get_test_result(): 
    global _TEST_RESULT 
   res = _TEST_RESULT 
   _TEST_RESULT = [] 
   return res"," 'Get the result of the last test run. 
 :return: TestResult'",'get test result and reset test_results'
"def savgol_filter(x, window_length, polyorder, deriv=0, delta=1.0, axis=(-1), mode='interp', cval=0.0): 
    if (mode not in ['mirror', 'constant', 'nearest', 'interp', 'wrap']): 
      raise ValueError(""mode   must   be   'mirror',   'constant',   'nearest'   'wrap'   or   'interp'."") 
   x = np.asarray(x) 
   if ((x.dtype != np.float64) and (x.dtype != np.float32)): 
      x = x.astype(np.float64) 
   coeffs = savgol_coeffs(window_length, polyorder, deriv=deriv, delta=delta) 
   if (mode == 'interp'): 
      y = convolve1d(x, coeffs, axis=axis, mode='constant') 
      _fit_edges_polyfit(x, window_length, polyorder, deriv, delta, axis, y) 
   else: 
      y = convolve1d(x, coeffs, axis=axis, mode=mode, cval=cval) 
   return y"," 'Apply Savitzky-Golay filter to a signal. 
 Parameters 
 x : array_like 
 The signal to be filtered. 
 window_length : int 
 The length of the window. 
 polyorder : int 
 The order of the polynomial. 
 deriv : int 
 The degree of the polynomial. 
 delta : float 
 The width of the filter. 
 axis : int 
 The axis to apply the filter to. 
 mode : string 
 The mode of the filter. 
 cval : float 
 The value to use for the filter output when the signal is out of range. 
 Returns 
 y : array 
 The filtered signal. 
 Notes 
 The Savitzky-Golay filter is a linear filter that approximates the signal 
 by a polynomial of order `polyorder`. The filter is applied to the signal 
 using a window of length `window_length` with a window of length `window_length`. 
 The filter is applied to the signal using a window of length `window_length` with a 
 window of length `window_length`. 
 References 
 .. [","'Apply a Savitzky-Golay filter to an array. 
 This is a 1-d filter.  If `x`  has dimension greater than 1, `axis` 
 determines the axis along which the filter is applied. 
 Parameters 
 x : array_like 
 The data to be filtered.  If `x` is not a single or double precision 
 floating point array, it will be converted to type `numpy.float64` 
 before filtering. 
 window_length : int 
 The length of the filter window (i.e. the number of coefficients). 
 `window_length` must be a positive odd integer. 
 polyorder : int 
 The order of the polynomial used to fit the samples. 
 `polyorder` must be less than `window_length`. 
 deriv : int, optional 
 The order of the derivative to compute.  This must be a 
 nonnegative integer.  The default is 0, which means to filter 
 the data without differentiating. 
 delta : float, optional 
 The spacing of the samples to which the filter will be applied. 
 This is only used if deriv > 0.  Default is 1.0. 
 axis : int, optional 
 The axis of the array `x` along which the filter is to be applied. 
 Default is -1. 
 mode : str, optional 
 Must be \'mirror\', \'constant\', \'nearest\', \'wrap\' or \'interp\'.  This 
 determines the type of extension to use for the padded signal to 
 which the filter is applied.  When `mode` is \'constant\', the padding 
 value is given by `cval`.  See the Notes for more details on \'mirror\', 
 \'constant\', \'wrap\', and \'nearest\'. 
 When the \'interp\' mode is selected (the default), no extension 
 is used.  Instead, a degree `polyorder` polynomial is fit to the 
 last `window_length` values of the edges, and this polynomial is 
 used to evaluate the last `window_length // 2` output values. 
 cval : scalar, optional 
 Value to fill past the edges of the input if `mode` is \'constant\'. 
 Default is 0.0. 
 Returns 
 y : ndarray, same shape as `x` 
 The filtered data. 
 See Also 
 savgol_coeffs 
 Notes 
 Details on the `mode` options: 
 \'mirror\': 
 Repeats the values at the edges in reverse order.  The value 
 closest to the edge is not included. 
 \'nearest\': 
 The extension contains the nearest input value. 
 \'constant\': 
 The extension contains the value given by the `cval` argument. 
 \'wrap\': 
 The extension contains the values from the other end of the array. 
 For example, if the input is [1, 2, 3, 4, 5, 6, 7, 8], and 
 `window_length` is 7, the following shows the extended data for 
 the various `mode` options (assuming `cval` is 0):: 
 mode       |   Ext   |         Input          |   Ext 
 \'mirror\'   | 4  3  2 | 1  2  3  4  5  6  7  8 | 7  6  5 
 \'nearest\'  | 1  1  1 | 1  2  3  4  5  6  7  8 | 8  8  8 
 \'constant\' | 0  0  0 | 1  2  3  4  5  6  7  8 | 0  0  0 
 \'wrap\'     | 6  7  8 | 1  2  3  4  5  6  7  8 | 1  2  3 
 .. versionadded:: 0.14.0 
 Examples 
 >>> from scipy.signal import savgol_filter 
 >>> np.set_printoptions(precision=2)  # For compact display. 
 >>> x = np.array([2, 2, 5, 2, 1, 0, 1, 4, 9]) 
 Filter with a window length of 5 and a degree 2 polynomial.  Use 
 the defaults for all other parameters. 
 >>> savgol_filter(x, 5, 2) 
 array([ 1.66,  3.17,  3.54,  2.86,  0.66,  0.17,  1.  ,  4.  ,  9.  ]) 
 Note that the last five values in x are samples of a parabola, so 
 when mode=\'interp\' (the default) is used with polyorder=2, the last 
 three values are unchanged.  Compare that to, for example, 
 `mode=\'nearest\'`: 
 >>> savgol_filter(x, 5, 2, mode=\'nearest\') 
 array([ 1.74,  3.03,  3.54,  2.86,  0.66,  0.17,  1.  ,  4.6 ,  7.97])'"
"def test_takes_kwargs(): 
    assert hug.introspect.takes_kwargs(function_with_kwargs) 
   assert (not hug.introspect.takes_kwargs(function_with_args)) 
   assert (not hug.introspect.takes_kwargs(function_with_neither)) 
   assert hug.introspect.takes_kwargs(function_with_both)"," 'Test that introspect.takes_kwargs() correctly detects whether a function 
 takes keyword arguments.'",'Test to ensure hug introspection can correctly identify when a function takes kwargs'
"def load_app(target): 
    global NORUN 
   (NORUN, nr_old) = (True, NORUN) 
   tmp = default_app.push() 
   try: 
      rv = load(target) 
      return (rv if callable(rv) else tmp) 
   finally: 
      default_app.remove(tmp) 
      NORUN = nr_old"," 'Loads an application and returns it. 
 If the application is a callable, it is returned directly. 
 Otherwise, the application is pushed onto the default application stack, 
 and the return value is the previous application.'","'Load a bottle application from a module and make sure that the import 
 does not affect the current default application, but returns a separate 
 application object. See :func:`load` for the target parameter.'"
"def compute_f1(precision, recall): 
    f1 = (((2.0 * precision) * recall) / T.maximum(1, (precision + recall))) 
   return f1"," 'Computes F1 score. 
 Parameters 
 precision : float 
 Recall 
 recall : float 
 Precision 
 f1 : float 
 F1 score 
 Examples 
 >>> from sympy.utilities.iterables import cartprod 
 >>> from sympy.utilities.iterables import product 
 >>> from sympy.utilities.iterables import cartprod 
 >>> from sympy.utilities.iterables import product 
 >>> from sympy.utilities.iterables import cartprod 
 >>> from sympy.utilities.iterables import product 
 >>> from sympy.utilities.iterables import cartprod 
 >>> from sympy.utilities.iterables import product 
 >>> from sympy.utilities.iterables import cartprod 
 >>> from sympy.utilities.iterables import product 
 >>> from sympy.utilities.iterables import cartprod 
 >>> from sympy.utilities.iterables import product 
 >>> from sympy.utilities.iterables import cartprod 
 >>> from sympy.utilities.iterables import product 
 >>> from sympy.util","'Computes the f1 score for the binary classification. 
 Computed as, 
 f1 = 2 * precision * recall / (precision + recall) 
 Parameters 
 precision : Variable 
 Precision score of the binary decisions. 
 recall : Variable 
 Recall score of the binary decisions. 
 Returns 
 f1 : Variable 
 f1 score for the binary decisions.'"
"def colorize(lead, num, color): 
    s = (u'%s=%-4s' % (lead, str(num))) 
   if ((num != 0) and ANSIBLE_COLOR and (color is not None)): 
      s = stringc(s, color) 
   return s", 'Colorize a number.','Print \'lead\' = \'num\' in \'color\''
"def hstack(tup): 
    arrs = [cupy.atleast_1d(a) for a in tup] 
   axis = 1 
   if (arrs[0].ndim == 1): 
      axis = 0 
   return concatenate(arrs, axis)"," 'Concatenate a list of arrays along the specified axis. 
 Parameters 
 tup : tuple of arrays 
 The list of arrays to concatenate. 
 Returns 
 The concatenated array. 
 Examples 
 >>> a = np.arange(10).reshape(5, 2) 
 >>> b = np.arange(5).reshape(2, 5) 
 >>> hstack((a, b)) 
 array([[ 0,  1], 
 [ 2,  3], 
 [ 4,  5], 
 [ 6,  7], 
 [ 8,  9]]) 
 See Also 
 concatenate, vstack, hstack_to_csc'","'Stacks arrays horizontally. 
 If an input array has one dimension, then the array is treated as a 
 horizontal vector and stacked along the first axis. Otherwise, the array is 
 stacked along the second axis. 
 Args: 
 tup (sequence of arrays): Arrays to be stacked. 
 Returns: 
 cupy.ndarray: Stacked array. 
 .. seealso:: :func:`numpy.hstack`'"
"def dump_psutil(): 
    output_file = (PROFILING_OUTPUT_FMT % get_filename_fmt()) 
   process_info = {} 
   for proc in psutil.process_iter(): 
      try: 
         pinfo = proc.as_dict(attrs=['pid', 'name', 'parent', 'status', 'io_counters', 'num_threads', 'cpu_times', 'cpu_percent', 'memory_info_ex', 'memory_percent', 'exe', 'cmdline']) 
      except psutil.NoSuchProcess: 
         pass 
      else: 
         for (info_name, info_data) in pinfo.iteritems(): 
            if hasattr(info_data, '_asdict'): 
               pinfo[info_name] = dict(info_data._asdict()) 
            else: 
               pinfo[info_name] = info_data 
         process_info[pinfo['pid']] = pinfo 
   netinfo = psutil.net_io_counters(pernic=True) 
   for (key, value) in netinfo.iteritems(): 
      netinfo[key] = value._asdict() 
   pids_to_show = [] 
   for (pid, pinfo) in process_info.iteritems(): 
      exe = str(pinfo['exe']) 
      if (('python' in exe) and ('w3af' in exe)): 
         pids_to_show.append(pid) 
   ps_mem_data = ps_mem_to_json(*get_memory_usage(pids_to_show, True)) 
   psutil_data = {'CPU': psutil.cpu_times()._asdict(), 'Load   average': os.getloadavg(), 'Virtual   memory': psutil.virtual_memory()._asdict(), 'Swap   memory': psutil.swap_memory()._asdict(), 'Network': netinfo, 'Processes': process_info, 'ps_mem': ps_mem_data, 'Thread   CPU   usage': get_threads_cpu_percent()} 
   json.dump(psutil_data, file(output_file, 'w'), indent=4, sort_keys=True)", 'Dump the profiling output to the file specified.','Dumps operating system information to file'
"def inputs(eval_data, data_dir, batch_size): 
    if (not eval_data): 
      filenames = [os.path.join(data_dir, ('data_batch_%d.bin' % i)) for i in xrange(1, 6)] 
      num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN 
   else: 
      filenames = [os.path.join(data_dir, 'test_batch.bin')] 
      num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL 
   for f in filenames: 
      if (not tf.gfile.Exists(f)): 
         raise ValueError(('Failed   to   find   file:   ' + f)) 
   filename_queue = tf.train.string_input_producer(filenames) 
   read_input = read_cifar10(filename_queue) 
   reshaped_image = tf.cast(read_input.uint8image, tf.float32) 
   height = IMAGE_SIZE 
   width = IMAGE_SIZE 
   resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, width, height) 
   float_image = tf.image.per_image_standardization(resized_image) 
   float_image.set_shape([height, width, 3]) 
   read_input.label.set_shape([1]) 
   min_fraction_of_examples_in_queue = 0.4 
   min_queue_examples = int((num_examples_per_epoch * min_fraction_of_examples_in_queue)) 
   return _generate_image_and_label_batch(float_image, read_input.label, min_queue_examples, batch_size, shuffle=False)"," 'Returns a batch of images and labels. 
 Args: 
 eval_data: A boolean, if True, returns a batch of images and labels for 
 evaluation. 
 data_dir: The directory containing the training data. 
 batch_size: The number of examples to return. 
 Returns: 
 A tuple of (images, labels) where images is a tensor of shape 
 [batch_size, height, width, 3] and labels is a tensor of shape 
 [batch_size].'","'Construct input for CIFAR evaluation using the Reader ops. 
 Args: 
 eval_data: bool, indicating if one should use the train or eval data set. 
 data_dir: Path to the CIFAR-10 data directory. 
 batch_size: Number of images per batch. 
 Returns: 
 images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size. 
 labels: Labels. 1D tensor of [batch_size] size.'"
"def scope_to_list(scope): 
    if isinstance(scope, list): 
      return [unicode_type(s) for s in scope] 
   if isinstance(scope, set): 
      scope_to_list(list(scope)) 
   elif (scope is None): 
      return None 
   else: 
      return scope.split(u'   ')"," 'Convert a scope to a list of strings. 
 :param scope: 
 :type scope: 
 :return: 
 :rtype: 
 :raises: 
 :param scope: 
 :type scope: 
 :return: 
 :rtype: 
 :raises: 
 :param scope: 
 :type scope: 
 :return: 
 :rtype: 
 :raises: 
 :param scope: 
 :type scope: 
 :return: 
 :rtype: 
 :raises: 
 :param scope: 
 :type scope: 
 :return: 
 :rtype: 
 :raises: 
 :param scope: 
 :type scope: 
 :return: 
 :rtype: 
 :raises: 
 :param scope: 
 :type scope: 
 :return: 
 :rtype: 
 :raises: 
 :param scope: 
 :type scope: 
 :return: 
 :rtype: 
 :raises: 
 :param scope: 
 :type scope: 
",'Convert a space separated string to a list of scopes.'
"@cython.locals(n=int, i=int, j=int) 
 def permutations(iterable): 
    pool = tuple(iterable) 
   n = len(pool) 
   indices = list(range(n)) 
   cycles = list(range(1, (n + 1)))[::(-1)] 
   (yield [pool[i] for i in indices]) 
   while n: 
      for i in reversed(range(n)): 
         j = (cycles[i] - 1) 
         if (j == 0): 
            indices[i:] = (indices[(i + 1):] + indices[i:(i + 1)]) 
            cycles[i] = (n - i) 
         else: 
            cycles[i] = j 
            (indices[i], indices[(- j)]) = (indices[(- j)], indices[i]) 
            (yield [pool[i] for i in indices]) 
            break 
      else: 
         return"," 'Permute an iterable. 
 >>> permutations([1, 2, 3]) 
 [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)] 
 >>> permutations([1, 2, 3, 4]) 
 [(1, 2, 3, 4), (1, 2, 4, 3), (1, 3, 2, 4), (1, 3, 4, 2), (1, 4, 2, 3), (1, 4, 3, 2), (2, 1, 3, 4), (2, 1, 4, 3), (2, 3, 1, 4), (2, 3, 4, 1), (2, 4, 1, 3), (2, 4, 3, 1), (3, 1, 2, 4), (3, ","'permutations(range(3), 2) --> (0,1) (0,2) (1,0) (1,2) (2,0) (2,1)'"
"def _push_assemble_error_status(status, ret, logs): 
    comment = 'An   error   occurred   pushing   your   image' 
   status['out'] = '' 
   try: 
      status['out'] += ('\n' + ret) 
      for err_log in logs: 
         if isinstance(err_log, dict): 
            if ('errorDetail' in err_log): 
               if ('code' in err_log['errorDetail']): 
                  msg = '\n{0}\n{1}:   {2}'.format(err_log['error'], err_log['errorDetail']['code'], err_log['errorDetail']['message']) 
               else: 
                  msg = '\n{0}\n{1}'.format(err_log['error'], err_log['errorDetail']['message']) 
               comment += msg 
   except Exception: 
      trace = traceback.format_exc() 
      status['out'] = 'An   error   occurred   while   parsing   error   output:\n{0}'.format(trace) 
   _invalid(status, comment=comment) 
   return status"," 'Push the image, and if it fails, assemble the error message. 
 :param status: The status dictionary to update. 
 :param ret: The error output from the push. 
 :param logs: The error logs from the push. 
 :rtype: dict'","'Given input in this form:: 
 u\'{""status"":""Pulling repository foo/ubuntubox""}: 
 ""image (latest) from foo/  ... 
 rogress"":""complete"",""id"":""2c80228370c9""}\' 
 construct something like that (load json data is possible):: 
 [u\'{""status"":""Pulling repository foo/ubuntubox""\', 
 {""status"":""Download"",""progress"":""complete"",""id"":""2c80228370c9""}]'"
"def url(context, link_url): 
    site = context['__CACTUS_SITE__'] 
   page = context['__CACTUS_CURRENT_PAGE__'] 
   url = site.get_url_for_page(link_url) 
   if (url is None): 
      link_url_index = os.path.join(link_url, 'index.html') 
      url_link_url_index = site.get_url_for_page(link_url_index) 
      if (url_link_url_index is None): 
         logger.warning('%s:   page   resource   does   not   exist:   %s', page.link_url, link_url) 
      url = link_url 
   if site.prettify_urls: 
      return url.rsplit('index.html', 1)[0] 
   return url"," 'Returns the URL for the given page. 
 If the page does not exist, it will try to find the index.html page 
 and return that. 
 :param context: The current context. 
 :type context: dict 
 :param link_url: The URL of the page. 
 :type link_url: str'","'Get the path for a page in the Cactus build. 
 We\'ll need this because paths can be rewritten with prettifying.'"
"def histograms_plot(): 
    (mu, sigma) = (100, 15) 
   x = (mu + (sigma * np.random.randn(10000))) 
   num_bins = 50 
   (n, bins, patches) = plt.hist(x, bins=num_bins, normed=1, color='green', alpha=0.6, label='hist') 
   y = mlab.normpdf(bins, mu, sigma) 
   plt.plot(bins, y, 'r--', label='line') 
   plt.legend(loc='upper   left', shadow=True) 
   plt.show() 
   return", 'Plot the histogram of the data and the normal distribution.','histograms plot'
"def test_array___setitem__(): 
    pass", 'Test __setitem__','TODO'
"def register_linker(name, linker): 
    if (name in predefined_linkers): 
      raise ValueError(('Linker   name   already   taken:   %s' % name)) 
   predefined_linkers[name] = linker"," 'Register a linker. 
 Parameters 
 name : str 
 The name of the linker. 
 linker : Linker 
 The linker to register.'",'Add a `Linker` which can be referred to by `name` in `Mode`.'
"def pkcs_mgf1(mgfSeed, maskLen, h): 
    if (not _hashFuncParams.has_key(h)): 
      warning('pkcs_mgf1:   invalid   hash   (%s)   provided') 
      return None 
   hLen = _hashFuncParams[h][0] 
   hFunc = _hashFuncParams[h][1] 
   if (maskLen > ((2 ** 32) * hLen)): 
      warning('pkcs_mgf1:   maskLen   >   2**32   *   hLen') 
      return None 
   T = '' 
   maxCounter = math.ceil((float(maskLen) / float(hLen))) 
   counter = 0 
   while (counter < maxCounter): 
      C = pkcs_i2osp(counter, 4) 
      T += hFunc((mgfSeed + C)) 
      counter += 1 
   return T[:maskLen]"," 'MGF1 is a cryptographic hash function used in the creation of a 
 message authentication code (MAC). It is specified in RFC 2898. 
 The MGF1 function is defined as follows: 
 MGF1(m, H) = H(m) | H(H(m)) | H(H(H(m))) | ... | H(H(H(H(m)))) 
 where H is the hash function and m is the message. 
 The MGF1 function is used to create a MAC by combining the message 
 with a secret key. The resulting MAC is a sequence of bytes that 
 is used to verify the integrity of the message. 
 Parameters 
 mgfSeed : a string of bytes 
 maskLen : a positive integer 
 h : a hash function name 
 Returns 
 a string of bytes. 
 Notes 
 This function is used to create a MAC for a message. The message is 
 combined with a secret key to create a MAC. The resulting MAC is 
 used to verify the integrity of the message. 
 Examples 
 >>> from Crypto.","'Implements generic MGF1 Mask Generation function as described in 
 Appendix B.2.1 of RFC 3447. The hash function is passed by name. 
 valid values are \'md2\', \'md4\', \'md5\', \'sha1\', \'tls, \'sha256\', 
 \'sha384\' and \'sha512\'. Returns None on error. 
 Input: 
 mgfSeed: seed from which mask is generated, an octet string 
 maskLen: intended length in octets of the mask, at most 2^32 * hLen 
 hLen (see below) 
 h      : hash function name (in \'md2\', \'md4\', \'md5\', \'sha1\', \'tls\', 
 \'sha256\', \'sha384\'). hLen denotes the length in octets of 
 the hash function output. 
 Output: 
 an octet string of length maskLen'"
"def _to_gapic_image(image): 
    if (image.content is not None): 
      return image_annotator_pb2.Image(content=image.content) 
   if (image.source is not None): 
      return image_annotator_pb2.Image(source=image_annotator_pb2.ImageSource(gcs_image_uri=image.source)) 
   raise ValueError('No   image   content   or   source   found.')", 'Convert an image annotation to a protobuf message.',"'Helper function to convert an ``Image`` to a gRPC ``Image``. 
 :type image: :class:`~google.cloud.vision.image.Image` 
 :param image: Local ``Image`` class to be converted to gRPC ``Image``. 
 :rtype: :class:`~google.cloud.grpc.vision.v1.image_annotator_pb2.Image` 
 :returns: gRPC ``Image`` converted from 
 :class:`~google.cloud.vision.image.Image`.'"
"def init_cachedir(base=None): 
    if (base is None): 
      base = __opts__['cachedir'] 
   needed_dirs = (base, os.path.join(base, 'requested'), os.path.join(base, 'active')) 
   for dir_ in needed_dirs: 
      if (not os.path.exists(dir_)): 
         os.makedirs(dir_) 
      os.chmod(base, 493) 
   return base", 'Initialize the cache directory','Initialize the cachedir needed for Salt Cloud to keep track of minions'
"def load_extensions(): 
    installed_extensions = [] 
   for entry_point in pkg_resources.iter_entry_points(u'mopidy.ext'): 
      logger.debug(u'Loading   entry   point:   %s', entry_point) 
      try: 
         extension_class = entry_point.load(require=False) 
      except Exception as e: 
         logger.exception((u'Failed   to   load   extension   %s:   %s' % (entry_point.name, e))) 
         continue 
      try: 
         if (not issubclass(extension_class, Extension)): 
            raise TypeError 
      except TypeError: 
         logger.error(u'Entry   point   %s   did   not   contain   a   valid   extensionclass:   %r', entry_point.name, extension_class) 
         continue 
      try: 
         extension = extension_class() 
         config_schema = extension.get_config_schema() 
         default_config = extension.get_default_config() 
         command = extension.get_command() 
      except Exception: 
         logger.exception(u'Setup   of   extension   from   entry   point   %s   failed,   ignoring   extension.', entry_point.name) 
         continue 
      installed_extensions.append(ExtensionData(extension, entry_point, config_schema, default_config, command)) 
      logger.debug(u'Loaded   extension:   %s   %s', extension.dist_name, extension.version) 
   names = (ed.extension.ext_name for ed in installed_extensions) 
   logger.debug(u'Discovered   extensions:   %s', u',   '.join(names)) 
   return installed_extensions", 'Load all installed extensions.',"'Find all installed extensions. 
 :returns: list of installed extensions'"
"def get_checks_places(osname, agentConfig): 
    try: 
      checksd_path = get_checksd_path(osname) 
   except PathNotFound as e: 
      log.error(e.args[0]) 
      sys.exit(3) 
   places = [(lambda name: os.path.join(agentConfig['additional_checksd'], ('%s.py' % name)))] 
   try: 
      sdk_integrations = get_sdk_integrations_path(osname) 
      places.append((lambda name: os.path.join(sdk_integrations, name, 'check.py'))) 
   except PathNotFound: 
      log.debug('No   sdk   integrations   path   found') 
   places.append((lambda name: os.path.join(checksd_path, ('%s.py' % name)))) 
   return places"," 'Returns a list of places to look for checks.py files. 
 :param osname: 
 :param agentConfig: 
 :return: 
 :rtype: 
 :type osname: str 
 :type agentConfig: dict'","'Return a list of methods which, when called with a check name, will each return a check path to inspect'"
"@require_admin_context 
 def group_type_access_remove(context, type_id, project_id): 
    group_type_id = _group_type_get_id_from_group_type(context, type_id) 
   count = _group_type_access_query(context).filter_by(group_type_id=group_type_id).filter_by(project_id=project_id).soft_delete(synchronize_session=False) 
   if (count == 0): 
      raise exception.GroupTypeAccessNotFound(group_type_id=type_id, project_id=project_id)"," 'Remove group type access. 
 :param context: security context 
 :param type_id: group type id 
 :param project_id: project id'",'Remove given tenant from the group type access list.'
"def _plot_traces(params): 
    params['text'].set_visible(False) 
   ax = params['ax'] 
   butterfly = params['butterfly'] 
   if butterfly: 
      ch_start = 0 
      n_channels = len(params['picks']) 
      data = (params['data'] * params['butterfly_scale']) 
   else: 
      ch_start = params['ch_start'] 
      n_channels = params['n_channels'] 
      data = (params['data'] * params['scale_factor']) 
   offsets = params['offsets'] 
   lines = params['lines'] 
   epochs = params['epochs'] 
   n_times = len(epochs.times) 
   tick_list = list() 
   start_idx = int((params['t_start'] / n_times)) 
   end = (params['t_start'] + params['duration']) 
   end_idx = int((end / n_times)) 
   xlabels = params['labels'][start_idx:] 
   event_ids = params['epochs'].events[:, 2] 
   params['ax2'].set_xticklabels(event_ids[start_idx:]) 
   ax.set_xticklabels(xlabels) 
   ylabels = ax.yaxis.get_ticklabels() 
   for line_idx in range(n_channels): 
      ch_idx = (line_idx + ch_start) 
      if (line_idx >= len(lines)): 
         break 
      elif (ch_idx < len(params['ch_names'])): 
         if butterfly: 
            ch_type = params['types'][ch_idx] 
            if (ch_type == 'grad'): 
               offset = offsets[0] 
            elif (ch_type == 'mag'): 
               offset = offsets[1] 
            elif (ch_type == 'eeg'): 
               offset = offsets[2] 
            elif (ch_type == 'eog'): 
               offset = offsets[3] 
            elif (ch_type == 'ecg'): 
               offset = offsets[4] 
            else: 
               lines[line_idx].set_segments(list()) 
         else: 
            tick_list += [params['ch_names'][ch_idx]] 
            offset = offsets[line_idx] 
         this_data = data[ch_idx] 
         ydata = (offset - this_data) 
         xdata = params['times'][:params['duration']] 
         num_epochs = np.min([params['n_epochs'], len(epochs.events)]) 
         segments = np.split(np.array((xdata, ydata)).T, num_epochs) 
         ch_name = params['ch_names'][ch_idx] 
         if (ch_name in params['info']['bads']): 
            if (not butterfly): 
               this_color = params['bad_color'] 
               ylabels[line_idx].set_color(this_color) 
            this_color = np.tile(params['bad_color'], (num_epochs, 1)) 
            for bad_idx in params['bads']: 
               if ((bad_idx < start_idx) or (bad_idx > end_idx)): 
                  continue 
               this_color[(bad_idx - start_idx)] = (1.0, 0.0, 0.0) 
            lines[line_idx].set_zorder(2) 
         else: 
            this_color = params['colors'][ch_idx][start_idx:end_idx] 
            lines[line_idx].set_zorder(3) 
            if (not butterfly): 
               ylabels[line_idx].set_color('black') 
         lines[line_idx].set_segments(segments) 
         lines[line_idx].set_color(this_color) 
      else: 
         lines[line_idx].set_segments(list()) 
   ax.set_xlim(params['times'][0], (params['times'][0] + params['duration']), False) 
   params['ax2'].set_xlim(params['times'][0], (params['times'][0] + params['duration']), False) 
   if butterfly: 
      factor = ((-1.0) / params['butterfly_scale']) 
      labels = np.empty(20, dtype='S15') 
      labels.fill('') 
      ticks = ax.get_yticks() 
      idx_offset = 1 
      if ('grad' in params['types']): 
         labels[(idx_offset + 1)] = '0.00' 
         for idx in [idx_offset, (idx_offset + 2)]: 
            labels[idx] = '{0:.2f}'.format(((((ticks[idx] - offsets[0]) * params['scalings']['grad']) * 10000000000000.0) * factor)) 
         idx_offset += 4 
      if ('mag' in params['types']): 
         labels[(idx_offset + 1)] = '0.00' 
         for idx in [idx_offset, (idx_offset + 2)]: 
            labels[idx] = '{0:.2f}'.format(((((ticks[idx] - offsets[1]) * params['scalings']['mag']) * 1000000000000000.0) * factor)) 
         idx_offset += 4 
      if ('eeg' in params['types']): 
         labels[(idx_offset + 1)] = '0.00' 
         for idx in [idx_offset, (idx_offset + 2)]: 
            labels[idx] = '{0:.2f}'.format(((((ticks[idx] - offsets[2]) * params['scalings']['eeg']) * 1000000.0) * factor)) 
         idx_offset += 4 
      if ('eog' in params['types']): 
         labels[(idx_offset + 1)] = '0.00' 
         for idx in [idx_offset, (idx_offset + 2)]: 
            labels[idx] = '{0:.2f}'.format(((((ticks[idx] - offsets[3]) * params['scalings']['eog']) * 1000000.0) * factor)) 
         idx_offset += 4 
      if ('ecg' in params['types']): 
         labels[(idx_offset + 1)] = '0.00' 
         for idx in [idx_offset, (idx_offset + 2)]: 
            labels[idx] = '{0:.2f}'.format(((((ticks[idx] - offsets[4]) * params['scalings']['ecg']) * 1000000.0) * factor)) 
      ax.set_yticklabels(labels, fontsize=12, color='black') 
   else: 
      ax.set_yticklabels(tick_list, fontsize=12) 
   if (params['events'] is not None): 
      _draw_event_lines(params) 
   params['vsel_patch'].set_y(ch_start) 
   params['fig'].canvas.draw() 
   if (params['fig_proj'] is not None): 
      params['fig_proj'].canvas.draw()", 'Plot the traces','Plot concatenated epochs.'
"def refine_Determinant(expr, assumptions): 
    if ask(Q.orthogonal(expr.arg), assumptions): 
      return S.One 
   elif ask(Q.singular(expr.arg), assumptions): 
      return S.Zero 
   elif ask(Q.unit_triangular(expr.arg), assumptions): 
      return S.One 
   return expr"," 'Return a determinant that is refined to be unitary, orthogonal, or 
 singular. 
 Examples 
 >>> from sympy import Matrix, Q 
 >>> from sympy.abc import x, y 
 >>> expr = Matrix([[x, y], [y, x]]) 
 >>> refine_Determinant(expr, Q.orthogonal) 
 1 
 >>> refine_Determinant(expr, Q.singular) 
 0 
 >>> refine_Determinant(expr, Q.unit_triangular) 
 1'","'>>> from sympy import MatrixSymbol, Q, assuming, refine, det 
 >>> X = MatrixSymbol(\'X\', 2, 2) 
 >>> det(X) 
 Determinant(X) 
 >>> with assuming(Q.orthogonal(X)): 
 ...     print(refine(det(X))) 
 1'"
"def get_rate_limit(): 
    return getattr(g, '_rate_limit', None)"," 'Return the rate limit for this client. 
 :return: A float or None if the rate limit is not set.'","'If available, returns a RateLimit instance which is valid for the 
 current request-response. 
 .. versionadded:: 0.0.7'"
"@with_setup(prepare_stdout) 
 def test_output_outlines_success_colorful(): 
    runner = Runner(join_path('zh-TW', 'success', 'outlines.feature'), verbosity=3, no_color=False) 
   runner.run() 
   assert_stdout_lines(u'\n\x1b[1;37m\u7279\u6027:   \u4e2d\u6587\u5834\u666f\u6a21\u677f                                 \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines.feature:3\x1b[0m\n\x1b[1;37m      \u4e2d\u6587\u5834\u666f\u6a21\u677f\u5716\u8868\u6e2c\u8a66                     \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines.feature:4\x1b[0m\n\n\x1b[1;37m      \u5834\u666f\u6a21\u677f:   \u7528\u8868\u683c\u63cf\u8ff0\u5834\u666f         \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines.feature:6\x1b[0m\n\x1b[0;36m            \u5982\u679c   \u8f38\u5165\u662f<\u8f38\u5165>                        \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines_steps.py:13\x1b[0m\n\x1b[0;36m            \u7576   \u57f7\u884c<\u8655\u7406>\u6642                              \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines_steps.py:22\x1b[0m\n\x1b[0;36m            \u90a3\u9ebd   \u5f97\u5230<\u7d50\u679c>                              \x1b[1;30m#   tests/functional/language_specific_features/zh-TW/success/outlines_steps.py:31\x1b[0m\n\n\x1b[1;37m      \u4f8b\u5982:\x1b[0m\n\x1b[0;36m         \x1b[1;37m   |\x1b[0;36m   \u8f38\u5165\x1b[1;37m   |\x1b[0;36m   \u8655\u7406\x1b[1;37m   |\x1b[0;36m   \u7d50\u679c                        \x1b[1;37m   |\x1b[0;36m\x1b[0m\n\x1b[1;32m         \x1b[1;37m   |\x1b[1;32m   \u4ec0\u9ebd\x1b[1;37m   |\x1b[1;32m   \u9019\u500b\x1b[1;37m   |\x1b[1;32m   \u529f\u80fd                        \x1b[1;37m   |\x1b[1;32m\x1b[0m\n\x1b[1;32m         \x1b[1;37m   |\x1b[1;32m   \u5176\u4ed6\x1b[1;37m   |\x1b[1;32m   \u9019\u88cf\x1b[1;37m   |\x1b[1;32m   \u4e00\u6a23                        \x1b[1;37m   |\x1b[1;32m\x1b[0m\n\x1b[1;32m         \x1b[1;37m   |\x1b[1;32m   \u6578\u64da\x1b[1;37m   |\x1b[1;32m   \u52d5\u4f5c\x1b[1;37m   |\x1b[1;32m   unicode\u8f38\u51fa!\x1b[1;37m   |\x1b[1;32m\x1b[0m\n\n\x1b[1;37m1   feature   (\x1b[1;32m1   passed\x1b[1;37m)\x1b[0m\n\x1b[1;37m3   scenarios   (\x1b[1;32m3   passed\x1b[1;37m)\x1b[0m\n\x1b[1;37m9   steps   (\x1b[1;32m9   passed\x1b[1;37m)\x1b[0m\n')", 'Testing the output of the outlines feature.','Language: zh-TW -> sucess outlines colorful'
"@utils.auth.requires_login 
 @blueprint.route('/upload_archive', methods=['POST']) 
 def upload_archive(): 
    files = flask.request.files 
   archive_file = get_tempfile(files['archive'], '.archive') 
   if tarfile.is_tarfile(archive_file): 
      archive = tarfile.open(archive_file, 'r') 
      names = archive.getnames() 
   elif zipfile.is_zipfile(archive_file): 
      archive = zipfile.ZipFile(archive_file, 'r') 
      names = archive.namelist() 
   else: 
      return (flask.jsonify({'status': 'Incorrect   Archive   Type'}), 500) 
   if ('info.json' in names): 
      tempdir = tempfile.mkdtemp() 
      labels_file = None 
      archive.extractall(path=tempdir) 
      with open(os.path.join(tempdir, 'info.json')) as data_file: 
         info = json.load(data_file) 
      (valid, key) = validate_archive_keys(info) 
      if (valid is False): 
         return (flask.jsonify({'status': ((""Missing   Key   '"" + key) + ""'   in   info.json"")}), 500) 
      weights_file = os.path.join(tempdir, info['snapshot   file']) 
      if ('model   file' in info): 
         model_file = os.path.join(tempdir, info['model   file']) 
      elif ('network   file' in info): 
         model_file = os.path.join(tempdir, info['network   file']) 
      else: 
         return (flask.jsonify({'status': 'Missing   model   definition   in   info.json'}), 500) 
      if ('labels   file' in info): 
         labels_file = os.path.join(tempdir, info['labels   file']) 
      job = PretrainedModelJob(weights_file, model_file, labels_file, info['framework'], username=utils.auth.get_username(), name=info['name']) 
      scheduler.add_job(job) 
      job.wait_completion() 
      shutil.rmtree(tempdir, ignore_errors=True) 
      return (flask.jsonify({'status': 'success'}), 200) 
   else: 
      return (flask.jsonify({'status': 'Missing   or   Incorrect   json   file'}), 500)", 'Upload a model archive and create a new job.','Upload archive'
